❑样本中输入变量（或自变量）的分布要与数据全集中输入变量（或自变量）的分布保持一致，或者说至少要高度相似。无论自变量是连续型变量还是类别型变量，其在样本中的分布要能代表其在数据全集里的分布。

❑样本中因变量（或目标变量）的值域或者种类的分布，也要与数据全集中目标变量值域或者种类的分布保持一致，或者说要高度相似。

❑缺失值的分布。样本中缺失值的分布（频率）要与数据全集中缺失值的分布（频率）保持一致，或者说至少要高度相似。

❑针对稀有事件建模时要采用抽样措施。由于抽样所造成的目标事件在样本中的浓度被人为放大了，样本中的事件与非事件的比例与数据全集中两者的比例将不一致，所以在建模过程中，数据分析师要记得使用加权的方法恢复新样本对全体数据集的代表性。在目前主流的数据挖掘软件里，对这种加权恢复已经做了自动处理，这给数据分析师带来了很大的便利。

正因为数据分析师要对比样本与全集的一致性，所以在数据分析挖掘实践中，会发生多次抽样的情况，以决定合适的样本。

8.3　分析数据的规模有哪些具体的要求

「分析数据的规模」与 8.2 节介绍的「抽样」有很大的关联性，但是，抽样的目的主要是降低数据集的规模，而本节要探讨的「分析数据的规模」，主要是指用于数据分析挖掘建模时最起码的数据规模大小。在数据挖掘实践中，如果分析数据太少，是不适合进行有价值的分析挖掘的。那么，对于分析数据的规模有没有一个大致的经验判断呢？

分析数据的规模，重点是考量目标变量所对应的目标事件的数量。比如在银行信用卡欺诈预警模型里，目标事件就是实际发生了信用欺诈的案例或者涉嫌欺诈的信用卡用户，而目标事件的数量就是分析样本中实际的欺诈案例的数量或者涉嫌欺诈的信用卡的用户数量。一般情况下，数据挖掘建模过程会将样本划分为 3 个子样本集，分别为训练集（Training Set）、验证集（Validation Set）、测试集（Testing Set）。不过，在具体的挖掘实践中，根据样本数量的大小，有时候也可以只将样本划分为两个子集，即训练集和验证集，对于模型的实践验证，通常是通过另外的时间窗口的新数据来进行测试的。相对来说，训练集的样本数量要比验证集的数量更多。训练集的数据量大概应该占到样本总数据量的 40%～70%。在理想的状况下，训练集里的目标事件的数量应该有 1000 个以上，因为在太少的目标事件样本基础上开发的模型很可能缺乏稳定性。但是，这些经验上的参考数据并不是绝对的，在数据挖掘的项目实践中数据分析师需要在这些经验值与实际的业务背景之间做出权衡或进行折中。比如，如果训练集里的目标事件数量少于 1000 个，只要分析师根据业务判断觉得可行也是可以进行分析挖掘的，只是需要更加关注模型的稳定性的检验。

另外，预测模型的自变量一般应控制在 8～20 个之间，因为太少的自变量会给模型的稳定性造成威胁，而任何一个自变量的缺失或者误差都可能引起模型结果的显著变动。但是，太多的自变量也会让模型因为复杂而变得不稳定。

前面说过，训练集里目标事件最好要在 1000 个以上，在此基础上，训练集样本的规模一般应该在自变量数量的 10 倍以上，并且被预测的目标事件至少是自变量数目的 6～8 倍。

正如之前所强调的，上述的参考数据源于经验，仅供参考。在数据挖掘实践中，数据分析师还应该综合考虑实际的业务背景和实际的数据质量、规模来进行综合的判断。

8.4　如何处理缺失值和异常值

如果说前面的 3 节内容谈到的数据处理问题并不会在每个分析场景都能明显引起分析师的关注，那么本节讨论的「数据缺失和异常值」却是几乎在每个数据分析、挖掘实践中，分析师都会碰到的、最常见的数据问题。

8.4.1　缺失值的常见处理方法

在数据分析挖掘实践中，数据样本里的数据缺失是常见的现象，而这其中有的是数据存储错误造成的，有的是原始数据本身就是缺省的，比如用户登记的信息不全。在大多数情况下，数据分析师需要对缺失数据进行处理；在个别情况下，比如应用决策树算法的时候，该算法本身允许数据缺失值直接进入分析挖掘，因为在这种情况下缺失值本身已经被看做是一个特定的属性类别了。下列一些方法是数据分析师常用的处理数据缺失值的方法：

❑数据分析师首先应该知道数据缺失的原因，只有知根知底，才可以从容、正确地处理缺失值。不同数据的缺失有不同的原因，因此也应该有不同的解读和解决方法，而不应该一概而论，眉毛胡子一把抓。举例来说，如果用户问卷里的缺失值是因为被调查者漏掉了一个问题选项，那么这个缺失值代表了用户没有回答该问题；而一个信用卡激活日期的缺失，不能表明是「丢失」了信用卡的激活日期，按照系统的计算逻辑来看，凡是还没有激活的信用卡，其激活日期都是记为缺失的，即 NULL；还有的缺失是因为系统本身的计算错误造成的，比如某个字段除以零，某个负数取对数等错误的数学运算。上述 3 种缺失场景有着完全不同的缺失原因，所代表的意义也不同，分析师只有真正找到了缺失的原因，才可以有的放矢，并采取相应的对策进行有效处理。

❑分析师基于数据缺失的原因进行正确查找后，还要对于数据的缺失进行判断。这种数据缺失是本身已经具有特定的商业意义呢？还是的确需要进行特别的处理。在上面所列举的 3 种完全不同的缺失原因里，很明显，信用卡激活日期的缺失其本身是具有特定商业意义的，这种缺失代表了该用户还没有激活信用卡，这个商业的含义非常明确，已经不用对此类缺失进行任何处理了；而诸如系统本身的计算错误所造成的缺失，比如，某个字段除以零，或某个负数取对数，就应该采取相应的措施修正计算错误，比如，重新定义计算逻辑，或者采用后面提到的一系列的处理方法。

❑直接删除带有缺失值的数据元组（或观察对象）。这种操作手法最大的好处在于删除带缺失值的观察对象后，留下来的数据全部是有完整记录的，数据很干净，删除的操作步骤也很简单方便。但是，此种操作手法最大的不足在于，如果数据缺失的比例很大，直接删除带有缺失值的观察值后剩下的用于分析挖掘用的数据集可能会太少，不足以进行有效的分析挖掘；其次，直接删除含有缺失值的观察对象很可能会丢失一些重要的信息，因为这些被删除的观察对象还可能包含了很多没有缺失的别的字段或者变量的属性，这些属性或者数据也是很有意义的；另外，在建模完成后进行业务应用时，如果用来打分的新数据也带有缺失值，那么先前完全基于不带缺失值的分析样本所搭建起来的预测模型，面对这些数据进行打分预测时，很有可能无法对此进行打分赋值。所以，直接删除带有缺失值的观察对象的方法只适用于建模样本里缺失值比例很少，并且后期打分应用中的数据的缺失值比例也很少的情况。

❑直接删除有大量缺失值的变量。这种方法是针对那些缺失值占比超过相当比例变量，比如缺失值超过 20% 或者更多的情况。但是采用这种方法之前需要仔细考虑，这种大规模的缺失是否有另外的商业背景和含义，比如前面提到的信用卡激活日期的缺失实际上表明这些用户还没有激活信用卡，那么这群用户是属于另外一个类别，即还未激活的用户群体，在这种情况下，轻率地删除就会丢失这群用户的重要信息，得不偿失。

❑对缺失值进行替换（Substitute）。这种方法包括利用全集中的代表性属性，诸如众数或者均值等，或者人为定义的一个数据去代替缺失值的情况。具体来说，包括：对于类别型变量（Category）而言，用众数或者一个崭新的类别属性来代替缺失值；对于次序型变量（Ordinal）和区间型变量（Interval）而言，用中间值、众数、最大值、最小值、用户定义的任意其他值、平均值或仅针对区间型变量来代替缺失值。上述对缺失值进行替换的做法最大的好处在于简单、直观，并且有相当的依据，比如说，众数本身就说明了该值出现的几率最大。但是，不管怎么说，这种替换毕竟是人为的替换，不能完全代表缺少数据本身真实的含义，所以也属于「不得已而为之」的策略。

❑对缺失值进行赋值（Impute）。这种方法将通过诸如回归模型、决策树模型、贝叶斯定理等去预测缺失值的最近替代值，也就是把缺失数据所对应的变量当做目标变量，把其他的输入变量作为自变量，为每个需要进行缺失值赋值的字段分别建立预测模型。从理论上看，该种方法最严谨，但是成本较高，其包括时间成本和分析资源的投入成本。是否采用该方法，取决于具体数据挖掘的业务背景、数据资源质量以及需要投入的力度。

8.4.2　异常值的判断和处理

数据样本中的异常值（Outlier）通常是指一个类别型变量（Category）里某个类别值出现的次数太少、太稀有，比如出现的频率只占 0.1% 或更少，或者指一个区间型变量（Interval）里某些取值太大，比如，互联网买家用户最近 30 天在线购买的交易次数，个别用户可以达到 3000 次，平均每天购买 100 次，相比数据全集里该字段均值为 2 次而言，这里的 3000 交易次数就属于异常值。

通常来讲，如果不把异常值清理掉，对于数据分析结论或者挖掘模型效果的负面影响是非常大的，很可能会干扰模型系数的计算和评估，从而严重降低模型的稳定性。

对于异常值的判断内容如下：

❑对于类别型变量（Category）来说，如果某个类别值出现的频率太小，太稀有，就可能是异常值。具体拿经验值来参考，一般某个类别值的分布占比不到 1% 或者更少就很可能是异常值了。当然，这还需要数据分析师根据具体项目的业务背景和数据实际分布作出判断和进行权衡。有些情况下，纵然某个类别值的占比很少，但是如果跟目标变量里的目标事件有显著的正相关关系，这种稀有类别值的价值就不是简单的异常值所可以代表的。

❑对于区间型变量（Interval）来说，最简单有效的方法就是把所有的观察对象按照变量的取值按从小到大的顺序进行排列，然后从最大的数值开始倒推 0.1% 甚至更多的观察值，这些最大的数值就很可能属于异常值，可再结合业务逻辑加以判断。另外一个常用的判断异常值的方法就是以「标准差」作为衡量的尺度，根据不同的业务背景和变量的业务含义，把超过均值 n 个标准差以上的取值定义为异常值，这里 n 的取值范围取决于具体的业务场景和不同变量的合理分布，比如超过均值在正负 4 个标准差以上的数值就要认真评估，确定其是否是异常值。

对于异常值的处理相对来说就比较简单，主要的措施就是直接删除。

需要提醒读者的是，在数据挖掘实践中，对于「异常值」的处理是辩证的，在多数情况下，异常值的删除可以有效降低数据的波动，使得处理后的建模数据更加稳定，从而提高模型的稳定性。但是，在某些业务场景下，异常值的应用却是另一个专门的业务方向。比如在前面章节里提到的信用体系中的恶意欺诈事件，从数据分析的角度来看那也是对异常值的分析挖掘应用。对这些有价值的异常值的分析应用包括利用聚类分析技术识别异常值，利用稀有事件的预测模型搭建去监控、预测异常值出现的可能性等。这些应用，将在第 9 章和第 10 章专门进行介绍。

8.5　数据转换

对于数据挖掘分析建模来说，数据转换（Transformation）是最常用、最重要，也是最有效的一种数据处理技术。经过适当的数据转换后，模型的效果常常可以有明显的提升，也正因为这个原因，数据转换成了很多数据分析师在建模过程中最喜欢使用的一种数据处理手段。另一方面，在绝大多数数据挖掘实践中，由于原始数据，在此主要是指区间型变量（Interval）的分布不光滑（或有噪声）、不对称分布（Skewed Distributions），也使得数据转化成为一种必需的技术手段。

按照采用的转换逻辑和转换目的的不同，数据转换主要可以分为以下四大类：

❑产生衍生变量。

❑改善变量分布特征的转换，这里主要指对不对称分布（Skewed Distributions）所进行的转换。

❑区间型变量的分箱转换。

❑针对区间型变量进行的标准化操作。

8.5.1　生成衍生变量

这类转换的目的很直观，即通过对原始数据进行简单、适当的数学公式推导，产生更加有商业意义的新变量。举个简单的例子，在对原始数据中的用户出生年月日进行处理时，把当前的年月日减去用户出生年月日，得到一个新的字段「用户年龄」，这个新的字段作为一个区间型变量（Interval）明显比原始变量用户出生年月日要更有商业含义，也更加适合进行随后的数据分析建模应用。一般常见的衍生变量如下。

❑用户月均、年均消费金额和消费次数。

❑用户在特定商品类目的消费金额占其全部消费金额的比例。

❑家庭人均年收入。

❑用户在线交易终止的次数占用户在线交易成功次数的比例。

❑用户下单付费的次数占用户下单次数的比例。

从中不难发现，得到这些衍生变量所应用到的数学公式都很简单，但是其商业意义都是很明确的，而且跟具体的分析背景和分析思路密切相关。

衍生变量的产生主要依赖于数据分析师的业务熟悉程度和对项目思路的掌控程度，是数据分析师用思想创造出来的「艺术品」。如果没有明确的项目分析思路和对数据的透彻理解，是无法找到有针对性的衍生变量的。

8.5.2　改善变量分布的转换

在数据挖掘实践中，大多数区间型变量（Interval）原始分布状态偏差都较大，而且是严重不对称的。这种大偏度，严重不对称的分布出现在自变量中常常会干扰模型的拟合，最终会影响模型的效果和效率，如图 8-1 所示。如果通过各种数学转换，使得自变量的分布呈现（或者近似）正态分布，并形成倒钟形曲线，如图 8-2 所示，那么模型的拟合常常会有明显的提升，转换后自变量的预测性能也可能得到改善，最终将会显著提高模型的效果和效率。

图　8-1　某区间型变量的原始分布图（明显的偏差大，严重不对称）

图　8-2　变量经过取对数的转换，呈现倒钟形的正态分布图

常见的改善分布的转换措施如下：

❑取对数（Log）。

❑开平方根（Square Root）。

❑取倒数（Inverse）。

❑开平方（Square）。

❑取指数（Exponential）。

8.5.3　分箱转换

对于区间型变量（Interval），除了进行上面提到的改善分布的转换措施之外，还可以进行另外的转换尝试，即分箱转换。

分箱转换（Binning）就是把区间型变量（Interval）转换成次序型变量（Ordinal），其转换的主要目的如下：

❑降低变量（主要是指自变量）的复杂性，简化数据。比如，有一组用户的年龄，原始数据是区间型的，从 20～80 岁，每 1 岁都是 1 个年龄段；如果通过分箱转换，每 10 岁构成 1 个年龄组，就可以有效简化数据。

❑提升自变量的预测能力。如果分箱恰当，是可以有效提升自变量和因变量的相关性的，这样就可以显著提升模型的预测效率和效果；尤其是当自变量与因变量之间有比较明显的非线性关系时，分箱操作更是不错的手段，可用于探索和发现这些相关性；另外，当自变量的偏度很大时，分箱操作也是值得积极尝试的方法。

从上面的分析可以看出，分箱操作的价值与改善分布转换的价值类似，都是努力提升自变量的预测能力，强化自变量与因变量的线性（或非线性）关系，从而可以明显提升预测模型的拟合效果。两者有异曲同工之处，在数据挖掘实践中，经常会对这两种方式分别进行尝试，择其优者而用之。

8.5.4　数据的标准化

数据的标准化（Normalization）转换也是数据挖掘中常见的数据转换措施之一，数据标准化转换的主要目的是将数据按照比例进行缩放，使之落入一个小的区间范围之内，使得不同的变量经过标准化处理后可以有平等分析和比较的基础。

最简单的数据标准化转换是 Min-Max 标准化，也叫离差标准化，是对原始数据进行线性变换，使得结果在 [0,1] 区间，其转换公式如下：

其中，max 为样本数据的最大值，min 为样本数据的最小值。

关于数据的标准化转换，将在 9.3.2 节详细介绍。

总地来说，数据转换的方式多种多样，操作起来简单、灵活、方便，在实践应用中的价值也是比较明显的。但是，它也有缺点，其中主要的缺点在于，在具体的数据挖掘实践中有些非线性转换如 Log 转换、平方根转换、多次方转换等的含义无法用清晰的商业逻辑和商业含义向用户（业务应用方）解释。比如，你无法解释「把消费者在线消费金额取对数」在商业上是什么意思，这在一定程度上影响了业务应用方对模型的接受程度和理解能力。

当然，瑕不掩瑜，毕竟预测模型的最终目的是预测的准确度和精确度，数据转换在商业解释中的这点小小的遗憾当然无损其在强大的数据处理中的重要价值。

8.6　筛选有效的输入变量

虽然「筛选有效的输入变量」属于模型搭建的技术问题，可以放在后面有关模型搭建的章节里做专门的介绍，但是这个问题在很大程度上也会涉及数据的清洗、整理、探索等数据处理的技巧，所以这里将「筛选有效的输入变量」作为数据处理技巧来进行深入讲解。

不同类型的模型对于输入变量的要求各不相同，在本书涉及的各种模型和各种项目中，鉴于预测（响应）和分类模型所涉及的变量的筛选最为复杂，最为常见，所以本节将聚焦预测（响应）和分类模型中的输入变量筛选进行深入讲解，至于聚类中的变量筛选将在 9.3.3 节做深入讲解，其他类型的模型和应用中的输入变量筛选相对来说非常直观和简单，将在相应章节中进行讲解。

8.6.1　为什么要筛选有效的输入变量

为什么要筛选有效的输入变量？有以下 3 个方面的理由：

❑筛选有效的输入变量是提高模型稳定性的需要。过多的输入变量很可能会带来干扰和过拟合等问题，这会导致模型的稳定性下降，模型的效果变差。所以，优质的模型一定是遵循输入变量少而精原则的。

❑筛选有效的输入变量是提高模型预测能力的需要。过多地输入变量会产生共线性问题，所谓共线性是指自变量之间存在较强的，甚至是完全的线性相关性。当自变量之间高度相关时，数据的小小变化，比如误差的发生都会引起模型参数严重震荡，明显降低模型的预测能力，关于共线性问题，将在 8.6.3 节做详细介绍。并且，共线性的发生也增加了对模型结果的解释困难，因为要更深入地分析和判断每个自变量对目标变量的影响程度。

❑当然，筛选有效的输入变量也是提高运算速度和运算效率的需要。

在采取各种评价指标筛选有价值的输入变量之前，可以先直接删除明显的无价值的变量，这些明显的无价值变量包括的内容如下：

❑常数变量或者只有一个值的变量。

❑缺失值比例很高的变量，比如缺失值高达 95%，或者视具体业务背景而定。

❑取值太泛的类别型变量，最常见的例子就是邮政编码，除非采取进一步措施将各个地区的编码整合，减少类别的数量，否则原始的邮政编码数据无法作为输入变量来提供起码的预测功能。

8.6.2　结合业务经验进行先行筛选

这是所有筛选自变量的方法中最核心、最关键、最重要的方法。在本书之前讲解的内容中也反复强调了业务经验和业务判断对数据挖掘的重要影响。正如数据挖掘商业实战的其他各个环节一样，筛选自变量的环节也应该引进业务专家的意见和建议，很多时候业务专家一针见血的商业敏感性可以有效缩小自变量的考察范围，准确圈定部分最有价值的预测变量，从而提高判断和筛选的效率。

另一方面，业务经验和业务专家的建议难免碎片化，也可能难以面面俱到，更关键的是业务经验和业务专家的建议也需要数据进行科学的验证。所以，在本章的后面的内容中，将详细介绍在数据挖掘实战领域里比较成熟、有效的方法和指标，用于筛选目标变量。在这里要强调的是，下面的具体介绍主要是从原理和算法上进行剖析的，读者只需要从思想上知道并了解这些方法背后的原理就可以了。在实战操作中，不需要大家运用这些最基础的公式进行繁琐的计算。目前有很多成熟的数据挖掘分析软件能够把这些繁琐的计算工作完成得很出色。作为数据分析人员只需要知道其中的原理、思路、分析方法就可以了。当然只有真正从思想上理解并掌握了这些具体的原理和思路，才可以在数据挖掘商业实战中游刃有余，得心应手；如果仅仅知其然，不知其所以然，在具体的数据挖掘商业实战中将会举步维艰，束手无策。

8.6.3　用线性相关性指标进行初步筛选

最简单、最常用的方法就是通过自变量之间的线性相关性指标进行初步筛选。其中，尤以皮尔逊相关系数（Pearson Correlation）最为常用。Pearson 相关系数主要用于比例型变量与比例型变量、区间型变量与区间型变量，以及二元变量与区间型变量之间的线性关系描述。其计算公式如下：

线性相关性的相关系数 r 的取值范围为 [-1，+1]，根据经验来看，不同大小的 r，表示不同程度的线性相关关系。

❑|r|＜0.3, 表示低度线性相关。

❑0.3≤|r|＜0.5，表示中低度线性相关。

❑0.5≤|r|＜0.8，表示中度线性相关。

❑0.8≤|r|＜1.0，表示高度线性相关。

在建模前的变量筛选过程中，如果自变量属于中度以上线性相关的（＞0.6 以上）多个变量，只需要保留一个就可以了。

上述相关系数的计算公式只是从状态上计算了变量之间的相关关系，但是相关系数是通过样本数据得到的计算结果，来自样本的统计结果需要通过显著性检验才能知道其是否适用于针对总体数据的相关性。关于类似的统计显著性问题，作为统计分析中的基本知识，不在本书的讨论范围之内，并且在目前所有的分析软件里都可以自动计算，有心的读者可以自己在实践中进行体会和学习。

需要强调的是，有时候尽管上述公式计算出来的相关系数 r 等于 0，也只能说明线性关系不存在，不能排除变量之间存在其他形式的相关关系，比如曲线关系等。

尽管线性相关性检验是模型的变量筛选中最常用也最直观的有效方法之一，但是在很多时候，某个自变量和因变量的线性相关性却很小，这时可以通过跟其他自变量结合在一起而让其成为预测力很强的自变量。正因为如此，在挑选输入变量的时候，应该多尝试不同的评价指标和不同的挑选方法，减少因采用单一方法而导致的误删除，避免在一棵树上吊死的情况发生。

8.6.4　R 平方

R 平方（R-Square），也叫做 R2 或 Coefficient of Multiple Determination，该方法将借鉴多元线性回归的分析算法来判断和选择对目标变量有重要预测意义及价值的自变量。

最通俗的解释，R2 表示模型输入的各自变量在多大程度上可以解释目标变量的可变性，R2 的取值范围在 [0,1] 之间，R2 越大，说明模型的拟合越好。R2 的计算公式如下：

在上述 R2 公式中，R2 表示回归方程拟合的好坏，R2∈(0,1)，R2 越大表示回归方程同样本观测值的拟合程度越好。R 又被称为因变量 Y 与自变量 X1,X2,…,Xp 的样本复相关系数，它表示整体的 X1,X2,…,Xp 和 Y 的线性关系。

在 R2 计算公式中：

yi 表示目标变量的真实值；

fi 表示模型的预测值；

表示目标变量真实值的均值；

SSE 称为残差平方和，自由度为 P，P 代表自变量的个数；

SST 称为总平方和，自由度为 N-1，N 代表样本数量；

SSR 称为回归平方和，自由度为 N-P-1。

总平方和 SST 反映了因变量（目标变量）Y 的波动程度，SST 是由回归平方和 SSR 和残差平方和 SSE 两部分组成的。其中，回归平方和 SSR 是由解释变量，即自变量，输入变量 X 所引起的，残差平方和 SSE 是由其他随机因素所引起的。

在回归方程中，回归平方和越大，回归效果越好，因此可构造如下的统计量：

在零假设 H0:β1=β2=…βp=0 成立时（β 为各自变量在回归方程中的回归系数），统计量服从自由度为（p,N-p-1）的 f 分布。如果给定显著水平 α，则否定域为 F＞F1-α（p,N-p-1）。

当 F 值没有落在否定域之中时，零假设 H0:β1=β2=…=βp=0 成立，表明解释变量（自变量）X1,X2,…,Xp 对因变量（目标变量）Y 的多元线性回归不成立，X1,X2,…,Xp 与 Y 之间没有显著的线性关系。

对于每个自变量 Xi 做偏回归显著性检验，其公式为：，其中，SSR-i 为剔除变量 Xi 之后的回归平方和，SSR-SSR-i 反映了在引入 Xi 之后，Xi 对于回归平方和的贡献。

分别检查各自变量的 Fi 是否都大于相应的 F0.05。

如果全部 Fi 都大于 F0.05，则结束。

如果经检查发现有几个自变量的 Fi 小于 F0.05，则每次只能删除其中的一个 Xi，这个 Xi 是所有自变量中其 Fi 最无显著性的，然后再重新用剩下的自变量进行回归的构建，如此反复，直到所有的有显著性意义的自变量都进入回归方程，而没有显著性意义的变量都被剔除为止。

8.6.5　卡方检验

卡方检验（Chi-Square Statistics）在统计学里属于非参数检验，主要用来度量类别型变量，包括次序型变量等定性变量之间的关联性以及比较两个或两个以上的样本率。其基本思想就是比较理论频数和实际频数的吻合程度或拟合度。作为数据挖掘中筛选自变量的重要方法，卡方检验主要是通过类别型目标变量，最常见的就是二元目标变量，0,1 与类别型自变量之间的关联程度来进行检验的，关联性大的类别型自变量就有可能是重要的自变量，可以通过初步的筛选进入下一轮的考察。卡方检验的公式如下：

其中，表示各交叉分类频数的观测值，表示各交叉分类频数的期望值，各交叉分类频数观测值与期望值的偏差为。

当样本量较大时，X2 统计量近似服从自由度为 (R-1)(C-1) 的 X2（卡方）分布。从上述公式可以看出，X2 的值与期望值、观测值和期望值之差有关，X2 值越大表明观测值与期望值的差异越大，相对应的 P-Value 就越小，而 P-Value 代表的是上述差异发生的偶然性。所以，通常讲，如果 P-Value 值的小于 0.01，同时 X2，即是卡方（Chi-Square）比较大，则说明可以拒绝该自变量与因变量之间相互独立的原假设，也就是说该类别型自变量与目标变量之间有比较强的关联性，因此可以认为该自变量可能值得输入模型。

8.6.6　IV 和 WOE

当目标变量是二元变量（Binary），自变量是区间型变量（Interval）时，可以通过 IV（Information Value）和 WOE（Weight of Evidence）进行自变量的判断和取舍。在应用 IV 和 WOE 的时候，需要把区间型自变量转换成类别型（次序型）自变量，同时要强调的是目标变量必须是二元变量（Binary），这两点是应用 IV 和 WOE 的前提条件。

举例来说，在一个「预测用户是否在信用卡使用上有信用欺诈嫌疑」的项目里，目标变量是「是否存在信用欺诈行为」，是个二元变量（0,1），0 代表没有欺诈，1 代表有欺诈；同时，自变量里有一个字段「用户的年收入」，在数据仓库的原始记录里，该字段「用户的年收入」是属于区间型变量（Interval）的，如果采用 WOE 和 IV 的指标方法判断其是否具有预测价值，即是否适合作为自变量放进模型里去预测，就需要先把这个区间型的变量「用户的年收入」进行转换，使其变成类别型变量（次序型变量），比如「分箱」成为具有 4 个区间的类别型变量，且这些变量分别为小于 20 000 元、[20 000,60 000)、[60 000,100 000），以及 100 000 元以上，共 4 类。

上述举例中的 4 类区间，又称为变量「用户的年收入」的 4 个属性（Attribute），针对每个属性（Attribute），可以计算样本数据里的 WOE，公式如下：

其中

在上述公式中，和分别代表在该属性值里，样本数据所包含的预测事件和非事件的数量；nevent 和 Nnonevent 分别代表在全体样本数据里所包含的预测事件和非事件的总量。

而一个变量的总的预测能力是通过 IV（Information Value）来表现的，它是该变量的各个属性的 WOE 的加权总和，IV 代表了该变量区分目标变量中的事件与非事件的能力，具体计算公式如下。

与 IV 有相似作用的一个变量是 Gini 分数（Gini Score），Gini 分数的计算步骤如下：

1）根据该字段里每个属性所包含的预测事件（Event）与非事件（Nonevent）的比率，按照各属性的比率的降序进行排列。比如，该字段共有 m 个属性，排序后共有 m 个组，每个组对应一个具体的属性，第一组就是包含预测事件比率最高的那个组。

2）针对排序后的每个组，分别计算该组内的事件数量和非事件数量。

3）计算 Gini 指数，其公式如下：

上述公式中，Nevent 和 Nnonevent 分别代表样本数据里总的事件数量和非事件数量。

总体来说，应用 IV、WOE、Gini Score3 个指标时，可以在数据挖掘实践中实现以下目标：

❑通过 WOE 的变化来调整出最佳的分箱阀值。通常的做法是先把一个区间型变量分成 10～20 个临时的区间，分别计算各自的 WOE 的值，然后根据 WOE 在各区间的变化趋势，做相应的合并，最终实现比较合理的区间划分。

❑通过 IV 值或者 Gini 分数，筛选出有较高预测价值的自变量，投入模型的训练中。

