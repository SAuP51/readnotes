

# 10 预测响应（分类）模型的典型应用和技术小窍门

卖弄杀周易阴阳谁似你，还有个未卜先知意。

——《桃花女》

10.1　神经网络技术的实践应用和注意事项

10.2　决策树技术的实践应用和注意事项

10.3　逻辑回归技术的实践应用和注意事项

10.4　多元线性回归技术的实践应用和注意事项

10.5　模型的过拟合及对策

10.6　一个典型的预测响应模型的案例分享

预测响应（分类）模型是数据挖掘实战中最常见的应用模型，它最直接地涉及了精细化运营中的客户分层以及随后的个性化区别对待，从某种意义上来说，基于预测响应（分类）模型的客户分层运营已经成为精细化运营的代名词。

本章围绕预测响应（分类）模型的典型应用，对神经网络、决策树、逻辑回归、多元线性回归等最常见的 4 种算法在数据挖掘实战应用中的优缺点和技术重点进行了分析、归纳，从而帮助读者在今后的项目实践中有的放矢，扬长避短。

不同的模型算法，需要不同的数据准备；不同的算法，输出不同的产出物；不同的算法，在实践应用中有各自独特的优势和不足之处。数据分析人员只有对这些有了足够的了解和掌握，才可以实现有效的数据挖掘实践应用。

10.1　神经网络技术的实践应用和注意事项

对神经网络的研究始于 20 世纪 40 年代，作为一门交叉学科，它是人类基于对其大脑神经认识理解的基础上，人工构造实现某种功能的网络模型。经过将近 70 年的发展，神经网络技术已经成为机器学习的典型代表，它不依照任何概率分布，而是模仿人脑功能进行抽象运算。

简单来讲，神经网络是一组互相连接的输入 / 输出单元，其中每个连接都会与一个权重相关联。在学习阶段，通过调整这些连接的权重，就能够预测输入观察值的正确类标号。因此可以理解为人工神经网络是由大量神经元通过丰富完善的连接、抽象、简化和模拟而形成的一种信息处理系统。

10.1.1　神经网络的原理和核心要素

人工神经网络的结构大致分为两大类：前向型网络和反馈型网络。

具体来说，所谓前向型网络，是指传播方向是从输入端传向输出端，并且没有任何的反馈；所谓反馈型网络是指在传播方向上除了从输入端传向输出端之外，还有回环或反馈存在。两种类型的网络原理图如图 10-1 所示。

图　10-1　人工神经网络的典型结构图

在上述的典型结构里，神经网络通过输入多个非线性模型，以及不同模型之间的加权互联，最终得到一个输出模型。具体来说，多元输入层是指一些自变量，这些自变量通过加权结合到中间的层次上，称为隐蔽层。隐蔽层中主要包含的是非线性函数，也叫转换函数或者挤压函数。隐蔽层就是所谓的黑箱（Black Box）部分，几乎没有人能在所有的情况下读懂隐蔽层中那些非线性函数是如何对自变量进行组合的，这是计算机思考代替人类思考的一个典型案例。

利用神经网络技术建模的过程中，有以下 5 个因素对模型的结果有重大影响 [1]：

❑层数。对于一定的输入层和输出层，需要有多少个隐蔽层，这点无论是在理论上，还是在实践中都非常有意义。虽然没有不变的规律，但是有经验的数据分析师通常要尝试不同的设置，力求找到满意的模型结构。

❑每层中输入变量的数量。太多的自变量很可能会造成模型的过度拟合，使得模型搭建时看上去很稳定，可是一旦用到新数据中，模型的预测与实际结果却相差很大，这时模型就失去了预测的价值和意义。所以，在使用神经网络建模之前，输入变量的挑选、精简非常重要。

❑联系的种类。神经网络模型中，输入变量可以有不同方向的结合，可以向前，可以向后，还可以平行。采用不同的结合方式，可能就会对模型的结果产生不同的影响。

❑联系的程度。在每一层中，其元素可以与他层中的元素完全联系，也可以部分联系。部分联系可以减少模型过度拟合的风险，但是也可能减弱模型的预测能力。

❑转换函数。转换函数也称为挤压函数，因为它能把从正无穷大到负无穷大的所有输入变量挤压为很小范围内的一个输出结果。这种非线性的函数关系有助于模型的稳定和可靠性。选择转换函数的标准很简单，即在最短时间内提供最好的结果函数。常见的转换函数包括阀值逻辑函数、双曲正切函数、S 曲线函数等。

大部分神经网络模型的学习过程，都是通过不断地改变权重来使误差达到总误差的最小绝对值的。比如，以常见的前向型网络模型为例，其设计原理如下：

❑隐蔽层的层数。从理论上讲，两层就足够了；在实践中，经常是一层隐蔽层就足够了。

❑每层内的输入变量。输出层的变量由具体分析背景来决定；隐蔽层的数量为输入数与输出数的乘积开平方；输入层的数量应该尽量精简，遵循少而精的原则，这在后面要详细阐述。

❑联系的程度。一般都选择所有层次间全部联系。

❑转换函数。选用逻辑斯蒂函数为主要转换函数，因为逻辑斯蒂函数可以提供在最短时间内的最佳拟合。

❑模型开发样本要足够充分，避免过拟合现象发生。

[1] 罗茂初。数据库营销 [M]. 北京：经济管理出版社，2007:239.

10.1.2　神经网络的应用优势

在数据挖掘实践应用中，人工神经网络的应用主要有以下优点：

❑有良好的自组织学习功能。神经网络可以根据外界数据的变化来不断修正自身的行为，对未经训练的数据模式的分类能力也比较强。

❑有比较优秀的在数据中挑选非线性关系的能力，能有效发现非线性的内在规律。在纷繁复杂的业务实践中，数据间非线性关系出现的机会远比线性关系多得多，神经网络的这种有效发现非线性关系的能力，大大提高了其在数据化运营等各种商业实践中的应用价值和贡献潜力。

❑由于神经网络具有复杂的结构，因此在很多实践场合中其应用效果都明显优于其他的建模算法；它对异常值不敏感，这是个很不错的「宽容」个性。

❑对噪声数据有比较高的承受能力。

10.1.3　神经网络技术的缺点和注意事项

虽然神经网络有上述这多优点，但是人无完人，金无足赤，它同样也有以下一些典型的不足之处需要引起数据分析师的注意：

❑神经网络需要比较长的模型训练时间，在面对大数据量时尤其如此。

❑对于神经网络模型来说少而精的变量才可以充分发挥神经网络的模型效率。但是，神经网络本身是无法挑选变量的。因此，对于神经网络的实际应用来讲，之前的变量挑选环节就必不可少了。虽然变量的选择对于任何一个模型的搭建来说都是很重要的环节，但是必须强调的是，对于神经网络模型来说尤为重要，这是由其复杂的内部结构决定的。

❑如果搭建模型后直接将其投入应用，可能会得不到想要的效果。为了确保模型投入应用后具有稳定的效果，最好先尝试几种不同的神经网络模型，经过多次验证后，再挑选最稳定的模型投入应用。

❑神经网络本身对于缺失值（Missing Value）比较敏感。所以，应用该技术时要注意针对缺失值进行适当的处理，或者赋值，或者替换，或者删除，参见本书 8.4.1 节。

❑它具有过度拟合（Over-Fitting）数据的倾向，可能导致模型应用于新数据时效率显著下降。鉴于此，针对神经网络模型的应用要仔细验证，在确保稳定的前提下才可以投入业务落地应用。

❑由于其结构的复杂性和结论的难以解释性，神经网络在商业实践中远远没有回归和决策树应用得广泛，人们对它的理解、接纳还有待提高。它也缺乏类似回归那样的丰富多样的模型诊断指标和措施。正因为如此，很多数据分析师视之为「黑盒子」，只是在实在无计可施的时候才「放手一搏」。

10.2　决策树技术的实践应用和注意事项

决策树模型是数据挖掘应用中常见的一种成熟技术，因其输出规则让人容易理解而备受数据分析师和业务应用方的喜欢和推崇。自从 1960 年 Hunt 等人提出概念学习系统框架方法（Concept Learning System Framework,CLSF）以来，决策树多种算法一直在不断发展、成熟，目前最常用的 3 种决策树算法分别是 CHAID、CART 和 ID3，包括后来的 C4.5，乃至 C5.0。

决策树，顾名思义，其建模过程类似一棵树的成长，从根部开始，到树干，到分叉，到继续细枝末节的分叉，最终到一片片的树叶。在决策树里，所分析的数据样本形成一个树根，经过层层分枝，最终形成若干个结点，每个结点代表一个结论。从决策树的根部到叶结点的一条路径就形成了对相应对象的类别预测。

10.2.1　决策树的原理和核心要素

构造决策树采用的是自顶向下的贪婪算法，它会在每个结点选择分类效果最好的属性对样本进行分类，然后继续这个过程，直到这棵树能准确地分类训练样本，或者所有的属性都已被用过。

决策树算法的核心是在对每个结点进行测试后，选择最佳的属性，并且对决策树进行剪枝处理。

最常见的结点属性选择方法（标准）有信息增益、信息增益率、Gini 指数、卡方检验（Chi-Square Statistics）等。在 10.2.2～10.2.4 节将对它们分别进行介绍。

决策树的剪枝处理包括两种方式：先剪枝（Prepruning）和后剪枝（Postpruning）。

所谓先剪枝，就是决策树生长之前，就人为定好树的层数，以及每个结点所允许的最少的样本数量等，而且在给定的结点不再分裂。

所谓后剪枝，是让树先充分生长，然后剪去子树，删除结点的分枝并用树叶替换。后剪枝的方法更常用。CART 算法就包含了后剪枝方法，它使用的是代价复杂度剪枝算法，即将树的代价复杂度看做是树中树叶结点的个数和树的错误率的函数。C4.5 使用的是悲观剪枝方法，类似于代价复杂度剪枝算法。

10.2.2　CHAID 算法

CHAID（Chi-Square Automatic Interaction Detector）算法历史较长，中文简称为卡方自动相互关系检测。CHAID 是依据局部最优原则，利用卡方检验来选择对因变量最有影响的自变量的，CHAID 应用的前提是因变量为类别型变量（Category）。

关于卡方检验的具体公式和原理，此处从略，详情可参考本书 8.6.5 节。

关于 CHAID 算法的逻辑，简述如下。

首先，对所有自变量进行逐一检测，利用卡方检验确定每个自变量和因变量之间的关系。具体来说，就是在检验时，每次从自变量里抽取两个既定值，与因变量进行卡方检验。如果卡方检验显示两者关系不显著，则证明上述两个既定值可以合并。如此，合并过程将会不断减少自变量的取值数量，直到该自变量的所有取值都呈现显著性为止。在对每个自变量进行类似处理后，通过比较找出最显著的自变量，并按自变量最终取值对样本进行分割，形成若干个新的生长结点。

然后，CHAID 在每个新结点上，重复上述步骤，对每个新结点重新进行最佳自变量挑选。整个过程不断重复，直到每个结点无法再找到一个与因变量有统计显著性的自变量对其进行分割为止，或者之前限度的条件得到满足，树的生长就此终止。

卡方检验适用于类别型变量的检验，如果自变量是区间型的变量（Interval），CHAID 改用 F 检验。

10.2.3　CART 算法

CART（Classification and Regression Trees）算法发明于 20 世纪 80 年代中期，中文简称分类与回归树。CART 的分割逻辑与 CHAID 相同，每一层的划分都是基于对所有自变量的检验和选择。但是，CART 采用的检验标准不是卡方检验，而是 Gini（基尼系数）等不纯度指标。两者最大的不同在于 CHAID 采用的是局部最优原则，即结点之间互不相干，一个结点确定了之后，下面的生长过程完全在结点内进行。而 CART 则着眼于总体优化，即先让树尽可能地生长，然后再回过头来对树进行修剪（Prune），这一点非常类似统计分析中回归算法里的反向选择（Backward Selection）。CART 所生产的决策树是二分的，即每个结点只能分出两枝，并且在树的生长过程中，同一个自变量可以反复多次使用（分割），这些都是不同于 CHAID 的特点。另外，如果自变量存在数据缺失（Missing）的情况，CART 的处理方式是寻找一个替代数据来代替（或填充）缺失值，而 CHAID 则是把缺失数值作为单独的一类数值。

10.2.4　ID3 算法

ID3（Iterative Dichotomiser）与 CART 发明于同一时期，中文简称迭代的二分器，其最大的特点在于自变量的挑选标准是基于信息增益度量的，即选择具有最高信息增益的属性作为结点的分裂（或分割）属性，这样一来，分割后的结点里分类所需的信息量就会最小，这也是一种划分纯度的思想。至于 C4.5，可以将其理解为 ID3 的发展版本（后继版），主要区别在于 C4.5 用信息增益率（Gain Ratio) 代替了 ID3 中的信息增益，主要的原因是使用信息增益度量有个缺点，就是倾向于选择具有大量值的属性，极端的例子，如对于 Member_id 的划分，每个 Id 都是一个最纯的组，但是这样的划分没有任何实际意义，而 C4.5 所采用的信息增益率就可以较好地克服这个缺点，它在信息增益的基础上，增加了一个分裂信息（Split Information）对其进行规范化约束。

10.2.5　决策树的应用优势

在数据挖掘的实践应用中，决策树体现了如下明显的优势和竞争力：

❑决策树模型非常直观，生成的一系列「如果…… 那么……」的逻辑判断很容易让人理解和应用。这个特点是决策树赢得广泛应用的最主要原因，真正体现了简单、直观、通俗、易懂。

❑决策树搭建和应用的速度比较快，并且可以处理区间型变量（Interval）和类别型变量（Category）。但是要强调的是「可以处理区间型变量」不代表「快速处理区间型变量」，如果输入变量只是类别型或次序型变量，决策树的搭建速度是很快的，但如果加上了区间型变量，视数据规模，其模型搭建速度可能会有所不同。

❑决策树对于数据的分布没有特别严格的要求。

❑对缺失值（Missing Value）很宽容，几乎不做任何处理就可以应用。

❑不容易受数据中极端值（异常值）的影响。

❑可以同时对付数据中线性和非线性的关系。

❑决策树通常还可以作为有效工具来帮助其他模型算法挑选自变量。决策树不仅本身对于数据的前期处理和清洗没有什么特别的要求和限制，它还会有效帮助别的模型算法去挑选自变量，因为决策树算法里结点的自变量选择方法完全适用于其他算法模型，包括卡方检验、Gini 指数、信息增益等。

❑决策树算法使用信息原理对大样本的属性进行信息量分析，并计算各属性的信息量，找出反映类别的重要属性，可准确、高效地发现哪些属性对分类最有意义。这一点，对于区间型变量的分箱操作来说，意义非常重大。关于分箱操作，请参考本书 8.5.3 节。

10.2.6　决策树的缺点和注意事项

事物都是具有两面性的，有缺点不可怕，关键在于如何扬长避短，数据分析师不仅要清楚知道决策树的缺点，更需要掌握相应的注意事项，才可能取长补短，达到事半功倍的效果。

❑决策树最大的缺点是其原理中的贪心算法。贪心算法总是做出在当前看来最好的选择，却并不从整体上思考最优的划分，因此，它所做的选择只能是某种意义上的局部最优选择。学术界针对贪心算法不断进行改进探索，但是还没有可以在实践中大规模有效应用的成熟方案。

❑如果目标变量是连续型变量，那么决策树就不适用了，最好改用线性回归算法去解决。

❑决策树缺乏像回归或者聚类那样的丰富多样的检测指标和评价方法，这或许是今后算法研究者努力的一个方向。

❑当某些自变量的类别数量比较多，或者自变量是区间型时，决策树过拟合的危险性会增加。针对这种情况，数据分析师需要进行数据转换，比如分箱和多次模型验证和测试，确保其具有稳定性。

❑决策树算法对区间型自变量进行分箱操作时，无论是否考虑了顺序因素，都有可能因为分箱丧失某些重要的信息。尤其是当分箱前的区间型变量与目标变量有明显的线性关系时，这种分箱操作造成的信息损失更为明显。

10.3　逻辑回归技术的实践应用和注意事项

回归分析，在此主要是指包括逻辑回归技术和多元线性回归技术，是数量统计学中应用最广泛的一个分析工具，也是数据分析挖掘实践中应用得最广泛的一种分析方法（技术）。尽管从狭隘的界定来看，回归分析技术属于统计分析的范畴，但是正如本书开头所阐述的那样，绝对地划清统计分析和数据挖掘的界线，对于数据分析挖掘实践来说是没有任何意义的。只要能解决实际的业务问题，只要能提升企业的运营效率，它就是好技术，况且目前在数据挖掘实践中也大量应用回归分析技术。因此，本节将专门讨论逻辑回归技术。

10.3.1　逻辑回归的原理和核心要素

当目标变量是二元变量（即是与否）的时候，逻辑回归分析是一个非常成熟的、可靠的主流模型算法。

对于二元（是与否）的目标变量来说，逻辑回归的目的就是要预测一组自变量数值相对应的因变量是「是」的概率，这个概率 P 是介于 [0,1] 之间的。如果要用线性回归方法来进行概率计算，计算的结果很可能是超出 [0,1] 范围的。在这种情况下，就需要用到专门的概率计算公式了，或叫 Sigmoid 函数，其计算公式如下：

上述概率算法可以确保二元目标变量的预测概率 P 是介于 [0,1] 之间的。

其中，β0 是常数，β1 到 βk 是自变量 x1 到 xk 各自所对应的系数。

按上述公式应用后的 Sigmoid 分布曲线如图 10-2 所示。

图　10-2　Sigmoid 分布曲线

接下来进一步深入理解，这里引入了可能性比率（ODDS）这个概念。

可能性比率（ODDS）是指一件事情发生的概率除以这件事情不发生的概率后得到的值，博彩活动中的赔率就是可能性比率，其在现实生活中是一个广为人知的应用案例。

可能性比率为 5，说明一件事件发生的可能性比不发生的可能性高 5 倍；

可能性比率为 0.2，说明一件事情发生的可能性为不发生的可能性的 1/5；

可能性比率小于 1，说明一件事情发生的概率低于 50%；

可能性比率大于 1，说明一件事件发生的概率高于 50%；

与概率不同的是，可能性比率的最小值为 0，但最大值可以是无穷大。

可能性比率是逻辑回归中连接自变量和因变量的纽带，我们可以从下面的公式演变中体会这句话的意思。

将上述两个公式合并，就会成为现在广泛应用的逻辑回归算法：

该公式也可以表现为：

逻辑回归使用的参数估计方法通常是最大似然法，利用最大似然法进行参数的估计时，通常有如下步骤：

设 Y 为 0-1 型变量，X=(x1,x2,…,xp) 是与 Y 相关的变量，n 组观测数据为 (xi1,xi2,…,xip；yi)(i=1,2,…,n)，yi 与 xi1,xi2,…,xip 的关系如下：

其中，函数 f (x) 是值域在 [0,1] 区间的单调递增函数，对于逻辑回归（Logistic Regression），有。

于是，yi 是均值为 πi=f (β0+β1xi1+β2xi2+…+βpxip) 的 0-1 分布，其概率函数为

P(yi=1)=πi

P(yi=0)=1-πi

可以把 yi 的概率函数合写为

于是 y1,y2,…,yn 的似然函数则为

对上述似然函数取对数，得

对于逻辑回归，将代入上式，得

上述式子被称为对数似然函数，其目的就是求出该式子的最大值，其中会涉及非线性方程组的求解，运算量非常大，所幸的是这些工作现在都有现成的软件可以代替人工计算了，数据分析师只需要知道其中的原理就可以了。

需要强调的是，对于通过上述最大似然法得到的参数估值，还需要进行相应的显著性检验，对于回归系数 βi 的估计值的显著性检验通常使用的是 Wald 检验，其公式为。

其中，D () 为回归系数 βi 的估计值的标准差。如果 βi 的估计值的 Wald 检验显著，通常来讲，变量对应的 P-Value 如果小于 0.05，这时可以认为该自变量对因变量的影响是显著的，否则影响不显著。

10.3.2　回归中的变量筛选方法

无论是线性回归，还是逻辑回归，在回归拟合的过程中，都要进行变量的筛选，并且有各种不同的筛选方法，其中最常见、最著名的 3 种方法分别是向前引入法（Forward Selection）、向后剔除法（Backward Elimination）、逐步回归法（Stepwise Selection）。

❑向前引入法（Forward Selection）。即采用回归模型逐个引入自变量。刚开始，模型中没有自变量，然后引入第一个自变量进入回归方程，并进行 F 检验和 T 检验，计算残差平方和。如果通过了检验，则保留该变量。接着引入第二个自变量进入回归模型中，重新构建一个新的估计方程，并进行 F 检验和 T 检验，同时计算残差平方和。从理论上说，增加一个新的自变量之后，回归平方和应该增加，残差平方和应该减少。引进一个新自变量前后的残差平方和之差额就是新引进的该自变量的偏回归平方和，如果改值明显偏大，说明新引进的该自变量对目标变量有显著影响，反之则没有显著影响。向前引入法最大的缺点是最先引入回归方程的变量在随后不会被剔除出去，这会对后面引入的变量的评估过程和结果造成干扰。

❑向后剔除法（Backward Elimination）。向后剔除法正好与向前引入法相反，即首先把所有的自变量一次性放进回归模型中进行 F 检验和 T 检验，然后逐个删除不显著的变量，删除的原则是根据其偏回归平方和的大小来决定的。如果偏回归平方和很大则保留，否则删除之。向后剔除法最大的缺点是可能会引入一些不重要的变量，并且变量一旦被剔除之后，就没有机会重新进入回归模型中了。

❑逐步回归法（Stepwise Selection）。该方法综合了上述两种方法的特点。自变量仍然是逐个进入回归模型中，在引入变量时需要利用偏回归平方和进行检验，只有显著时才可以加入。当新的变量加入模型之后，又要重新对原来的老变量进行偏回归平方和的检验，一旦某变量变得不显著时就要立即删除该变量。如此循环往复，直到留下来的老变量均不可删除，并且新的变量也无法加入为止。

10.3.3　逻辑回归的应用优势

相比于数据挖掘建模常用的其他算法如决策树、神经网络、邻近记忆推理等，逻辑回归技术是最成熟、应用最广泛的，也是数据分析师和数据化运营业务人员最为熟悉的。在各种新的数据挖掘算法层出不穷的今天，逻辑回归技术仍然具有强大的活力和最广泛的业务应用基础。

10.3.4　逻辑回归应用中的注意事项

逻辑回归实践应用中的注意事项如下：

❑建模数据量不能太少，目标变量中每个类别所对应的样本数量要足够充分，才能支持建模。

❑要注意排除自变量中的共线性问题。关于共线性问题，可参考本书 8.7 节。

❑异常值（Outliers）会给模型带来很大干扰，应该删除。

❑逻辑回归模型本身不能处理缺失值（Missing Value），所以应用逻辑回归算法的时候，要注意针对缺失值进行适当的处理，或者赋值，或者替换，或者删除，可参考本书 8.4.1 节。

10.4　多元线性回归技术的实践应用和注意事项

之所以本章在最后才介绍线性回归模型，主要的原因在于线性回归是逻辑回归的基础，同时，线性回归也是数据挖掘中常用的处理预测问题的有效方法。线性回归与逻辑回归最大的区别，也是最直观的区别在于目标变量的类型，线性回归所针对的目标变量是区间型的（Interval），而逻辑回归所针对的目标变量是类别型的（Category）。另外，线性回归模型与逻辑回归模型的主要区别如下：

❑线性回归模型的目标变量与自变量之间的关系假设是线性关系的，而逻辑回归模型中目标变量与自变量之间的关系是非线性的。

❑在线性回归中通常会假设，对应于自变量 X 的某个值，目标变量 Y 的观察值是服从正态分布的；但是，在逻辑回归中，目标变量 Y 是服从二项分布 0 和 1 或者多项分布的。

❑在逻辑回归中，不存在线性回归里常见的残差。

❑在参数的估值上，线性回归通常采用的是最小平方法，而逻辑回归通常采用的是最大似然法。

10.4.1　线性回归的原理和核心要素

线性回归包括一元线性回归和多元线性回归，在数据分析挖掘的业务实践中，用得更多的是多元线性回归。

「多元线性回归」是描述一个区间型目标变量（Interval Variable）Y 是如何随着一组自变量 X1，X2，…，Xp 的变化而变化。把目标变量 Y 与自变量 X1，X2，…，Xp 联系起来的公式就是多元线性回归方程。

在目标变量 Y 的变化中包括两个部分：系统性变化和随机变化。系统性变化是由自变量引起的；而自变量不能解释的那部分变化就是所谓的残差，该部分可以认为是随机变化。

在多元线性回归方程中，目标变量 Y 与一组自变量之间的线性函数关系，可以用如下公式表示：

Y=β0+β1x1+β2x2+…+βpxp+ε

其中，Y 是目标变量，X1，X2，…，Xp 是自变量，β0 是常数（截距），β0,β2,…，βp, 是每个自变量的系数（权重），ε 是随机误差。

常用来估算多元线性回归方程中自变量系数的方法就是最小平方法，即找出一组参数（与 β1,β2,…，βp 相对应），使得目标变量 Y 的实际观察值与回归方程的预测值之间总的方差最小。

对于多元线性回归方程的检验，一般从模型的解释程度、回归方程的总体显著性和回归系数的显著性等方面进行检验。

❑模型的解释程度，又称回归方程的拟合度检验。R 的平方（R-Square），也叫做 R2 或 Coefficient of Multiple Determination 表示拟合度的优劣，其取值范围为 [0,1]。关于 R2 的详细介绍，请参考本书 8.6.4 节。需要强调的是，R2 的数值与自变量的个数有关，自变量的个数越多，R2 越大，这在一定程度上削弱了 R2 的评价能力，因此在实践中通常要考虑剔除自变量数目影响后的 R2，即修正的 R2（Adjustable R2）。

❑回归方程的总体显著性检验。主要是检验目标变量与自变量之间的线性关系是否显著，也就是自变量的系数是否不全为 0，其原假设为：H0:β1=β2=…=βp=0；而其备选假设为：H1:βp 不全为 0。该检验利用 F 检验完成。

❑回归方程系数的显著性检验。回归方程系数的显著性检验要求对所有的回归系数分别进行检验。如果某个系数对应的 P 值小于理论显著性水平 α 值，则可认为在显著性水平 α 条件下，该回归系数是显著的。

10.4.2　线性回归的应用优势

线性回归模型作为应用最为广泛的算法，其主要的优势如下：

❑通俗易懂。多元线性回归模型非常容易被解读，其自变量的系数直接跟权重挂钩，因此很容易解释每个自变量对于目标变量的预测价值大小（贡献大小），解读出的这些信息可以为数据化运营提供有效的思考方向。

❑速度快，效率高。相比于其他的建模算法而言，多元线性回归的计算速度是最快的。

❑可以作为查找异常值的有效工具。那些与多元线性回归方程的预测值相差太大的观察值通常值得进一步考察，确定其是否是异常值。

10.4.3　线性回归应用中的注意事项

线性回归应用中的注意事项如下：

❑算法对于噪声和异常值比较敏感。因此，在实践应用中，回归之前应该努力消除噪声和异常值，确保模型的稳定和准确度。

❑该算法只适合处理线性关系，如果自变量与目标变量之间有比较强烈的非线性关系，直接利用多元线性回归是不合适的。不过，在这种情况下，可以尝试对自变量进行一定的转换，比如取对数、开平方、取平方根等，尝试用多种不同的运算进行转换。

❑多元线性回归的应用还有一些前提假设：自变量是确定的变量，而不是随机变量，并且自变量之间是没有线性相关性的；随机误差项具有均值为 0 和等方差性；随机误差呈正态分布等。

10.5　模型的过拟合及对策

模型的过拟合（Over Fitting）是指模型在训练集里的表现让人非常满意，但是一旦应用到真实业务实践中，效果会大打折扣。换成学术化语言描述，就是模型对样本数据拟合得非常好，但是对于样本数据外的应用数据，拟合效果非常差。在数据分析挖掘业务实践中，即为模型搭建时的表现看上去非常好，但是应用到具体业务实践时，模型的效果显著下降，包括准确率、精度、效果等都显著下降了。

过拟合现象是数据挖掘中常见的一种挫折，尤其是在预测响应（分类）模型的应用场景里。在模型的实践应用中如果发生了模型的过拟合，不仅会大幅度降低模型的效果和效率，也会严重浪费运营业务资源，同时，还会严重打击数据分析师的自信心和影响力。所以，数据分析师应该比较清楚地了解过拟合产生的主要原因以及可以采用的相应措施，尽量去避免过拟合的发生。

总的来说，过拟合产生的主要原因如下：

❑建模样本抽取错误。包括但不限于样本数量太少，抽样方法错误，抽样时没有足够正确地考虑业务场景或业务特点等，以致抽出的样本数据不能足够有效地代表业务逻辑或业务场景。

❑样本里的噪声数据干扰过大。样本噪声大到模型过分记住了噪声特征，反而忽略了真实的输入输出间的关系。

❑在决策树模型的搭建过程中，如果对于决策树的生长没有合理的限制和修剪，由着决策树自由的生长，那有可能会使每片叶子里只包含单纯的事件数据（Event）或非事件数据（No Event）。可以想象，这种决策树当然是可以完美匹配（拟合）训练数据的，但是一旦应用到新的业务真实数据中，效果就会一塌糊涂。

❑建模时的逻辑假设到了应用模型时已经不能成立了。任何预测模型都是在假设的基础上才可以搭建和应用的，常用的假设包括：假设历史数据可以推测未来，假设业务环节没有发生显著变化，假设建模数据与后来的应用数据是相似的等。如果上述假设违反了业务场景，那么根据这些假设搭建的模型当然是无法有效应用的。

❑建模时使用了太多的输入变量。这同第二点噪声数据有些类似，数据挖掘新人常常犯这个错误，自己不做分析判断，把所有的变量交给软件或者机器去「撞大运」。须知，一个稳定优良的模型一定要遵循建模输入变量少而精的原则。

上面的原因都是现象，其实本质只有一个，那就是对业务理解错误造成的，无论是抽样，还是噪声，还是决策树、神经网络等，如果我们对于业务背景和业务知识了解得非常透彻，一定是可以避免绝大多数过拟合现象产生的。因为在模型从确定需求、思路讨论、搭建到业务应用验证的各个环节中，都是可以通过业务敏感来防止过拟合产生的。

入世，出世，都是一样的道，所谓的道从来不曾离开我们半步，只是看我们自身是否足够清净，足够醒悟，足够真实而已。佛法有八万四千法门，不过是不同的方便路径，归根结底，佛法的根本只是认识我们与生俱来的本来面目，真如自性。

过拟合的产生，有种种原因，不一而足，对其进行分类和剖析只是为了方便而已，防止过拟合的终极思路就是真正透彻理解业务背景和业务逻辑，有了这个根本，我们一定可以正确抽样，发现并排除噪声数据，一定可以在决策树、神经网络等算法中有效防止过拟合的产生。

当然，除了透彻了解业务本质外，还有一些技术层面的方法来防止过拟合的产生，虽然是「术」层面上的内容，但是很多人热衷于这些技巧，所以，在这里也顺便讲解如下：

❑最基本的技术手段，就是合理、有效地抽样；包括分层抽样、过抽样等，从而用不同的样本去检验模型。

❑事前准备几个不同时间窗口、不同范围的测试数据集和验证数据集，然后在不同的数据集里分别对模型进行交叉检验，这是目前业界防止过拟合的最常用的手段。

❑建模时目标观测值的数量太少，如何分割训练集和验证集的比例，需要建模人员灵活掌握。

❑如果数据太少，谨慎使用神经网络模型，只有拥有足够多的数据，神经网络模型才可以有效防止过拟合的产生。并且，使用神经网络时，一定要事先有效筛选输入变量，千万不能一股脑把所有的变量都放进去。

10.6　一个典型的预测响应模型的案例分享

10.6.1　案例背景

某垂直细分的 B2B 网站平台，其商业模式是通过买卖双方在平台上产生交易而对卖家抽取交易提成费。对于该网站平台来说，促成买卖双方的线上成交是该平台的价值所在，网站平台的发展和盈利最终取决于是否能有效且规模化地促成买卖双方的线上成交并持续成交。

要有效且规模化地促成买卖双方在线成交，该网站平台有许多事情要做，包括吸引优质卖家、吸引广大有采购意愿的优质买家、帮助卖家在平台上更好地展示商品、帮助买家更快更有效地匹配所需要的卖家、优化网站交易流程以方便交易更有效、提供风险控制措施，保障双方交易的安全等。这里提到的每一个目的其实都是包含着一揽子的分析课题和项目开发的，需要数据分析团队在内的所有相关部门协同合作来实现。

本案例所要分享的就是其中一个细分的项目：初次成交的预测模型和运营应用。对于该平台上的卖家来说，从最开始的注册、发布商品信息，到后期的持续在线获得订单和在线成交，其中有一个结点对于卖家来说是至关重要，具有突破性的，那就是第一次在线成交，也叫初次成交转化，这个初次成交对于卖家的成功体验和激励的价值是不言而喻的；另外，从网站平台的运营方来说，卖家的初次成交也是网站运营工作的一个重要考察环节和考察指标，只有初次成交的卖家数量越多，周期越短，才可以有效保障后期持续性、规模化在线成交的可能性。本着上述背景和考虑思路，网站平台运营方希望通过数据分析找出短期内最有可能实现初次成交的卖家群体，分析其典型特征，运营方可以据此对卖家群体进行分层的精细化运营。最终的目的是一方面希望可以通过数据化运营有效提升单位时间段内初次成交的卖家数量，另一方面为今后的卖家培养找出一些运营可以着力的「抓手」，以帮助卖家有效成长。

10.6.2　基本的数据摸底

为了慎重起见，数据分析团队与运营方协商，先针对网站平台的某一个细分产品类目的卖家进行初次成交的专题分析。视分析和建模的应用效果，再决定后期是否推广到全站的卖家。

因此，本次专题分析只针对代号为 120023 的细分产品类目卖家，根据网站平台的运营规律和节奏，初步的分析思路是通过对第 N-1 月份的卖家行为数据和属性数据的分析，寻找它们与卖家第 N 个月有实际的在线初次成交之间的关系。

在进行数据摸底后发现，截止当时项目进行时，代号为 120023 的细分产品类目卖家共有 170 000 家，交易次数为 0，即是还没有发生初次成交的卖家，经过连续几个月的数据观察，发现每个月实现初次成交的卖家基本上稳定在 2000 家左右。如果基于总共 170 000 家来计算每个月初次成交的转换率，大约在 1.12%。

根据数据分析师的项目经验以及运营方的业务判断，总数 170 000 的大池子里应该是可以通过数据分析找出一些简单的阀值过滤掉一批最不可能近期实现初次成交的卖家群体的。通过业务经验和连续几个月对重点字段的数据摸底，得到了如下结果：

❑月度登录「即时通信工具」达 10 天次以上的潜在卖家，平均每月大概为 50 000 人，其中在次月实现初次成交的用户有 1900 人左右（对比原始数据每月大概 170 000 的潜在卖家，次月实现（初次成交）的用户有 2000 人左右；浓缩过滤后只保留 50 000 人（过滤了大约 71% 的近期可能性很小的大部分卖家），但是次月实现初次成交的用户只过滤掉 5%；换句话说，通过设置阀值月度登录即时通信工具达到 10 天次以上，初次成交的转换率就从原始的 1.12% 提升到 3.5% 左右。并且这个阀值的设立只是丢失了 5% 的初次成交卖家。找到这个阀值的意义在于，基于 3.5% 的转换率搭建的模型相比在原始转换率 1.12% 基础上搭建的模型来说要更加准确，更容易发现自变量与因变量之间的关系。

❑来自两个特定省份 A 省和 B 省的卖家，其初次成交的转换率约为 3.3%，所覆盖的初次成交卖家数为 70% 左右，即是丢失了将近 30% 的初次成交卖家。

❑可交易 Offer 占比大于等于 0.5 的卖家，其初次成交的转换率约为 3.7%，所覆盖的初次成交卖家数为 85% 左右。

基于上述的一些数据摸底和重要发现，数据分析师与业务方沟通后，决定设置阀值为月度登录即时通信工具达到 10 天次以上，在此基础上尝试数据分析挖掘建模和后期应用。

在数据摸底环节中，还有一个重要的基础性工作，那就是与业务方一起列出潜在的分析字段和分析指标，如图 10-3 所示 [1]。这个工作是后期分析挖掘的基础，可圈定大致的分析指标和分析字段的范围，并据此进行数据的抽取工作。之所以强调要与业务方一起列出潜在的分析字段和分析指标，是因为在项目的前期阶段，业务方的业务经验和灵感非常重要，可以协助数据分析师考虑得更加全面和周详。

图　10-3　初步分析字段一览

在上述原始字段的基础上，数据分析师通过走访业务方，以及经过资深业务专家的检验，增添了一些重要的衍生变量如下：

❑类目专注度。公式是卖家该类目下总的有效商品 Offer 数量除以该卖家在网站中总的有效商品 Offer。因为有足够的理由相信，类目专注度越高，越容易产生成交。

❑优质商品 Offer 占比。公式是卖家的优质 Offer 数量除以该卖家总的有效商品的 Offer 数量。因为有足够的理由相信，优质的商品 Offer 越多，越容易产生成交。

❑可在线交易 Offer 的占比。公式是卖家的可在线交易 Offer 数量除以该卖家总的有效商品的 Offer 数量。

[1] 限于业务方的商业隐私，这些字段和指标的中文含义就不详述了。

10.6.3　建模数据的抽取和清洗

在完成了前期摸底和变量罗列之后，接下来的工作就是抽取建模数据和熟悉、清洗数据环节了。这个环节的工作量是最大的，它和随后的数据转换环节，所需要消耗的时间占整个数据分析建模项目时间的 70%，甚至更多。

抽取、熟悉、清洗数据的目的主要包括：熟悉数据的分布特征和数据的基本统计指标、发现数据中的缺失值（及规模）、发现数据中的异常值（及规模）、发现数据中明显与业务逻辑相矛盾的错误。这样最终就可以得到比较干净的数据，从而提高随后分析的准确性和后期模型搭建的效果了。

在本项目的数据清洗过程中，发现了以下的数据错误：

❑Company_Reg_Capital 这个字段有少数的样本夹杂了中文，与绝大多数观察值中的数字格式不一致，容易引起机器的误判，需要直接把这些少数样本删除。

❑Credit_Status 这个字段有将近 40% 是空缺的，经过业务讨论，决定直接删除该字段。

❑Bu_Name 这个字段是中文输入，属于类别型变量，为了后期数据分析需要，将其转化为数字格式的类别型变量。

❑Credit_Balance_Amt 有将近 20% 的观察值是 N，而其余观察值是区间型数字变量，经过走访数据仓库相关人员，确认这些为 N 的观察值实际上应该是 0。为了后期数据分析需要，将该字段所有为 N 的观察值替换成 0。

同时，对原始变量进行基本的统计观察，图 10-4 是各字段的基本统计指标一览表。

图　10-4　各字段的基本统计指标一览表

10.6.4　初步的相关性检验和共线性排查

在该阶段进行初步的相关性检验，主要有 3 个目的：一是进行潜在自变量之间的相关性检验后，高度相关的自变量就可以择一进入模型，而不需要都放进去。二是通过相关性检验，排除共线性高的相关字段，为后期的模型搭建做好前期的基础清查工作。三是，如果潜在自变量与目标变量之间的高度线性相关，则可以作为筛选自变量的方法之一进行初步筛选。

图 10-5 是相关性检验的部分截屏，从中可以发现，tradable_grade45_offer_bu 与 valid_sale_offer_cnt 线性相关系数为 0.668 53，且 P 值小于 0.000 1，这说明这两变量之间有比较强的线性相关性，在后续的建模中至多只能二选一，也就是说只能挑选出来一个作为潜在的自变量，然后根据其他筛选自变量的方法综合考虑是否最终进入模型中。

图　10-5　相关性检验的截屏图

10.6.5　潜在自变量的分布转换

本环节主要是针对前面的基础统计结论，包括偏度 Skewness 和峰度 Kurtosis 进行分箱转换、以正态分布为目的的转换，以及其他形式的转换。

比如，在前面的基础统计结论里，我们发现：

Valid_Sale_Offer_Cnt 偏度（Skewness）为 17.008，峰度 Kurtosis 为 438.62，这样的分布非常不均衡，不利于后期模型的拟合，因此需要对这些分布不均匀的变量进行转换，（如图 10-6 和图 10-7）。

图　10-6　变量 Valid_Sale_Offer_Cnt 的原始分布图

图　10-7　变量 Valid_Sale_Offer_Cnt 取对数后的分布图

10.6.6　自变量的筛选

自变量的筛选有很多方法，比如本书第 8.6 节就具体分享了各种不同筛选输入变量的方法。在数据挖掘商业实战中，通常的做法是分别采用多种方法，这样可以防止单一筛选方法有可能遗漏一些重要的变量。

在本项目里，数据分析师采用了多种筛选方法逐一尝试、对比，最终得到了以下一些重要变量，并将其作为自变量收入模型当中，如表 10-1 所示。

10.6.7　响应模型的搭建与优化

在本项目的模型搭建过程中，数据分析师分别尝试了 3 种不同的模型工具，即决策树、逻辑回归及神经网络，在每一种工具里又分别尝试了不同的算法或参数调整，经过反复的比较和权衡，得到了比较满意的模型结论。具体内容参考 10.6.8 节的结论分析。

关于模型优化的详细方法论，可参考本书第 7 章。

10.6.8　冠军模型的确定和主要的分析结论

经过比较和权衡，最终的冠军模型，即投入落地应用的模型是逻辑回归模型，相应的模型响应率曲线图，如图 10-8 所示，模型捕获率曲线图，如图 10-9 所示，模型 lift 曲线图，如图 10-10 所示。关于如何解读模型捕获率曲线、响应率曲线和 Lift 曲线，可参考本书 7.4.4 节的详细介绍。

图　10-8　模型响应率曲线图

图　10-9　模型捕获率曲线图

图　10-10　模型 Lift 曲线图

之所以最终选择逻辑回归模型作为冠军模型，主要是基于两方面的理由：一方面是逻辑回归模型的效果，即提升率、捕获率及转化率与最高的神经网络模型相差无几，另一方面是逻辑回归的可解释性远远高于神经网络模型，这一点对于落地应用中的业务方来说尤为重要。

模型的最终确定，还需要经过最新的真实数据验证，数据分析师用选好的冠军模型来对最新月度的真实数据进行模拟打分验证，结果表明冠军模型非常稳定，表现非常出色，具体验证结果如图 10-11 所示。

图　10-11　模型应用到新数据后的捕获率曲线效果图

10.6.9　基于模型和分析结论基础上的运营方案

基于模型的效果和主要自变量的业务含义，本项目落地应用方案包括两部分，即卖家基于概率分数的分层，以及在分层基础上的相应运营措施和重点，具体内容讲解如下。

根据模型打分后的次月初次成交概率的分数高低，对潜在成交卖家进行分层精细化运营。比如，模型打分最高的 10% 的卖家，是最有可能在次月实现初次成交突破的，运营方对该类群体的运营方针应该是临门一脚式的一击即中，也就是与流量资源团队合作，给这批优质客户群体提供更大的流量，有效提升初次成交转化率。

对于模型打分的概率分数为 10%～30% 之间的群体，这类群体没有前 10% 的卖家在次月实现初次成交的可能性那么高，但也是仅次于前 10% 卖家的，根据模型中有价值的输入变量的业务含义，运营方应该作出相应的运营策略，即对于基础操作不够的，要通过运营提升相关的基础操作完成率；对于活跃度不够的，要通过相关的运营帮助卖家提升其活跃度等。

对于模型打分概率在 30% 之后的群体，尤其是 40% 之后的群体，由于其在近期实现初次成交的可能性很低，考虑到运营方的运营资源有限，无法面面俱到，所以针对这一群体，不能急功近利，需要有长期培育的心理准备。运营方可以通过线上广而告之的讲座、社区活动等，让这类卖家逐步完善自己的基础建设和深化其参与度，最终完成从量变到质变的转化。

10.6.10　模型落地应用效果跟踪反馈

初期模型，针对代号为 120023 的细分产品类目的卖家运营的测试效果不错，在此基础上又对模型进行了调整，因涉及企业商业隐私，具体技术手段在此略去，然后延伸到全站全行业应用，经过数据分析团队、业务运营团队等相关部门的通力合作，模型落地应用效果反馈不错。

针对 Top30% 的优质卖家进行重点运营后，在随后两周对运营效果进行验证，发现各个行业运营后的效果提升（初次成交突破的卖家数量）显著，效果对比如图 10-12 所示。相比未做专门运营的自然增长效果，本次重点运营的活动效果总体平均提升了 99%。

图　10-12　基于模型的精细化运营后的效果对比图



