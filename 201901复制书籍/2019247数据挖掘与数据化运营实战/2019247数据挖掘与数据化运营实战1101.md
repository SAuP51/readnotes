8.6.7　部分建模算法自身的筛选功能

除了上述这些具体的、直接的指标计算和参考的方法之外，在数据挖掘商业实战中，还有一种「借力」的巧妙方法，那就是借助于一些成熟的算法进行初步的运算，利用模型的初步结果筛选出有价值的自变量，再把这些经过初期过滤的自变量放进模型和算法中进行真正意义上的建模和验证工作。

可供「借力」的算法或者模型包括决策树模型、回归（含线性回归和逻辑回归）模型等，在建模前期的变量筛选阶段，借力可以帮助初选出有价值的自变量。需要强调的是，在这些场景中，这些算法工具和模型可能无法实现最终的预测（分类）功能，而仅仅是用作自变量的初步筛选。

比如线性回归和逻辑回归，算法本身通过不断地增加或者剔除变量，来检验各输入变量对于预测的价值，这就是所谓的 Stepwise 算法，但是，即便如此，最好在使用之前先进行人为的初步筛选，从而把精简后的变量交给算法去选择。在大数据量建模的时候尤其要如此。

8.6.8　降维的方法

在数据挖掘的实战中，面对数量庞大的原始变量，除了上述种种指标及思路外，还有一种方法也会经常被应用，那就是数据降维，具体来说，包括主成分分析和变量聚类等。其中，对于主成分分析，已在 2.3.8 节中进行了详细介绍；对于变量聚类，将在 8.7 节的共线性问题中做专门介绍。

通过采取降维的措施和方法，可以有效精简输入变量的数目，在一定程度上实现有效筛选模型输入变量的目标。

8.6.9　最后的准则

本节到目前为止，谈到了数据挖掘实战中常见的筛选输入变量的各种方法和原理，这些分析技术层面的技巧和工具的熟练应用可以有效提高我们筛选输入变量的效率和质量。但是，业务环境千差万别，应用场景纷繁复杂，很多时候我们既要考虑技术层面的指标及判断方法，同时又要受实战环境中诸多因素的影响和制约，包括时间、资源、成本和目标等。

有些时候，尽管通过上述的分析技术可发现某个变量很重要，但是具体实战中也可能会选择放弃，个中的原因可能会涉及环境因素，比如说该变量的收集要花费太长的时间，或者花费过多的成本，那么权衡下来，就有可能放弃该变量。毕竟，只要最终的模型能满足初期的业务需求就可以了，模型的优化和提升是需要兼顾和权衡其他因素的制约的。

既要贯彻落实上述种种有效的筛选输入变量的方法和原理，又要在数据挖掘商业实战中综合考虑诸多环境因素和制约条件，并加以权衡和折中，这就是筛选输入变量的方法和原理中最后的准则。这个准则体现了筛选变量的过程是个辩证的、丰富多彩的、充满活力的过程，体现了数据分析挖掘强大的生命力和勃勃生机。

8.7　共线性问题

共线性问题是困扰模型预测能力的一个常见问题。所谓共线性，又叫多重共线性，是指自变量之间存在较强的，甚至完全的线性相关关系。当自变量之间高度相关时，模型参数会变得不稳定，模型的预测能力会降低。同时，严重的共线性增加了对于模型结果的解释成本，因为它致使很难确切分辨每个自变量对因变量的影响。所以，在建模前期的变量筛选环节，就要对共线性问题引起足够的重视，并采取有效措施尽量加以避免。

需要强调的是，理论上来讲，输入变量之间除了存在共线性之外，完全可能存在其他各种非线性的关系，这些非线性的关系也很可能如共线性一样影响模型的预测能力。但是，我们无法完全掌握这些非线性关系，所以，只能以考察它们之间的线性关系为基础来排除一些主要的线性关系的变量。

8.7.1　如何发现共线性

常见的识别共线性的方法如下：

❑相关系数的方法。最常见的就是皮尔逊相关系数（Pearson Correlation），详细内容请参考 8.6.3 节，对于线性相关指标的详细讨论。

❑通过模型结论的观察。比如，在回归模型中，如果回归系数的标准差过大，就可能意味着变量之间存在着共线性问题。

❑主成分分析方法。在主成分分析方法中，主成分里的系数，也就是主成分载荷大小能从一定程度上反映出各个变量的相关性。比如，第一主成分中，某几个原始变量的主成分载荷系数较大，且数值相近，就有可能在其中隐藏着共线性问题。

❑根据业务经验判断的原本应该没有预测作用的变量突然变得有很强的统计性，那其中就有可能隐藏着共线性问题。

❑对变量进行聚类。通过对区间型变量进行聚类，同一类中的变量之间具有较强的相似性，也就可能隐藏着共线性问题。

8.7.2　如何处理共线性

水至清则无鱼，人至察则无徒，对于数据挖掘实战中出现的共线性问题，也需要本着中庸之道灵活处理。轻微的共线性是可以容忍的。比如说模型拟合度较高，样本量大的时候，轻微的共线性可以适当的采用视而不见的方法。但是，当样本量较少，很轻微的共线性问题都有可能导致参数的不稳定。如果发生严重的共线性问题，一般采取以下措施：

❑对相关变量进行取舍。高度共线性的相关变量，可以选择保留对业务方最有价值、最有意义的变量，而过滤掉相关变量。

❑对相关变量组合，生成一个新的综合性变量。

❑当我们利用相关变量通过线性的方式衍生出新的变量时，要记得两者之间的共线性问题，并且及时删除相关的原始变量，不要将其投入到模型中。在实践应用中这种情况会经常出现，也很容易被人忽视。

❑尝试对相关变量进行一些形式的转换（参考 8.5 节），恰当的转换可以在一定程度上减少甚至去除共线性关系。

第 9 章　聚类分析的典型应用和技术小窍门

物以类聚，人以群分。

——《战国策·齐策三》

9.1　聚类分析的典型应用场景

9.2　主要聚类算法的分类

9.3　聚类分析在实践应用中的重点注意事项

9.4　聚类分析的扩展应用

9.5　聚类分析在实际应用中的优势和缺点

9.6　聚类分析结果的评价体系和评价指标

9.7　一个典型的聚类分析课题的案例分享

从本章开始到第 13 章，将针对常见的分析（课题或算法）类型分别进行详细介绍，包括典型应用、案例、模型的评价指标和体系、相关技术应用在实践中的优点和缺点、主流的应用场景和扩展的应用场景等，还有一些重点技术要领和小窍门。

本章则是针对聚类分析的上述相关问题来展开讲解和进行总结的。之所以把聚类分析作为第一个专题来进行探讨，主要是想强调聚类分析技术在数据分析挖掘中的重要性和常用性，聚类技术一方面本身就是一种模型技术，通过有效聚类后的结果常常就可以直接指导落地应用实践；另一方面聚类技术又常常作为数据分析过程中前期进行数据摸底和数据清洗、数据整理（数据转换）的工具。鉴于聚类技术在实践应用中的上述多样性、多元性，数据分析师应该要对该技术的实践应用有比较深刻的认识和比较熟练地掌握。

9.1　聚类分析的典型应用场景

可以说，聚类分析的典型应用场景是非常普遍的，业务团队几乎每天都要碰到。比如说，把付费用户按照几个特定的维度，如利润贡献、用户年龄、续费次数等进行聚类划分，得到不同特征的群体。举个例子：在将付费用户进行聚类划分后，其中一个群体占总的付费用户人数的 40%，其特征是用户年龄在 25 岁左右，利润贡献不大，但是续费次数多；还有一个群体，占总的付费用户人数的 15%，而该群的特征是用户年龄在 40 岁以上，利润贡献比较大，但是续费次数不多。对于运营方来说，这两个典型群体都是可以「着力」的目标群体，并且分别有不同的运营思路和业务价值。对于第一个群体，虽然利润贡献不大，但是由于续费次数多，其表现出来的产品忠诚度对于企业和产品来说非常重要、非常可贵，因此针对该群体的重要运营目的应该是稳中有升，同时积极预防其流失，密切监控相应的流失率，并且还要进一步分析挖掘该群体的其他特征，从而可以有效复制该群体的规模，针对其 25 岁左右的年龄这个特点，可以考虑在运营方式和内容上更加贴近年轻人的喜好和兴趣；而针对后一个群体，虽然利润贡献大，但是很不稳定，续费次数少，对企业和产品的忠诚度不高，因此针对该群体的运营重点应该是采取积极措施提升续费率，提升其忠诚度。而该群体「40 岁以上的年龄」这个特点，也为相应的运营方式和运营内容的设计提供了比较准确的参考范围。

从上述简单的案例中，可以看出聚类分析的一个重要用途就是针对目标群体进行多指标的群体划分，而类似这种目标群体的分类常常就是精细化运营、个性化运营的基础和核心，只有进行了正确的分类，才可以有效进行个性化和精细化的运营、服务及产品支持等，从这个角度来看，聚类分析技术对于数据化运营而言是非常重要、非常基础的。

总地来说，聚类分析技术在数据化运营实践中常见的业务应用场景如下。

❑目标用户的群体分类：通过为特定运营目的和商业目的所挑选出的指标变量进行聚类分析，把目标群体划分成几个具有明显特征区别的细分群体，从而可以在运营活动中为这些细分群体采用精细化、个性化的运营和服务，最终提升运营的效率和商业的效果。

❑不同产品的价值组合：企业可以按照不同的商业目的，并依照特定的指标变量来为众多的产品种类进行聚类分析，把企业的产品体系进一步细分成具有不同价值、不同目的多维度的产品组合，并且可在此基础上分别制定相应的产品开发计划、运营计划和服务规划。

❑探测、发现孤立点、异常值：孤立点就是指相对于整体数据对象而言的少数数据对象，这些对象的行为特征与整体的数据行为特征很不一致。虽然在一般的数据处理过程中会把孤立点作为噪声而剔除出去，但是在许多业务领域里，孤立点的价值非常重要。比如说，互联网的风险管理里，就非常强调对于风险的预防和预判，而相关的风险控制分析中的孤立点很多时候又是风险的最大嫌疑和主要来源。及时发现这些特殊行为对于互联网的风险管理来说至关重要。比如，某 B2C 电商平台上，比较昂贵的、频繁的交易，就有可能隐含着欺诈的风险成分，需要风控部门提前关注、监控，防患于未然。

9.2　主要聚类算法的分类

聚类算法的深入研究到今天已经持续了半个多世纪，聚类技术也已经成为最常用的数据分析技术之一。其各种算法的提出、发展、演化也使得聚类算法家族「家大口阔，人丁兴旺」。下面就针对目前数据分析和数据挖掘业界主流的认知将聚类算法进行介绍。

9.2.1　划分方法

给定具有 n 个对象的数据集，采用划分方法（Partitioning Methods）对数据集进行 k 个划分，每个划分（每个组）代表一个簇，k≤n，并且每个划分（每个簇）至少包含一个对象，而且每个对象一般来说只能属于一个组。对于给定的 k 值，划分方法一般要做一个初始划分，然后采取迭代重新定位技术，通过让对象在不同组间移动来改进划分的准确度和精度。一个好的划分原则是：同一个簇中对象之间的相似性很高（或距离很近），而不同簇的对象之间相异度很高（或距离很远）。目前主流的划分方法如下。

❑K-Means 算法：又叫 K 均值算法，这是目前最著名、使用最广泛的聚类算法。在给定一个数据集和需要划分的数目 k 后，该算法可以根据某个距离函数反复把数据划分到 k 个簇中，直到收敛为止。K-Means 算法用簇中对象的平均值来表示划分的每个簇，其大致的步骤是，首先从随机抽取的 k 个数据点作为初始的聚类中心（种子中心），然后计算每个数据点到每个种子中心的距离，并把每个数据点分配到距离它最近的种子中心；一旦所有的数据点都被分配完成，每个聚类的聚类中心（种子中心）按照本聚类（本簇）的现有数据点重新计算；这个过程不断重复，直到收敛，即满足某个终止条件为止，最常见的终止条件是误差平方和（SSE）局部最小。

❑K-Medoids 算法：又叫 K 中心点算法，该算法用最接近簇中心的一个对象来表示划分的每个簇。K-Medoids 算法与 K-Means 算法的划分过程相似，两者最大的区别是 K-Medoids 算法是用簇中最靠近中心点的一个真实的数据对象来代表该簇的，而 K-Means 算法是用计算出来的簇中对象的平均值来代表该簇的，这个平均值是虚拟的，并没有一个真实的数据对象具有这些平均值。

9.2.2　层次方法

在给定 n 个对象的数据集后，可用层次方法（Hierarchical Methods）对数据集进行层次分解，直到满足某种收敛条件为止。按照层次分解的形式不同，层次方法又可以分为凝聚层次聚类和分裂层次聚类：

❑凝聚层次聚类：又叫自底向上方法，一开始将每个对象作为单独的一类，然后相继合并与其相近的对象或类，直到所有小的类别合并成一个类，即层次的最上面，或者达到一个收敛，即终止条件为止。

❑分裂层次聚类：又叫自顶向下方法，一开始将所有对象置于一个簇中，在迭代的每一步中，类会被分裂成更小的类，直到最终每个对象在一个单独的类（簇）中，或者满足一个收敛，即终止条件为止。

层次方法最大的缺陷在于，合并或者分裂点的选择比较困难，对于局部来说，好的合并或者分裂点的选择往往并不能保证会得到高质量的全局的聚类结果，而且一旦一个步骤（合并或分裂）完成，它就不能被撤销了。

9.2.3　基于密度的方法

传统的聚类算法都是基于对象之间的距离，即距离作为相似性的描述指标进行聚类划分，但是这些基于距离的方法只能发现球状类型的数据，而对于非球状类型的数据来说，只根据距离来描述和判断是不够的。鉴于此，人们提出了一个密度的概念，基于密度的方法（Density-Based Methods），其原理是：只要邻近区域里的密度（对象的数量）超过了某个阀值，就继续聚类。换言之，给定某个簇中的每个数据点（数据对象），在一定范围内必须包含一定数量的其他对象。该算法从数据对象的分布密度出发，把密度足够大的区域连接在一起，因此可以发现任意形状的类。该算法还可以过滤噪声数据（异常值）。基于密度的方法的典型算法包括 DBSCAN（Density-Based Spatial Clustering of Application with Noise）以及其扩展算法 OPTICS（Ordering Points to Identify the Clustering Structure）。其中，DBSCAN 算法会根据一个密度阀值来控制簇的增长，将具有足够高密度的区域划分为类，并可在带有噪声的空间数据库里发现任意形状的聚类。尽管此算法优势明显，但是其最大的缺点就是，该算法需要用户确定输入参数，而且对参数十分敏感。

9.2.4　基于网格的方法

基于网格的方法（Grid-Based Methods）将把对象空间量化为有限数目的单元，而这些单元则形成了网格结构，所有的聚类操作都是在这个网格结构中进行的。该算法的优点是处理速度快，其处理时间常常独立于数据对象的数目，只跟量化空间中每一维的单元数目有关。基于网格的方法的典型算法是 STING（Statistical Information Grid）算法。该算法是一种基于网格的多分辨率聚类技术，将空间区域划分为不同分辨率级别的矩形单元，并形成一个层次结构，且高层的低分辨率单元会被划分为多个低一层次的较高分辨率单元。这种算法从最底层的网格开始逐渐向上计算网格内数据的统计信息并储存。网格建立完成后，则用类似 DBSCAN 的方法对网格进行聚类。

9.3　聚类分析在实践应用中的重点注意事项

在数据化运营实践中，由于针对大规模数据集所采用的聚类算法主要是 K-Means 算法应用，因为其简洁、高效、易理解、易实施。因此，除非特别说明，本章所展开讲解的聚类技术的具体内容都是针对 K-Means 算法进行分析和阐述的。

9.3.1　如何处理数据噪声和异常值

K-Means 算法对噪声和异常值非常敏感，这些个别数据对于平均值的影响非常大，相对而言，K - 中心点的方法不像 K-Means 算法，它不是求样本的平均值，而是用类中最接近于中心点的对象来代表类，因此 K - 中心点的方法对于噪声和异常值没有 K-Means 算法那么敏感。鉴于 K-Means 算法的这一局限性，我们应用该算法时需要特别注意这些数据噪声和异常值。

针对聚类中的数据噪声和异常值，常用的处理方法如下：

❑直接删除那些比其他任何数据点都要远离聚类中心点的异常值。为了防止误删的情况发生，数据分析师需要在多次的聚类循环中监控这些异常值，然后依据业务逻辑与多次的循环结果进行对比，再决定是否删除这些异常值。

❑随机抽样的方法也可以较好地规避数据噪声的影响。因为是随机抽样，作为稀有事件的数据噪声和异常值能被随机抽进样本中的概率会很小，这样随机抽出的样本就比较干净。针对该随机样本进行聚类分析时不仅可以避免数据噪声的误导和干扰，而且其聚类后的结果作为聚类模型可以应用到剩余的数据集中，完成对整个数据集的聚类划分。利用这种随机抽样方式得到的聚类模型，在应用于整个数据集时至少有以下两种方式。

1）直接用该聚类模型对剩余的数据集进行判断，也就是把剩余的数据分配给那些离它们最近的聚类中心，这种方法最简单、最直观、最快捷。

2）利用监督学习中的分类器的原理，每个聚类被认为是一个类别，已经参与聚类的这些随机抽样数据则被看做是学习样本，由此产生的分类器可以用于判断剩余的那些数据点最适合放进哪个类别或者哪个聚类群体中。这种方式相比第一种方式来说比较费时，尤其是当聚类出来的群体较多的时候，利用分类器的原理去分别判断时会更加耗时，不过其作为一种思路和方法倒是未尝不可。

9.3.2　数据标准化

在数据化运营的商业实战中，参与聚类的变量绝大多数都是区间型变量（Interval），不同区间型变量之间的数量单位不同，如果不加处理直接进行聚类，很容易造成聚类结果的失真。比如，长度单位有的是公里，有的是毫米；质量单位有的是吨，有的是克；一般而言，变量的单位越小，变量可能的值域就越大，对聚类结果的影响也就越大。为了避免对度量单位选择的依赖，在聚类之前所要采取的一个重要的技术措施就是进行数据标准化。

数据标准化是聚类分析中最重要的一个数据预处理步骤，这主要是因为它不仅可以为聚类计算中的各个属性赋予相同的权重，还可以有效化解不同属性因度量单位不统一所带来的潜在的数量等级的差异，这些差异如果不处理，会造成聚类结果的失真。

数据的标准化有多种不同的方式，其中，尤以标准差标准化最常用。标准差标准化，又叫 Z-Score 标准化（Zero-Mean Normalization），经过这种方法处理后的数据符合标准正态分布，即均值为 0，标准差为 1，其转化公式如下：

其中，μ 为所有样本数据的均值，σ 为所有样本数据的标准差。

9.3.3　聚类变量的少而精

在聚类分析中，参与聚类的指标变量不能太多，如果太多，一方面会显著增加运算的时间，更重要的是变量之间或多或少的相关性会严重损害聚类的效果，并且太多的变量参与其中会使随后的聚类群体的业务解释变得很复杂。鉴于此，聚类之前，如何精心挑选特定的少数变量参与聚类是聚类分析技术应用中的又一个关键点。

具体到数据化运营的聚类实践中，要如何落实聚类变量少而精的原则呢？以下一些经验可以作为参考。

❑紧紧围绕具体分析目的和业务需求挑选聚类变量。在分析展开之前，密切保持与业务需求方的沟通，借鉴业务方的业务经验和业务直觉，直接排除大量无关的指标变量，锁定与项目需求关系最密切的核心变量。任何数据挖掘项目都是有明确挖掘任务定义的，聚类分析也是如此，在聚类之前应该有明确的聚类应用目的，然后根据这个目的挑选一些相应的字段。举个简单的例子，如果在 10 000 个用户样本中，想从产品使用习惯不同的角度来细分群体，以此调整我们的客户服务，可以优先考虑把产品使用频率、产品档次、主要损耗件的类别等作为其中的聚类字段；而如果要从不同的购买习惯的角度来划分群体，以供营销策划参考，则会把付费的方式、产品档次、是否响应促销等作为优先考虑的聚类字段。这个案例主要是想说明，对于任何具体的聚类项目，都应该事先在脑海里有一些相应的基本核心字段可以与该项目相匹配，而不能不管是什么项目、什么任务、什么目的，一股脑把所有变量统统放进去，这种胡子眉毛一把抓的做派是没有任何意义的。

❑通过相关性检测，可防止相关性高的变量同时进入聚类计算。比如，在互联网行业的分析中，登录次数、在线时长、PV 浏览量等这些变量相互之间都是明显相关的，只取其中一个变量就足够了。

❑数据分析也好，数据挖掘也罢，其本身是充满想象艺术的，所谓一半是科学，一半是艺术，相信你在聚类实践中也会体会这个特点。数据分析在很多时候是需要一些衍生变量来画龙点睛的。我们常常容易从现有的数据库中提取现成的字段，而经常忘记一些衍生的新字段，如比率。很多时候，我们的分析中有太多直接提取的绝对值字段，而常会忘记增添一些有价值的相对值（比率）字段，什么时候要考虑哪些有价值的比率字段，这需要业务知识和挖掘经验来支持的。

❑主成分分析，作为一种常用的降维方法，可以在聚类之前进行数据的清理，帮助有效精简变量的数量，确保参与聚类运算变量的少而精。然而，任何事物都是具有两面性的，主成分分析在帮助聚类算法精简输入变量数目的同时，也会造成聚类结论的可解释性、可理解性上相对于原始变量而言更复杂，在直观上不容易理解。

9.4　聚类分析的扩展应用

前面内容中谈到的聚类分析都是在典型业务场景中的应用。除此以外，聚类分析还有更多的扩展应用，这些扩展应用有的能显著提升单纯聚类分析所无法实现的商业应用价值，有的可作为辅助工具提升其他建模工具的应用效果，而且效果很显著，还有的突破了常规聚类应用的场景，参与到个性化推荐的应用中了。聚类分析技术的这些扩展应用，生动体现了数据挖掘分析技术在业务实践中的生命力，也对数据分析师提出了自我专业提升的方向和思路，即与时俱进、紧贴业务需求、以不变的聚类原理，从容应对万变的业务场景和业务需求。

9.4.1　聚类的核心指标与非聚类的业务指标相辅相成

聚类分析技术在实践应用中有个比较明显的不足之处，那就是参与聚类的变量数目不能多，需要坚持少而精的原则，否则不仅运算耗时，而且聚类的效果也不好。但是，另一方面，从业务需求的实际出发，业务应用应让尽可能多的指标进入分析范围，这样得到的信息更丰富、更全面，也才更有可能发现业务线索。那如何协调两者的矛盾呢？

在实践中，已经有了比较成熟且行之有效的方法可以较好地解决上述矛盾。一方面坚持参与聚类的变量少而精的原则，另一方面把非聚类的业务指标与聚类结果一起拿来分析、提炼、挖掘，这种相辅相成的做法在聚类分析的应用实践中已经得到了普遍的认可和采用。

具体来说，先通过用户行为属性里的核心字段进行聚类分群，在得到比较满意的聚类分群结果之后，针对每个具体细分的对象群体，再分别考察用户的会员属性，包括年龄、性别、地域、收入、爱好等一系列的基础信息。如果这些属性在聚类细分后的群体里有显著的区别或特征，将会明显丰富仅仅依靠参与聚类的少数字段所能揭示的业务特征和线索。

当然，在具体的聚类分析业务实践中，是否采用这种聚类核心指标与非聚类的业务指标相辅相成的策略，要视具体的分析目的和分析背景而定，但是这种相互结合的方法在大多数的项目实践中被证明是一种简单、有效、快捷的好办法，值得信赖。

9.4.2　数据的探索和清理工具

前面的内容已经多次提到，聚类技术不仅仅是一种模型技术，可以直接应用于相应的业务需求和项目目的；同时，聚类技术也可以作为一种数据清理工具，在其他数据模型分析的前期，可使用聚类技术进行数据的探索、清理工作，作为其他建模技术有效应用的「清道夫」。聚类技术的这种基础性价值，主要表现在以下几个方面：

❑聚类技术产生的聚类类别可以作为一个新的字段加入其他的模型搭建过程中，在适当的项目场景里，这种新的类别字段很可能会有效提高建模的效率和增强效果。

❑聚类技术产生的聚类类别在合适的项目场景里，可以作为细分群体的建模依据，并且通常来说，细分建模的模型精度常常比整体建模的模型精度要高些。

❑聚类技术的应用本身就是数据探索和熟悉的过程，这个过程对于其他算法的模型搭建来说常常也是必不可少的。而且这种基于聚类技术对数据的认知比盲目的、没有体系的数据认知要来得更加有效率、有章法。

❑聚类技术针对变量的聚类是精简变量的有效方法。变量聚类用来检验变量之间的关系，目的是对数量较多的变量进行分类。归于同一组里的变量之间关系紧密，组内变量间的相关性会很高；而不同组群里的变量间相异性很大，即组间变量相互独立。变量聚类的结果可以用作减少变量的依据和方法，在利用变量聚类产生的几个类别中，每个类别里只选取有代表性的变量作为模型的输入变量，就可大大减少输入变量的数量，有利于提升建模的效率。在 SAS 里，变量聚类可以用简单的代码来实现：PROC VARCLUS DATA=table A。

❑聚类技术还可以用来检查数据的共线性问题。关于共线性问题，已经在第 8 章里进行了详细讲解。识别共线性的方法很多，聚类技术只是其中的一种。具体来说，通过变量聚类，同一组里的变量相似性明显，因此如果将同一聚类组里的变量同时放入建模过程中，就很有可能会产生共线性的问题。通过变量聚类，可以有效锁定可能发生共线性的一些变量，从而通过取舍，减少共线性的产生。

9.4.3　个性化推荐的应用

个性化推荐是电子商务时代产生的一个新的专业方向，在很多互联网公司里，个性化推荐已经作为一个单独的部门独立于数据分析部门之外了。个性化推荐目前已经产生了诸多的相关算法，其中以协同过滤算法最为普及。聚类分析的思想和原理也可以用到个性化推荐的应用场景里，我们来看以下的业务场景。

在电子商务平台上，买家与卖家如何高效、精准匹配是个性化推荐的核心任务。当买家进入平台浏览第一个页面时，个性化推荐就需要计算其可能感兴趣的卖家或者特定商品页面，或者特定店面的页面，并第一时间把与之相关的页面发送到买家面前。一般情况下，通过对买家的历史浏览行为进行统计分析，可以确定其感兴趣的特定商品大类，但在此基础上如何进一步精确锁定商品大类下面的具体小类呢？聚类技术提供了一个独特的思路和方法。通过历史数据对该商品大类的买家进行聚类分析，找出不同小类目的买家细分群体（聚类结果），然后用这个聚类模型去判别这个新的买家最可能属于哪个细分群体，再去匹配跟该细分群体最相近的卖家或者卖家的商品小类目，这就是聚类思想在个性化推荐中的应用思路。当然在具体的项目操作中，数据的清理是非常复杂的，前期的阀值确定和规则梳理也非常关键。在个性化推荐的大场景里，聚类技术只是其中的一个思路或环节，不过，聚类技术能突破传统的应用场景，尝试应用于类似个性化推荐之类的崭新的业务需求方面，正体现了包括聚类技术在内的数据分析挖掘技术与时俱进的活力和生命力。

9.5　聚类分析在实际应用中的优势和缺点

聚类分析的优势在实践应用中是很明显的，无论是从其原理上来理解，还是从其应用的普遍程度上来看。尤其是针对大数据集的时候，K-Means 算法几乎是目前最主流的算法和应用了。具体来讲，其应用优势体现在以下几个方面：

❑目前聚类技术已经比较成熟，算法也比较可靠，而且长期的商业实践应用已经证明它是一个不错的数据群体细分的工具和方法。

❑聚类技术不仅本身是一种模型技术，可以直接响应业务需求，提出细分的具体方案来指导实践；同时，聚类技术还经常作为数据分析前期的数据摸底和数据清洗的有效思想和工具。这种多样性的特点使得聚类技术的应用场景更加丰富，其价值也因此更加明显。

❑如果聚类技术应用得好，其聚类的结果比较容易用商业和业务的逻辑来理解和解释。可理解、可解释在数据化运营实践中非常重要，它决定了业务应用方是否可以理解模型的结论，在此基础上才谈得上业务方是否真心支持、全力配合、共同推进数据分析（模型）的有效地落地应用。

❑K-Means 算法具有简洁、高效的特点。K-Means 算法的时间复杂度是 O (tkn)，其中，t 是循环次数，也就是算法收敛时已经迭代的次数；k 是聚类的个数，也就是聚类的类别数量；n 是数据点的个数，也就是样本数量。由于 t 和 k 都要远远小于 n，所以 K-Means 算法的时间复杂度与数据集的大小是线性相关的。

❑K-Means 算法是一个不依赖顺序的算法。给定一个初始类分布，无论样本算法的顺序如何，聚类过程结束后的数据分区结果都会是一样的。

K-Means 算法有这么多的好处，那它的劣势又有哪些呢？

尽管在众多的聚类算法中，尤其是针对大数据集的应用场景里，K-Means 算法几乎是唯一主流的算法，但是其本身也有一些缺点和不足，主要表现在以下几个方面：

❑数据分析师需要事先指定聚类的数目 k。在实践中，要测试多个不同的 k 值才能根据效果比较来选择最合适的 k 值，这个过程有可能会比较耗时。

❑算法对数据噪声和异常值比较敏感。异常值是数据中那些与其他数据点相隔很远的数据点，其可能是数据采集时的失误，也可能是本质不同的数据。由于 K-Means 算法是采用均值作为每个聚类的聚类中心的，所以异常值会严重干扰正常的聚类中心的计算，造成聚类失真。

9.6　聚类分析结果的评价体系和评价指标

正如第 7 章里谈到的，每一个算法都有自身的优势和局限性，因此没有哪个算法是永远优于其他算法的。在聚类分析的实际应用中，针对聚类结果的评估也有很多的维度和指标。但是，从数据化运营的实践经验来看，任何模型的评估，包括聚类分析的评估既要考虑统计学意义上的指标、维度，同时更要关注其实践效果上的价值及业务背景下的价值。尤其是对于聚类项目来说，它跟分类（预测）项目的一个显著不同之处在于，后者的评判有训练集、验证集、测试集的客观参照，而对于聚类结果的评判来说，一个对象分配到 A 类与分配到 B 类，中间并没有太明确、太客观的参照依据。鉴于此，聚类结果的评判常常更加复杂和困难。下面就来介绍一下常用的聚类评估方法及其指标体系。

9.6.1　业务专家的评估

聚类分析的结果评估首先要跟相应的落地应用场景相结合。尽管目前关于聚类的评价指标和评价体系已经比较成熟，但是总体来说，业务专家的评估才是最重要的评价层面。这一方面是由数据化运营的最终目的即落地应用效果所决定的，另一方面也是由聚类技术本身（与分类、预测技术相比，一个对象到底应该分到 A 簇，还是 B 簇，中间没有明显的效果区别）的特点决定的。

业务专家虽然可能不太了解聚类原理，但是他们对于具体对象的大概所属群体特征还是有非常深刻的商业直觉和业务敏锐性的。如果对于聚类的结果，多数业务专家都不满意、不认可、看不懂，那么这个聚类的结果很可能是有问题的，是值得怀疑的。虽然对于每个业务专家来说，他们的评判非常主观，但是采用全体专家平均分的技术手段，是可以比较有效降低主观因素对于聚类效果评价的影响的。

业务专家对聚类结果进行评判时不仅仅只是对结果的合理性、理解性进行评判，更重要的是常常会结合具体应用的业务场景来进行评判。很多时候，尽管聚类的结果看上去很合理，很容易理解，很符合业务逻辑，但是如果没有落地应用价值，或者说没有落地应用的前景，那这个聚类的结果仍然是不合格的，是无法满足业务需求的。举例来说，如果业务分析需求的目的是找出产品付费用户的网络行为特征，并根据该特征有效发现、复制潜在的付费用户，而聚类的结果只是从付费用户中发现了不同群体的产品使用特征和续费特征，尽管这些发现都是正确的、符合业务逻辑的，都是满足聚类评价技术指标的，但是这种发现对于当初的分析目的而言是没有价值的，是不合格的，因为该结果并没有实现当初的分析目的 —— 发现付费用户群体的典型的网络行为特征，从而可以让业务方、运营方有方向、有目标地去锁定潜在的付费用户群体。

9.6.2　聚类技术上的评价指标

从 9.2 节中讲解了，不同的聚类算法遵循不同的聚类原理和思路，因此它们必然也会有不同的评价标准和评价指标。鉴于 K-Means 算法和凝聚层次聚类算法在数据化运营实践中占绝对的主流应用地位，其中 K-Means 算法比后者应用更广泛，因此本节主要针对这两种算法的效果进行总结，当然这些指标的思路对于其他聚类算法而言也是有积极的借鉴和参考价值的。

❑RMSSTD（Root-Mean-Square Standard Deviation）：群体中所有变量的综合标准差，RMSSTD 越小表明群体内（簇内）个体对象的相似程度越高，聚类效果越好。计算公式如下：

其中，Si 代表第 i 个变量在各群内的标准差之和，p 为变量数量。

❑R-Square：聚类后群体间差异的大小，也就是聚类结果可以在多大比例上解释原数据的方差，R-Square 越大表明群体间（簇间）的相异性越高，聚类效果就越好。计算公式如下：

其中，W 代表聚类分组后的各组内部的差异程度，B 代表聚类分组后各组之间的差异程度，T 代表聚类分组后所有数据对象总的差异程度，并且 T=W+B。

按照聚类的思想来看，一个好的聚类结果，应该是在 R-Square∈[0,1] 的范围内，并且 R-Square 越接近 1 越好，这说明了各个群类之间的差异，即 B 越大，而同组内（群内）各对象间的差异，即 W 越小，这正是聚类分析所希望达到的效果。计算公式如下：

其中，p 代表有 p 个指标（变量），n 代表有 n 个组员，代表总体平均值。

❑SPR（Semi Partial R-Square）：该指标适用于层次方法中的凝聚层次聚类算法，它表示当原来两个群体合并成新群体的时候，其所损失的群内相似性的比例。一般来说，SPR 越小，表明合并成新的群体时，损失的群内相似性比例越小，新群体内的相似性越高，聚类效果就越好。

❑Distance Between Clusters：该指标适用于层次方法中的凝聚层次聚类算法，它表示在要合并两个细分群体（簇）时，分别计算两个群体的中心，以求得两个群体的距离。一般来说，距离越小说明两个群体越适合合并成一个新群体。虽然该指标主要应用于层次方法中的凝聚层次聚类算法，但是从其算法原理来看，该指标也可应用于其他聚类算法中，包括 K-Means 算法，也就是说，在 K-Means 算法的聚类结果里，一样可以有这个指标，用于显示聚类的结果里各个群体间是否有足够的距离。这个指标越大，说明聚类分群效果越好。

上面总结的 4 个主要评价指标只是在聚类分析实践应用中最常用的指标，并不是针对聚类结果的全部评价指标，在实践应用中还有更多的指标可以供我们参考，其中最重要的是从业务背景的角度所提出来的指标，比如，特定群体的数量不能太少，聚类的结果要有很好的业务解释性等。另外，不同的数据挖掘软件或聚类软件，也会自带一些相关的指标，在实际应用中，数据分析师通常都是相互参考，再结合业务逻辑和业务专家的意见做综合评价的。

9.7　一个典型的聚类分析课题的案例分享

9.7.1　案例背景

A 公司推出了一个在线转账的产品，用户通过该产品在线转账时交易费用相比普通的网银要便宜。在经过一段时间的测试性运营之后，企业积累了一定数量的、使用该产品的付费用户数据，现在产品运营团队需要基于该批实际使用的付费用户数据，来分析找出有价值的特定群体，进而通过精细化运营提升付费用户数量。

由于该产品上线时间很短，业务方对于付费用户的特点并不十分清楚，另外前期运营阶段并没有做专门的定向推广，所以常规的分类（响应）模型并不适合当前的业务场景。在此情况下，数据分析师想到通过聚类分析技术锁定部分特征明显的目标群体，通过精细化运营促进付费用户的有效增长。

