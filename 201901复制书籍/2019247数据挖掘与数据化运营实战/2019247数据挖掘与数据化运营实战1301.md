对于逻辑回归，将代入上式，得

上述式子被称为对数似然函数，其目的就是求出该式子的最大值，其中会涉及非线性方程组的求解，运算量非常大，所幸的是这些工作现在都有现成的软件可以代替人工计算了，数据分析师只需要知道其中的原理就可以了。

需要强调的是，对于通过上述最大似然法得到的参数估值，还需要进行相应的显著性检验，对于回归系数 βi 的估计值的显著性检验通常使用的是 Wald 检验，其公式为。

其中，D () 为回归系数 βi 的估计值的标准差。如果 βi 的估计值的 Wald 检验显著，通常来讲，变量对应的 P-Value 如果小于 0.05，这时可以认为该自变量对因变量的影响是显著的，否则影响不显著。

10.3.2　回归中的变量筛选方法

无论是线性回归，还是逻辑回归，在回归拟合的过程中，都要进行变量的筛选，并且有各种不同的筛选方法，其中最常见、最著名的 3 种方法分别是向前引入法（Forward Selection）、向后剔除法（Backward Elimination）、逐步回归法（Stepwise Selection）。

❑向前引入法（Forward Selection）。即采用回归模型逐个引入自变量。刚开始，模型中没有自变量，然后引入第一个自变量进入回归方程，并进行 F 检验和 T 检验，计算残差平方和。如果通过了检验，则保留该变量。接着引入第二个自变量进入回归模型中，重新构建一个新的估计方程，并进行 F 检验和 T 检验，同时计算残差平方和。从理论上说，增加一个新的自变量之后，回归平方和应该增加，残差平方和应该减少。引进一个新自变量前后的残差平方和之差额就是新引进的该自变量的偏回归平方和，如果改值明显偏大，说明新引进的该自变量对目标变量有显著影响，反之则没有显著影响。向前引入法最大的缺点是最先引入回归方程的变量在随后不会被剔除出去，这会对后面引入的变量的评估过程和结果造成干扰。

❑向后剔除法（Backward Elimination）。向后剔除法正好与向前引入法相反，即首先把所有的自变量一次性放进回归模型中进行 F 检验和 T 检验，然后逐个删除不显著的变量，删除的原则是根据其偏回归平方和的大小来决定的。如果偏回归平方和很大则保留，否则删除之。向后剔除法最大的缺点是可能会引入一些不重要的变量，并且变量一旦被剔除之后，就没有机会重新进入回归模型中了。

❑逐步回归法（Stepwise Selection）。该方法综合了上述两种方法的特点。自变量仍然是逐个进入回归模型中，在引入变量时需要利用偏回归平方和进行检验，只有显著时才可以加入。当新的变量加入模型之后，又要重新对原来的老变量进行偏回归平方和的检验，一旦某变量变得不显著时就要立即删除该变量。如此循环往复，直到留下来的老变量均不可删除，并且新的变量也无法加入为止。

10.3.3　逻辑回归的应用优势

相比于数据挖掘建模常用的其他算法如决策树、神经网络、邻近记忆推理等，逻辑回归技术是最成熟、应用最广泛的，也是数据分析师和数据化运营业务人员最为熟悉的。在各种新的数据挖掘算法层出不穷的今天，逻辑回归技术仍然具有强大的活力和最广泛的业务应用基础。

10.3.4　逻辑回归应用中的注意事项

逻辑回归实践应用中的注意事项如下：

❑建模数据量不能太少，目标变量中每个类别所对应的样本数量要足够充分，才能支持建模。

❑要注意排除自变量中的共线性问题。关于共线性问题，可参考本书 8.7 节。

❑异常值（Outliers）会给模型带来很大干扰，应该删除。

❑逻辑回归模型本身不能处理缺失值（Missing Value），所以应用逻辑回归算法的时候，要注意针对缺失值进行适当的处理，或者赋值，或者替换，或者删除，可参考本书 8.4.1 节。

10.4　多元线性回归技术的实践应用和注意事项

之所以本章在最后才介绍线性回归模型，主要的原因在于线性回归是逻辑回归的基础，同时，线性回归也是数据挖掘中常用的处理预测问题的有效方法。线性回归与逻辑回归最大的区别，也是最直观的区别在于目标变量的类型，线性回归所针对的目标变量是区间型的（Interval），而逻辑回归所针对的目标变量是类别型的（Category）。另外，线性回归模型与逻辑回归模型的主要区别如下：

❑线性回归模型的目标变量与自变量之间的关系假设是线性关系的，而逻辑回归模型中目标变量与自变量之间的关系是非线性的。

❑在线性回归中通常会假设，对应于自变量 X 的某个值，目标变量 Y 的观察值是服从正态分布的；但是，在逻辑回归中，目标变量 Y 是服从二项分布 0 和 1 或者多项分布的。

❑在逻辑回归中，不存在线性回归里常见的残差。

❑在参数的估值上，线性回归通常采用的是最小平方法，而逻辑回归通常采用的是最大似然法。

10.4.1　线性回归的原理和核心要素

线性回归包括一元线性回归和多元线性回归，在数据分析挖掘的业务实践中，用得更多的是多元线性回归。

「多元线性回归」是描述一个区间型目标变量（Interval Variable）Y 是如何随着一组自变量 X1，X2，…，Xp 的变化而变化。把目标变量 Y 与自变量 X1，X2，…，Xp 联系起来的公式就是多元线性回归方程。

在目标变量 Y 的变化中包括两个部分：系统性变化和随机变化。系统性变化是由自变量引起的；而自变量不能解释的那部分变化就是所谓的残差，该部分可以认为是随机变化。

在多元线性回归方程中，目标变量 Y 与一组自变量之间的线性函数关系，可以用如下公式表示：

Y=β0+β1x1+β2x2+…+βpxp+ε

其中，Y 是目标变量，X1，X2，…，Xp 是自变量，β0 是常数（截距），β0,β2,…，βp, 是每个自变量的系数（权重），ε 是随机误差。

常用来估算多元线性回归方程中自变量系数的方法就是最小平方法，即找出一组参数（与 β1,β2,…，βp 相对应），使得目标变量 Y 的实际观察值与回归方程的预测值之间总的方差最小。

对于多元线性回归方程的检验，一般从模型的解释程度、回归方程的总体显著性和回归系数的显著性等方面进行检验。

❑模型的解释程度，又称回归方程的拟合度检验。R 的平方（R-Square），也叫做 R2 或 Coefficient of Multiple Determination 表示拟合度的优劣，其取值范围为 [0,1]。关于 R2 的详细介绍，请参考本书 8.6.4 节。需要强调的是，R2 的数值与自变量的个数有关，自变量的个数越多，R2 越大，这在一定程度上削弱了 R2 的评价能力，因此在实践中通常要考虑剔除自变量数目影响后的 R2，即修正的 R2（Adjustable R2）。

❑回归方程的总体显著性检验。主要是检验目标变量与自变量之间的线性关系是否显著，也就是自变量的系数是否不全为 0，其原假设为：H0:β1=β2=…=βp=0；而其备选假设为：H1:βp 不全为 0。该检验利用 F 检验完成。

❑回归方程系数的显著性检验。回归方程系数的显著性检验要求对所有的回归系数分别进行检验。如果某个系数对应的 P 值小于理论显著性水平 α 值，则可认为在显著性水平 α 条件下，该回归系数是显著的。

10.4.2　线性回归的应用优势

线性回归模型作为应用最为广泛的算法，其主要的优势如下：

❑通俗易懂。多元线性回归模型非常容易被解读，其自变量的系数直接跟权重挂钩，因此很容易解释每个自变量对于目标变量的预测价值大小（贡献大小），解读出的这些信息可以为数据化运营提供有效的思考方向。

❑速度快，效率高。相比于其他的建模算法而言，多元线性回归的计算速度是最快的。

❑可以作为查找异常值的有效工具。那些与多元线性回归方程的预测值相差太大的观察值通常值得进一步考察，确定其是否是异常值。

10.4.3　线性回归应用中的注意事项

线性回归应用中的注意事项如下：

❑算法对于噪声和异常值比较敏感。因此，在实践应用中，回归之前应该努力消除噪声和异常值，确保模型的稳定和准确度。

❑该算法只适合处理线性关系，如果自变量与目标变量之间有比较强烈的非线性关系，直接利用多元线性回归是不合适的。不过，在这种情况下，可以尝试对自变量进行一定的转换，比如取对数、开平方、取平方根等，尝试用多种不同的运算进行转换。

❑多元线性回归的应用还有一些前提假设：自变量是确定的变量，而不是随机变量，并且自变量之间是没有线性相关性的；随机误差项具有均值为 0 和等方差性；随机误差呈正态分布等。

10.5　模型的过拟合及对策

模型的过拟合（Over Fitting）是指模型在训练集里的表现让人非常满意，但是一旦应用到真实业务实践中，效果会大打折扣。换成学术化语言描述，就是模型对样本数据拟合得非常好，但是对于样本数据外的应用数据，拟合效果非常差。在数据分析挖掘业务实践中，即为模型搭建时的表现看上去非常好，但是应用到具体业务实践时，模型的效果显著下降，包括准确率、精度、效果等都显著下降了。

过拟合现象是数据挖掘中常见的一种挫折，尤其是在预测响应（分类）模型的应用场景里。在模型的实践应用中如果发生了模型的过拟合，不仅会大幅度降低模型的效果和效率，也会严重浪费运营业务资源，同时，还会严重打击数据分析师的自信心和影响力。所以，数据分析师应该比较清楚地了解过拟合产生的主要原因以及可以采用的相应措施，尽量去避免过拟合的发生。

总的来说，过拟合产生的主要原因如下：

❑建模样本抽取错误。包括但不限于样本数量太少，抽样方法错误，抽样时没有足够正确地考虑业务场景或业务特点等，以致抽出的样本数据不能足够有效地代表业务逻辑或业务场景。

❑样本里的噪声数据干扰过大。样本噪声大到模型过分记住了噪声特征，反而忽略了真实的输入输出间的关系。

❑在决策树模型的搭建过程中，如果对于决策树的生长没有合理的限制和修剪，由着决策树自由的生长，那有可能会使每片叶子里只包含单纯的事件数据（Event）或非事件数据（No Event）。可以想象，这种决策树当然是可以完美匹配（拟合）训练数据的，但是一旦应用到新的业务真实数据中，效果就会一塌糊涂。

❑建模时的逻辑假设到了应用模型时已经不能成立了。任何预测模型都是在假设的基础上才可以搭建和应用的，常用的假设包括：假设历史数据可以推测未来，假设业务环节没有发生显著变化，假设建模数据与后来的应用数据是相似的等。如果上述假设违反了业务场景，那么根据这些假设搭建的模型当然是无法有效应用的。

❑建模时使用了太多的输入变量。这同第二点噪声数据有些类似，数据挖掘新人常常犯这个错误，自己不做分析判断，把所有的变量交给软件或者机器去「撞大运」。须知，一个稳定优良的模型一定要遵循建模输入变量少而精的原则。

上面的原因都是现象，其实本质只有一个，那就是对业务理解错误造成的，无论是抽样，还是噪声，还是决策树、神经网络等，如果我们对于业务背景和业务知识了解得非常透彻，一定是可以避免绝大多数过拟合现象产生的。因为在模型从确定需求、思路讨论、搭建到业务应用验证的各个环节中，都是可以通过业务敏感来防止过拟合产生的。

入世，出世，都是一样的道，所谓的道从来不曾离开我们半步，只是看我们自身是否足够清净，足够醒悟，足够真实而已。佛法有八万四千法门，不过是不同的方便路径，归根结底，佛法的根本只是认识我们与生俱来的本来面目，真如自性。

过拟合的产生，有种种原因，不一而足，对其进行分类和剖析只是为了方便而已，防止过拟合的终极思路就是真正透彻理解业务背景和业务逻辑，有了这个根本，我们一定可以正确抽样，发现并排除噪声数据，一定可以在决策树、神经网络等算法中有效防止过拟合的产生。

当然，除了透彻了解业务本质外，还有一些技术层面的方法来防止过拟合的产生，虽然是「术」层面上的内容，但是很多人热衷于这些技巧，所以，在这里也顺便讲解如下：

❑最基本的技术手段，就是合理、有效地抽样；包括分层抽样、过抽样等，从而用不同的样本去检验模型。

❑事前准备几个不同时间窗口、不同范围的测试数据集和验证数据集，然后在不同的数据集里分别对模型进行交叉检验，这是目前业界防止过拟合的最常用的手段。

❑建模时目标观测值的数量太少，如何分割训练集和验证集的比例，需要建模人员灵活掌握。

❑如果数据太少，谨慎使用神经网络模型，只有拥有足够多的数据，神经网络模型才可以有效防止过拟合的产生。并且，使用神经网络时，一定要事先有效筛选输入变量，千万不能一股脑把所有的变量都放进去。

10.6　一个典型的预测响应模型的案例分享

10.6.1　案例背景

某垂直细分的 B2B 网站平台，其商业模式是通过买卖双方在平台上产生交易而对卖家抽取交易提成费。对于该网站平台来说，促成买卖双方的线上成交是该平台的价值所在，网站平台的发展和盈利最终取决于是否能有效且规模化地促成买卖双方的线上成交并持续成交。

要有效且规模化地促成买卖双方在线成交，该网站平台有许多事情要做，包括吸引优质卖家、吸引广大有采购意愿的优质买家、帮助卖家在平台上更好地展示商品、帮助买家更快更有效地匹配所需要的卖家、优化网站交易流程以方便交易更有效、提供风险控制措施，保障双方交易的安全等。这里提到的每一个目的其实都是包含着一揽子的分析课题和项目开发的，需要数据分析团队在内的所有相关部门协同合作来实现。

本案例所要分享的就是其中一个细分的项目：初次成交的预测模型和运营应用。对于该平台上的卖家来说，从最开始的注册、发布商品信息，到后期的持续在线获得订单和在线成交，其中有一个结点对于卖家来说是至关重要，具有突破性的，那就是第一次在线成交，也叫初次成交转化，这个初次成交对于卖家的成功体验和激励的价值是不言而喻的；另外，从网站平台的运营方来说，卖家的初次成交也是网站运营工作的一个重要考察环节和考察指标，只有初次成交的卖家数量越多，周期越短，才可以有效保障后期持续性、规模化在线成交的可能性。本着上述背景和考虑思路，网站平台运营方希望通过数据分析找出短期内最有可能实现初次成交的卖家群体，分析其典型特征，运营方可以据此对卖家群体进行分层的精细化运营。最终的目的是一方面希望可以通过数据化运营有效提升单位时间段内初次成交的卖家数量，另一方面为今后的卖家培养找出一些运营可以着力的「抓手」，以帮助卖家有效成长。

10.6.2　基本的数据摸底

为了慎重起见，数据分析团队与运营方协商，先针对网站平台的某一个细分产品类目的卖家进行初次成交的专题分析。视分析和建模的应用效果，再决定后期是否推广到全站的卖家。

因此，本次专题分析只针对代号为 120023 的细分产品类目卖家，根据网站平台的运营规律和节奏，初步的分析思路是通过对第 N-1 月份的卖家行为数据和属性数据的分析，寻找它们与卖家第 N 个月有实际的在线初次成交之间的关系。

在进行数据摸底后发现，截止当时项目进行时，代号为 120023 的细分产品类目卖家共有 170 000 家，交易次数为 0，即是还没有发生初次成交的卖家，经过连续几个月的数据观察，发现每个月实现初次成交的卖家基本上稳定在 2000 家左右。如果基于总共 170 000 家来计算每个月初次成交的转换率，大约在 1.12%。

根据数据分析师的项目经验以及运营方的业务判断，总数 170 000 的大池子里应该是可以通过数据分析找出一些简单的阀值过滤掉一批最不可能近期实现初次成交的卖家群体的。通过业务经验和连续几个月对重点字段的数据摸底，得到了如下结果：

❑月度登录「即时通信工具」达 10 天次以上的潜在卖家，平均每月大概为 50 000 人，其中在次月实现初次成交的用户有 1900 人左右（对比原始数据每月大概 170 000 的潜在卖家，次月实现（初次成交）的用户有 2000 人左右；浓缩过滤后只保留 50 000 人（过滤了大约 71% 的近期可能性很小的大部分卖家），但是次月实现初次成交的用户只过滤掉 5%；换句话说，通过设置阀值月度登录即时通信工具达到 10 天次以上，初次成交的转换率就从原始的 1.12% 提升到 3.5% 左右。并且这个阀值的设立只是丢失了 5% 的初次成交卖家。找到这个阀值的意义在于，基于 3.5% 的转换率搭建的模型相比在原始转换率 1.12% 基础上搭建的模型来说要更加准确，更容易发现自变量与因变量之间的关系。

❑来自两个特定省份 A 省和 B 省的卖家，其初次成交的转换率约为 3.3%，所覆盖的初次成交卖家数为 70% 左右，即是丢失了将近 30% 的初次成交卖家。

❑可交易 Offer 占比大于等于 0.5 的卖家，其初次成交的转换率约为 3.7%，所覆盖的初次成交卖家数为 85% 左右。

基于上述的一些数据摸底和重要发现，数据分析师与业务方沟通后，决定设置阀值为月度登录即时通信工具达到 10 天次以上，在此基础上尝试数据分析挖掘建模和后期应用。

在数据摸底环节中，还有一个重要的基础性工作，那就是与业务方一起列出潜在的分析字段和分析指标，如图 10-3 所示 [1]。这个工作是后期分析挖掘的基础，可圈定大致的分析指标和分析字段的范围，并据此进行数据的抽取工作。之所以强调要与业务方一起列出潜在的分析字段和分析指标，是因为在项目的前期阶段，业务方的业务经验和灵感非常重要，可以协助数据分析师考虑得更加全面和周详。

图　10-3　初步分析字段一览

在上述原始字段的基础上，数据分析师通过走访业务方，以及经过资深业务专家的检验，增添了一些重要的衍生变量如下：

❑类目专注度。公式是卖家该类目下总的有效商品 Offer 数量除以该卖家在网站中总的有效商品 Offer。因为有足够的理由相信，类目专注度越高，越容易产生成交。

❑优质商品 Offer 占比。公式是卖家的优质 Offer 数量除以该卖家总的有效商品的 Offer 数量。因为有足够的理由相信，优质的商品 Offer 越多，越容易产生成交。

❑可在线交易 Offer 的占比。公式是卖家的可在线交易 Offer 数量除以该卖家总的有效商品的 Offer 数量。

[1] 限于业务方的商业隐私，这些字段和指标的中文含义就不详述了。

10.6.3　建模数据的抽取和清洗

在完成了前期摸底和变量罗列之后，接下来的工作就是抽取建模数据和熟悉、清洗数据环节了。这个环节的工作量是最大的，它和随后的数据转换环节，所需要消耗的时间占整个数据分析建模项目时间的 70%，甚至更多。

抽取、熟悉、清洗数据的目的主要包括：熟悉数据的分布特征和数据的基本统计指标、发现数据中的缺失值（及规模）、发现数据中的异常值（及规模）、发现数据中明显与业务逻辑相矛盾的错误。这样最终就可以得到比较干净的数据，从而提高随后分析的准确性和后期模型搭建的效果了。

在本项目的数据清洗过程中，发现了以下的数据错误：

❑Company_Reg_Capital 这个字段有少数的样本夹杂了中文，与绝大多数观察值中的数字格式不一致，容易引起机器的误判，需要直接把这些少数样本删除。

❑Credit_Status 这个字段有将近 40% 是空缺的，经过业务讨论，决定直接删除该字段。

❑Bu_Name 这个字段是中文输入，属于类别型变量，为了后期数据分析需要，将其转化为数字格式的类别型变量。

❑Credit_Balance_Amt 有将近 20% 的观察值是 N，而其余观察值是区间型数字变量，经过走访数据仓库相关人员，确认这些为 N 的观察值实际上应该是 0。为了后期数据分析需要，将该字段所有为 N 的观察值替换成 0。

同时，对原始变量进行基本的统计观察，图 10-4 是各字段的基本统计指标一览表。

图　10-4　各字段的基本统计指标一览表

10.6.4　初步的相关性检验和共线性排查

在该阶段进行初步的相关性检验，主要有 3 个目的：一是进行潜在自变量之间的相关性检验后，高度相关的自变量就可以择一进入模型，而不需要都放进去。二是通过相关性检验，排除共线性高的相关字段，为后期的模型搭建做好前期的基础清查工作。三是，如果潜在自变量与目标变量之间的高度线性相关，则可以作为筛选自变量的方法之一进行初步筛选。

图 10-5 是相关性检验的部分截屏，从中可以发现，tradable_grade45_offer_bu 与 valid_sale_offer_cnt 线性相关系数为 0.668 53，且 P 值小于 0.000 1，这说明这两变量之间有比较强的线性相关性，在后续的建模中至多只能二选一，也就是说只能挑选出来一个作为潜在的自变量，然后根据其他筛选自变量的方法综合考虑是否最终进入模型中。

图　10-5　相关性检验的截屏图

10.6.5　潜在自变量的分布转换

本环节主要是针对前面的基础统计结论，包括偏度 Skewness 和峰度 Kurtosis 进行分箱转换、以正态分布为目的的转换，以及其他形式的转换。

比如，在前面的基础统计结论里，我们发现：

Valid_Sale_Offer_Cnt 偏度（Skewness）为 17.008，峰度 Kurtosis 为 438.62，这样的分布非常不均衡，不利于后期模型的拟合，因此需要对这些分布不均匀的变量进行转换，（如图 10-6 和图 10-7）。

图　10-6　变量 Valid_Sale_Offer_Cnt 的原始分布图

图　10-7　变量 Valid_Sale_Offer_Cnt 取对数后的分布图

10.6.6　自变量的筛选

自变量的筛选有很多方法，比如本书第 8.6 节就具体分享了各种不同筛选输入变量的方法。在数据挖掘商业实战中，通常的做法是分别采用多种方法，这样可以防止单一筛选方法有可能遗漏一些重要的变量。

在本项目里，数据分析师采用了多种筛选方法逐一尝试、对比，最终得到了以下一些重要变量，并将其作为自变量收入模型当中，如表 10-1 所示。

10.6.7　响应模型的搭建与优化

在本项目的模型搭建过程中，数据分析师分别尝试了 3 种不同的模型工具，即决策树、逻辑回归及神经网络，在每一种工具里又分别尝试了不同的算法或参数调整，经过反复的比较和权衡，得到了比较满意的模型结论。具体内容参考 10.6.8 节的结论分析。

关于模型优化的详细方法论，可参考本书第 7 章。

10.6.8　冠军模型的确定和主要的分析结论

经过比较和权衡，最终的冠军模型，即投入落地应用的模型是逻辑回归模型，相应的模型响应率曲线图，如图 10-8 所示，模型捕获率曲线图，如图 10-9 所示，模型 lift 曲线图，如图 10-10 所示。关于如何解读模型捕获率曲线、响应率曲线和 Lift 曲线，可参考本书 7.4.4 节的详细介绍。

图　10-8　模型响应率曲线图

图　10-9　模型捕获率曲线图

图　10-10　模型 Lift 曲线图

之所以最终选择逻辑回归模型作为冠军模型，主要是基于两方面的理由：一方面是逻辑回归模型的效果，即提升率、捕获率及转化率与最高的神经网络模型相差无几，另一方面是逻辑回归的可解释性远远高于神经网络模型，这一点对于落地应用中的业务方来说尤为重要。

模型的最终确定，还需要经过最新的真实数据验证，数据分析师用选好的冠军模型来对最新月度的真实数据进行模拟打分验证，结果表明冠军模型非常稳定，表现非常出色，具体验证结果如图 10-11 所示。

图　10-11　模型应用到新数据后的捕获率曲线效果图

10.6.9　基于模型和分析结论基础上的运营方案

基于模型的效果和主要自变量的业务含义，本项目落地应用方案包括两部分，即卖家基于概率分数的分层，以及在分层基础上的相应运营措施和重点，具体内容讲解如下。

根据模型打分后的次月初次成交概率的分数高低，对潜在成交卖家进行分层精细化运营。比如，模型打分最高的 10% 的卖家，是最有可能在次月实现初次成交突破的，运营方对该类群体的运营方针应该是临门一脚式的一击即中，也就是与流量资源团队合作，给这批优质客户群体提供更大的流量，有效提升初次成交转化率。

对于模型打分的概率分数为 10%～30% 之间的群体，这类群体没有前 10% 的卖家在次月实现初次成交的可能性那么高，但也是仅次于前 10% 卖家的，根据模型中有价值的输入变量的业务含义，运营方应该作出相应的运营策略，即对于基础操作不够的，要通过运营提升相关的基础操作完成率；对于活跃度不够的，要通过相关的运营帮助卖家提升其活跃度等。

对于模型打分概率在 30% 之后的群体，尤其是 40% 之后的群体，由于其在近期实现初次成交的可能性很低，考虑到运营方的运营资源有限，无法面面俱到，所以针对这一群体，不能急功近利，需要有长期培育的心理准备。运营方可以通过线上广而告之的讲座、社区活动等，让这类卖家逐步完善自己的基础建设和深化其参与度，最终完成从量变到质变的转化。

10.6.10　模型落地应用效果跟踪反馈

初期模型，针对代号为 120023 的细分产品类目的卖家运营的测试效果不错，在此基础上又对模型进行了调整，因涉及企业商业隐私，具体技术手段在此略去，然后延伸到全站全行业应用，经过数据分析团队、业务运营团队等相关部门的通力合作，模型落地应用效果反馈不错。

针对 Top30% 的优质卖家进行重点运营后，在随后两周对运营效果进行验证，发现各个行业运营后的效果提升（初次成交突破的卖家数量）显著，效果对比如图 10-12 所示。相比未做专门运营的自然增长效果，本次重点运营的活动效果总体平均提升了 99%。

图　10-12　基于模型的精细化运营后的效果对比图

第 11 章　用户特征分析的典型应用和技术小窍门

君子知微知彰，知柔知刚。

——《周易·系辞》（下）

11.1　用户特征分析所适用的典型业务场景

11.2　用户特征分析的典型分析思路和分析技术

