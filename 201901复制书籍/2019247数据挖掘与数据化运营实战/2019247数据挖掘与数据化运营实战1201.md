9.7.2　基本的数据摸底

数据分析师与运营方协商，针对前期测试性运营时所产生的那部分实际付费的用户来整理特征，根据业务逻辑推测和业务经验判断，大致整理出了 15 个可能的特征字段。

在进行聚类分析之前，先对部分异常值进行了删除处理。关于异常值的详细介绍，可参考 8.4.2 节的内容。

由于在聚类分析中参与聚类的变量不能太多，同时考虑到聚类样本数量有限，因此本项目实际聚类的变量数量为 4 个。更多的其他变量指标可以在聚类完成后进行群体描述时添加进来，再进行群体特征分析。在聚类之前，针对所有数值型变量进行相关性检验，对于高度线性相关的变量只保留一个进入聚类过程。

考虑到企业的商业隐私，下面展示的分析过程和聚类结果是基于抽样的部分样本得到的，聚类中的群体数量不代表企业的真实用户规模，特此说明。

9.7.3　基于用户样本的聚类分析的初步结论

聚类分析过程是个不断探索、反复尝试的过程，经过多次比较，最终决定基于 login freq、tp index、pv3 个指标来进行聚类，在聚类完成后根据有价值的群体再增加其他有意义的字段，来进行特征描述，并得到如表 9-1 所示的聚类结论。

上述样本共聚类成 8 个群体，整体的 Over-All R-Square 为 0.755 34，说明群体间的差异性比较明显。考虑到部分群体内样本数量太少，在实际应用中可以忽略不计，所以上述聚类结论中比较有代表性的群体为：

❑第 7 组，聚类样本中该组共有 192 个用户，占样本总量的 30%。该组 RMSSTD（Root-Mean-Square Standard Deviation）为 0.436 632，该组 login freq 均值为 148.92，tp index 均值为 11.34，pv 均值为 8133.23。

❑第 3 组，聚类样本中该组共有 51 个用户，占样本总量的 8%。该组 RMSSTD（Root-Mean-Square Standard Deviation）为 0.495 173，该组 login freq 均值为 138.17，tp index 均值为 68.98，pv 均值为 8028.51。该组与第 7 组中的各指标非常类似（除了 tp index 差别较大之外）。另外，这两组都是可以作为优质用户群体合并的，这是后来业务方的理解。

❑第 5 组，聚类样本中该组共有 304 个用户，占样本总量的 47%。该组 RMSSTD（Root-Mean-Square Standard Deviation）为 0.258068，该组 Login freq 均值为 21.32，tp index 均值为 2.56，pv 均值为 1808.43。

上述 3 组共占样本总量的 85%，具有相当的代表性。

在上述基本聚类的基础上，又增加了业务方认为值得考虑的一些其他变量到这 3 个群组中，来进行特征描述，最后总共得到 5 类典型付费用户群体。经过与业务方的讨论，决定挑选其中两个群体的特征指标进行目标用户圈定，并进行具体的精细化运营。这两个群体的典型特征如下：

❑群体 A。占样本数量的 15%，主要特征为：全部是企业俱乐部用户，全部有在线交易历史，俱乐部年限小于 4 年，登录次数大于 110 次，pv 量大于 10 000，进一步过滤发现，绝大部分有 P4P 消耗记录。

❑群体 B。占样本数量的 10%，主要特征为：全部是个人俱乐部用户，全部有在线交易历史，俱乐部年限小于 2 年，登录次数大于 100 次，并且俱乐部指数小于 80。

运营方根据上述两个群体特征，从参考数据中提取出了满足上述特征阀值的 20000 潜在目标受众，并进行了为期 1 周的定向在线运营活动，通过 3 轮营销推广活动（不同的渠道，不同的文案），产生了以下运营成果。

在为期 1 周的定向运营活动里，这 20 000 名目标受众中共 937 名用户被成功转化为付费用户，付费转化率为 4.69%，与前期无特定目标的即时通讯工具群体运营的付费转化率不足 1% 进行对比，可以看到这次运营活动的效率提升是非常明显的。

为了让读者能更清楚、直观地认识到本次在线精细化运营的价值，在这里还分享一下传统领域的营销推广转换率数据，以便于大家参考和比较。2006 年，世界知名的计算机硬件和软件系统服务提供商 Oracle 公司在中国市场做过一次比较成功的定向运营推广活动。先向 200 000 企业高层相关人士通过 EDM（电子邮件营销）、Banner（官方网站的横幅广告）、直邮、电话访问以及夹报广告等各种传播方式传达 Oracle 的产品理念，经过两个月运营，有 20 000 目标受众对此表示出了兴趣，其阅读了宣传资料或者访问了产品宣传网站，进一步跟进，有 2000 人填写了反馈问卷，并下载了相关资料，最终，Oracle 得到了 900 个销售机会。Oracle 公司本次为期 3 个月的定向运营，得到的销售计划转化率为 0.45%。

第 10 章　预测响应（分类）模型的典型应用和技术小窍门

卖弄杀周易阴阳谁似你，还有个未卜先知意。

——《桃花女》

10.1　神经网络技术的实践应用和注意事项

10.2　决策树技术的实践应用和注意事项

10.3　逻辑回归技术的实践应用和注意事项

10.4　多元线性回归技术的实践应用和注意事项

10.5　模型的过拟合及对策

10.6　一个典型的预测响应模型的案例分享

预测响应（分类）模型是数据挖掘实战中最常见的应用模型，它最直接地涉及了精细化运营中的客户分层以及随后的个性化区别对待，从某种意义上来说，基于预测响应（分类）模型的客户分层运营已经成为精细化运营的代名词。

本章围绕预测响应（分类）模型的典型应用，对神经网络、决策树、逻辑回归、多元线性回归等最常见的 4 种算法在数据挖掘实战应用中的优缺点和技术重点进行了分析、归纳，从而帮助读者在今后的项目实践中有的放矢，扬长避短。

不同的模型算法，需要不同的数据准备；不同的算法，输出不同的产出物；不同的算法，在实践应用中有各自独特的优势和不足之处。数据分析人员只有对这些有了足够的了解和掌握，才可以实现有效的数据挖掘实践应用。

10.1　神经网络技术的实践应用和注意事项

对神经网络的研究始于 20 世纪 40 年代，作为一门交叉学科，它是人类基于对其大脑神经认识理解的基础上，人工构造实现某种功能的网络模型。经过将近 70 年的发展，神经网络技术已经成为机器学习的典型代表，它不依照任何概率分布，而是模仿人脑功能进行抽象运算。

简单来讲，神经网络是一组互相连接的输入 / 输出单元，其中每个连接都会与一个权重相关联。在学习阶段，通过调整这些连接的权重，就能够预测输入观察值的正确类标号。因此可以理解为人工神经网络是由大量神经元通过丰富完善的连接、抽象、简化和模拟而形成的一种信息处理系统。

10.1.1　神经网络的原理和核心要素

人工神经网络的结构大致分为两大类：前向型网络和反馈型网络。

具体来说，所谓前向型网络，是指传播方向是从输入端传向输出端，并且没有任何的反馈；所谓反馈型网络是指在传播方向上除了从输入端传向输出端之外，还有回环或反馈存在。两种类型的网络原理图如图 10-1 所示。

图　10-1　人工神经网络的典型结构图

在上述的典型结构里，神经网络通过输入多个非线性模型，以及不同模型之间的加权互联，最终得到一个输出模型。具体来说，多元输入层是指一些自变量，这些自变量通过加权结合到中间的层次上，称为隐蔽层。隐蔽层中主要包含的是非线性函数，也叫转换函数或者挤压函数。隐蔽层就是所谓的黑箱（Black Box）部分，几乎没有人能在所有的情况下读懂隐蔽层中那些非线性函数是如何对自变量进行组合的，这是计算机思考代替人类思考的一个典型案例。

利用神经网络技术建模的过程中，有以下 5 个因素对模型的结果有重大影响 [1]：

❑层数。对于一定的输入层和输出层，需要有多少个隐蔽层，这点无论是在理论上，还是在实践中都非常有意义。虽然没有不变的规律，但是有经验的数据分析师通常要尝试不同的设置，力求找到满意的模型结构。

❑每层中输入变量的数量。太多的自变量很可能会造成模型的过度拟合，使得模型搭建时看上去很稳定，可是一旦用到新数据中，模型的预测与实际结果却相差很大，这时模型就失去了预测的价值和意义。所以，在使用神经网络建模之前，输入变量的挑选、精简非常重要。

❑联系的种类。神经网络模型中，输入变量可以有不同方向的结合，可以向前，可以向后，还可以平行。采用不同的结合方式，可能就会对模型的结果产生不同的影响。

❑联系的程度。在每一层中，其元素可以与他层中的元素完全联系，也可以部分联系。部分联系可以减少模型过度拟合的风险，但是也可能减弱模型的预测能力。

❑转换函数。转换函数也称为挤压函数，因为它能把从正无穷大到负无穷大的所有输入变量挤压为很小范围内的一个输出结果。这种非线性的函数关系有助于模型的稳定和可靠性。选择转换函数的标准很简单，即在最短时间内提供最好的结果函数。常见的转换函数包括阀值逻辑函数、双曲正切函数、S 曲线函数等。

大部分神经网络模型的学习过程，都是通过不断地改变权重来使误差达到总误差的最小绝对值的。比如，以常见的前向型网络模型为例，其设计原理如下：

❑隐蔽层的层数。从理论上讲，两层就足够了；在实践中，经常是一层隐蔽层就足够了。

❑每层内的输入变量。输出层的变量由具体分析背景来决定；隐蔽层的数量为输入数与输出数的乘积开平方；输入层的数量应该尽量精简，遵循少而精的原则，这在后面要详细阐述。

❑联系的程度。一般都选择所有层次间全部联系。

❑转换函数。选用逻辑斯蒂函数为主要转换函数，因为逻辑斯蒂函数可以提供在最短时间内的最佳拟合。

❑模型开发样本要足够充分，避免过拟合现象发生。

[1] 罗茂初。数据库营销 [M]. 北京：经济管理出版社，2007:239.

10.1.2　神经网络的应用优势

在数据挖掘实践应用中，人工神经网络的应用主要有以下优点：

❑有良好的自组织学习功能。神经网络可以根据外界数据的变化来不断修正自身的行为，对未经训练的数据模式的分类能力也比较强。

❑有比较优秀的在数据中挑选非线性关系的能力，能有效发现非线性的内在规律。在纷繁复杂的业务实践中，数据间非线性关系出现的机会远比线性关系多得多，神经网络的这种有效发现非线性关系的能力，大大提高了其在数据化运营等各种商业实践中的应用价值和贡献潜力。

❑由于神经网络具有复杂的结构，因此在很多实践场合中其应用效果都明显优于其他的建模算法；它对异常值不敏感，这是个很不错的「宽容」个性。

❑对噪声数据有比较高的承受能力。

10.1.3　神经网络技术的缺点和注意事项

虽然神经网络有上述这多优点，但是人无完人，金无足赤，它同样也有以下一些典型的不足之处需要引起数据分析师的注意：

❑神经网络需要比较长的模型训练时间，在面对大数据量时尤其如此。

❑对于神经网络模型来说少而精的变量才可以充分发挥神经网络的模型效率。但是，神经网络本身是无法挑选变量的。因此，对于神经网络的实际应用来讲，之前的变量挑选环节就必不可少了。虽然变量的选择对于任何一个模型的搭建来说都是很重要的环节，但是必须强调的是，对于神经网络模型来说尤为重要，这是由其复杂的内部结构决定的。

❑如果搭建模型后直接将其投入应用，可能会得不到想要的效果。为了确保模型投入应用后具有稳定的效果，最好先尝试几种不同的神经网络模型，经过多次验证后，再挑选最稳定的模型投入应用。

❑神经网络本身对于缺失值（Missing Value）比较敏感。所以，应用该技术时要注意针对缺失值进行适当的处理，或者赋值，或者替换，或者删除，参见本书 8.4.1 节。

❑它具有过度拟合（Over-Fitting）数据的倾向，可能导致模型应用于新数据时效率显著下降。鉴于此，针对神经网络模型的应用要仔细验证，在确保稳定的前提下才可以投入业务落地应用。

❑由于其结构的复杂性和结论的难以解释性，神经网络在商业实践中远远没有回归和决策树应用得广泛，人们对它的理解、接纳还有待提高。它也缺乏类似回归那样的丰富多样的模型诊断指标和措施。正因为如此，很多数据分析师视之为「黑盒子」，只是在实在无计可施的时候才「放手一搏」。

10.2　决策树技术的实践应用和注意事项

决策树模型是数据挖掘应用中常见的一种成熟技术，因其输出规则让人容易理解而备受数据分析师和业务应用方的喜欢和推崇。自从 1960 年 Hunt 等人提出概念学习系统框架方法（Concept Learning System Framework,CLSF）以来，决策树多种算法一直在不断发展、成熟，目前最常用的 3 种决策树算法分别是 CHAID、CART 和 ID3，包括后来的 C4.5，乃至 C5.0。

决策树，顾名思义，其建模过程类似一棵树的成长，从根部开始，到树干，到分叉，到继续细枝末节的分叉，最终到一片片的树叶。在决策树里，所分析的数据样本形成一个树根，经过层层分枝，最终形成若干个结点，每个结点代表一个结论。从决策树的根部到叶结点的一条路径就形成了对相应对象的类别预测。

10.2.1　决策树的原理和核心要素

构造决策树采用的是自顶向下的贪婪算法，它会在每个结点选择分类效果最好的属性对样本进行分类，然后继续这个过程，直到这棵树能准确地分类训练样本，或者所有的属性都已被用过。

决策树算法的核心是在对每个结点进行测试后，选择最佳的属性，并且对决策树进行剪枝处理。

最常见的结点属性选择方法（标准）有信息增益、信息增益率、Gini 指数、卡方检验（Chi-Square Statistics）等。在 10.2.2～10.2.4 节将对它们分别进行介绍。

决策树的剪枝处理包括两种方式：先剪枝（Prepruning）和后剪枝（Postpruning）。

所谓先剪枝，就是决策树生长之前，就人为定好树的层数，以及每个结点所允许的最少的样本数量等，而且在给定的结点不再分裂。

所谓后剪枝，是让树先充分生长，然后剪去子树，删除结点的分枝并用树叶替换。后剪枝的方法更常用。CART 算法就包含了后剪枝方法，它使用的是代价复杂度剪枝算法，即将树的代价复杂度看做是树中树叶结点的个数和树的错误率的函数。C4.5 使用的是悲观剪枝方法，类似于代价复杂度剪枝算法。

10.2.2　CHAID 算法

CHAID（Chi-Square Automatic Interaction Detector）算法历史较长，中文简称为卡方自动相互关系检测。CHAID 是依据局部最优原则，利用卡方检验来选择对因变量最有影响的自变量的，CHAID 应用的前提是因变量为类别型变量（Category）。

关于卡方检验的具体公式和原理，此处从略，详情可参考本书 8.6.5 节。

关于 CHAID 算法的逻辑，简述如下。

首先，对所有自变量进行逐一检测，利用卡方检验确定每个自变量和因变量之间的关系。具体来说，就是在检验时，每次从自变量里抽取两个既定值，与因变量进行卡方检验。如果卡方检验显示两者关系不显著，则证明上述两个既定值可以合并。如此，合并过程将会不断减少自变量的取值数量，直到该自变量的所有取值都呈现显著性为止。在对每个自变量进行类似处理后，通过比较找出最显著的自变量，并按自变量最终取值对样本进行分割，形成若干个新的生长结点。

然后，CHAID 在每个新结点上，重复上述步骤，对每个新结点重新进行最佳自变量挑选。整个过程不断重复，直到每个结点无法再找到一个与因变量有统计显著性的自变量对其进行分割为止，或者之前限度的条件得到满足，树的生长就此终止。

卡方检验适用于类别型变量的检验，如果自变量是区间型的变量（Interval），CHAID 改用 F 检验。

10.2.3　CART 算法

CART（Classification and Regression Trees）算法发明于 20 世纪 80 年代中期，中文简称分类与回归树。CART 的分割逻辑与 CHAID 相同，每一层的划分都是基于对所有自变量的检验和选择。但是，CART 采用的检验标准不是卡方检验，而是 Gini（基尼系数）等不纯度指标。两者最大的不同在于 CHAID 采用的是局部最优原则，即结点之间互不相干，一个结点确定了之后，下面的生长过程完全在结点内进行。而 CART 则着眼于总体优化，即先让树尽可能地生长，然后再回过头来对树进行修剪（Prune），这一点非常类似统计分析中回归算法里的反向选择（Backward Selection）。CART 所生产的决策树是二分的，即每个结点只能分出两枝，并且在树的生长过程中，同一个自变量可以反复多次使用（分割），这些都是不同于 CHAID 的特点。另外，如果自变量存在数据缺失（Missing）的情况，CART 的处理方式是寻找一个替代数据来代替（或填充）缺失值，而 CHAID 则是把缺失数值作为单独的一类数值。

10.2.4　ID3 算法

ID3（Iterative Dichotomiser）与 CART 发明于同一时期，中文简称迭代的二分器，其最大的特点在于自变量的挑选标准是基于信息增益度量的，即选择具有最高信息增益的属性作为结点的分裂（或分割）属性，这样一来，分割后的结点里分类所需的信息量就会最小，这也是一种划分纯度的思想。至于 C4.5，可以将其理解为 ID3 的发展版本（后继版），主要区别在于 C4.5 用信息增益率（Gain Ratio) 代替了 ID3 中的信息增益，主要的原因是使用信息增益度量有个缺点，就是倾向于选择具有大量值的属性，极端的例子，如对于 Member_id 的划分，每个 Id 都是一个最纯的组，但是这样的划分没有任何实际意义，而 C4.5 所采用的信息增益率就可以较好地克服这个缺点，它在信息增益的基础上，增加了一个分裂信息（Split Information）对其进行规范化约束。

10.2.5　决策树的应用优势

在数据挖掘的实践应用中，决策树体现了如下明显的优势和竞争力：

❑决策树模型非常直观，生成的一系列「如果…… 那么……」的逻辑判断很容易让人理解和应用。这个特点是决策树赢得广泛应用的最主要原因，真正体现了简单、直观、通俗、易懂。

❑决策树搭建和应用的速度比较快，并且可以处理区间型变量（Interval）和类别型变量（Category）。但是要强调的是「可以处理区间型变量」不代表「快速处理区间型变量」，如果输入变量只是类别型或次序型变量，决策树的搭建速度是很快的，但如果加上了区间型变量，视数据规模，其模型搭建速度可能会有所不同。

❑决策树对于数据的分布没有特别严格的要求。

❑对缺失值（Missing Value）很宽容，几乎不做任何处理就可以应用。

❑不容易受数据中极端值（异常值）的影响。

❑可以同时对付数据中线性和非线性的关系。

❑决策树通常还可以作为有效工具来帮助其他模型算法挑选自变量。决策树不仅本身对于数据的前期处理和清洗没有什么特别的要求和限制，它还会有效帮助别的模型算法去挑选自变量，因为决策树算法里结点的自变量选择方法完全适用于其他算法模型，包括卡方检验、Gini 指数、信息增益等。

❑决策树算法使用信息原理对大样本的属性进行信息量分析，并计算各属性的信息量，找出反映类别的重要属性，可准确、高效地发现哪些属性对分类最有意义。这一点，对于区间型变量的分箱操作来说，意义非常重大。关于分箱操作，请参考本书 8.5.3 节。

10.2.6　决策树的缺点和注意事项

事物都是具有两面性的，有缺点不可怕，关键在于如何扬长避短，数据分析师不仅要清楚知道决策树的缺点，更需要掌握相应的注意事项，才可能取长补短，达到事半功倍的效果。

❑决策树最大的缺点是其原理中的贪心算法。贪心算法总是做出在当前看来最好的选择，却并不从整体上思考最优的划分，因此，它所做的选择只能是某种意义上的局部最优选择。学术界针对贪心算法不断进行改进探索，但是还没有可以在实践中大规模有效应用的成熟方案。

❑如果目标变量是连续型变量，那么决策树就不适用了，最好改用线性回归算法去解决。

❑决策树缺乏像回归或者聚类那样的丰富多样的检测指标和评价方法，这或许是今后算法研究者努力的一个方向。

❑当某些自变量的类别数量比较多，或者自变量是区间型时，决策树过拟合的危险性会增加。针对这种情况，数据分析师需要进行数据转换，比如分箱和多次模型验证和测试，确保其具有稳定性。

❑决策树算法对区间型自变量进行分箱操作时，无论是否考虑了顺序因素，都有可能因为分箱丧失某些重要的信息。尤其是当分箱前的区间型变量与目标变量有明显的线性关系时，这种分箱操作造成的信息损失更为明显。

10.3　逻辑回归技术的实践应用和注意事项

回归分析，在此主要是指包括逻辑回归技术和多元线性回归技术，是数量统计学中应用最广泛的一个分析工具，也是数据分析挖掘实践中应用得最广泛的一种分析方法（技术）。尽管从狭隘的界定来看，回归分析技术属于统计分析的范畴，但是正如本书开头所阐述的那样，绝对地划清统计分析和数据挖掘的界线，对于数据分析挖掘实践来说是没有任何意义的。只要能解决实际的业务问题，只要能提升企业的运营效率，它就是好技术，况且目前在数据挖掘实践中也大量应用回归分析技术。因此，本节将专门讨论逻辑回归技术。

10.3.1　逻辑回归的原理和核心要素

当目标变量是二元变量（即是与否）的时候，逻辑回归分析是一个非常成熟的、可靠的主流模型算法。

对于二元（是与否）的目标变量来说，逻辑回归的目的就是要预测一组自变量数值相对应的因变量是「是」的概率，这个概率 P 是介于 [0,1] 之间的。如果要用线性回归方法来进行概率计算，计算的结果很可能是超出 [0,1] 范围的。在这种情况下，就需要用到专门的概率计算公式了，或叫 Sigmoid 函数，其计算公式如下：

上述概率算法可以确保二元目标变量的预测概率 P 是介于 [0,1] 之间的。

其中，β0 是常数，β1 到 βk 是自变量 x1 到 xk 各自所对应的系数。

按上述公式应用后的 Sigmoid 分布曲线如图 10-2 所示。

图　10-2　Sigmoid 分布曲线

接下来进一步深入理解，这里引入了可能性比率（ODDS）这个概念。

可能性比率（ODDS）是指一件事情发生的概率除以这件事情不发生的概率后得到的值，博彩活动中的赔率就是可能性比率，其在现实生活中是一个广为人知的应用案例。

可能性比率为 5，说明一件事件发生的可能性比不发生的可能性高 5 倍；

可能性比率为 0.2，说明一件事情发生的可能性为不发生的可能性的 1/5；

可能性比率小于 1，说明一件事情发生的概率低于 50%；

可能性比率大于 1，说明一件事件发生的概率高于 50%；

与概率不同的是，可能性比率的最小值为 0，但最大值可以是无穷大。

可能性比率是逻辑回归中连接自变量和因变量的纽带，我们可以从下面的公式演变中体会这句话的意思。

将上述两个公式合并，就会成为现在广泛应用的逻辑回归算法：

该公式也可以表现为：

逻辑回归使用的参数估计方法通常是最大似然法，利用最大似然法进行参数的估计时，通常有如下步骤：

设 Y 为 0-1 型变量，X=(x1,x2,…,xp) 是与 Y 相关的变量，n 组观测数据为 (xi1,xi2,…,xip；yi)(i=1,2,…,n)，yi 与 xi1,xi2,…,xip 的关系如下：

其中，函数 f (x) 是值域在 [0,1] 区间的单调递增函数，对于逻辑回归（Logistic Regression），有。

于是，yi 是均值为 πi=f (β0+β1xi1+β2xi2+…+βpxip) 的 0-1 分布，其概率函数为

P(yi=1)=πi

P(yi=0)=1-πi

可以把 yi 的概率函数合写为

于是 y1,y2,…,yn 的似然函数则为

对上述似然函数取对数，得

