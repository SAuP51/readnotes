决策树技术在数据化运营中的主要用途体现在：作为分类、预测问题的典型支持技术，它在用户划分、行为预测、规则梳理等方面具有广泛的应用前景，决策树甚至可以作为其他建模技术前期进行变量筛选的一种方法，即通过决策树的分割来筛选有效地输入自变量。

关于决策树的详细介绍和实践中的注意事项，可参考本书 10.2 节。

2.3.2　神经网络

神经网络（Neural Network）是通过数学算法来模仿人脑思维的，它是数据挖掘中机器学习的典型代表。神经网络是人脑的抽象计算模型，我们知道人脑中有数以百亿个神经元（人脑处理信息的微单元），这些神经元之间相互连接，使得人的大脑产生精密的逻辑思维。而数据挖掘中的「神经网络」也是由大量并行分布的人工神经元（微处理单元）组成的，它有通过调整连接强度从经验知识中进行学习的能力，并可以将这些知识进行应用。

简单来讲，「神经网络」就是通过输入多个非线性模型以及不同模型之间的加权互联（加权的过程在隐蔽层完成），最终得到一个输出模型。其中，隐蔽层所包含的就是非线性函数。

目前最主流的「神经网络」算法是反馈传播（Backpropagation），该算法在多层前向型（Multilayer Feed-Forward）神经网络上进行学习，而多层前向型神经网络又是由一个输入层、一个或多个隐蔽层以及一个输出层组成的，「神经网络」的典型结构如图 2-1 所示。

图　2-1　「神经网络」的典型结构图

由于「神经网络」拥有特有的大规模并行结构和信息的并行处理等特点，因此它具有良好的自适应性、自组织性和高容错性，并且具有较强的学习、记忆和识别功能。目前神经网络已经在信号处理、模式识别、专家系统、预测系统等众多领域中得到广泛的应用。

「神经网络」的主要缺点就是其知识和结果的不可解释性，没有人知道隐蔽层里的非线性函数到底是如何处理自变量的，「神经网络」应用中的产出物在很多时候让人看不清其中的逻辑关系。但是，它的这个缺点并没有影响该技术在数据化运营中的广泛应用，甚至可以这样认为，正是因为其结果具有不可解释性，反而更有可能促使我们发现新的没有认识到的规律和关系。

在利用「神经网络」技术建模的过程中，有以下 5 个因素对模型结果有着重大影响：

❑层数。

❑每层中输入变量的数量。

❑联系的种类。

❑联系的程度。

❑转换函数，又称激活函数或挤压函数。

关于这 5 个因素的详细说明，请参考本书 10.1.1 节。

「神经网络」技术在数据化运营中的主要用途体现在：作为分类、预测问题的重要技术支持，在用户划分、行为预测、营销响应等诸多方面具有广泛的应用前景。

关于神经网络的详细介绍和实践中的注意事项，可参考本书 10.1 节。

2.3.3　回归

回归（Regression）分析包括线性回归（Linear Regression），这里主要是指多元线性回归和逻辑斯蒂回归（Logistic Regression）。其中，在数据化运营中更多使用的是逻辑斯蒂回归，它又包括响应预测、分类划分等内容。

多元线性回归主要描述一个因变量如何随着一批自变量的变化而变化，其回归公式（回归方程）就是因变量与自变量关系的数据反映。因变量的变化包括两部分：系统性变化与随机变化，其中，系统性变化是由自变量引起的（自变量可以解释的），随机变化是不能由自变量解释的，通常也称作残值。

在用来估算多元线性回归方程中自变量系数的方法中，最常用的是最小二乘法，即找出一组对应自变量的相应参数，以使因变量的实际观测值与回归方程的预测值之间的总方差减到最小。

对多元线性回归方程的参数估计，是基于下列假设的：

❑输入变量是确定的变量，不是随机变量，而且输入的变量间无线性相关，即无共线性。

❑随机误差的期望值总和为零，即随机误差与自变量不相关。

❑随机误差呈现正态分布 [1]。

如果不满足上述假设，就不能用最小二乘法进行回归系数的估算了。

逻辑斯蒂回归（Logistic Regression）相比于线性回归来说，在数据化运营中有更主流更频繁的应用，主要是因为该分析技术可以很好地回答诸如预测、分类等数据化运营常见的分析项目主题。简单来讲，凡是预测「两选一」事件的可能性（比如，「响应」还是「不响应」；「买」还是「不买」；「流失」还是「不流失」），都可以采用逻辑斯蒂回归方程。

逻辑斯蒂回归预测的因变量是介于 0 和 1 之间的概率，如果对这个概率进行换算，就可以用线性公式描述因变量与自变量的关系了，具体公式如下：

与多元线性回归所采用的最小二乘法的参数估计方法相对应，最大似然法是逻辑斯蒂回归所采用的参数估计方法，其原理是找到这样一个参数，可以让样本数据所包含的观察值被观察到的可能性最大。这种寻找最大可能性的方法需要反复计算，对计算能力有很高的要求。最大似然法的优点是在大样本数据中参数的估值稳定、偏差小，估值方差小。

关于线性回归和逻辑回归的详细介绍和在实践应用中的注意事项，可参考本书 10.3 节和 10.4 节。

[1] 正态分布也称常态分布，是具有两个参数 m 和 s2 的连续型随机变量分布，第一个参数 m 是服从正态分布的随机变量的均值，第二个参数 s2 是此随机变量的方差，服从正态分布的随机变量的概率规律为取与 m 邻近的值的概率大，而取离 m 越远的值的概率越小；s 越小，分布越集中在 m 附近，s 越大，分布越分散。

2.3.4　关联规则

关联规则（Association Rule）是在数据库和数据挖掘领域中被发明并被广泛研究的一种重要模型，关联规则数据挖掘的主要目的是找出数据集中的频繁模式（Frequent Pattern），即多次重复出现的模式和并发关系（Cooccurrence Relationships），即同时出现的关系，频繁和并发关系也称作关联（Association）。

应用关联规则最经典的案例就是购物篮分析（Basket Analysis），通过分析顾客购物篮中商品之间的关联，可以挖掘顾客的购物习惯，从而帮助零售商更好地制定有针对性的营销策略。

以下列举一个简单的关联规则的例子：

婴儿尿不湿→啤酒 [支持度 = 10%，置信度 = 70%]

这个规则表明，在所有顾客中，有 10% 的顾客同时购买了婴儿尿不湿和啤酒，而在所有购买了婴儿尿不湿的顾客中，占 70% 的人同时还购买了啤酒。发现这个关联规则后，超市零售商决定把婴儿尿不湿和啤酒摆放在一起进行促销，结果明显提升了销售额，这就是发生在沃尔玛超市中「啤酒和尿不湿」的经典营销案例。

上面的案例是否让你对支持度和置信度有了一定的了解？事实上，支持度（Support）和置信度（Confidence）是衡量关联规则强度的两个重要指标，它们分别反映着所发现规则的有用性和确定性。其中支持度：规则 X→Y 的支持度是指事物全集中包含 X∪Y 的事物百分比。支持度主要衡量规则的有用性，如果支持度太小，则说明相应规则只是偶发事件。在商业实战中，偶发事件很可能没有商业价值；置信度：规则 X→Y 的置信度是指既包含了 X 又包含了 Y 的事物数量占所有包含了 X 的事物数量的百分比。置信度主要衡量规则的确定性（可预测性），如果置信度太低，那么从 X 就很难可靠地推断出 Y 来，置信度太低的规则在实践应用中也没有太大用处。

在众多的关联规则数据挖掘算法中，最著名的就是 Apriori 算法，该算法具体分为以下两步进行：

（1）生成所有的频繁项目集。一个频繁项目集（Frequent Itemset）是一个支持度高于最小支持度阀值（min-sup）的项目集。

（2）从频繁项目集中生成所有的可信关联规则。这里可信关联规则是指置信度大于最小置信度阀值（min-conf）的规则。

关联规则算法不但在数值型数据集的分析中有很大用途，而且在纯文本文档和网页文件中，也有着重要用途。比如发现单词间的并发关系以及 Web 的使用模式等，这些都是 Web 数据挖掘、搜索及推荐的基础。

2.3.5　聚类

聚类（Clustering）分析有一个通俗的解释和比喻，那就是「物以类聚，人以群分」。针对几个特定的业务指标，可以将观察对象的群体按照相似性和相异性进行不同群组的划分。经过划分后，每个群组内部各对象间的相似度会很高，而在不同群组之间的对象彼此间将具有很高的相异度。

聚类分析的算法可以分为划分的方法（Partitioning Method）、层次的方法（Hierarchical Method）、基于密度的方法（Density-based Method）、基于网格的方法（Grid-based Method）、基于模型的方法（Model-based Method）等，其中，前面两种方法最为常用。

对于划分的方法（Partitioning Method），当给定 m 个对象的数据集，以及希望生成的细分群体数量 K 后，即可采用这种方法将这些对象分成 K 组（K≤m），使得每个组内对象是相似的，而组间的对象是相异的。最常用的划分方法是 K-Means 方法，其具体原理是：首先，随机选择 K 个对象，并且所选择的每个对象都代表一个组的初始均值或初始的组中心值；对剩余的每个对象，根据其与各个组初始均值的距离，将它们分配给最近的（最相似）小组；然后，重新计算每个小组新的均值；这个过程不断重复，直到所有的对象在 K 组分布中都找到离自己最近的组。

层次的方法（Hierarchical Method）则是指依次让最相似的数据对象两两合并，这样不断地合并，最后就形成了一棵聚类树。

聚类技术在数据分析和数据化运营中的主要用途表现在：既可以直接作为模型对观察对象进行群体划分，为业务方的精细化运营提供具体的细分依据和相应的运营方案建议，又可在数据处理阶段用作数据探索的工具，包括发现离群点、孤立点，数据降维的手段和方法，通过聚类发现数据间的深层次的关系等。

关于聚类技术的详细介绍和应用实践中的注意事项，可参考本书第 9 章。

2.3.6　贝叶斯分类方法

贝叶斯分类方法（Bayesian Classifier）是非常成熟的统计学分类方法，它主要用来预测类成员间关系的可能性。比如通过一个给定观察值的相关属性来判断其属于一个特定类别的概率。贝叶斯分类方法是基于贝叶斯定理的，已经有研究表明，朴素贝叶斯分类方法作为一种简单贝叶斯分类算法甚至可以跟决策树和神经网络算法相媲美。

贝叶斯定理的公式如下：

其中，X 表示 n 个属性的测量描述；H 为某种假设，比如假设某观察值 X 属于某个特定的类别 C；对于分类问题，希望确定 P (H|X)，即能通过给定的 X 的测量描述，来得到 H 成立的概率，也就是给出 X 的属性值，计算出该观察值属于类别 C 的概率。因为 P (H|X) 是后验概率（Posterior Probability），所以又称其为在条件 X 下，H 的后验概率。

举例来说，假设数据属性仅限于用教育背景和收入来描述顾客，而 X 是一位硕士学历，收入 10 万元的顾客。假定 H 表示假设我们的顾客将购买苹果手机，则 P (H|X) 表示当我们知道顾客的教育背景和收入情况后，该顾客将购买苹果手机的概率；相反，P (X|H) 则表示如果已知顾客购买苹果手机，则该顾客是硕士学历并且收入 10 万元的概率；而 P (X) 则是 X 的先验概率，表示顾客中的某个人属于硕士学历且收入 10 万元的概率；P (H) 也是先验概率，只不过是任意给定顾客将购买苹果手机的概率，而不会去管他们的教育背景和收入情况。

从上面的介绍可见，相比于先验概率 P (H)，后验概率 P (H|X) 基于了更多的信息（比如顾客的信息属性），而 P (H) 是独立于 X 的。

贝叶斯定理是朴素贝叶斯分类法（Naive Bayesian Classifier）的基础，如果给定数据集里有 M 个分类类别，通过朴素贝叶斯分类法，可以预测给定观察值是否属于具有最高后验概率的特定类别，也就是说，朴素贝叶斯分类方法预测 X 属于类别 Ci 时，表示当且仅当

P(Ci|X)＞P(Cj|X)1≤j≤m，j≠i

此时如果最大化 P (Ci|X)，其 P (Ci|X) 最大的类 Ci 被称为最大后验假设，根据贝叶斯定理

可知，由于 P (X) 对于所有的类别是均等的，因此只需要 P (X|Ci) P (Ci) 取最大即可。

为了预测一个未知样本 X 的类别，可对每个类别 Ci 估算相应的 P (X|Ci) P (Ci)。样本 X 归属于类别 Ci，当且仅当

P(Ci|X)＞P(Cj|X)1≤j≤m，j≠i

贝叶斯分类方法在数据化运营实践中主要用于分类问题的归类等应用场景。

2.3.7　支持向量机

支持向量机（Support Vector Machine）是 Vapnik 等人于 1995 年率先提出的，是近年来机器学习研究的一个重大成果。与传统的神经网络技术相比，支持向量机不仅结构简单，而且各项技术的性能也明显提升，因此它成为当今机器学习领域的热点之一。

作为一种新的分类方法，支持向量机以结构风险最小为原则。在线性的情况下，就在原空间寻找两类样本的最优分类超平面。在非线性的情况下，它使用一种非线性的映射，将原训练集数据映射到较高的维上。在新的维上，它搜索线性最佳分离超平面。使用一个适当的对足够高维的非线性映射，两类数据总可以被超平面分开。

支持向量机的基本概念如下：

设给定的训练样本集为 {(x1,y1),(x2,y2),…,(xn,yn)}，其中 xi∈Rn,y∈{-1,1}。

再假设该训练集可被一个超平面线性划分，设该超平面记为 (w,x)+b=0。

支持向量机的基本思想可用图 2-2 的两维情况举例说明。

图　2-2　线性可分情况下的最优分类线

图中圆形和方形代表两类样本，H 为分类线，H1、H2，分别为过各类样本中离分类线最近的样本并且平行于分类线的直线，它们之间的距离叫做分类间隔（Margin）。所谓的最优分类线就是要求分类线不但能将两类正确分开（训练错误为 0），而且能使分类间隔最大。推广到高维空间，最优分类线就成了最优分类面。

其中，距离超平面最近的一类向量被称为支持向量（Support Vector），一组支持向量可以唯一地确定一个超平面。通过学习算法，SVM 可以自动寻找出那些对分类有较好区分能力的支持向量，由此构造出的分类器则可以最大化类与类的间隔，因而有较好的适应能力和较高的分类准确率。

支持向量机的缺点是训练数据较大，但是，它的优点也是很明显的 —— 对于复杂的非线性的决策边界的建模能力高度准确，并且也不太容易过拟合 [1]。

支持向量机主要用在预测、分类这样的实际分析需求场景中。

[1] 过拟合，是指模型在训练的时候对样本「模拟」过好，不能反映真实的输入输出函数关系，所以一旦模型面对新的应用数据的时候，就表现为不准确的程度较大。

2.3.8　主成分分析

严格意义上讲，主成分分析（Principal Components Analysis）属于传统的统计分析技术范畴，但是正如本章前面所阐述的，统计分析与数据挖掘并没有严格的分割，因此在数据挖掘实战应用中也常常会用到这种方式，从这个角度讲，主成分分析也是数据挖掘商业实战中常用的一种分析技术和数据处理技术。

主成分分析会通过线性组合将多个原始变量合并成若干个主成分，这样每个主成分都变成了原始变量的线性组合。这种转变的目的，一方面是可以大幅降低原始数据的维度，同时也在此过程中发现原始数据属性之间的关系。

主成分分析的主要步骤如下：

1）通常要先进行各变量的标准化工作，标准化的目的是将数据按照比例进行缩放，使之落入一个小的区间范围之内，从而让不同的变量经过标准化处理后可以有平等的分析和比较基础。关于数据标准化的详细介绍，可参考本书 8.5.4 节和 9.3.2 节。

2）选择协方差阵或者相关阵计算特征根及对应的特征向量。

3）计算方差贡献率，并根据方差贡献率的阀值选取合适的主成分个数。

4）根据主成分载荷的大小对选择的主成分进行命名。

5）根据主成分载荷计算各个主成分的得分。

将主成分进行推广和延伸即成为因子分析（Factor Analysis），因子分析在综合原始变量信息的基础上将会力图构筑若干个意义较为明确的公因子；也就是说，采用少数几个因子描述多个指标之间的联系，将比较密切的变量归为同一类中，每类变量即是一个因子。之所以称其为因子，是因为它们实际上是不可测量的，只能解释。

主成分分析是因子分析的一个特例，两者的区别和联系主要表现在以下方面：

❑主成分分析会把主成分表示成各个原始变量的线性组合，而因子分析则把原始变量表示成各个因子的线性组合。这个区别最直观也最容易记住。

❑主成分分析的重点在于解释原始变量的总方差，而因子分析的重点在于解释原始变量的协方差。

❑在主成分分析中，有几个原始变量就有几个主成分，而在因子分析中，因子个数可以根据业务场景的需要人为指定，并且指定的因子数量不同，则分析结果也会有差异。

❑在主成分分析中，给定的协方差矩阵或者相关矩阵的特征值是唯一时，主成分也是唯一的，但是在因子分析中，因子不是唯一的，并且通过旋转可以得到不同的因子。

主成分分析和因子分析在数据化运营实践中主要用于数据处理、降维、变量间关系的探索等方面，同时作为统计学里的基本而重要的分析工具和分析方法，它们在一些专题分析中也有着广泛的应用。

2.3.9　假设检验

假设检验（Hypothesis Test）是现代统计学的基础和核心之一，其主要研究在一定的条件下，总体是否具备某些特定特征。

假设检验的基本原理就是小概率事件原理，即观测小概率事件在假设成立的情况下是否发生。如果在一次试验中，小概率事件发生了，那么说明假设在一定的显著性水平下不可靠或者不成立；如果在一次试验中，小概率事件没有发生，那么也只能说明没有足够理由相信假设是错误的，但是也并不能说明假设是正确的，因为无法收集到所有的证据来证明假设是正确的。

假设检验的结论是在一定的显著性水平下得出的。因此，当采用此方法观测事件并下结论时，有可能会犯错，这些错误主要有两大类：

❑第 Ⅰ 类错误：当原假设为真时，却否定它而犯的错误，即拒绝正确假设的错误，也叫弃真错误。犯第 Ⅰ 类错误的概率记为 α，通常也叫 α 错误，α=1 - 置信度。

❑第 Ⅱ 类错误：当原假设为假时，却肯定它而犯的错误，即接受错误假设的错误，也叫纳伪错误。犯第 Ⅱ 类错误的概率记为 β，通常也叫 β 错误。

上述这两类错误在其他条件不变的情况下是相反的，即 α 增大时，β 就减小；α 减小时，β 就增大。α 错误容易受数据分析人员的控制，因此在假设检验中，通常会先控制第 Ⅰ 类错误发生的概率 α，具体表现为：在做假设检验之前先指定一个 α 的具体数值，通常取 0.05，也可以取 0.1 或 0.001。

在数据化运营的商业实践中，假设检验最常用的场景就是用于「运营效果的评估」上，本书第 12 章将针对最常见、最基本的假设检验形式和技术做出比较详细的梳理和举例。

2.4　互联网行业数据挖掘应用的特点

相对于传统行业而言，互联网行业的数据挖掘和数据化运营有如下的一些主要特点：

❑数据的海量性。互联网行业相比传统行业第一个区别就是收集、存储的数据是海量的，这一方面是因为互联网的使用已经成为普通人日常生活和工作中不可或缺的一部分，另一方面更是因为用户网络行为的每一步都会被作为网络日志记录下来。海量的数据、海量的字段、海量的信息，尤其是海量的字段，使得分析之前对于分析字段的挑选和排查工作显得无比重要，无以复加。如何大浪淘沙挑选变量则为重中之重，对此很难一言以蔽之的进行总结，还是用三分技术，七分业务来理解吧。本书从第 7～12 章，几乎每章都用大量的篇幅讨论如何在具体的分析课题和项目中选择变量、评估变量、转换变量，乃至如何通过清洗后的核心变量完成最终的分析结论（挖掘模型）。

❑数据分析（挖掘）的周期短。鉴于互联网行业白热化的市场竞争格局，以及该行业相对成熟的高级数据化运营实践，该行业的数据分析（挖掘）通常允许的分析周期（项目周期）要明显短于传统行业。行业技术应用飞速发展，产品和竞争一日千里，都使该行业的数据挖掘项目的时间进度比传统行业的项目模式快得多。一方面要保证挖掘结果的起码质量，另一方面要满足这个行业超快的行业节奏，这也使得传统的挖掘分析思路和步调必须改革和升华，从而具有鲜明的 Internet 色彩。

❑数据分析（挖掘）成果的时效性明显变短。由于互联网行业的用户行为相对于传统行业而言变化非常快，导致相应的数据分析挖掘成果的时效性也比传统行业明显缩短。举例来说，互联网行业的产品更新换代很多是以月为单位的，新产品层出不穷，老产品要及时下线，因此，针对具体产品的数据分析（挖掘）成果的时效性也明显变短；或者说，用户行为变化快，网络环境变化快，导致模型的维护和优化的时间周期也明显变短，传统行业里的「用户流失预测模型」可能只需要每年更新优化一次，但是在互联网行业里类似的模型可能 3 个月左右就有必要更新优化了。

❑互联网行业新技术、新应用、新模式的更新换代相比于传统行业而言更加迅速、周期更短、更加具有颠覆性，相应地对数据分析挖掘的应用需求也更为苛刻，且要多样化。以中国互联网行业的发展为例，作为第一代互联网企业的代表，新浪、搜狐、雅虎等门户网站的 Web 1.0 模式（传统媒体的电子化）从产生到被以 Google、百度等搜索引擎企业的 Web 2.0 模式（制造者与使用者的合一）所超越，前后不过 10 年左右的时间，而目前这个 Web 2.0 模式已经逐渐有被以微博为代表 Web 3.0 模式（SNS 模式）超越的趋势。具体到数据分析所服务的互联网业务和应用来说，从最初的常规、主流的分析挖掘支持，到以微博应用为代表的新的分析需求，再到目前风头正健的移动互联网的数据分析和应用，互联网行业的数据分析大显身手的天地在不断扩大，新的应用源源不断，新的挑战让人们应接不暇，这一切都要求数据分析师自觉、主动去学习、去充实、去提升自己、去跟上互联网发展的脚步。

第 3 章　数据化运营中常见的数据分析项目类型

千举万变，其道一也。

——《荀子·儒效》

3.1　目标客户的特征分析

3.2　目标客户的预测（响应、分类）模型

3.3　运营群体的活跃度定义

3.4　用户路径分析

3.5　交叉销售模型

3.6　信息质量模型

3.7　服务保障模型

3.8　用户（买家、卖家）分层模型

3.9　卖家（买家）交易模型

3.10　信用风险模型

3.11　商品推荐模型

3.12　数据产品

3.13　决策支持

数据化运营中的数据分析项目类型比较多，涉及不同的业务场景、业务目的和分析技术。在本章中，按照业务用途的不同将其做了一个大概的分类，并针对每一类项目的特点和具体采用的分析挖掘技术进行了详细的说明和举例示范。

一个成功的数据分析挖掘项目，首先要有准确的业务需求描述，之后则要求项目相关人员自始至终对业务有正确的理解和判断，所以对于本章所分享的所有分析项目类型以及对应的分析挖掘技术，读者只有在深刻理解和掌握相应业务背景的基础上才可以真正理解项目类型的特点、目的，以及相应的分析挖掘技术合适与否。

对业务的理解和思考，永远高于项目的分类和分析技术的选择。

3.1　目标客户的特征分析

目标客户的特征分析几乎是数据化运营企业实践中最普遍、频率最高的业务分析需求之一，原因在于数据化运营的第一步（最基础的步骤）就是要找准你的目标客户、目标受众，然后才是相应的运营方案、个性化的产品与服务等。是不加区别的普遍运营还是有目标有重点的精细化运营，这是传统的粗放模式与精细的数据化运营最直接、最显性的区别。

在目标客户的典型特征分析中，业务场景可以是试运营之前的虚拟特征探索，也可以是试运营之后来自真实运营数据基础上的分析、挖掘与提炼，两者目标一致，只是思路不同、数据来源不同而已。另外，分析技术也有一定的差异。

对于试运营之前的虚拟特征探索，是指目标客户在真实的业务环境里还没有产生，并没有一个与真实业务环境一致的数据来源可以用于分析目标客户的特点，因此只能通过简化、类比、假设等手段，来寻找一个与真实业务环境近似的数据来源，从而进行模拟、探索，并从中发现一些似乎可以借鉴和参考的目标用户特征，然后把这些特征放到真实的业务环境中去试运营。之后根据真实的效果反馈数据，修正我们的目标用户特征。一个典型的业务场景举例就是 A 公司推出了一个在线转账产品，用户通过该产品在线转账时产生的交易费用相比于普通的网银要便宜些。在正式上线该转账产品之前，产品运营团队需要一个初步的目标客户特征报告。很明显，在这个时刻，产品还没有上线，是无法拥有真实使用该产品的用户的，自然也没有相应数据的积累，那这个时候所做的目标客户特征分析只能是按照产品设计的初衷、产品定位，以及运营团队心中理想化的猜测，从企业历史数据中模拟、近似地整理出前期期望中的目标客户典型特征，很明显这里的数据并非来自该产品正式上线后的实际用户数据（还没有这些真实的数据产生），所以这类场景的分析只能是虚拟的特征分析。具体来说，本项目先要从企业历史数据中寻找有在线交易历史的买卖双方，在线行为活跃的用户，以及相应的一些网站行为、捆绑了某知名的第三方支付工具的用户等，然后根据这些行为字段和模拟的人群，去分析我们期望的目标客户特征，在通过历史数据仓库的对比后，准确掌握该目标群体的规模和层次，从而提交运营业务团队正式运营。

对于试运营之后的来自真实运营数据基础上的用户特征分析，相对而言，就比上述的模拟数据分析来得更真实更可行，也更贴近业务实际。在该业务场景下，数据的提取完全符合业务需求，且收集到的用户也是真实使用了该产品的用户，基于这些真实用户的分析就不是虚拟的猜测和模拟了，而是有根有据的铁的事实。在企业的数据化运营实践中，这后一种场景更加普遍，也更加可靠。

对于上面提到的案例，在经过一段时间的试运营之后，企业积累了一定数量使用该产品的用户数据。现在产品运营团队需要基于该批实际的用户数据，整理分析出该产品的核心目标用户特征分析报告，以供后期运营团队、产品开发团队、服务团队更有针对性、更有效地进行运营和服务。在这种基于真实的业务场景数据基础上的客户特征分析，有很多分析技术可以采用（本书第 11 章将针对「用户特征分析」进行专题介绍，分享其中最主要的一些分析技术），但是其中采用预测模型的思路是该场景与上述「虚拟场景」数据分析的一个不同，上述「虚拟场景」数据分析一般来说是无法进行预测模型思路的探索的。

