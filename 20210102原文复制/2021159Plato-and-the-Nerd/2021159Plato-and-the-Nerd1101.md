Determinism

· · · in which I argue that determinism is a property of models not of the physical world; that determinism is an extremely valuable property, one that has historically delivered considerable payoffs in engineering and science; that deterministic models may not be usefully predictive because of chaos and complexity; that families of deterministic models that embrace both discrete and continuous behaviors are incomplete; and that nondeterministic models, used explicitly and judiciously, play an essential role in engineering.

10.1 Laplace's Demon

The previous three chapters show definitively that we cannot know everything. Of course, each of us already knew that about ourselves, but these results are more fundamental. They assert that not everything is knowable. If we limit our study to computational processes, those given algorithmically by software, then Turing's result shows that we cannot tell what some programs will do just by looking at the code. If we broaden our study to formal mathematical models, then Gödel's result shows that we will always be able to construct models that we cannot determine to be true or false (or, worse, that end up being both true and false). Moreover, cardinality arguments show that there are many more possible information-processing functions than there are computer programs, mathematical models, or even descriptions in any given language. Consequently, some functions are not computations, cannot be modeled mathematically, and cannot even be described (completely) in any language we have. Unless nature, for some inexplicable reason, limits itself to only the tiny subset of functions that are computable and describable in a language we have, nature will inevitably continue to throw things at us that we cannot understand. To a scientist, this may be frustrating, but to an engineer, it simply means that the horizon for creativity is infinitely far away. There are no bounds to what we can do because we can continue to invent new languages and formalisms, and because they will never be complete, we will never be finished.

Because we can't know everything, we need systematic ways to deal with uncertainty. In the next chapter, I will directly address how to model uncertainty with probabilities. Before we can do that, however, we need to address the question of whether uncertainty is caused by the limits of what we can know or some intrinsic randomness in the world or in our models of the world. Many of the mathematical models and computer programs that we construct are deterministic, which suggests that we should be able to know quite a bit about them unless they fall into the Turing and Gödel traps. Most computer programmers strive to avoid these traps, yielding understandable programs, and also to create deterministic programs. However, this notion of determinism is not a simple one. We can't confront uncertainty without first confronting determinism.

Determinism is a deceptively simple idea that has vexed thinkers for a long time. Broadly, determinism in the physical world is the principle that everything that happens is inevitable, preordained by some earlier state of the universe or by some deity. For centuries, philosophers have debated the implications of this principle, particularly insofar as it undermines the notion of free will. If the world is deterministic, then presumably we cannot be held individually accountable for our actions because they are preordained. 1

Determinism is quite a subtle concept, as is the notion of free will. John Earman, in his Primer on Determinism , admits defeat in getting a handle on the concept:

This is already enough to make strong the suspicion that a real understanding of determinism cannot be achieved without simultaneously constructing a comprehensive philosophy of science. Since I have no such comprehensive view to offer, I approach the task I have set myself with humility. And also with the cowardly resolve to issue disclaimers whenever the going gets too rough. But even in a cowardly approach, determinism wins our unceasing admiration in forcing to the surface many of the more important and intriguing issues in the length and breadth of the philosophy of science. (Earman, 1986, p. 21)

But Earman insists that「determinism is a doctrine about the nature of the world.」I will circumvent the most treacherous difficulties by instead adopting a principle that I first learned from the Austrian computer scientist Hermann Kopetz, who asserted that determinism is a property of models and not a property of the physical world. This thesis does not diminish my fascination with the deep questions that Earman addresses, but it certainly does make it easier to apply the concept of determinism to engineered systems.

As a property of models, determinism is relatively easy to define:

A model is deterministic if given an initial state of the model, and given all the inputs that are provided to the model, the model defines exactly one possible behavior .

In other words, a model is deterministic if it is not possible for it to react in two or more ways to the same conditions. Only one reaction is possible. In this definition, I have italicized words that must be defined within the modeling paradigm to complete the definition, specifically,「state,」「input,」and「behavior.」

For example, if the state of a particle is its position x ( t ) in a Euclidean space at a time t , where both time and space are continuums, and if the input F ( t ) is a force applied to the particle at each instant t , and the behavior is the motion of the particle through space, then Newton's second law, equation (4096), is a deterministic model.

Two useful variations of this idea are evident immediately. First, a model may not have any inputs, in which case it is called a「closed model.」For example, if we assume that the universe is everything there is, then any model of the universe cannot have any inputs. Nothing outside the universe exists to provide those inputs. A closed model is deterministic if given an initial state, exactly one behavior is possible.

Second, a deterministic model may be reversible. In this case, given the state of the model at any particular time, and given the inputs at all times (if there are any inputs), both the past and future of the model are uniquely defined. There is only one possible past and only one possible future. Put another way, in a closed reversible deterministic model, the behavior for all time is determined by the state at any one time.

One reason that this simple concept has been so problematic is that all too often, when speaking of determinism, the speaker is confusing the map for the territory. To even speak of determinism, we must define「input,」「state,」and「behavior.」How can we define these things for an actual physical system? Any way we define them requires constructing a model. Hence, an assertion about determinism will actually be an assertion about the model not about the thing being modeled. Only a model can be unambiguously deterministic, which underscores Earman's struggle to pin down the concept.

Consider that any given physical system inevitably has more than one valid model. For example, a particle to which we are applying a force exhibits deterministic motion under Newton's second law but not under quantum mechanics, where the position of the particle will be given probabilistically. However, under quantum mechanics, the evolution of the particle's wave function is deterministic, following the Schrödinger equation (I will say more about this later). If the「state」and「behavior」of our model are the wave function, then the model is deterministic. If instead the state and behavior are the particle's position, then the model is nondeterministic. It makes no sense to assign determinism as a property to the particle. It is a property of the model.

If I have a deterministic model that is faithful to some physical system, then this model may have a particularly valuable property: the model may predict how the system will evolve in time in reaction to some input stimulus. This predictive power of a deterministic model is a key reason to seek deterministic models.

We already know that some deterministic models are not predictable. For example, Turing showed that we cannot predict, for all computer programs, whether the program execution will halt for a particular input, even if the program is deterministic. It turns out that there are many more deterministic models that are also not predictable because of chaos and complexity.

In chapter 2 , I made a distinction between the engineering and scientific uses of models. An engineer seeks a physical system to match a model, whereas a scientist seeks a model to match a physical system. For these two uses, determinism plays different roles. For an engineer, the determinism of a model is useful because it facilitates building confidence in the model . In chapter 4 , I talked about logic gates as deterministic models of electrons sloshing around in silicon. The determinism of the logic gate model is valuable: it enables circuit designers to use Boolean algebra to build confidence in circuit designs that have billions of transistors. The model predicts behaviors perfectly , in that an engineer can determine how a logic gate model will react to any particular input, given any initial state.

Of course, the usefulness of the logic gate model also depends on our ability to build silicon structures that are extremely faithful to the model. We have learned to control the sloshing of electrons in silicon so that, with high confidence, a circuit will emulate the logic gate model billions of times per second and operate without error for years.

Herein lies an essential difference between science and engineering. To a scientist, for a deterministic model to be useful, it must faithfully describe the behavior of a given physical system. To an engineer, for a deterministic model to be useful, it must be possible to construct a physical system that is faithful to the model. In both cases,「faithful」means that the behaviors of the model and the physical system match to a high degree of accuracy. However, the goals are different, and therefore deterministic models will be useful to an engineer in situations where the same models are not useful to a scientist and vice versa.

Some of the most valuable engineering models are deterministic. In addition to logic gates, we also have digital machines, instruction set architectures, and programming languages, most of which are deterministic models. The Turing machines in chapter 8 are also deterministic. The determinism of all these models has proved extremely valuable historically. The information technology revolution is built on the determinism of these models.

For a scientist, fundamentally, when considering the use of deterministic models, it matters quite a lot whether the physical system being modeled is also deterministic. Is the sloshing of electrons in silicon deterministic? If only a model can be unambiguously deterministic, then how can we answer this question? The fact is that almost all established laws of physics are deterministic models, and most are also reversible. Ohm's law for resistors and Faraday's law for inductors from chapter 2 are both reversible deterministic models. For a resistor, if I define the input to be a voltage and the output to be the current, then Ohm's law gives me a deterministic model of a resistor. The model is described by equation (1024). Newton's laws of motion and Einstein's theories of relativity are deterministic. Interestingly, even basic laws used to study the thermodynamics of gasses such as Boyle's law and Charles' law are deterministic, although they do not require the underlying motion of gas molecules to be deterministic. They define state, input, and output in terms of pressure, temperature, and volume not in terms of positions and momentums of the gas molecules. Even quantum mechanics is almost entirely deterministic, in that the evolution of the wave function as defined by the Schrödinger equation is deterministic.

The question of whether the physical world is deterministic has remained unanswered for a long time. In the early 1800s, the French scientist Pierre-Simon Laplace made an argument for determinism in the universe. Laplace argued that if someone (a demon) were to know the precise location and velocity of every particle in the universe, then the past and future locations and velocities for each particle would be completely determined and could be calculated from the laws of classical mechanics (Laplace, 1901). Is this true?

As I've pointed out before, the laws of classical mechanics, such as Newton's second law, equation (4096), are wrong. They need to be adjusted using Einstein's relativity to be precise, although the imprecision will be insignificant for most applications of classical mechanics. Moreover, the notions of position and velocity that underly the notion of「state」in classical mechanics are undermined by quantum mechanics, although again only significantly at extremely small scales. If the question is the fundamental scientific question of whether the world is deterministic, then any imprecision, no matter how small, matters.

What about the probabilistic nature of the wave function in quantum mechanics? Does this undermine the idea of a deterministic universe? Stephen Hawking argues that it does not:

At first, it seemed that these hopes for a complete determinism would be dashed by the discovery early in the 20th century that events like the decay of radioactive atoms seemed to take place at random. It was as if God was playing dice, in Einstein's phrase. But science snatched victory from the jaws of defeat by moving the goal posts and redefining what is meant by a complete knowledge of the universe. (Hawking, 2002)

Hawking is referring to the fact that the Schrödinger equation, which describes how a wave function evolves in time, is deterministic.「In quantum theory, it turns out one doesn't need to know both the positions and the velocities [of the particles].」It is enough to know how the wave function evolves in time.

Although I can't possibly explain it fully (I'm not convinced that anyone can), it is worth a brief aside on wave functions because they represent the only established nondeterminism that I know of in widely accepted models of fundamental physics. In quantum mechanics, the position of a particle in space is not described simply as a point in a three-dimensional Euclidean geometry but rather by a wave function, a squiggle that changes shape and moves in space over time. The square of the value of the wave function at a point in space and time represents the relative probability of finding the particle at that position at that time. 2 The use of probabilities, a subtle concept that I discuss in chapter 11 , implies that the position of the particle is nondeterministic, and the wave function gives the relative likelihoods that an observer will find the particle at any particular point in space.

In 1926, Erwin Schrödinger, a Nobel Prize-winning Austrian physicist, published what is now a key centerpiece of quantum mechanics, the Schrödinger equation. This equation describes the evolution in time and space of the wave function, and as Hawking points out, that evolution is deterministic.

The wave function represents probabilities, and its interpretation is fraught with difficulties. In what is now called the Copenhagen interpretation, originally proposed in the years 1925 to 1927 by the Danish physicist Niels Bohr and the German physicist Werner Heisenberg, the state of a system continues to be defined by probabilities until an external observer observes the state, and only at that point do the probabilities influence the outcome. Prior to being observed, all possible outcomes represented by the probabilities continue to remain possible. This requires an「observer」who is somehow separate from the system and measures the position of the particle. The usual interpretation of a probability is that it specifies the likelihood of observing a particular outcome of an experiment. Under this interpretation, a probability makes sense only if the experiment is performed and the outcome is observed. How can any observer be separate from the physical system?

Schrödinger pointed out difficulties of the Copenhagen interpretation, famously illustrating them with what is now called「Schrödinger's cat.」In this thought experiment, a cat is locked in a chamber containing a mechanism that will release a poison if a particular radioactive atom decays, emitting radiation. The decay of a radioactive atom is governed by a wave function, so the decay event is governed by probabilities. The probabilities evolve deterministically according to the Schrödinger equation, but under the Copenhagen interpretation, no actual experiment governed by those probabilities occurs until an observer observes the system. Because all possibilities remain possible until an observer observes the system, Schrödinger argued that the cat must be both alive and dead until such observation occurs. Only then does it become one or the other.

These difficulties have led to endless debateabout the meaning of the wave function, with a variety of sometimes bizarre positions emerging. Some of these positions posit that the observer somehow lives outside of physics, that the observer is God, and that this observer is the essence of human cognition. In the 1950s, the physicist Hugh Everett III dispensed with the observer, bundling observer and observed under a single wave function that evolves deterministically under the Schrödinger equation. This view is often called the「many worlds」view because it can be interpreted to mean that all outcomes exist simultaneously in an infinite number of parallel universes. In this view, we can take the state of a system to be its wave function, and then quantum dynamics is deterministic.

So Laplace's question still stands, except that now we have to update it to consider the「state」of the system to be represented by its wave function not by the positions and velocities of its particles, and we have to account for the curvature of space time no matter how small. If we make these adjustments, is the resulting model of the universe deterministic? The question of whether the physical world itself is deterministic is probably unanswerable. 3 However, we can answer the question of whether any particular model of the universe is deterministic. We need to keep distinct the map and the territory.

In 2008, David Wolpert used Georg Cantor's diagonalization technique to prove that Laplace's demon cannot exist (Wolpert, 2008). His proof relies on the observation that such a demon, were it to exist, would have to exist in the very physical world that it predicts. This results in a self-referentiality that yields contradictions, not unlike Turing's undecidability discussed in chapter 8 and Gödel's incompleteness theorems discussed in chapter 9 . In fact, the result is a kind of essential incompleteness that must result from any deterministic model of the universe, similar to Hawking's observation quoted earlier. Binder (2008), in a review of Wolpert's work, observes poignantly,「It is possible, though, that these various theories, along with all that we have learned in physics and other scientific disciplines, will yet merge into the best science can do: a theory of almost everything.」That theory, incomplete as it is, today consists almost entirely of deterministic models.

These debates are fascinating and more philosophical than scientific, but they are largely irrelevant to the engineering use of models. The value of a deterministic logic gate model does not depend at all on whether the sloshing of electrons in silicon is deterministic. It depends only on whether we can build silicon structures that emulate the model with high confidence. We do not need and cannot achieve perfection. As Box and Draper say, all models are wrong, but some models are useful, and logic gates have proved extremely useful.

Although determinism can help predict how a system will evolve in time, I will show in section 10.2 that even a deterministic model may not predict future behavior well. It may be foiled by a phenomenon called chaos; by complexity, where it simply becomes impractical to compute the predictions; or even more simply by an accumulation of error. In such cases, nondeterministic models may become valuable.

Every model has a bounded regime of validity. Newton's laws are accurate at modest speeds and scales, but even a system that is well described by Newton's laws may evolve to be outside the regime of validity of the model. Suppose that we apply a modest constant force for a long time to an object in space. Then Newton's second law predicts that the velocity will grow without bound, eventually exceeding the speed of light, in violation of Einstein's special theory of relativity. Compared with an actual physical system, the error in the model's prediction will become arbitrarily large. Nevertheless, for a reasonably short time horizon, with small forces and large masses, Newton's second law will predict behavior extremely well, and such prediction is valuable. Similarly, Einstein's general theory of relativity explains gravity at large scales, whereas quantum mechanics explains interactions of matter at small scales, and attempts to unify these remain unsatisfactory.

Deterministic models may also be foiled by complexity. A classic example from thermodynamics is the pressure exhibited by a gas in a chamber. This can be modeled as collisions of individual molecules with each other and with the walls of the chamber. In Laplace's day, these collisions would have been governed by Newton's deterministic laws of motion, but such models are intractable. Any attempt to compute the individual motions of even a relatively small number of molecules under such laws of motion would overwhelm even the most powerful computers today. As a consequence, physicists consider these motions to be nondeterministic and rely on the statistics of large numbers of random events to exhibit behaviors that are well modeled deterministically by Boyle's and Charles' laws. The emergence of deterministic models from large numbers of nondeterministic behaviors is a consequence of the law of large numbers , considered in the next chapter. Our ability to model transistors as deterministic switches relies on similar statistical arguments.

Complex behaviors can arise from even simple models, which can exhibit a phenomenon called「chaos.」Interestingly, the inability to make predictions for chaotic models despite their determinism can actually be a valuable property. The technology of encryption, which obscures the content of messages, depends on both determinism (to ensure that the message can be decrypted by the intended recipient) and an inability to make predictions (to protect the message from eavesdroppers). Interestingly, although cryptographers today depend on deterministic models, they hope the physical world is actually nondeterministic and some form of「true randomness」can be tapped to make stronger encryption techniques. True randomness, it turns out, is extremely difficult to achieve.

A nondeterministic model may also lend itself to prediction, but instead of predicting a single behavior, it predicts a family of behaviors. Each behavior in the family is possible. A nondeterministic model of a coin toss, for example, simply states that both heads and tails are possible. A deterministic model of a coin toss remains elusive. Even if we laboriously construct one using some exact model of the shape and material properties of the coin and the surface on which it lands, the predictive value of the model would be poor because even the smallest error in our model could drastically change the outcome of a toss. Despite its uselessness, Karl Popper, high priest of scientific positivism, insists on such a model for the toss of a die:

One sometimes hears it said that the movements of the planets obey strict laws, whilst the fall of a die is fortuitous, or subject to chance. In my view the difference lies in the fact that we have so far been able to predict the movement of the planets successfully, but not the individual results of throwing dice. In order to deduce predictions one needs laws and initial conditions; if no suitable laws are available or if the initial conditions cannot be ascertained, the scientific way of predicting breaks down. In throwing dice, what we lack is, clearly, sufficient knowledge of initial conditions. With sufficiently precise measurements of initial conditions it would be possible to make predictions in this case also. (Popper, 1959, p. 198)

The root of this insistence is a firm belief in the determinism of the underlying physical system. However, predictions can only be made with models, and no such model will be faithful to the physical system in any useful way, so an irreconcilable gap remains here.

Regardless of whether the underlying physical world is deterministic, a nondeterministic model may be augmented with probabilities, which attach a measure of our uncertainty about the outcome. The unfair coin toss considered in chapter 7 , where we expect only 1 of 10 tosses to produce tails, can be modeled with probability 0.1 for tails and 0.9 for heads. I will explore probabilistic models in chapter 11 , but for now let's just focus on deterministic models.

10.2 The Butterfly Effect

Studying atmospheric effects with the goal of being able to predict weather better, Edward Norton Lorenz came to a disheartening conclusion that prediction beyond a few days was simply not possible, despite that his models were deterministic. Working as a research meteorologist at MIT in the early 1960s, Lorenz was among the first to use well-developed mathematical models of convection and thermal effects in fluids to build computer simulations of weather. He noticed, however, that his models would yield radically different behaviors if he started the simulations with minutely different initial states.

[T]wo states differing by imperceptible amounts may eventually evolve into two considerably different states. If, then, there is any error whatever in observing the present state—and in any real system such errors seem inevitable—an acceptable prediction of an instantaneous state in the distant future may well be impossible. (Lorenz, 1963)

People later called this extreme sensitivity to initial conditions「the butterfly effect」after a metaphor put forth in the title of a talk by Lorenz, where the turbulence created in the air by the wing of a butterfly could cause a tornado. The butterfly wing changed the initial conditions just enough to make the difference between the tornado forming and the tornado not forming. The tornado would not have formed had the butterfly not flown.

Since the time of Lorenz's initial experiments, computers, mathematical models, and data gathered about weather have all improved by many orders of magnitude. Yet it is still true that predictions beyond about 14 days of weather patterns like rain and wind are not reliable. Indistinguishable initial conditions can lead to radically different weather.

A common feature of models that have such extreme sensitivity to initial conditions is that their behavior can appear random, capricious even. For this reason, the phenomenon is often called「chaos,」although the models are actually deterministic. Models of fluid flow, as occurs, for example, as air moves around on earth making weather, frequently exhibit chaos. These models can capture the general pattern of behavior of a system but not the details. The turbulence that you feel in an airplane, for example, has the character of highly random motion (see figure 10.1 ). Even the most detailed model will not be able to meaningfully predict that motion.

Even simple models can exhibit extreme sensitivity to initial conditions. Figure 10.2 shows the trajectory of a billiard ball on a table with a fixed circular obstacle in the middle. In this case, a small variation in the angle of the initial path of the ball eventually results in a completely different trajectory around the table. Although the starting trajectory of the ball on the left is almost imperceptibly different between the solid line and the dotted line, a radically different trajectory results.

Lorenz's studies of chaos all involved physical systems operating in a continuum of space and time. It turns out that purely digital systems can also exhibit chaotic behavior. The electrical engineer Solomon Wolf Golomb, whom we encountered in chapter 2 for his famous quote,「You will never strike oil by drilling through the map」(Golomb, 1971), figured out that surprisingly simple digital logic circuits could generate bit patterns that appeared to be random (Golomb, 1967).

I first learned about Golomb's「linear feedback shift registers」in the early 1980s when I was at Bell Labs designing modems, devices to transmit bit sequences over an ordinary telephone channel that had been designed to carry human voice signals not bit sequences. It turns out that modems behave much better on seemingly random bit sequences, where there are no repeating patterns. Golomb had figured a way to make almost any bit pattern look completely random using a simple logic circuit called a「scrambler.」The original bit sequence can be easily extracted using a similar logic circuit called a「descrambler」at the receiving end. I was so impressed with the elegant simplicity (and the Boolean algebra that could be used to analyze these circuits) that I made an oil painting with a scrambler circuit and LED lights embedded in the canvas (see figure 10.3 ). The LED lights exhibit a seemingly random pattern that repeats itself only every 14 hours. The circuit operates reliably to this day.

Figure 10.1

Turbulence in a vortex from the tip of an airplane wing tracked with the help of colored smoke. [Image by NASA Langley Research Center, released into the public domain.]

Pseudorandom patterns were also used in a much more serious art work shown in figure 10.4 , a light sculpture by the American artist Leo Villareal. The sculpture consists of 25,000 LED lights installed in 2013 on the San Franscisco Bay Bridge. The lights are controlled by a computer to create patterns that were designed to never repeat during the entire intended two-year lifetime of the installation.

Golomb's circuits generate pseudorandom bit sequences. They seem random but are not. They produce digital chaos. Pseudorandom bit sequences are central to simulation, computer games, cryptography, and even some art. In computer games, for example, they create an illusion of random things happening, whereas in fact the game is completely deterministic.

Figure 10.2

A billiard table with a fixed circular obstacle in the middle.

Figure 10.3

Pseudo Random, oil, TTL circuits, and LEDs on canvas, 1981, by the author.

Figure 10.4

The Bay Lights, light sculpture by Leo Villareal (2013).

In the early 1980s, Stephen Wolfram noticed a connection between Golomb's circuits and cellular automata. Cellular automata are simple digital models with a rectangular grid of bits, where each bit gets repeatedly updated by computing some logic function of the neighboring bits. A famous example of a cellular automaton is Conway's Game of Life, developed in 1970 by the British mathematician John Horton Conway. Conway's game is an astonishingly simple deterministic closed model that exhibits tantalizingly lifelike behavior. It captured the imagination of many people, including Wolfram, who devoted much of the rest of his career to studying cellular automata and related phenomena.

The game has a rectangular grid of cells that are either alive (shown as black squares) or dead (white squares). An initial state has some cells alive and some dead, as shown in figure 10.5 . At each step of the game, the cells are updated according to the following rules:

Any live cell with fewer than two live neighbors dies.

Any live cell with two or three live neighbors lives on to the next step.

Any live cell with more than three live neighbors dies.

Any dead cell with exactly three live neighbors becomes a live cell.

Conway metaphorically associated these rules with life, where underpopulation, overpopulation, and reproduction could all change the state of a cell. Despite the simple rules, the game exhibits surprisingly complex behavior. As the game proceeds, patterns may become stable, like the Block and Beehive shown in the figure, or they may move across the grid, as in the Spaceship. They can also oscillate between two repeating patterns, and they can exhibit seemingly random, chaotic behavior for quite a long time. It is mesmerizing to watch one of these games play out.

Figure 10.5

A snapshot of Conway's Game of Life.

Conway's game is purely digital, easily realized on a computer. The fact that such simple rules can exhibit such complex behavior inspired Wolfram, who in his 2002 book, A New Kind of Science , concludes that「all is computation」(Wolfram, 2002). More specifically, Wolfram postulates that all natural processes can be constructed out of simple rules, and the complexities arise because of the chaos that such rules can induce. He makes a compelling and engaging case, but of course the ultimate「truth」of such a postulate would depend on digital physics.

Despite chaos, many engineered systems are predictable with high confidence with time horizons of years. Transistors are good examples. Although any detailed model of the underlying motion of electrons in the silicon will be chaotic, the macroscopic behavior of a transistor is simple. It functions as a switch. A gasoline engine in a car is another example. The explosions in the cylinders are highly chaotic, but they reliably deliver controllable power to the powertrain. Harnessing chaos is a key goal of engineering and, if Wolfram is right, of nature as well.

10.3 Incompleteness of Determinism

Laplace believed that nature can be fully described by deterministic models. Wolfram goes further and argues that nature behaves like computational models that are also deterministic. The set of deterministic computational models is much smaller than the set of deterministic models. The set of computational models excludes continuums, for example. So Wolfram's claim is more aggressive than Laplace's.

In both cases, the models may exhibit chaos, so they are capable of describing immensely complex behavior. But the chaos also limits the utility of the models as predictors, making Laplace's demon a difficult concept to accept. Nevertheless, the models are deterministic.

In Laplace's world, time and space are continuums through which objects move. In Wolfram's world, time and space are discrete grids of cells that are updated in a step-by-step fashion. What happens if we assume that the world has both kinds of behaviors, discrete and continuous? I will give a simple example that suggests that in such a world, determinism is incomplete. Specifically, a set of deterministic models that describes the world using a mixture of discrete and continuous behaviors has「holes」in it, situations that should be able to be modeled deterministically but cannot be. To patch these holes, we either have to disallow discrete behaviors altogether, asserting that they do not occur in the physical world, or we have to embrace digital physics and sacrifice almost all known physical models, including relativity and quantum mechanics, both of which model space and time as continuums.

As an example of a model with both continuous and discrete behaviors, consider the collisions of billiard balls, as shown in figure 10.6 . Suppose that the left ball is moving toward the right ball with momentum P and the right ball is sitting still, as illustrated in figure 10.6 (a). Assume that the surface is frictionless, so the momentum of each ball remains constant until a collision occurs. As long as no collision occurs, the behavior is continuous.

Suppose that we model a collision as a discrete event. That is, we assume that the collision occurs in an instant, having no duration in time. Such a model needs to determine the momentum of the balls after the collision as a function of their momentums before the collision.

Figure 10.6

Collision of ideal billiard balls on a frictionless surface.

Assume that the balls are ideally elastic , meaning that no kinetic energy is lost when they collide. In this case, Newton's laws require that both energy and momentum be conserved, in the sense that the total momentum and energy in the balls must be the same after the collision as before. 4 In the physical world, some of the kinetic energy will be converted to heat due to friction, but here we will assume that doesn't happen or that so little kinetic energy is lost that we can neglect it.

Assuming the masses are the same, there are two possible results from the collision that preserve both momentum and energy. One result is that the left ball passes right through the right ball without interacting with it. This outcome could happen, for example, if the ball were actually a neutrino rather than a billiard ball. However, for billiard balls, this outcome is extremely unlikely, so we are justified in rejecting this possibility. The only other result that conserves both momentum and energy is that the balls exchange momentums, as shown in figure 10.6 (c). The left ball is now still, and the right ball is moving away at the same velocity that the left ball was approaching before the collision.

Now suppose that the two balls have different masses. It turns out that in this case, there are still exactly two possible solutions, one where the left ball passes through the right ball and the other where they bounce. Let's again choose the more reasonable solution where they bounce. Because the masses are different, after the collision, both balls will be moving. If the left ball is heavier than the right, then they will both be moving to the right. If the left ball is lighter than the right, then they will be moving in opposite directions. In both cases, their speeds after the collision are uniquely determined by Newton's requirement that both momentum and energy be conserved. Hence, the model is deterministic.

If there are more than two balls, however, then the situation gets much more interesting. Consider a thought experiment where two billiard balls are approaching a third ball from opposite sides, as illustrated here 5 :

Assume that the center ball is sitting still and the two outer balls collide with the center ball at the same time. To keep things simple, let's start with the assumption that all three balls have the same mass. What will happen?

I hope you have enough practical experience with billiard balls that your intuition matches mine. I would expect in this situation that the two outer balls will bounce off the center one and move away from it at the same speed that they had been approaching it. Thus, after the collision, the situation will look like this:

But coming up with a discrete model that predicts this behavior turns out to not be so easy.

A first attempt will be to simply use the same technique that we used with two balls, where the balls exchange momentums when they collide. However, if the collisions are simultaneous, then the left and right balls will exchange momentums with the center ball at the same time, and these momentums will have opposite signs, canceling each other out. Thus, all three balls will suddenly stop. This solution fails to conserve both momentum and energy.

An alternative way to handle this situation is to treat the two simultaneous collisions as a sequence of two-ball collisions with no time elapsing between the collisions. As shown in figure 10.7 (b), when the collisions occur, we can arbitrarily pick one of the collisions to handle first, ignoring the other collision. Suppose we handle the left collision first, ignoring the right collision, as indicated in the figure. The left ball transfers its momentum P to the middle ball and stops. Without time elapsing, we find ourself in state (c) in the figure, where the middle and right balls are traveling toward one another and colliding. Now there is only one collision, so we handle it in (d), leaving us in state (e), where the balls have exchanged momentums. Again, without time elapsing, a new collision occurs, which we handle in (f), leaving us in state (g). After time elapses, we find ourselves in state (h), where the left and right balls are moving away from the center ball, which has not moved and remains still. This behavior is the one we expected intuitively, where the two balls are moving away at equal speeds after the collision.

In this solution, if the masses of the balls are all the same, then it does not matter which of the two collisions we handle first. Here comes the rub. If the masses of the balls are not the same, then the solutions are not the same. If we handle the left collision first, then we get one solution. If we handle the right collision first, then we get a different solution, with all three balls moving at different speeds.

Suppose, for example, that the center ball weighs twice as much as the left and right balls. To be concrete, let's suppose that the center ball weighs two kilograms and the outer balls weigh one kilogram each (these billiard balls are quite heavy, but nice round numbers make the math easier). Suppose that the left and right balls are approaching the center ball at one meter per second so that they collide simultaneously with the center ball. First, notice that this scenario is completely symmetric, and the same intuitive solution works, where the two outside balls bounce off the center one so that after the collision, they are moving away from the center ball at one meter per second, and the center ball remains still. This solution conserves both momentum and energy.

However, this is not the solution we get using the strategy shown in figure 10.7 . In that strategy, we handle the left collision first, and then without time elapsing, we handle the second and third collisions that result. I will spare you the nerd storm, but after the sequence of collisions in the figure, the left ball will be moving to the left at about 0.48 meters per second, the middle ball will also be moving to the left, but more slowly, at about 0.37 m/s, and the right ball will be moving to the right at about 1.22 m/s. If you do the math, you can verify that with this solution, both momentum and energy are conserved. 6 Notice that with this solution, the situation is no longer symmetric, although the starting state was symmetric.

So what happens if we handle the right collision first? In that case, we will end up with the mirror image asymmetric solution, where the middle ball is moving to the right. This solution also conserves momentum and energy.

We now have a real conundrum. We have three possible outcomes: a symmetric one derived intuitively and two mirror-image asymmetric ones derived using the strategy of figure 10.7 . Newton's laws give us no basis for preferring any one of these solutions. All three solutions, and many more, are consistent with Newton's laws. They all conserve momentum and energy. Because there is more than one allowed behavior, Newton's laws (with discrete collisions) result in a nondeterministic model.

How do we know which behavior will match some physical experiment? Here's where things get tricky. To conduct such an experiment, we will have to ensure that the collisions are actually simultaneous. This will be difficult to do (impossible, in fact, given quantum mechanical uncertainty principles). First, suppose that the collisions are not actually simultaneous but are just close in time. That is, one of the two outer balls collides with the center ball just before the other outer ball arrives. In this case, instead of a single collision among three balls, a sequence of collisions occurs between two balls. This makes the problem easier to solve because when only two balls are colliding, only one outcome after the collision is possible that conserves both momentum and energy (barring the tunneling outcome, where the balls pass through one another). Consequently, if the collisions are not simultaneous, the model remains deterministic. Only one final behavior is allowed by the model.

If the masses of the balls are different, then the behavior is different if the left ball collides first than if the right ball collides first. This will be true no matter how small the time is between collisions. Let's call the time of the left collision t L and the time of the right collision t R . Let the time difference be d = t L − t R . Consider a sequence of experiments where d is always positive (the right ball always collides first), and d approaches zero, getting smaller and smaller. As d gets close to zero, there will be little difference between the outcomes of these experiments. Changing d from, say, one nanosecond to 0.5 nanoseconds will not change the outcome much. Eventually, as d gets small, there is no significant difference between the experimental outcomes, so the sequence of experiments seem to be converging to behavior that should be the behavior when d = 0. 7 It would seem that the limiting behavior should be the one unique behavior when the collisions are simultaneous.

Figure 10.7

One of two orderings for handling collisions among three balls.

However, it is not. If we repeat the same sequence of experiments, but this time we require d to always be negative, then again we get a sequence of behaviors that are closer and closer together, and they appear to be converging to a behavior, but they do not converge to the same behavior as the previous sequence of experiments.

In the limiting case, when d reaches zero, the collisions become simultaneous. At this point, the behavior will depend on which sequence of experiments we are conducting, the one where d > 0 or the one where d < 0. These two sequences of experiments converge to different behaviors as d approaches zero. When the collisions become simultaneous, we have no basis for choosing between these two possible limiting cases, so we have to assume they are both possible. They both conserve momentum and energy.

In the single unique experiment where d is exactly zero, there is more than one possible outcome from the model, so the model is nondeterministic. However, if d is not zero, no matter how small it is, then the model is deterministic. A mathematician would call the set of all these deterministic models incomplete because this set does not contain its own limit points. In the limit, when d exactly hits zero, the model becomes nondeterministic. The set of deterministic models has a hole at exactly the point where d = 0.

Note that it will not do to just disallow d = 0 because to do so we would have to disallow t 1 and t 2 to vary smoothly. Assuming time is a continuum, as nearly all current models of physics do, t 1 can cross t 2 , in which case there is a point where t 1 = t 2 , and hence d = 0. Note that this point is even harder to avoid if we quantize time, as required by digital physics. However, we don't want to do that anyway because doing so sacrifices almost all of modern physics, including Newton's laws, the Schrödinger equation, and Einstein's relativity.

I argued in section 9.3 that a close approximation of a process does not have the same properties as the process unless no significant difference is found between a continuum and a countable set. This billiard ball thought experiment reinforces that arbitrarily close approximations can in fact be quite different from the real thing. If the「real thing」is simultaneous collisions, then this thought experiment shows that all arbitrarily close approximations to it are deterministic, but the real thing is not. Moreover, we can make two scenarios, one with d < 0 and one with d > 0, that are arbitrarily close to one another in all their parameters but that exhibit wildly different (but still deterministic) behaviors. We are forced to conclude that coming arbitrarily close, in「arbitrarily fine detail」in the words of Deutsch, does not achieve the「real thing.」In this billiard ball experiment, all close approximations of simultaneous collisions are deterministic, but the actual simultaneous collisions are not.

Any model for this physical system that treats the collisions discretely will suffer this problem. What exactly does it mean to treat the collisions discretely? In this case, it means that the model talks about time before and after the collision, but the collision itself does not occupy any time. It occurs instantaneously. An instant before the collision, we have a certain energy-momentum arrangement, and an instant after, we have another energy-momentum arrangement, and the model only requires that total energy and momentum are conserved.

An alternative is to not treat the collision as a discrete event. Instead of being instantaneous, it takes time. The collision starts when the molecules of the balls are close enough to begin to affect one another, and it ends when they have moved far enough apart that they no longer significantly affect one another. We can build this kind of model using either classical mechanics (Newton's laws) or quantum mechanics (using the Schrödinger equation) to describe the continuous evolution of a wave function. Both approaches will yield a model that is deterministic but extremely sensitive to initial conditions, and therefore chaotic.

The nondiscrete classical mechanics solution is reasonably easy to understand. Suppose the balls are ever so slightly springy. That is, when the molecules of one ball get close enough to those of the other ball to start interacting, the molecules of the balls get squished together, like a spring being compressed. The balls slow down. The spring compression temporarily converts kinetic energy to potential energy, so energy is still conserved. As the springs that are the balls compress, the motion of the balls slows until the balls stop. The springs then start to decompress, converting the potential energy back to kinetic energy and pushing the balls apart. A model like this is deterministic, but it is extremely sensitive to the initial positions, speeds, and springiness of the balls. If those are slightly off, then a radically different behavior will result.

So we have a choice. We can either accept a discrete model of the collisions, in which case we lose predictability to nondeterminism, or we can reject discreteness, constructing a detailed model of molecular interactions, in which case we lose predictability to chaos. In both cases, we lose predictability. The model that accepts discrete collisions is much simpler than the one that models molecular interactions, so it seems like the simpler model that admits nondeterminism is the better choice.

10.4 The Hard and the Soft of Determinism

Determinism focuses our attention on a single behavior, a single reaction, a single「right answer」to the question of how a system will react to a stimulus. As an intellectual tool, it is valuable. Being able to identify the「right answer」is essential to Popper's principle of falsifiability in science. An experiment falsifies a theory if its behavior deviates by more than measurement error from the right answer predicted by the theory.

In the complementary engineering use of models, a deterministic model defines the「correct behavior」of a system. Any physical system that deviates significantly from that correct behavior is a flawed implementation of the model. A clear definition of correct behavior is a principle that underlies all of digital technology. The very notion of「digital」discretizes the messy physical world, unambiguously differentiating zero and one, yes and no, right and wrong. It is the ultimate of Serres' hard versus soft.

However, we have to carefully avoid the tarpit that results when we conflate the map with the territory. Determinism is a clear and unambiguous property of models but a muddy and treacherous property of physical systems. Nearly all fundamental models in physics are deterministic, and the question remains open whether intrinsically nondeterministic behaviors are found in the physical world. This question is philosophical more than technical because any nondeterministic model of the physical world may simply reflect an unknown unknown, some hidden variable that determines the outcome but remains invisible to us. 8 Because our knowledge of the physical world can never be complete, as observed by Hawking, we can never definitively assert that the physical world is nondeterministic. Hidden variables or unknown laws of physics may later reveal themselves as our technology and ability to measure and observe the physical world improve.

Regardless of whether the physical world is deterministic, humans have learned to build physical devices, transistors, that exhibit behavior that is extraordinarily faithful to a discrete, digital, deterministic model. The transistors in your laptop computer can switch on and off billions of times per second and operate for years without deviating from the correct behavior defined by a deterministic model. Such fidelity to a deterministic model is unrivaled by any other human-created artifact.

Thus, determinism in models is valuable, but this does not in any way imply that engineers should forgo nondeterministic models. In fact, nondeterminism is an essential tool for overcoming the limitations of determinism. For example, if a deterministic model is too complex to analyze, a nondeterministic model may be more valuable. A nondeterministic model exhibits multiple behaviors, but if all of these behaviors are demonstrably acceptable, then a nondeterministic model is just as good for building confidence in a design.

Nondeterminism has also played a central role in the theory and practice of computer science, even though Turing machines and algorithms are usually deterministic. Concurrent software, where multiple programs execute simultaneously, can be extremely difficult or even impossible to model deterministically, for example.

Nondeterminism even plays a central role in one of the key open questions in computer science: whether P equals NP. This question was apparently first raised in a 1956 letter written by Kurt Gödel to John von Neumann. The question is whether it is always true that a problem where the solution is easy to check is also easy to solve. The「N」in「NP」stands for「nondeterministic,」but it would take us too far afield to fully explain this question here. If you are interested, Wikipedia has a nice article.

In science, the models of a theory must predict the behavior of a physical system, or else the theory is not falsifiable and therefore not scientific, according to Popper. However, deterministic models do not necessarily yield predictability. Deterministic models are often extremely sensitive to initial conditions, exhibiting chaos. Even purely discrete, digital, and computational deterministic models can exhibit extremely complex chaotic behaviors.

In engineering, in contrast to science, predictability may be less important than repeatability. Repeatability ensures that an engineered system will behave as designed, with high confidence. This is valuable even if the design is able to exhibit behaviors that are too complex to predict.

According to Wolfram, the ability that digital technologies have to exhibit extremely complex behaviors justifies a belief in a「new kind of physics」that models the physical world in a purely discrete, computational way. I have argued in chapter 8 that I find it extremely unlikely that nature limits itself to the small set of processes that are computational, but even if it doesn't, computational models of the physical world are still valuable. Simulation models of weather, for example, are deterministic, chaotic, discrete, and computational, and nevertheless do a remarkable job of predicting weather for at least a few days into the future. Even if their predictive value is limited, these models lend enormous insight into the processes of nature, even if the mechanisms of the model do not match the mechanisms of nature.

Perhaps more disturbing than chaos is the incompleteness of deterministic models. Any modeling framework rich enough to include Newton's laws of motion, if it also admits discrete behaviors, is incomplete. Therefore, the family of deterministic models has「holes,」limiting cases that cannot be modeled deterministically. The philosophical implications of this observation seem profound. It implies that models of the physical world that admit both discrete and continuous behaviors must also admit nondeterminism.

Both chaos and nondeterminism limit the predictive value of models. It appears that both chaos and nondeterminism are unavoidable. Because our ability to predict is limited, our vision of the future will always remain uncertain. Any rich enough modeling framework for practical usage is going to have to deal with uncertainty. In the next chapter, I examine how to confront and manage uncertainty.

__________

1 Author and neuroscientist Sam Harris, in his short 2012 book Free Will , argues that even without determinism, free will does not exist. His argument is quite compelling but off topic for this book.

2 For the position of a particle, this is actually a probability density not a probability. The probability of finding the particle at any specific point in space and time is zero (see chapter 11 ).

3 For a concise summary of the various interpretations that have been put forth, see Hoefer (2016). For a more in-depth study of determinism in physical models, Earman's Primer on Determinism remains a good analysis of determinism in various physical theories (Earman, 1986).

4 The momentum of a ball is the product of its velocity and its mass. The energy of a ball is half of the product of the mass and the square of the velocity.

5 The details of this thought experiment are given in Lee (2016). I will spare you the nerd storm here, but if you want to check the conclusions, please see that article. Penrose (1989) also used multiball collisions to show that the notion of determinism even in classical mechanics is problematic. His examples are a bit more complicated because they occur in more dimensions.

6 The total momentum in the system after these collisions is −0.48 × 1 − 0.37 × 2 + 1.22 × 1 = 0, same as the starting momentum. The total energy in the system after the collisions is ((−0.48) 2 × 1 + (−0.37) 2 × 2 + (1.22) 2 × 1)/2 = 1, same as the starting energy.

7 Technically, such a sequence of experiments, where the difference between them becomes vanishingly small, is called a Cauchy sequence, after the French mathematician Augustin-Louis Cauchy. A space of models is said to be complete if every Cauchy sequence in the space converges to a model in the space. This space of deterministic models is incomplete, as proved in Lee (2016).

8 In quantum mechanics, Bell's theorem, named after the Irish physicist John Stewart Bell, uses quantum entanglement to rule out hidden variables as the source for experimentally observed randomness in quantum systems. Specifically, these experiments show that taking a measurement at one point in space can instantly affect the outcome of another experiment at a remote location, seemingly in violation of the speed-of-light limits on communication. Einstein called this property of quantum physics「spooky action at a distance.」Interestingly, although Bell's theorem seems to indicate that randomness in the physical world is real, an equally explanatory resolution is that the world is actually deterministic in an extremely strong way, where every particle carries with it since its inception all the outcomes of all possible measurements that might ever be taken on it any time in the future. Hence, even this result does not definitively prove that randomness is real in the world.

11

