Shadows on the Wall

· · · in which I examine the very idea of「facts」and「truths,」showing that: collective wisdom about them can be better than individual wisdom; a narrative about facts can be more interesting than the facts themselves; facts and truths may be invented or even designed, not just discovered; facts and truths may be wrong; and it can cost billions to show that facts are true. And, oh yes, nerds are misunderstood, and science and engineering get confused.

1.1 Nerds

I am a nerd. According to the Merriam-Webster dictionary, a nerd is

an unstylish, unattractive, or socially inept person; especially: one slavishly devoted to intellectual or academic pursuits.

a person who is very interested in technical subjects, computers, etc.

Who but a nerd would start a book with a quote from the dictionary? I wouldn't expect a nerd to write very well, particularly not for a general audience. Actually, I'm quite sure this book would be much better if it were written by someone else. But I can't get anyone else to write it, so I will compensate by quoting the writing of others, even from dictionaries.

The previous definition was presumably written by a trustworthy expert on the subject of nerdiness. We expect that the publishers of dictionaries go to some effort to ensure that the definitions are written by experts. In contrast, we cannot assume that a Wikipedia page about nerds would be written by experts. Anyone with Internet access can modify the contents of a Wikipedia page. There is no vetting of expertise. Nevertheless, the page for「nerd」largely concurs with Merriam-Webster but offers more:

Though originally derogatory,「Nerd」is a stereotypical term, but as with other pejoratives, it has been reclaimed and redefined by some as a term of pride and group identity.

Now I am reassured that I can be proud to be a nerd. Continuing with an etymology,

The first documented appearance of the word「nerd」is as the name of a creature in Dr. Seuss's book If I Ran the Zoo (1950), in which the narrator Gerald McGrew claims that he would collect「a Nerkle, a Nerd, and a Seersucker too」for his imaginary zoo. [citations to Merriam-Webster and the American Heritage Dictionary] The slang meaning of the term dates to the next year, 1951, when Newsweek magazine reported on its popular use as a synonym for「drip」or「square」in Detroit, Michigan…. At some point, the word took on connotations of bookishness and social ineptitude.

I like this narrative better than the dictionary definition because it focuses on how the word came about and how it evolved rather than what it is. Most facts are more interesting when we understand how they came to be facts rather than just accepting them as if they were always there.

But is this Wikipedia article authoritative? The article points out that American satirist「Weird Al」Yankovic's song「White and Nerdy」states that editing Wikipedia is a stereotypical nerd interest. I am therefore reassured that this article is likely written by experts on nerdiness.

There is a big difference in style between a dictionary definition and a narrative about culture. A dictionary definition is usually understood to give a fact, a truth. Merriam-Webster defines「definition」as「an explanation of the meaning of a word, phrase, etc.」This definition gives definitions the aura of facts and truths, deemphasizing their instability and fluidity with human culture.

Most of us approach technology as if it too were a compendium of facts and truths. We assume that technology advances because people discover more facts and truths. Because the discovery of facts and truths is the realm of science, science therefore drives technology. But I doubt that the technology of Wikipedia came about as a consequence of the discovery of facts and truths.

Wikipedia is a software system created by Jimmy Wales and Larry Sanger, who put the first version online in 2001. I don't know them, but one of my prejudices is that many software people are nerds, so there is a reasonable chance they too are nerds.

According to the Wikipedia article on Wikipedia, Sanger coined its name as「a portmanteau of wiki and encyclopedia.」Following the link to the page on「wiki,」we learn that a wiki is a website that「allows collaborative modification of its content and structure directly from the web browser.」That page tells us that「wiki」is a Hawaiian word meaning「quick」and credits Ward Cunningham with inventing the wiki. I hope you will agree that it would be odd to say that Cunningham「discovered」the wiki, so presumably this was not an advance creditable to science. But the nuanced relationship between discovery and invention and between science and engineering is not always so clear.

Cunningham's 2001 book with Bo Leuf describes the wiki concept as follows:

A wiki invites all users to edit any page or to create new pages within the wiki Web site, using only a plain-vanilla Web browser without any extra add-ons. Wiki promotes meaningful topic associations between different pages by making page link creation almost intuitively easy and showing whether an intended target page exists or not. A wiki is not a carefully crafted site for casual visitors. Instead, it seeks to involve the visitor in an ongoing process of creation and collaboration that constantly changes the Web site landscape. (Leuf and Cunningham, 2001)

I love that web browsers come in「plain vanilla」flavors. I wonder what other flavors are available.

This description starts to give us the sense that a wiki, and particularly Wikipedia, is as much a cultural artifact as a technological one. And as with all cultural artifacts, it didn't suddenly pop into existence at the instant of invention. In fact, inventions almost never do and nearly always have a strongly cultural element. The「meaningful topic associations between different pages」were in fact already present in the World Wide Web, which according to its Wikipedia article was「invented by English scientist Tim Berners-Lee in 1989.」

It is interesting that this article identifies Sir Timothy John Berners-Lee (he was knighted by Queen Elizabeth II for his work) as a「scientist.」His recognized contributions were certainly not of the nature of discovery of facts and truths and certainly not about the natural world, the main focus of science. Berners-Lee did get a bachelor of arts degree in physics, unquestionably a science subject, from Oxford, so I suppose calling him a scientist is justified. But I see no evidence that he is a successful scientist.

Or maybe I am misunderstanding what it means to be a scientist. Returning to trusty old Merriam-Webster, a scientist is「a person who is trained in a science and whose job involves doing scientific research or solving scientific problems.」Hmm… Not very helpful. Looking up「science,」we find it is「knowledge about or study of the natural world based on facts learned through experiments and observation.」By this definition, if Berners-Lee's career goal was to study the natural world, then I would have to conclude that his career has not (yet) been very successful. If, in contrast, his career goal was to invent and engineer artifacts that had never before existed, then he has been spectacularly successful, richly deserving the knighthood. He created mechanisms that are today used by nearly every person in the developed world. He changed the world.

Berners-Lee's contributions are arguably more cultural than technical. The cultural context of the web and Wikipedia goes back even further. Vannevar Bush, 1 in a 1945 article「As We May Think,」states,

Wholly new forms of encyclopedias will appear, ready-made with a mesh of associative trails running through them, ready to be dropped into the memex and there amplified. (Bush, 1945)

The memex is Bush's hypothetical microfilm viewer that has a structure analogous to that of hypertext, the essential feature of Berners-Lee's web. Berners-Lee's technical contribution was to make Bush's vision a reality.

So why is Berners-Lee identified as a scientist? Possibly whoever wrote the Wikipedia article intended this as an honorific in the sense that「engineer」would not be. In his wonderful book, The Black Swan , from which I get the title of this book, Nassim Taleb used the term「Platonicity」for the「desire to cut reality into crisp shapes」(Taleb, 2010). My classification of people, including myself, as「engineers」or「scientists」(or even as「nerds」) stems from such Platonicity. But humans are complex and defy classification. You may be surprised that I, a nerd, am also an amateur artist (see figure 1.1 ).

Taleb argues that Platonicity, the desire to categorize, the obsessive focus on taxonomy,「makes us think that we understand more than we actually do.」

What I call Platonicity, after the ideas (and personality) of the philosopher Plato, is our tendency to mistake the map for the territory, to focus on pure and well-defined「forms,」whether objects, like triangles, or social notions, like utopias (societies built according to some blueprint of what「makes sense」), even nationalities.

The arbitrariness of categories such as「scientist」and「engineer」is an example of Platonicity. It makes us sanguine in our understanding of the world, but it can be misleading. In the rest of this chapter, I will focus on the difficulties in distinguishing discovery from invention, invention from design, and scientist from engineer.

Figure 1.1

Self-portrait of a nerd. Acrylic on canvas (2007).

1.2 Artificial and Natural

Herbert Simon, a hugely influential twentieth-century thinker and winner of both the Turing Award in computer science and the Nobel Prize in economics, in his book, The Sciences of the Artificial , makes a distinction between「artificial」and「natural」phenomena:

The thesis is that certain phenomena are「artificial」in a very specific sense: they are as they are only because of a system's being molded, by goals or purposes, to the environment in which it lives. (Simon, 1996)

A system is artificial if it is「being molded, by goals or purposes.」But in this statement, who does the molding? Simon presupposes it is humans. It could instead be God or some other teleological cause, and then no distinction would exist between the「sciences of the natural」and the「sciences of the artificial.」One could even take as a definition of God as He who molds, by goals or purpose, our entire natural world. But there is a distinction between the artificial and the natural. A big one, Simon says.

Simon's examples of artificial phenomena include political systems, economies, engineered artifacts, and administrative organizations. The「molding」of such systems「by goals or purposes」is the process of design .

Everyone designs who devises courses of action aimed at changing existing situations into preferred ones. (Simon, 1996)

Engineering , as a discipline, is fundamentally about design in this sense. Wikipedia, which as you may have realized by now, is my first recourse for many research problems, defines engineering this way:

Engineering is the application of mathematics, empirical evidence and scientific, economic, social, and practical knowledge in order to invent, innovate, design, build, maintain, research, and improve structures, machines, tools, systems, components, materials, and processes. (retrieved March 1, 2016)

It then points out the obvious,「the discipline of engineering is extremely broad,」and gives the origin of the term:

The term Engineering is derived from the Latin ingenium , meaning「cleverness」and ingeniare , meaning「to contrive, devise.」

Note that the word is not derived from「engine,」as many people might assume. Instead,「engine」is derived from the same Latin roots.

Wikipedia effectively leverages recent triumphs of engineering. But is it authoritative? It creates collective wisdom, subjugating the role of individual experts, If you are old enough, you may remember the encyclopedias of the twentieth century, such as the Encyclopedia Britannica . According to Wikipedia, the Britannica

is written by about 100 full-time editors and more than 4,000 contributors, who have included 110 Nobel Prize winners and five American presidents.

The Britannica is built in a very different way than Wikipedia. The editors recruit top experts to contribute to articles. The focus is on the individual experts, who, through their reputation, lend authority to the text.

Not Wikipedia. Anyone can edit a Wikipedia page. So how can these pages have any authority? At one level, Wikipedia replaces authority with accountability. Figure 1.2 shows the edit history of the Wikipedia page for Engineering quoted earlier. Notice near the bottom of the figure the most recent edit of this page, on February 25, 2016. This edit is annotated with the comment,「Reverting possible vandalism.」Indeed, the previous edit, which was made less than a minute earlier, has the comment,「This is all a lie.」That edit removed quite a bit of text, including the previous definition of Engineering, and replaced it with,「This is al a lie」[sic]. The entire history of these edits is accessible on the Wikipedia site.

Figure 1.2

Edit history for the Wikipedia page on Engineering, retrieved March 1, 2016.

So who reversed the vandalism? The user is identified as「ClueBot NG.」Clicking on that name reveals the page shown in figure 1.3 . It turns out that ClueBot NG is a「bot,」which Wikipedia defines as「a software application that runs automated tasks (scripts) over the Internet.」On the page in figure 1.3 , listed as item 8, is a description of the「Vandalism Detection Algorithm,」which evidently is a piece of software that classifies an edit as either「vandalism」or「not vandalism,」and if it is vandalism, it reverses the edit. The methods used to perform classification are statistical machine learning methods, linchpins of the currently hot area of data science. I will examine how this mechanism is analogous to an immune system and why that is important in chapter 9 , and I will explain the principles behind how it works in chapter 11 .

Figure 1.3

User page for the most recent editor shown in figure 1.2 , retrieved March 1, 2016.

It is worth pausing and reflecting on how profoundly different all of this is compared with a twentieth-century encyclopedia. I will have more to say about this later, but one of the trends of the twenty-first century is the subjugation of the individual expert, the high authority, the intellectual hero. In the twentieth century, the phrase「triumphs of modern physics」would evoke in our minds Einstein, Bohr, Schrödinger, Heisenberg, and a few others. Of course, many others contributed, many of them also recognized as heroes. Experts do contribute to Wikipedia articles, but their text may be modified and elaborated on by anyone, including vandals. And readers rarely check to see who wrote the text. The authority of the author seems to be irrelevant. The text reflects a collective wisdom, not an individual one.

We now face an interesting conundrum. Which is closer to the truth, the collective wisdom or the individual one? This question gets ensnarled by what we mean by「truth.」The answer is different if truths can be created rather than just being discovered.

1.3 Design and Discovery

In contrast to Simon's「sciences of the artificial,」the「sciences of the natural」study what nature has given us. The goal is to uncover the「secrets of nature,」presupposed to exist disembodied, independent of humans. These secrets occupy neither time nor space; they do not come into existence when they are discovered, occupying a time, nor do they exist at the place they are discovered.

The idea that these secrets exist disembodied dates back at least to Plato, who postulated ideal「Forms,」objective and eternal truths that are impossible to know completely. Plato states that these Forms are the only objective truths, and that the ultimate goal of a「philosopher」(a lover of knowledge) is to understand these forms. The Forms represent the most accurate reality, and Plato calls knowledge of them「the Good.」

Plato's Allegory of the Cave (see figure 1.4 ) suggests that human perception of reality is always imperfect. In the allegory, prisoners are chained with their backs to a low wall (bottom right in the figure). Their heads face the blank wall of the cave on which shadows are cast (top right) from a fire (top center). The shadows are of puppets and figures manipulated from behind the low wall, and the shadows on the cave wall constitute the only knowledge of reality that the prisoners experience. If a prisoner is released and can face the fire creating the light, then he will resist accepting the reality of the fire or the puppets casting shadows. If the prisoner is further allowed to exit the cave, then he will be blinded by the sun and further resist the reality of that external「ideal」world. And if a prisoner does accept some of the truths he is exposed to and tries to convey those truths to the prisoners behind the low wall, then they will reject his ideas as absurd.

Figure 1.4

Plato's Allegory of the Cave by Jan Saenredam, 1604. [ c The Trustees of the British Museum.]

The allegory underscores the difficulty of achieving the Good and explains why those individuals who do successfully convince others of some newly discovered, seemingly objective truth, the Einsteins and Bohrs of the world, are eventually deemed to be heroes.

Simon contrasts these objective truths with those of artificial phenomena.「If natural phenomena have an air of ‘necessity' about them in their subservience to natural law, artificial phenomena have an air of ‘contingency' in their malleability by environment.」Their「subservience to natural law」presupposes a Platonic, disembodied existence of that natural law, regardless of whether it can be known.

A contrasting view is that laws of nature are models, created by humans, for how the physical world works. This view has gained considerable currency in the last few decades, perhaps beginning with Thomas Kuhn's groundbreaking and controversial 1962 book, The Structure of Scientific Revolutions , which postulated that scientific theories are framed by「paradigms,」which are very much human ways of thinking about the world.

In either case, calling natural laws「laws」is a bit odd. It's almost as if nature is required to follow them, as citizens are required to follow the laws of a state. But what happens when nature violates a law of nature? Nature is not punished! Instead, the law becomes invalid. Imagine if a state worked that way. Each time a driver exceeded a speed limit, the speed limit would become invalid. But what about「laws of nature」? How can an ideal truth become invalid? And yet we've seen laws of nature become invalid many times.

David Deutsch, a British physicist at Oxford and a pioneer of quantum computing, in his 2011 book, The Beginning of Infinity , argues that science is more about「good explanations」than about laws of nature. Deutsch attributes these explanations to humans rather than some preexisting disembodied truth:

Discovering a new explanation is inherently an act of creativity. (Deutsch, 2011, p. 7)

A「law of nature,」in contrast, has a more humble role, that of codifying a connection between the past and the future:

[A]ny purported law of nature — true or false — about the future and the past is a claim that they「resemble」each other by both conforming to that law. (Deutsch, 2011, p. 6, emphasis in the original)

Deutsch goes further to reject empiricism, arguing that neither laws of nature nor good explanations are derived from observation of the physical world:

Experience is indeed essential to science, but its role is different from that supposed by empiricism. It is not the source from which theories are derived. Its main use is to choose between theories that have already been guessed. (Deutsch, 2011)

Here Deutsch echoes the thesis put forth earlier by the Austrian-British philosopher of science Karl Popper (1902–1994), who stated that these guesses, hypotheses about nature, arise from「creative intuition」and can only be tested empirically after they have been advanced (Popper, 1959). Deutsch points out that most of reality in the physical world is not directly observable to any human being. Black holes, quarks, and nuclear fusion in the sun involve scales, forces, and temperatures that no human has ever experienced. They are just as inaccessible as Platonic Forms, observable at best only as shadows on the wall. It cannot possibly be observations of black holes that lead to our theories about them, because we cannot observe them.

Plato recognized that the ideal truths of Forms could not be fully known by humans. But because they cannot be fully known by humans, isn't it more practical to view what we do know about nature as human-constructed models or what Deutsch calls good explanations? This would be more humble, tacitly acknowledging that even our most fervently held beliefs about nature are subject to improvement. I'm not saying that there are no truths, but just that we should always be required to question them.

Such humility is intrinsic in the Wikipedia model of collective wisdom. No individuals, no authorities, no matter how many accolades follow their names, can be trusted as the holders of「the truth.」Knowledge should and must evolve. And as knowledge evolves, doesn't「the truth」also evolve? Of course, a realist might reply that it is belief not knowledge that evolves. For that realist, if a belief isn't true, it isn't (and never was) knowledge.

Deutsch points out that deference to authority was replaced during the enlightenment by empiricism, where ideas and theories are based on testing and experience. But he argues that experience alone is insufficient because so much of the physical world operates in conditions that cannot support human life and therefore cannot be directly experienced.

Empiricism never did achieve its aim of liberating science from authority. It denied the legitimacy of traditional authorities, and that was salutary. But unfortunately it did this by setting up two other false authorities: sensory experience and whatever fictitious process of「derivation,」such as induction, one imagines is used to extract theories from experience. (Deutsch, 2011)

Again echoing Popper, he argues that rather than deriving knowledge from experience, knowledge comes from a fundamentally creative process of conjecture, followed, often much later, by cleverly devised experiments that support the guesses (usually by failing to falsify them). Such clever experiments usually observe only indirect side effects. In other words, knowledge is engineered.

The next chapter will focus on the role that models play in both science and engineering, but for now suffice it to say that there is a tension here between design and discovery . Some artificial phenomena are not explicitly designed but rather emerge accidentally from human activity. The field of economics, for example, is full of the study of such emergent phenomena. The rules by which these artificial phenomena operate are arguably discovered rather than designed. And although many other artificial phenomena are designed, surely they too are subject to natural laws, which in the Platonic view at least must be discovered. But our knowledge of the natural laws is imperfect, so we must construct models of those laws using mathematics, for example. Are not these models designed?

Sir Isaac Newton's laws of motion are now known to be violated by nature, in that they fail at relativistic speeds and quantum scales. Those laws take the form of mathematical formulae, such as Newton's second law, 2

which states that force equals mass times acceleration. This looks like an expression of a Platonic Form, but it is wrong! Despite being wrong, we don't hesitate to say that Newton「discovered」it, and people would look at us funny if we said that Newton「invented」it.

For some phenomena, we do not hesitate to use the word「invention」for their discovery. Consider the transistor, credited to John Bardeen, Walter Brattain, and William Shockley of Bell Labs (see figure 1.5 ). 3 They got the 1956 Nobel Prize in Physics「for their researches on semiconductors and their discovery of the transistor effect.」They demonstrated the transistor effect with a device made of gold and germanium, although today transistors are realized mostly using silicon crystals with carefully introduced impurities called dopants. Note the careful phrasing in the Nobel Prize citation,「their discovery of the transistor effect.」Nobel Prizes are not issued for inventions, only for discoveries. Yet most of us would say that the transistor was invented at Bell Labs in the 1950s.

But actually, a U.S. patent was awarded to Julius Lilienfeld in 1930 for the 1925 invention of a type of transistor now called a field-effect transistor (FET) (Lilienfeld, 1930). So if the transistor effect is a Platonic Form, then it was actually discovered earlier by Julius Lilienfeld. But patents are not issued for discoveries, only for inventions. According to the U.S. Patent and Trademark Office, you cannot patent「laws of nature, natural phenomena, and abstract ideas.」A patent may be issued for a「novel use」of a law of nature, of course, because presumably everything is subject to the laws of nature. Such a novel use is an invention, not a discovery.

To be fair to the Nobel Prize committee, Lilienfeld's FET was actually significantly different from the one developed by Bardeen, Brattain, and Shockley. The Bell Labs transistor was of a type known today as a bipolar transistor. Interestingly, today, bipolar transistors are used only in niche applications because of their significantly higher energy requirements. FETs are used much more commonly, having become the workhorses for digital technology. An iPhone today contains billions of FETs.

Figure 1.5

John Bardeen, Walter Brattain, and William Shockley, who received the 1956 Nobel Prize in Physics for the discovery of the transistor effect.

It is not uncommon for inventions and discoveries to be messy in this way. The intellectual history of an idea is rarely clear, and yet, as a culture, we insist on singling out the「heroes」who bring these ideas to the fore. 4

The tension between discovery and invention is not new. Is the transistor effect a「natural phenomenon」that has always existed, waiting to be discovered? As far as I know, nobody has ever found in nature sandwiches of gold and germanium or doped silicon operating as transistors. So transistors must not be natural phenomena. But when these sandwiches are constructed by man, nature takes over and regulates the movement of electrons in the material so as to realize the transistor effect. So a transistor appears to be a novel use of a law of nature.

But the movement of electrons in crystalline materials with impurities was not well understood until the transistor had been fabricated and studied; in fact, the phenomenon continues to be better understood to this day under more study. Is a natural law that had no manifestation in nature prior to a human-constructed invention, and that was not understood as a natural law until after such construction, a Platonic Form? It seems to me questionable to claim that the transistor effect exists and has always existed, timeless and disembodied. To me, this stretches my understanding of the word「exists」beyond the breaking point.

This debate about natural laws dates back a long time. Aristotle, a student of Plato's, questioned the Platonic ideal Forms, arguing that knowledge is based on the study of particular things, and that generalizations arise from that study rather than preexisting in a disembodied Form. Aristotle used the term「natural philosophy」for the study of phenomena in the natural world, what we now call「science.」Aristotle's world of facts is extensible; it can grow with study of the natural world. Plato's world of facts is fixed; all the facts are there as Forms, with many of them still waiting to be discovered.

But it seems that facts can become wrong. Despite being wrong, Newton's second law is amazingly useful. Engineers use it all the time to design cars, airplanes, robots, bridges, toys, and so on. It provides a good model for how particular things behave, as long as they are not traveling near the speed of light and are not being examined at subatomic scales. It cannot be a Platonic Form because it is violated by nature. It can, however, constitute knowledge in the Aristotelian sense because it generalizes nicely the behavior of macroscopic objects.

1.4 Engineering and Science

Simon says that design is about「changing existing situations into preferred ones.」But what do we mean by「preferred」situations? In political systems, this may be highly subjective. In engineered systems, it may be much more objective. A political leader may prefer a situation where all immigrants are kept out, even when there is no objective evidence that this makes anything better for anyone. Engineers, by contrast, are often called on to defend their preferences with objective measures, such as lower cost or reduced energy consumption. Simon's「preferred situations」are open. But it is not uncommon in popular culture to assume that engineers primarily optimize preexisting designs. A somewhat silly joke underscores this point:

Question : What is the difference between an optimist, a pessimist, and an engineer?

Answer : An optimist sees a glass half full. A pessimist sees a glass half empty. An engineer sees a glass that is twice as big as it needs to be.

This joke plays on our preconception that engineers prefer whatever costs less. Many engineered systems, however, are「preferred」despite lacking any objective measures showing them to be better than preceding「existing situations.」Apple's iPhone, for example, did not make phone calls better than its Nokia predecessors, and its battery life was distinctly shorter. And it certainly didn't cost less! It was「preferred」because of nonobjective properties. It was fundamentally a creative contribution to humanity, not an optimization. And yet it was most certainly an engineered artifact.

When using the phrase「sciences of the artificial」for the creation and study of human-made phenomena, Simon laments the pejorative connotations of the term「artificial,」saying「our language seems to reflect man's deep distrust of his own products.」Arguably, distrust of the products of nature is equally justified, as suggested by the poem 「Unnatural」 by Stephen Dunn. But as a father of teenagers with iPhones, I can attest that distrust of the artificial is real. Trusted or not, there is no question that smartphones are transformative. The iPhone (and its subsequent competitors), together with other recent innovations in wireless communications and computer systems, enable us to carry nearly all of human published information in our pockets. To call this「transformative」seems like an extraordinary understatement. It is much more a triumph of engineering,「the sciences of the artificial,」than of the sciences of the natural. Yet there is little, if any, invention in the iPhone. Nearly every important aspect of the phone existed already in other products when it was introduced. The iPhone is much more the result of design than either invention or discovery.

UNNATURAL

I'm sure Nature has disapproved of me for years, as if it had overheard one of my silent screeds against it, and my insistence that only the artificial has a real shot at becoming more than we started with, designed, revised, something completely itself. If it could speak, Nature might say it contains lilies, the strange beauty of swamps, the architectural art of spiders, the many et ceteras that make the world the world. Nothing man-made can compete, Nature might say. Oh Nature has been known to go on and on. And if it wanted to push things further, it could cite our sleek perfection of bombs and instruments of torture, our nature so human we hide behind words that disguise and justify. But that's as generous as I want to be in giving Nature its say. I've seen it randomly play its violence card — natural, no-motive crimes with hail and rain and vicious winds, taking out, say, trailer courts and playing fields and homes for the elderly. So I want to be heard and overheard, this time for real, out loud, in fact right in Nature's face, to say I prefer the artifice in what's called artificial, the often concealed skill involved, without which we'd have no accurate view of ourselves, or of lilies in the pond.

— Stephen Dunn

Yet in current Western culture, it seems that most people respect an inventor more than an engineer and a scientist more than an inventor. Colin Macilwain, in an article in Nature , attributes to William Wulf, former president of the U.S. National Academy of Engineering, the following statement:

There is a general attitude among the scientific community that science is superior to engineering. (Macilwain, 2010)

This attitude spills out from the scientific community to the general culture. We use the term「rocket scientist」for extremely smart people, although most of what the people who put together the space program do is engineering. Macilwain goes on,

Wulf attributes this partly to the「linear」model of innovation, which holds that scientific discovery leads to technology, which in turn leads to human betterment. This model is as firmly entrenched in policy-makers' minds as it is intellectually discredited. As any engineer will tell you, innovations, such as aviation and the steam engine, commonly precede scientific understanding of how things work.

It is hard to point to any scientific discovery that led to the iPhone, in the sense that every scientific discovery it depends on was already in widespread use in other products. Nevertheless, it is easy to find evidence that popular culture assumes that this linear model of innovation is in fact how things work. For example, About.com, an advertising-funded website centered around articles on a huge variety of subjects, collects reader commentary. On the question of「Engineer vs Scientist - What's the Difference?」some of the reader answers are: 5

Scientists are the ones who create the theories, engineers are the ones who implement them. They compliment [sic] each other…

Science is a lot of high level theory and engineering is implementation and optimization.

Engineers deal with math, efficiency and optimization while Scientist [sic] deal with「what is possible.」

Engineers trained [sic] for Using tools, where Scientists are trained for Making them.

Scientists develop theories and work to verify them, Engineers search in these theories to「optimize」things in real life.

A scientist invents a law and an engineer applies it.

Scientist for invention of new theories [sic]. Engineers for applying those theories for piratical [sic] applications.

These views are clearly not authoritative, but rather are reflective of popular perception. Note the contrast between the style of this website and that of Wikipedia. This one is a portal for individual wisdom (and stupidity), whereas Wikipedia is a portal for collective wisdom.

Kuhn, a highly regarded historian of science and a philosopher, in his 1962 book, The Structure of Scientific Revolutions , echoed what Wulf claimed was the「general attitude among the scientific community,」stating that certain kinds of scientific measurement tasks are「hack work to be relegated to engineers or technicians」(Kuhn, 1962). To Kuhn, clearly engineers were a rung down the ladder from scientists. But like his repeated reference in the same text to scientists as「men,」we can forgive this disparaging remark about engineers because at the time he was writing, this view was standard in contemporary culture and had a strong element of truth.

Kuhn addressed the question of what is science, stating,「to a very great extent the term ‘science' is reserved for fields that do progress in obvious ways.」But, he points out, many fields progress in obvious ways:

Part of our difficulty in seeing the profound differences between science and technology must relate to the fact that progress is an obvious attribute of both fields.

Kuhn rejects the pervasive idea that the progress of science is toward some Platonic truth:

We may … have to relinquish the notion, explicit or implicit, that changes of paradigm carry scientists and those who learn from them closer and closer to the truth.

If truth is not the goal, then what gives「progress」its directionality? Kuhn postulates that science may actually have no goal, an observation that he recognizes will be difficult for many people to swallow.

We are all deeply accustomed to seeing science as the one enterprise that draws constantly nearer to some goal set by nature in advance.

He then draws an analogy between the progress of science and Darwin's theory of evolution:

The Origin of Species recognized no goal set either by God or nature.

The lack of a goal for science may be a shock, but for technology, it seems easier to accept. It is hard to postulate any ultimate Platonic「truth」of technology, any goal that when reached finishes the field. Technology progresses if once it is known how to make certain things, this knowledge is not forgotten.

In a 1984 book, the philosopher John Searle supports Wulf and Kuhn about this twentieth-century view of science:

「Science」has become something of an honorific term, and all sorts of disciplines that are quite unlike physics and chemistry are eager to call themselves「sciences.」A good rule of thumb to keep in mind is that anything that calls itself「science」probably isn't — for example, Christian science, or military science, and possibly even cognitive science or social science. (Searle, 1984, p. 11)

Spencer Klaw, in his 1968 book, The New Brahmins — Scientific Life in America , writes about「the awe that scientists now inspire,」where

science has become a form of established religion, and scientists its priests and ministers. (Klaw, 1968, p. 12)

Many disciplines seek to emulate the methods of science, hoping for similar payoffs. The「scientific method,」where a hypothesis is formed and experiments are designed to attempt to falsify the hypothesis, is useful in many disciplines that have little connection with natural science. But the value of the scientific method is often not as great in these nonsciences. Referring to social science, Searle observes,「the methods of the natural sciences have not given the kind of payoff in the study of human behavior that they have in physics and chemistry」(Searle, 1984, p. 71).

Popper, before Kuhn, stressed that the core of the scientific method is falsifiability. A theory or postulate is scientific only if it is falsifiable, according to Popper. To be falsifiable, at least the possibility of an empirical experiment that could disprove the theory must exist. For example, the postulate that「all swans are white」is not supported by any number of observations of white swans. But the postulate is falsifiable because an experiment may find a black swan. Hence, it is a scientific theory, albeit a false one.

Kuhn rejects Popper's conclusion that a scientific theory is rejected by falsification, arguing that even in the face of evidence against it, a theory will not be rejected until a replacement theory is invented:

[T]he act of judgment that leads scientists to reject a previously accepted theory is always based upon more than a comparison of that theory with the world. The decision to reject one paradigm is always simultaneously the decision to accept another, and the judgment leading to that decision involves the comparison of both paradigms with nature and with each other. (Kuhn, 1962, pp. 77–88)

Kuhn is saying that even if an experiment seems to falsify a hypothesis, scientists will not reject the hypothesis until they have a replacement hypothesis. He says,「If any and every failure to fit were ground for theory rejection, all theories ought to be rejected at all times.」

Popper's emphasis on using experiments to falsify hypotheses is healthy. Well-constructed experiments undermine astrology, phrenology, and many other pseudosciences. But as Kuhn points out, experimental evidence is always subject to interpretation. If there is no new paradigm aligning with the experiments, then the experimental results are more likely to be viewed as errors than falsifications.

Experiments are also useful in the「sciences of the artificial.」Engineers and computer scientists do perform experiments but not usually with an eye toward falsification or to compare against nature. The mere fact that you do experiments does not make you a scientist.

In its narrower usage, as reflected by the Merriam-Webster definition previously quoted, the word「science」refers to the study of nature, not to the study or creation of artificial artifacts. Under this interpretation, many of the disciplines that call themselves a「science,」including computer science, are not, even if they do experiments and use the scientific method.

To be sure, beginning with the information technology revolution in the 1990s, the role of engineering has been changing. I believe that this is because digital technology and software have created an explosion of possibilities in the「sciences of the artificial.」There is nothing natural about being able to communicate instantaneously with another person nearly anywhere on the planet. There is nothing natural about being able to see inside the human body. There is nothing natural about being able to carry all of human published information in your pocket. These are all the results of engineering more than science. More important, they are creative products, not inevitable consequences of scientific discovery.

Nevertheless, science still captures our imaginations and delivers spectacular results. The announcement on February 11, 2016, of the detection of gravitational waves emitted by colliding black holes, for example, got a great deal of press (see, e.g., the New York Times article by Overbye [2016]). Gravitational waves were predicted by Einstein more than a century ago, but detecting them has turned out to be astonishingly difficult. The announced detection was accomplished by the Laser Interferometer Gravitational-Wave Observatory (LIGO), at a cost of approximately $1.1 billion. The detected wave lasted one fifth of a second, and analysis indicates that it was produced by a collision between two black holes a billion light years away. This style of science is unlikely to have the practical consequences that early twentieth-century science had. It is「pure science,」in that it seeks knowledge for its own sake.

As might be expected, the high cost of this project has drawn some criticism. Horgan (2016) subtitled his column that reported this result in Scientific American

Was the gravitational-wave experiment worth its $1.1-billion cost if it merely confirms that Einstein was right?

In his article, he quotes chemist Ashutosh Jogalekar, who blogs as Curious Wavefunction:

Some sources are already calling the putative finding one of the most important discoveries in physics of the last few decades. Let me not mince words here: if that is indeed the case, then physics is in bad shape.

Horgan goes on:

In an email to me, a historian of technology was more blunt:「So a 100 year old theory has been confirmed experimentally—big whup. Did anyone think Einstein was wrong? There wasn't any controversy, was there? Was anyone credible claiming that spacetime isn't curved, or that black holes don't exist? I can get that this was quite an experimental trick and technological feat. But this isn't doing anything to convince me that public funds spent on this stuff wouldn't be better spent on medical research. Or clean fuels, or any number of things that would apply scientific expertise toward justice or the alleviation of human suffering.

The acknowledgment that this experiment was「quite an experimental trick and technological feat」is interesting. It raises the question, is the contribution of LIGO science or engineering? The basic method used, laser interferometry, has been understood by scientists as a way to measure gravitational waves since the 1970s. But building a system with adequate sensitivity was not easy.

Given that Einstein's model predicted gravitational waves 100 years ago, that there seems to be no controversy among scientists about the correctness of this prediction, and that the laser interferometry technique for measuring gravitational waves has been known for decades, it may appear that no new science resulted from the $1.1 billion investment. But it is probably not the validation of the existence of gravitational waves that is really the scientific contribution, but rather the demonstration of a new modality for observing events in the universe that were previously invisible to us. Specifically, this experiment has given the first observation of two black holes merging. That such events occur is perhaps not surprising, but most certainly intellectual value can be found in the first demonstration of a new kind of telescope into the universe that is capable of observing such events.

So instead, we should probably view the $1.1 billion as an investment in the engineering of a new device that can now enable a new form of astronomical observation. And the device is quite a triumph of engineering.

Let me try to explain the magnitude of the engineering challenge that the LIGO team faced. First, two detectors were built 3,000 kilometers apart so that the difference in time of arrival of gravitational waves at the two detectors would provide an indication of the direction of the source, and so that entirely independent observations could corroborate each other. Building two detectors can't be more than twice as hard as building one, so this was not the biggest challenge.

Each detector consists of an L-shaped ultra-high-vacuum cavity 4 kilometers long on each side (this alone is not easy to build; see figure 1.6 ). It uses laser interferometry to measure extremely slight distortions in space-time caused by passing gravitational waves; these distortions change the distance between the two ends of the 4-km cavity ever so slightly, by much less than the diameter of a proton! By measuring this change in distance, once all other possible causes for the change in distance have been eliminated, one can infer that the change in distance was caused by a passing gravitational wave distorting space-time. To minimize spurious causes for changes in distance, each detector has to be completely isolated from sources of vibration such as seismic events and human activity such as automotive traffic. Even the most minor such vibration would render the instrument useless.

It is hard to make the case that a gravitational wave telescope will improve (or even affect) the human condition in any tangible way. Nevertheless, the project may in fact have practical and tangible impact by contributing improvements in engineering methods. The ability to detect such extremely small variations in distance surely has applications elsewhere.

Figure 1.6

LIGO gravitational wave detector in Livingston, Louisiana. [Courtesy Caltech/MIT/LIGO Laboratory.]

NASA, whose main mission (I believe) is space exploration in the name of science, frequently uses their contributions to technology development as further justification for the expenditure on space exploration. They claim contributions to light-emitting diodes (LEDs), infrared ear thermometers, artificial limbs, ventricular assist devices, anti-icing systems for aircraft, safety grooving on highways, improved automotive tires, chemical detectors, land mine removal, firefighter gear, and many other technologies (NASA, 2016). To me, this reads as a substantial contribution to technology, irrespective of the contribution to science.

Assuming that LIGO is a triumph, who is the hero? The article announcing the measurement of gravitational waves in Physical Review Letters , published on February 11, 2016, has 1,019 authors (Abbott et al., 2016). The author list occupies 5 of the 16 pages of the article. It is hard to identify an「Einstein」from this list. According to the Boston Globe , Rainer Weiss, now a Professor Emeritus at MIT, is credited by many scientists with being the mastermind of the project, over significant protests from Weiss, who demurs that many people contributed a great deal (Moskowitz, 2016). Assuming Weiss is right, the LIGO project is a form of collective rather than individual wisdom, much like a Wikipedia article. And it is likely that most of these authors would self-identify as「scientists」and not as「engineers.」To me, most if not all of these 1,019 authors are engineers as well as scientists, defying Platonicity.

An engineered artifact such as an iPhone is similarly a form of collective wisdom. It is impossible to identify all the individuals who contributed significant technical content to the iPhone, but I'm sure it is many more than 1,019.

In a famous essay, Leonard Edward Read (1898–1983), libertarian and founder of the Foundation for Economic Education (FEE), accounted for the technical contributions required to make a humble wooden pencil (Read, 1958). Written from the point of view of the pencil, it starts with,「Not a single person on the face of this earth knows how to make me.」He then chronicles the processes and materials that go into fabricating a pencil:

My family tree begins with what in fact is a tree, a cedar of straight grain that grows in Northern California and Oregon. Now contemplate all the saws and trucks and rope and the countless other gear used in harvesting and carting the cedar logs to the railroad siding. Think of all the persons and the numberless skills that went into their fabrication: the mining of ore, the making of steel and its refinement into saws, axes, motors; the growing of hemp and bringing it through all the stages to heavy and strong rope…

He goes on to explain how the wood is milled, kiln dried, and tinted; how the graphite is mined and then mixed with clay and sulfonated tallow; how the lacquer paint is made from castor beans and castor oil; how the label is made with carbon black mixed with resins; how the metal is mined and refined; and how the eraser is made from rape seed oil, sulfur chloride, rubber, pumice, and cadmium sulfide.

And an iPhone is much more complicated than a pencil. Evidently, even Steve Jobs wouldn't know how to make an iPhone (or even a pencil). In a reference to the「invisible hand」of the economist Adam Smith (1723–1790), Read continues:

There is a fact still more astounding: The absence of a master mind, of anyone dictating or forcibly directing these countless actions which bring me into being. No trace of such a person can be found. Instead, we find the invisible hand at work.

Such an engineered artifact is an embodiment of collective wisdom even more extreme than Wikipedia, where at least a log is kept of the individual contributions.

Although we can't even trace the forces behind the invisible hand, widespread recognition exists that many of these forces are driven by people's technical skills. Cultivating such talent is a prerequisite for a modern economy. Today, policymakers and much of the public recognize the value in Science, Technology, Engineering, and Mathematics (STEM) education. This term bundles together a broad set of technical disciplines. It still puts Science first, but this may be as much about being able to pronounce the acronym as it is about relative priorities. Indeed, Liana Heitin blogs that STEM was originally SMET, which perhaps better reflected perceived priorities but was not so euphonious (Heitin, 2015).

Much of the political motivation in STEM may be pragmatic; it's more about being able to get jobs than it is about intellectual search. But we may be underestimating the intellectual search that is intrinsic in the「sciences of the artificial.」Without the engineering tour-de-force of the LIGO project, we would not have humankind's first gravitational telescope. Maybe this telescope will reveal other colliding black holes and other phenomena that may help us better understand the origin of the universe. So indeed, sometimes engineering does precede science rather than the other way around.

We see many other indicators of a shifting attitude toward technology and engineering. In the twentieth century, an「institute of technology」would be viewed as primarily a vocational school rather than a center of intellectual activity. MIT and Caltech changed that notion, and we are even starting to see「technical high schools」emerge as much more than vocational training.

Technology and engineering are distinctly not about discovering preexisting, disembodied truths. They are about creating things, processes, and ideas that never before existed. Pursuit of the Platonic Good, the preexisting, fixed world of Forms, is no longer what is driving humanity forward. We are instead creating knowledge and facts that never before existed, embodied or not.

In the next chapter, I focus on the relationship between discovery and invention. A key theme of that chapter is to understand the role of models in engineering and science. My essential claim is that models are invented, and when those models are modeling physical phenomena, the corresponding physical phenomena, not the models, are discovered. And even those physical phenomena may be brand new, as was the case with the transistor.

__________

1 Vannevar Bush, in the Wikipedia article on him, is identified as an「engineer, inventor and science administrator.」Bush, who died in 1974, was a towering figure. He was an MIT professor, dean of the MIT School of Engineering, and founder of Raytheon, a major U.S. defense contractor. During World War II, Bush coordinated several thousand scientists in the application of science to warfare. He started the Manhattan Project, which led to the development of nuclear weapons. At the end of World War II, Bush pressed for increased government support for science. His arguments led to the creation of the National Science Foundation (NSF), which today is one of the premier supporters of research in science and engineering.

2 My former PhD thesis advisor, Dave Messerschmitt, once told me that when you publish a book, every equation you put in the book cuts your readership in half. I will call this principle「Messerschmitt's Law,」although Dave tells me he did not discover this law. But I first heard it from him. Throwing caution to the wind, I am putting in an equation, but in an attempt to have some discipline, I will number each equation with an estimate of the remaining readership. Here, I've assumed optimistically a starting readership of 8,192, so the presence of this equation has cut it to 4,096. The next equation will be numbered 2,048. These are powers of 2 to make it easier to evenly divide by 2 each time and to underscore that I really am a nerd. If and when I get down to equation (1), I can write whatever I want because I will presumably have no more readers. As a side note, my PhD thesis had several dozen equations in it. It makes me wonder whether Dave ever read it.

3 Shockley moved in 1956 from New Jersey to Palo Alto, California, and started Shockley Semiconductor Laboratory in what would later become known as Silicon Valley. Eight of Shockley's employees left his company in 1957, the year I was born, to found Fairchild Semiconductor, the first successful high-tech company in Silicon Valley. Many other Silicon Valley giants, including Intel, were founded by former Fairchild employees. Arguably, Shockley's move to Palo Alto was the founding of Silicon Valley.

4 And then become embarrassed when those heroes disappoint us, as Shockley did by becoming a proponent of eugenics later in his life.

5 http://chemistry.about.com/od/educationemployment/fl/Engineer-vs-Scientist-Whats-the-Difference.htm , updated June 29, 2015, retrieved March 1, 2016.

2

