Probability and Possibility

· · · in which I consider the meaning of probability, which I argue is fundamentally a model of uncertainty about a system and not directly a model of that system; and in which I reconsider continuums and argue that probabilistic models over continuums reinforce the conclusion that digital physics is extremely unlikely.

11.1 The Bayesians and the Frequentists

Scientists seek models of the physical world. Even if the physical world is actually deterministic, Laplace's agenda to explain the natural world with deterministic models is foiled by complexity, the incompleteness of determinism, the unpredictability of chaos, Wolpert's proof of the impossibility of Laplace's demon, or indeed all four. If we put aside the incompleteness of determinism (by disallowing models with discrete behaviors), then Laplace's agenda may still be alive but only as a philosophical question. It ceases to be a question of science or engineering because either the explainer (the demon) is impossible (per Wolpert) or any explanation has little or no predictive value due to chaos. Models without predictive value cannot be falsified and hence fail Popper's test for being scientific. Add to this Hawking's observation that Gödel's theorem implies that models of nature will never be complete and Turing's result that we cannot tell what some programs will do just by looking at the code, and it seems we have no choice but to accept uncertainty.

Note that we have to accept uncertainty even if we tenaciously cling to Einstein's「God does not play dice.」Regardless of whether the physical world has a phenomenon of random chance, our models cannot be certain. Once we have models that embrace uncertainty, those models will be robust to the philosophical question of whether chance exists in the physical world. The models will work whether it does or not.

Engineers, in contrast to scientists, seek physical realizations of models. In the previous chapter, I argued that determinism is a valuable property of models, and hence if we can find physical realizations that are faithful to deterministic models, we get a powerful partnership. However, there are limits to this approach, just as there are limits to the scientific use of deterministic models. In addition to the unpredictability of chaos, we can easily find ourselves in a situation where we just don't know enough about the physical realizations to be able to construct faithful deterministic models. In this situation, it is valuable to be able to explicitly model our lack of knowledge. This is what probabilistic models do.

Embracing uncertainty also gives us a way to manage complexity. Given any intrinsically complex system, such as the Airbus A350 considered in chapter 6 , even if we could construct a deterministic model of its behavior, that model would likely be incomprehensible. By embracing uncertainty, we change the goal. Instead of seeking certainty, we seek confidence.

But what is uncertainty, and how do we model it? Nondeterministic models give us a simple way to model uncertainty. We simply create models that have more than one allowed behavior. However, this is often too simple. Nondeterministic models, by themselves, give no indication about which of the allowed behaviors is more likely. In fact, they are missing a notion of likelihood altogether.

So now I could play a dirty trick on you. I could define randomness to be uncertainty, uncertainty to be likelihood, likelihood to be probability, and probability to be randomness. I could couch all that in fancy language so you don't even notice that it's circular. I could even throw the words「stochastic」and「measure」to lend gravitas, and you would still not know what I'm talking about. Instead, I'm going to be up front about it. Probability is circularly defined. It is an axiomatic formal system with solid, well-understood mathematical properties. It's only the interpretation, how to apply the model to the physical world, that is difficult.

Probability is a surprisingly controversial theory from a philosophical perspective, despite that it is well established and mature mathematically. One view, which happens to be my view, is that probability is a formal model for quantifying what we don't know. Perhaps the reason for the controversy is that any time we talk about what we don't know, we can't really know what we are talking about.

We can begin by understanding the distinction between nondeterminism, which talks only about possibilities , and probability, which attempts to quantify uncertainty. How uncertain are we actually? Connor, channeling Michel Serres, points out that the distinction between probability and possibility is arguably one of the many hard versus soft oppositions, as discussed in chapter 4 .

The gap between probability「uncertain but calculable」and possibility「certain but incalculable」is itself graspable only as another inflection of the hard and the soft. (Connor, 2009)

Probabilities quantify known unknowns. The unknown unknowns can only be handled with possibilities. Possibilities are modeled with nondeterminism, but to have a calculable theory of uncertainty, we need probabilities. For example, probability theory enables us to become more certain about something as we gather data while still preserving room for learning from still more data, leaving a residual uncertainty. The method for doing this is credited to Reverend Thomas Bayes, an eighteenth-century English statistician and theologian. Interestingly, the modern mathematical formulation is due to Laplace, who was apparently hedging his bets about whether his demon could actually remove all uncertainty.

I used probabilities in chapter 7 , where I talked about fair and unfair coins. In that chapter, the fair coin had a probability 0.5 of heads, whereas the unfair coin had a probability 0.9 of heads. What do these numbers really mean? In answering this question, we run amuck of a long-standing philosophical debate. Although experts in probability theory pretty much agree on the mathematical machinery that they use, they disagree on the basic meaning of these numbers. These experts fall into two camps: the frequentists and the Bayesians. In the Bayesian model, a probability is a quantified estimate of uncertainty. In the frequentist model, a probability is a statement about how repeated experiments are likely to turn out. Let me explain.

Consider the statement「the probability of heads is 0.5.」To a frequentist, this means「in repeated tosses of the coin, on average, half of the outcomes will be heads.」To a Bayesian, this same statement means「I have no idea whether a toss will yield heads or tails, so I have no reason to expect one outcome over the other.」The frequentist statement is about repeated independent experiments, whereas the Bayesian's statement is about uncertainty, about what we don't know.

Consider now the statement about the unfair coin,「the probability of heads is 0.9.」To a frequentist this means,「If I repeatedly toss the coin, on average, 9 out of 10 outcomes will be heads.」To a Bayesian this same statement means,「I believe strongly that a coin toss will very likely yield heads.」The number 0.9 is a quantification of the strength of this belief.

If you toss the unfair coin N times, both frequentists and Bayesians expect to see heads N × 0.9 times if N is large enough. This is an informal statement of a central tenet of probability called the law of large numbers . Specifically, this law asserts that for large enough N , the actual number of heads, call it M , will be close to N × 0.9. Here,「will be close to」means that the probability that M/N differs from 0.9 by more than some small number is very small. 1 Even more specifically,「very small」means that for any particular choice of , we can make this probability as small as we like by choosing a large enough N .

Despite the agreement between the frequentists and Bayesians about the law of large numbers, deep differences exist. The frequentists take the law of large numbers to be the essential definition of probability. A probability is exactly a statement about repeated experiments, no more, no less. A Bayesian, however, uses a probability as a measure of uncertainty, a subjective concept, and hence can interpret the probability to have meaning even if there is only one experiment.

Suppose you are tossing the unfair coin of chapter 7 , which has a probability of heads of 0.9. Suppose that your first toss of the unfair coin yields tails. I will be surprised. Suppose now that the second toss again yields tails. I will be even more surprised. Suppose the third toss again yields tails. Now I am astonished. The frequentists and Bayesians would agree that this sequence of events is extremely unlikely, and they both assign a probability of 0.1 × 0.1 × 0.1 = 0.001 (one in a thousand) to this outcome. Both would say,「Wow, that was really unlikely,」and they might suspect that something is amiss, but then they would part ways.

Both frequentists and Bayesians use probabilistic models to learn about the world. A frequentist might approach the coin conundrum by designing a scientific experiment to test the hypothesis that the coin you are tossing has a probability of heads of 0.9. He would call this hypothesis the「null hypothesis」and would then postulate an alternative hypothesis, for example, that the coin is actually a fair coin. 2 Then the experiment would begin. You would toss the coin once. Suppose it comes up heads. The frequentist would dispassionately observe that the probability of that occurrence was high, 0.9. Suppose the next toss yields tails. The frequentist would say,「Well, that was unlikely.」The probability that a single coin toss yields tails is 0.1, but more interestingly, in two repetitions of the experiment, we saw an outcome of tails. The probability of seeing at least one tail in two coin tosses is 0.1 × 0.9 + 0.9 × 0.1 + 0.1 × 0.1 = 0.19 (because there are three ways that could have happened).

This probability of 0.19, or 19%, is called a「 p -value.」It is a measure of the likelihood of seeing an observation at least as extreme as what was observed (one tail in two tosses) given the null hypothesis. The probability of 19% is small but not small enough to reject the null hypothesis. It could still be true that the coin is the unfair one.

The experiment would continue. You toss the coin again. Suppose it comes up tails again. We have now seen two tails in three coin tosses, which is quite unlikely if the coin is unfair in this way. Examining all the possible ways that three coin tosses could yield at least two tails, we come up with a p -value of 0.028, or 2.8%. The frequentist would now say,「Hmm… this p -value is below my threshold of 5% for rejecting the null hypothesis. I therefore conclude that the coin you are tossing is not the unfair coin I thought it was.」

The frequentist's approach to this problem is objective and scientific, in the sense of Popper. He formulated a hypothesis, designed an experiment to falsify it, and rejected the hypothesis when the data indicated that the hypothesis was likely false.

The Bayesian, however, would approach this same problem in quite a different way. She too would suspect that the coin you have tossed is not the unfair coin she thought it was. But she would start by quantifying her initial uncertainty. Let's suppose she sold you an unfair coin that has probability of heads of 0.9. She therefore believes that the coin you are tossing is very likely to be that particular unfair coin. She assigns a number to this belief, saying, for example, that she is 80% sure you are tossing the coin she sold you. This is a subjective judgement that she calls a prior probability . It is a measure of uncertainty prior to any observation of data.

Armed with this prior probability, the experiment begins. Suppose that your first coin toss comes up heads. The probability that the unfair coin comes up heads is 0.9, so she is not surprised. This seems to reinforce her prior probability. She now uses Bayes' formula, which gives her a specific way to update her prior probability, taking into account the new data.

Let U denote the assertion that the coin is the unfair coin she sold you. The Bayesian's prior probability is p ( U ) = 0.8. Let H denote the outcome that the coin toss yields heads. If the coin is unfair, then the probability of this outcome is written p ( H | U ) = 0.9, which is read「the probability is 0.9 of getting heads given that the coin is unfair.」Bayes' formula then gives our Bayesian a way to update her prior probability as follows:

The left side of this equation, p ( U | H ), is read「the probability that the coin is the unfair one given that a coin toss yielded heads.」This new probability is an updated belief called the posterior probability . It quantifies our uncertainty about whether the coin is the unfair one after observing data. Bayes' formula, therefore, gives us a systematic way to update our subjective beliefs upon making observations of the world.

We have enough information to calculate the numerator on the right side because we know that p ( H | U ) = 0.9 and p ( U ) = 0.8. The only hard part is the denominator, which is the probability of seeing heads regardless of whether the coin is unfair. This probability is a weighted average of probability of seeing heads if the coin is unfair (0.9) and the probability of seeing heads if the coin is fair (0.5), where the weights are the probability that the coin is unfair (0.8) and the probability that the coin is fair (0.2), respectively. So the denominator works out to 0.9 × 0.8 + 0.5 × 0.2 = 0.82.

Putting it all together, evaluating (2), our Bayesian calculates the posterior probability to be 0.878. She is now 87.8% sure that the coin is unfair. The observation of heads has strengthened her conviction that the coin is unfair. Before observing any data, she was 80% sure. Now she is 87.8% sure.

Let the experiment continue. Suppose that the next coin toss yields tails. This outcome is less likely under the assumption that the coin is unfair, so it should result in a lowered confidence that the coin is unfair. Our Bayesian will again apply Bayes' formula (2). She won't bore us with the details but reports that Bayes' formula gives us a new posterior probability of 0.59. She is now only 59% sure that the coin is unfair. Notice that the outcome of tails diminished her confidence by more than an outcome of heads reinforced her confidence. This is because an outcome of tails is much more unlikely than an outcome of heads, so observing this outcome carries more information. In fact, in chapter 7 , I showed how Shannon calculated that observing a tail carries about 22 times as much information as observing heads (3.32 bits vs. 0.15 bits), assuming the unfair coin. 3 Because there is more information in this observation, our Bayesian learns more and adjusts her prior probability more.

Continuing the experiment, suppose that the next coin toss again yields tails. Then by Bayes' formula, after observing this, our Bayesian will be only 22% sure that the coin is unfair. She now actually believes it is more likely that the coin is fair than unfair. The frequentist came to the same conclusion and rejected the hypothesis that the coin is unfair. But the frequentist had no mechanism to take into account the prior information that the Bayesian sold you an unfair coin. The frequentist's experiment is therefore more objective, but it also omits information that we actually have.

If we continue the experiment and observe yet another tail, our Bayesian will only be 5% sure that the coin is unfair. She is now quite sure that the coin is not the unfair coin she sold you. But it took more observations to reach that level of confidence because her prejudice had to be overcome by the data. The frequentist had no mechanism for taking into account that prejudice.

The Bayesian approach embraces subjectivity. This is consistent with an interpretation of probability as a measure of uncertainty rather than a measure of a percentage of outcomes of repeated experiments. Uncertainty is necessarily subjective. No objective physical reality can be uncertain about anything. The notion of uncertainty is a human cognitive notion.

The Bayesian interpretation of probability is completely invulnerable to the debate about whether the physical world is actually deterministic. If the world really is deterministic, then the frequentists are on a slippery slope. What does it mean to repeat an experiment? If the starting conditions of each repetition are the same and the world is deterministic, then the outcomes should also be the same. This makes the frequentist's experiment useless. So clearly the starting conditions need to be different. But how are they different? How much are they different? It seems that the variability of initial conditions would be the only source of differing outcomes, but the frequentist ignores this variability. Or is it just that the frequentist is uncertain about those starting conditions? But then, isn't the frequentist also faced with subjective uncertainty?

The frequentist interpretation of probability is problematic if you assume a deterministic physical world. The Bayesian interpretation is not. The Bayesian approach uses probability models to quantify uncertainty, and regardless of whether the physical world is deterministic, there is no shortage of uncertainty. Moreover, the Bayesian's approach systematizes learning , which is the process of reducing uncertainty. We can learn from data in less ad hoc ways. It's no wonder that the Bayesian approach dominates in the field of machine learning, where computer programs continually update probabilistic models of the world. The vandalism detector of Wikipedia that we saw in chapter 1 is an example of such a machine learning application.

Frequentists and Bayesians use pretty much the same mathematical framework, including Bayes' formula (2). Although their experimental methodology differs for the previous coin toss experiment, often they don't differ at all between the two camps. It is odd to see two major camps of intellectuals who disagree so fundamentally and yet almost always agree. It's like Dr. Seuss's The Butter Battle Book , where an arms race breaks out between two camps that differ only on which side the bread should be buttered on. The difference between the Bayesian and the frequentists is more philosophical, but at that level the difference is profound.

The Shannon notion of information dovetails nicely with the Bayesian interpretation. The intuitive notion of information is that it counters ignorance. Receiving information results in updating our model of the world or learning. The more information we receive, the more we update our model. If a toss of the unfair coin yields heads, then we are not surprised. This is what we expected. The information content is low (only 0.15 bits, as calculated in chapter 7 ). 4 So we don't update the model by much (80% to 87.8% in the earlier example). In contrast, if the toss yields tails, then we are a bit surprised. That was not a likely outcome, according to the probability of 0.1 that we assigned to it. The information content is higher (3.32 bits, as calculated in chapter 7 ). 5 We update the model by quite a bit more. If we fail to learn something from repeated observations of tails and fail to update our model, then we are just being stubborn and dogmatic.

In fact, Bayesian probability gives us a way to understand dogma. If I am initially absolutely sure that your coin is unfair, then my prior probability is p ( U ) = 1, and Bayes' formula provides no possibility for learning. In Bayes' formula (2), we will find that if p ( U ) = 1, then p ( H | U ) = p ( H ), so the posterior probability p ( U | H ) will equal the prior probability p ( U ). No matter what the outcome of coin tosses, Bayes' formula will not change our minds. Even Reverend Bayes cannot overcome stubborn dogmatism. If you are determined not to learn, then you won't learn, no matter what you observe in the world. Bayes' formula proves this.

Karl Popper, in The Logic of Scientific Discovery , objected strongly to the Bayesian approach (which he called the Laplacean approach). He argued that the Bayesian interpretation of probability is subjective, and subjectivity has no place in science.

It treats the degree of probability as a measure of the feelings of certainty or uncertainty, of belief or doubt, which may be aroused in us by certain assertions or conjectures. In connection with some nonnumerical statements, the word「probable」may be quite satisfactorily translated in this way; but an interpretation along these lines does not seem to me very satisfactory for numerical probability statements. (Popper, 1959, p. 135)

In other words, such「feelings」should not be assigned numbers. He then credits the English economist John Maynard Keynes, whose 1921 A Treatise on Probability refined Laplace's and Bayes' notion by interpreting a probability as a「degree of rational belief.」To Popper, this interpretation can more rationally be assigned a number, but it is still subjective. Popper minces no words in preferring the frequentists' approach, which he asserts is objective:

I declare my faith in an objective interpretation ; chiefly because I believe that only an objective theory can explain the application of the probability calculus within empirical science. (Popper, 1959, p. 137, emphasis in the original)

He then admits that「the subjective theory … is faced by fewer logical difficulties than is the objective theory,」but that this is because they are「non-empirical … they are tautologies.」It is a theory built on its own assumptions, like an axiomatic theory in mathematics. Popper declares this to be「utterly unacceptable.」

The frequentist perspective seems to have the advantage of better testability, appealing to Popper's preference for empirical methods. Specifically, a Bayesian approach always requires a prior probability, which is used without being tested. But instead of confirming or falsifying a hypothesis, a Bayesian will adjust the prior probability based on new evidence. How is this less empirical than the frequentist approach? A frequentist will treat repeated coin tosses as a scientific experiment designed to falsify the hypothesis that the coin is unfair. But when should falsification occur? When should the hypothesis be rejected? Frequentists use an ad hoc measure, where thresholds of 5% or 1% for the p -value are common. How is this less subjective? Even with the unfair coin, a string of tails is certainly possible. It's just not likely.

Bayes' rule provides a systematic way to learn from the observations. The Bayesian approach is consistent with the observation that all models are wrong and provides a way to improve the models. The objective approach only provides a way to reject the model, but as Kuhn points out, because all models are wrong, all hypotheses should be rejected. Nevertheless, even the frequentists aren't so rigorous and fall back on ad-hoc confidence measures such as thresholds for the p -value to determine when to reject a hypothesis.

Laplace distinctly adopted a subjective approach that Popper deems unacceptable, but this is actually a consistent position for Laplace to take. After all, Laplace believed in a deterministic world governed by deterministic models and predictable by his demon. In such a world, repeated experiments are pointless. But Laplace recognized that we don't know the initial conditions for the experiments exactly. We are uncertain about those conditions, and his probabilities model exactly that uncertainty, not some intrinsic chance in the world, where God plays dice.

The eighteenth-century Scottish philosopher David Hume supports Laplace's position (and likely influenced Laplace):

Though there be no such thing as Chance in the world; our ignorance of the real cause of any event has the same influence on the understanding, and begets a like species of belief or opinion.

There is certainly a probability, which arises from a superiority of chances on any side; and according as this superiority increases, and surpasses the opposite chances, the probability receives a proportionable increase, and begets still a higher degree of belief or assent to that side, in which we discover the superiority. [in An Enquiry Concerning Human Understanding ]

Popper's position, unlike Laplace's, emphasizes statistics rather than uncertainty. To Popper, probability is not about what we don't know but rather about aggregate behavior of large numbers of individually deterministic players. This point of view is nicely illustrated by his description of a waterfall:

Imagine a waterfall. We may discern some odd kind of regularity: the size of the currents composing the fall varies; and from time to time a splash is thrown off from the main stream; yet throughout all such variations a certain regularity is apparent which strongly suggests a statistical effect. Disregarding some unsolved problems of hydrodynamics (concerning the formation of vortices, etc.) we can, in principle, predict the path of any volume of water — say a group of molecules — with any desired degree of precision, if sufficiently precise initial conditions are given. Thus we may assume that it would be possible to foretell of any molecule, far above the waterfall, at which point it will pass over the edge, where it will reach bottom, etc. In this way the path of any number of particles may, in principle, be calculated; and given sufficient initial conditions we should be able, in principle, to deduce any one of the individual statistical fluctuations of the waterfall. (Popper, 1959, p. 202)

Popper accepts Laplace's deterministic world, but he really picked a difficult illustration. Models of fluid flow are notoriously chaotic (see figure 10.1 ), so Popper's「any desired degree of precision」is not really achievable, no matter how precise the initial conditions are. They would have to be perfect. So here we see starkly the debate. The Bayesian says that we don't know where the molecule will go (whether we can know is irrelevant) and uses probability to model that uncertainty. Popper says we can know, but we are interested in the aggregate behavior of many molecules, and we use probability to model the aggregate behavior.

Although both perspectives have merit, I personally find the Bayesian perspective more compelling for three reasons. First, the Bayesian approach embraces the notions of information and learning. In Popper's approach, any description of the aggregate behavior of deterministic molecules is either wrong or right, and it can be tested by observing the molecules in the waterfall. If through observation we find that our aggregate model is wrong, then we can update the model and try again. But that update (for Popper) must occur in a meta theory, outside the theory of probability, because probability does not model what we know and don't know, it just models aggregate behavior. The update of the model becomes subjective and possibly capricious because in principle the theory can't help us perform that update. It is ironic that many machine learning techniques used today, like those used in the vandalism detector of Wikipedia (see chapter 1 ), use Bayesian models. The irony is that machine learning algorithms are completely mechanized, operating without human intervention, and yet, according to Popper, they are subjective. It is hard to reconcile these observations.

Second, the Bayesian approach makes more sense when talking about rare events. A Bayesian can say something like,「The probability of a major earthquake in San Francisco in the next 30 years is 63%」(Field and Milner, 2008). I don't see how a frequentist could rationally make such a statement. For sure, such a statement is not falsifiable and is not a statement about aggregate behavior of many individually deterministic behaviors. 6 Such a statement, if it is backed up by rigorous research, reflects the aggregate opinion of many experts and the use of computer simulation models that are informed by prior experience with real earthquakes. However, there is nowhere near enough such prior experience to adopt a frequentist's interpretation of the probability. I nevertheless find statements about rare events useful (if scary). They quantify what we know and don't know, and reasoning about rare events is an essential part of engineering safety-critical systems, such as the Airbus A350.

Third and finally, I find the Bayesian perspective more compelling because it covers situations that seem to be simply not well handled by the frequentist interpretation. Suppose, for example, that a coin is flipped, but the outcome of the flip is obscured by a cup before you can observe it. What now is the probability that you will see heads when the cup is removed? No matter how many cup-removal experiments you perform, the outcome will always be the same, so it seems that the frequentist would have to say that the probability of heads is either zero or one, but we don't know which it is. The Bayesian, however, has no difficulty with this situation. The probability of observing heads is the same as it was before the coin was flipped.

11.2 Continuums, Again

So far, I have only considered probabilities of events with a finite number of possible outcomes. A coin toss yields either heads or tails. An earthquake either occurs or not. The real world, however, is often messier. A coin could get stuck in the mud and yield neither heads nor tails. Earthquakes are occurring all the time, although fortunately most of them are too small to feel. Many of the things we don't know have more than a finite number of possible outcomes. Probability theory needs some adaptation to reason about those.

Consider a thought experiment. Suppose you throw a dart and you measure the distance between where you are standing and where the dart lands. This process is physical, and I will define its「output」to be the final distance to the dart. Assume that distance is a continuum (see chapter 9 ), you can measure the distance precisely, and you measure the distance in inches as a real number. For example, the dart may land 120.5 inches away, which is 10 feet plus half an inch.

Now, what is the likelihood that the distance to the dart is an integer? Intuitively, I hope you see that this is extremely unlikely. In fact, with a reasonable probabilistic model of this process, the probability that the outcome is an integer is zero. There are vastly more noninteger distances than integer distances.

OK, you may say, let's measure the distance in millimeters instead of inches. What is the likelihood now that the distance is an integer? If the measurement is precise, then the probability will again be zero. There are still vastly more noninteger distances than integer distances. In fact, the probability of getting an integer will be zero for any precision of measurement.

However, things get even weirder. Suppose I throw the dart, and it lands at exactly 120.123 inches. Note that I do not require that we actually be able to measure this distance. The dart had to land somewhere, and some oracle knows that it landed at 120.123 inches, even if I don't know this. In a reasonable probabilistic model, the probability that the dart will land at 120.123 inches is zero. In fact, the probability that it will land at any specific distance is zero, and yet it lands somewhere. Wherever it lands has a probability of zero. Don't give up on probability yet. Bear with me.

For problems with a finite number of possible outcomes, such as a coin toss, a probability is a number between zero and one, where zero means that the event will not occur, and one means that it always occurs. With my dart experiment, we can no longer interpret a probability of zero to mean that the event will not occur. Under that interpretation, the dart could not land. It would have to remain suspended in mid air because every point where it could land has a probability of zero, but it does land.

Before I give you the details, let me point out a flaw in my argument already. I have asked you to imagine a physical scenario, that of throwing a dart, and use it to draw conclusions about a model, the model of distance measures in a continuum. I am asking you to confuse the map with the territory.

OK, we are going to have to pick one. Do we want to do this experiment in the physical world or in the world of models? If we do it in the physical world, then we face a number of difficulties. First, I asked you to measure the distance precisely . You can't do that in the physical world. Even the LIGO experiment makes imprecise measurements of distance, although through an extraordinary feat of engineering and a $1.1 billion investment, they have reduced the imprecision to much less than the diameter of a proton, but not to zero.

To avoid these difficulties, let's do the experiment in the world of models. Now it is a thought experiment. Now that we are in the world of models, we can assume that the dart has a distance, a real number, even if we can't measure it precisely.

The essential problem here is that distance to the dart has a truly vast number of possible values. In fact, it has an uncountably infinite number of possible values. But some values are more likely than others. It is unlikely that the dart will land one mile from me. It is also unlikely that it will land very close, say on my feet. How can we model this variability in likelihoods if all distances have a probability of zero?

In probability theory, the way this situation is handled is using a probability density rather than a probability. Specifically, I talk about the probability that the dart will land between 9 and 11 feet from me. Perhaps that probability is 0.68. A frequentist would interpret this to mean that in repeated dart throws, 68% will land between 9 and 11 feet. A Bayesian would interpret this as a belief that it is a bit more likely that the next throw will land between 9 and 11 feet than that it will land outside that range. Either way, I assign a probability to a range rather than to a particular number. Every individual number will have a probability of zero, but a range can have a probability bigger than zero.

A probability density function for our dart-throwing experiment is shown in figure 11.1 . That figure has a characteristic shape called a「bell curve,」which suggests that the dart is most likely to land in the vicinity of 10 feet because that is where the curve is highest. As we get farther away from 10 feet in either direction, the probability density decreases, but it actually never gets to zero. In such a curve, the area under the curve indicates the probability of landing within a range. For example, the area of the shaded region is about 0.68, indicating that there is a probability of 68% of landing between 9 and 11 feet. Notice that the value of the curve at a particular point is not the probability that the dart will land at that point. The probability of landing at any one point is zero. The probability of landing in a range is the area under the curve for that range.

Figure 11.1

Probability density function for the dart distance experiment.

Recall from chapter 7 that Shannon's model of information states that a rare event carries more information than a common event. If an event has a probability of zero, for example, the event that the dart lands at 120.123 inches, then an observation of that event carries an infinite amount of information, as measured in bits. Shannon nevertheless uses a finite number to model the information content in the dart throw. This finite number is a continuous entropy, which as I explained in section 7.4 does not measure information in bits. Continuous entropy is more like a probability density than a probability. We can interpret it as an information density, not as a measure of the information content (in bits) of an individual observation. This allows us to compare information densities, just as we can compare likelihoods. If the dart lands one mile from me rather than 10 feet, then it is a truly remarkable event, carrying quite a lot of information, although the probability of both outcomes is zero. Using densities allows me to make such comparisons.

The particular shape of the curve in figure 11.1 , the bell curve, is special in probability. Shannon's notion of entropy helps to explain why. The bell-shaped probability density function is called a「normal distribution」(presumably because it is more「normal」than abnormal distributions) or a「Gaussian distribution,」after the nineteenth-century German mathematician Carl Friedrich Gauss. As it happens, the normal distribution is the most random of all distributions that have a fixed mean and variance and no other constraints. 7 If you compare the entropy of two random variables with the same mean and variance, one with a normal distribution and the other with some other distribution, the one with the normal distribution has higher entropy and hence is more random.

11.3 Impossibility and Improbability

In the dart thought experiment, when we toss the dart, it lands somewhere. The distance from me to where it lands is a random number selected from a continuum of possible distances. Assuming that distances form a continuum, the probability that any particular distance occurs is zero. However, a particular distance does occur. So in a continuum, a probability of zero does not indicate impossibility but rather indicates extreme improbability.

Now, risking confusing the map for the territory, if we assume that continuums exist in the physical world, then we would have to conclude that any particular thing that happens in the physical world has a probability of zero, at least according to the Bayesian model of probability. Perhaps this is fundamentally why precisely exact measurement of anything in the physical world is not possible. It would imply certainty about something that is extremely improbable. Under the Bayesian interpretation,「probability zero」means that we are almost certain that the event in question cannot occur. How could we have confidence in a measurement that reveals something that we are quite sure cannot have occurred? Probability theorists actually use such terminology, where they talk about something that will「almost surely」not occur, by which they mean it has a probability of zero, but it is still possible for it to occur.

Perhaps this is the biggest reason that the Bayesian interpretation of probability appeals to me so much more than the frequentist interpretation. All models are wrong, so certainty is unachievable. Nevertheless, some models are useful, and we use them to build confidence. Models and experiments can narrow our ignorance, but certainty is a fiction. We need to be open to continuing to learn. The Bayesian model gives us a systematic way to do that.

This is also why I do not believe that the digital physics explanation of the physical world is likely to be correct or useful (see chapter 8 ). Unless we believe that nature limits itself to countable sets, a postulate that is not testable, then the probability that any naturally occurring process is a computation is zero. It is not impossible, but it is extremely improbable. There are vastly many more processes than there are computations.

Human cognition is a naturally occurring physical process. This reasoning, therefore, leads to the conclusion that the probability that human cognition is computation is zero. There are so many fewer computations than physical processes that it is extremely unlikely that nature somehow ended up using only processes from the limited set that we call computation. It is not impossible, but it is extremely improbable.

So we should certainly not just assume that cognition is computation, as many people today do. Instead, we need evidence to support the claim. What kind of evidence? How much evidence? Bayes' formula provides guidance because it explains how to update our beliefs based on observations.

But Bayes' formula now presents us with a serious conundrum. Suppose that in equation (2) we let U represent the statement「cognition is achievable using computer programs.」Then the above cardinality reasoning seems to require us to assert that the prior probability p ( U ) is zero. If we take this to be the prior, then no observation H will result in any posterior probability p ( U | H ) that differs from zero. We are stuck with a dogmatic position that cognition is not computation, and no amount of evidence will change our minds.

I don't like dogma. Under the Bayesian interpretation, a prior probability is subjective. It is not derived from a formula or an experiment. If we have any doubt, therefore, we should not set the prior probability to zero. Based on my cardinality argument, however, it seems that our prior should be small. Because there are so many more physical processes than computations, it is unlikely but not impossible that cognition is computation. We might, for example, choose a prior of 1%, p ( U ) = 0.01. This says we believe there is a 1% chance that cognition is computation. What evidence then is required to convince us that cognition is actually computation? What observation H will yield a posterior p ( U | H ) greater than 50%, convincing us it is more likely that cognition is computation than that it is not?

Bayes' formula (2) states that our belief gets updated when we observe some experimental outcome H by multiplying our prior by 8

Let's call this quantity E the「evidence.」If E has a value larger than one, then it is evidence in favor of U . If E has a value smaller than one, then it is evidence against U . If E equals one, then it is no evidence at all because our posterior probability will be the same as the prior probability.

In section 9.3 , I considered whether the Turing test could help us determine whether a computer program has consciousness. I concluded in that section that if consciousness is not an externally observable property of the brain, then passing the Turing test provides no evidence of consciousness in a program. So if U is the postulate that a program has consciousness, and H is a passing of the Turing test, then it must be that p ( H | U )/ p ( H ) = 1 or equivalently that p ( H | U ) = p ( H ). What does this mean?

The quantity p ( H | U ) is the probability of passing the Turing test given that the program has consciousness, and p ( H ) is the probability of passing the Turing test regardless of whether the program has consciousness. Thus, for these two quantities to be equal, we just need it to be true that whether a computer program can pass the Turing test does not depend on whether the program has consciousness.

Indeed, many computer programs have been created that quite convincingly pass the Turing test, at least within limited domains. To my knowledge, none of the authors of these programs claims that their program has consciousness.

In a famous example, in the 1960s, the MIT computer scientist Joseph Weizenbaum wrote a computer program called ELIZA that was capable of engaging humans in a conversation. ELIZA would respond to humans in a manner bearing a remarkable resemblance to an empathic psychologist. Using a modern online implementation of ELIZA called「elizabot,」I had the following short conversation with ELIZA: 9

ELIZA: Please tell me what's been bothering you.

YOU: I think my computer is smarter than me.

ELIZA: Do computers worry you?

YOU: Yes

ELIZA: You are sure.

YOU: No, not really.

ELIZA: Are you saying no just to be negative?

ELIZA was the first example of programs that are now called「chatterbots.」Chatterbots are widely used for customer service call centers, airline reservation systems, toys (such as Hello Barbie ), and automated personal assistants such as Apple's Siri and Amazon's Echo. Malicious chatterbots appear in chat rooms for surreptitious advertising and to entice people to reveal personal information.

Weizenbaum claimed that ELIZA demonstrates that the appearance of human-like intelligence does not in fact imply the existence of human-like intelligence:

[M]achines are made to behave in wondrous ways, often sufficient to dazzle even the most experienced observer. But once a particular program is unmasked, once its inner workings are explained in language sufficiently plain to induce understanding, its magic crumbles away; it stands revealed as a mere collection of procedures, each quite comprehensible. The observer says to himself「I could have written that.」With that thought he moves the program in question from the shelf marked 「intelligent,」to that reserved for curios, fit to be discussed only with people less enlightened than he. (Weizenbaum, 1966)

Weizenbaum's goal with ELIZA was to demonstrate that the appearance of intelligence was easy to achieve and therefore should not be construed as evidence of real intelligence:

The object of this paper is to cause just such a reevaluation of the program about to be「explained.」Few programs ever needed it more. (Weizenbaum, 1966)

An aspect of Weizenbaum's statement is disturbing. He seems to be claiming that if a program is comprehensible, then it must not be intelligent. This implies that if intelligence is computation, then it must be a form of computation that we can't understand. Doesn't this in effect assert that we will never understand intelligence?

The notion of understanding human intelligence is self-referential because the process doing the understanding must itself be intelligent. This notion therefore is likely vulnerable to the sort of incompleteness that Gödel found in formal languages, Hawking applied to physics, and Wolpert found in Laplace's determinism. If this incompleteness is real and we will never fully understand intelligence, then Weizenbaum's criterion is valid, and we should dismiss intelligence in a program if we fully understand the program. However, I know from experience that it is not hard to write programs that nobody can fully understand. I've written quite a few myself, so this criterion is not really all that limiting. Certainly programs that exhibit digital chaos, as discussed in section 10.2 , have inexplicable behaviors.

So what kind of evidence is needed to convince us that cognition is computation? Assuming that our prior is low (e.g. p ( U ) = 0.01), then we need some sort of observation H , where the evidence E = p ( H | U )/ p ( H ) is much bigger than one. In other words, we need for p ( H | U ) to be much bigger than p ( H ). Hence, we need an observation H that is much more likely if U is true than if U is not true. To get us to a posterior of 50%, where it is equally likely that cognition is computation as that it is not, we need an observation H that is 50 times more likely if U is true than if U is not true. When the prior is small, the burden of proof is higher than if the prior is more moderate.

Alternatively, we can use many observations of weaker evidence. If the Turing test provides any evidence at all, it is at best weak evidence, where p ( H | U ) is only slightly larger than p ( H ). Equivalently, passing the Turing test is only slightly more likely if cognition is computation than if it is not. In the face of weak evidence, E is only slightly bigger than one, so if the prior is small, the posterior will still be small. It will take many instances of such weak evidence to overcome the small prior.

One alternative interpretation is that chatterbots approximate cognition, and as the software gets more sophisticated, the approximation gets arbitrarily close to actual cognition. Is getting arbitrarily close sufficient? I argued in section 9.3 that we cannot assume a close approximation of a process has the same properties as the process itself unless no significant difference is found between a continuum and a countable set. This observation is reinforced in section 10.3 , where a billiard ball thought experiment has the property that all close approximations of simultaneous collisions are deterministic but the actual simultaneous collisions are not. If a close approximation to cognition is not actually cognition, then it is easy to show that the Turing test provides no evidence that a program achieves cognition. Under these assumptions, we can, in principle, make a computer program where p ( H ) is as close as we like to p ( H | U ). As the program gets more sophisticated, the evidence for cognition E gets arbitrarily close to one, which is the value for no evidence at all.

So things don't look good for the claim that cognition is computation. If the Turing test can provide only weak evidence and the prior is low, then it will take a large number of observations to build support for this claim. If the Turing test provides no evidence, then what other experiment should we conduct to update our low prior probability that cognition is computation? The evidence is not there, so the claim that cognition is computation is ultimately faith based. A claim that cognition is computation amounts to an unreasonably high prior, given the cardinality argument and no effective method for updating the prior with experimental evidence. This is the essence of faith.

I'm an engineer not a scientist. My goal isn't to model the physical world or replicate natural processes such as human cognition. My goal instead is to make physical systems that do interesting and useful things. I believe we can do interesting things with computation even if we can't replicate human cognition. In fact, I will argue in the next chapter that we probably don't even want to replicate human cognition in computers. We can do many more useful things with computers that complement human capabilities. In chapter 9 , I argued that the concept of semantics makes the human-computer partnership powerful indeed. Digital technology, together with the stack of hardware and software paradigms that humans have constructed already, is a truly rich medium for modeling, even if the total number of such models is tiny compared with the possibilities. We know how to build faithful physical realizations for almost all of these models. Using Bayesian reasoning, we also know how to model what we don't know about these physical realizations. This gives us a truly expressive toolkit for creativity. I believe we have barely scratched the surface of what we can do with it.

__________

1 Nerds like to use the Greek letter (epsilon) for small differences. This even creeps into the vernacular, where a Nerd may assert, for example,「I am within epsilon of finishing this book.」

2 Actually, the alternative hypothesis could be much broader. It could be that the probability of heads is some unknown value less than 0.9. This hypothesis includes even the possibility that the coin is some other unfair coin, for example, one where the probability of heads is 0.1. This would not change the experiment or its conclusions in any way.

3 In chapter 7 , we assumed the coin was the unfair one. We can adjust the information content calculation to take into account that we are only 80% sure that the coin is unfair. With this adjustment, the probability of heads becomes p ( H ) = 0.82 instead of 0.9. The information content in an outcome of heads becomes 0.29 bits instead of 0.15. Correspondingly, the probability of tails becomes p ( T ) = 0.18 instead of 0.1, and the information content in observing tails drops from 3.32 bits to 2.47 bits.

4 Or 0.29 bits after adjusting the calculation to use the prior.

5 Or 2.47 bits after adjusting to use the prior.

6 Unless, I suppose, we adopt Everett's many worlds interpretation of quantum mechanics, which postulates that many worlds exist in parallel at the same time and in the same space as our own. Hence, there are many San Franciscos, some of which will have earthquakes and some of which will not. However, even that theory does not permit any observation of the many worlds, so it is still not falsifiable.

7 The mean, also called the expected value, is the center of mass of the probability density function. For a normal distribution, the mean is where the distribution peaks, or equivalently, the outcome where it is equally likely to get a result above as below it. For a frequentist, the mean is the average value of an infinite number of experimental outcomes. For a Bayesian, the term「expected value」makes more sense than average, as it captures the subjectivity of probabilities. The variance is the average (or expected value) of the square of the outcomes minus the mean. It measures how variable the outcomes are likely to be. If the variance is large, then many outcomes will be far from the mean, whereas if it is small, then most outcomes are close to the mean.

8 By Messerschmitt's law (see footnote on page 14 ), it doesn't matter what I write from here on. I am now truly feeling free.

9 http://www.masswerk.at/elizabot/

12

