## 0601. Evolution and Revolution

· · · in which I argue that technology revolutions differ from scientific revolutions in that paradigms appear and disappear much more rapidly; new paradigms do not necessarily replace old ones; and the crises that trigger new paradigms do not arise so much from the discovery of anomalies but rather from increasing complexity and technology-driven opportunity.

0601 进化与革命

我认为技术革命与科学革命的区别在于，其范式出现和消失的速度要快得多；同时，新范式不一定要取代旧范式；引发新范式的危机并不是因为异常现象的发现，而是因为复杂性以及由技术驱动的机遇在持续增加。

### 6.1 Normal Engineering

In his Structure of Scientific Revolutions , Thomas Kuhn calls the research that is firmly grounded in an established paradigm「normal science.」The LIGO gravitational wave detector discussed in chapter 1 , with its firm grounding in Einstein's general theory of relativity, despite its monumental scale, qualifies under Kuhn's scheme as normal science.

Kuhn asserts that adherence to a paradigm is essential to normal science:

Without commitment to a paradigm there could be no normal science. (Kuhn, 1962, p. 100)

He calls normal science「mopping up operations」and「puzzle solving」and asserts that this is what engages most scientists throughout their careers. The paradigms within which they operate provide the framework for these operations.

We can similarly define「normal engineering」to be the process of design and optimization within an established methodology and an established set of rules. Given a requirement for, say, a web page with some interactive features, a software engineer is hired to design the HTML and JavaScript code for the web page. This sort of engineering is easily and effectively outsourced, and a whole industry has emerged in India to carry out such normal engineering.

Although normal engineering is routine, it nevertheless demands skill and benefits from talent. When designing a web page, for example, aesthetics are often as important as functionality. Malcolm McCullough, in his 1996 book Abstracting Craft , focuses on this aspect of normal engineering, observing that digital media, including the technology for creating web pages and other digital artifacts, offer a whole new form of craftsmanship. Unlike the physical crafts of, say, pottery and woodworking, this form of craft works with abstract media, the zeros and ones of computing. But like the physical crafts, abstract craft admits mastery and aesthetics.

Although normal science certainly admits mastery, it is a real stretch to say it admits aesthetics. A scientist may object, observing correctly that personal taste is involved in the selection of experiments to perform, the manner in which they are performed, and the way the results are presented to the scientific community. I have to agree that there is aesthetics in all of this, but the end product of normal science is not an artifact subject to aesthetic judgment. It is, for example, the LIGO validation of Einstein's prediction of gravitational waves. The goal of such validation is not to please the human senses or to stir the soul. It is to reaffirm the Platonic truth of a prevailing paradigm in physics. Kuhn asserts that the object of normal science「is to solve a puzzle for whose very existence the validity of the paradigm must be assumed. Failure to achieve a solution discredits only the scientist and not the theory」(Kuhn, 1962, p. 80). How indeed would LIGO be viewed if it failed to detect any gravitational waves? Would it have undermined Einstein's theory of relativity? Probably not.

A failure to create an effective or successful interactive web page would discredit the software engineers assigned to the task. It would not undermine the paradigm of the web or of the HTML and JavaScript languages. Success in such a project requires some technology, but even more it requires craftsmanship.

Craftsmanship is human skill creating artifacts that did not previously exist. But the craftsmanship in normal engineering is distinctly different from innovation. A beautiful web page that is a pleasure to interact with is not necessarily innovative and almost certainly does not constitute an invention, just as normal science does not seek novelties:

Normal science does not aim at novelties of fact or theory and, when successful, finds none. (Kuhn, 1962, p. 52)

Craftsmanship and aesthetics can have as much or more impact on the success of an engineering task as innovation. One of the factors in the success of the iPhone is undoubtedly the aesthetic physical design, credited to Jonathan Ive. Amazingly, Apple managed to patent this design, stamping it as an invention. The patent contains one claim, the entire text of which is,「The ornamental design for a portable display device, as shown and described」(Akana et al., 2012). In my opinion, this is an abomination that goes against any reasonable notion of what constitutes an invention. The U.S. Patent and Trademark Office should be ashamed of itself.

Of course, not all of engineering admits aesthetics easily. The design of a sewage handling system for a building usually has only one aesthetic goal: make it invisible. Even so, occasionally even plumbing is used as an aesthetic medium. Witness the Pompidou Center in Paris, which exposes its guts in a bold and aesthetically driven reversal of conventional practice in architecture (see figure 6.1 ). But with digital media, aesthetic elements are much more common than in other branches of engineering.

As with any craft, mastery of digital media can have an enormous effect on the outcome of a project. But mastery of a craft is quite orthogonal to innovation. Innovation can occur within the framework of an established paradigm, of course. But a truly game-changing innovation, such as the stored-program computer credited to von Neumann or the World Wide Web credited to Berners-Lee, is more like Kuhn's paradigm shifts than like practice within a paradigm. These innovations change the practice of normal engineering for many successor engineers. The question I will address next is what brings about these paradigm shifts. It turns out that the situation in engineering is quite different from that in science.

Figure 6.1

The Pompidou Center in Paris exposes the building's mechanical functions for aesthetic reasons. The building was designed by Richard Rogers, Renzo Piano, Gianfranco Franchini, and their teams, and opened in 1977. [Image licensed under CC BY-SA 3.0 by「Reinraum.」From https://en.wikipedia.org/w/index.php?curid=37297406 .]

6.1 常态工程

在《科学革命的结构》一书中，托马斯·库恩称建立在既定范式基础上的科学研究是「常态科学」。我们在第 1 章讨论过的激光干涉引力波天文台探测器研究项目是建立在爱因斯坦广义相对论的坚实理论基础之上的，尽管该项科学研究规模巨大，但在库恩的理论体系下，仍然是一门常态科学。

库恩认为，坚持一种范式对于常态科学而言是必不可少的：

如果没有对一种范式的保证，就没有常态科学。（库恩，1962：100）

他称常态科学为「清扫行动」和「解除迷惑」，并断言这正是大多数科学家在其整个职业生涯中所从事的工作。他们所采用的范式为这些活动提供了指导框架。

我们可以类似地将「常态工程」定义为在一套既定的方法和一套既定的规则中进行设计和优化的过程。例如，如果一个网页需要具备某些交互功能，就应聘请一名软件工程师为该网页设计 HTML 和 JavaScript 代码。这类工程可以被很容易且有效地外包出去，例如，印度就形成了一个完整的行业从事这种常态工程。

虽然常态工程大都是常规工作，但它还是需要技能和人才的。例如，在设计网页时，它的审美性常常和它的功能性一样重要。马尔科姆·麦卡洛在他 1996 年出版的名为《抽象化工艺》（Abstracting Craft）的书中，重点阐释了常态工程这一主题。他观察到数字媒介，包括创建网页和其他数字产品的技术，提供了一种全新的工艺形式。例如，与陶艺和木工等的物理工艺不同，这种全新的工艺形式使用了 0 和 1 的计算这种抽象媒介。但是就像物理工艺一样，抽象工艺也是精湛并具有审美性的。

虽然常态科学肯定会考虑工艺的精湛性，但如果说它也会考虑美学的问题可能就有些夸大其词了。某位科学家可能会对此提出异议，他正确地观察到个人品位在选择要进行的实验、进行实验的方式以及向科学界呈现实验结果的方式中都有反映。我不得不承认，所有这一切都是有美学价值的，但是我们不能说常态科学的最终产物就是受美学判断支配的人工制品。例如，激光干涉引力波天文台探测器研究项目验证了爱因斯坦对引力波的预测。这种验证不是为了取悦人类的感官，也不是为了激发人类的灵感，而是为了重申物理学中主流范式的柏拉图式真理。库恩断言，常态科学的目标在于，「解决难题，就其存在而言，必须假定这个范式是有效的。未能找到解决办法只会使科学家蒙羞，而科学理论并不会受到什么影响」（库恩，1962:80）。如果激光干涉引力波天文台探测器研究项目探测不到任何引力波，那么人们将如何看待它？这会破坏爱因斯坦的相对论吗？我想大概不会。

如果不能成功地创建一个有效的交互式网页，就会使负责该任务的软件工程师名誉扫地。但是，这不会破坏万维网或 HTML 和 JavaScript 语言的范式。然而，要成功地完成这样一个任务需要一些技术，但更需要工艺。

工艺指的是人类创造以前不存在的人工制品的技能。但是，常态工程的工艺与创新有着明显的不同。一个出色的网页会让用户产生互动的乐趣，但它并不一定具有创新性，而且几乎肯定不构成一项发明，这与常态科学并不追求新奇性的道理是一样的：

常态科学不以新奇的事实或理论为目标。而且，当取得成功的时候，也根本找不出任何新奇的东西。（库恩，1962：52）

工艺、美学对工程任务能否成功的影响与创新一样大，甚至是更大。苹果手机成功的一个主要因素无疑是其充满美学的外观设计，这都要归功于乔纳森·伊夫。令人感到惊讶的是，苹果公司居然设法为这一设计申请了专利，并将其标榜为一项发明。该专利包含一个声明，全文是「如图所示和描述的便携式显示装置的外观设计」（赤名等人，2012）。在我看来，这是一个令人讨厌的声明，它有悖于任何关于发明的合理概念。美国专利和商标局应该为批准该项专利的行为感到羞愧。

当然，并非所有的工程都能轻易地接纳美学。例如，建筑物中污水处理系统的设计通常只会考虑一个美学目标：让该系统隐形。即使如此，有时甚至连管道也会被用作美学的媒介。看看巴黎蓬皮杜艺术中心那些大胆的、唯美主义的建筑设计风格吧，它们都大幅扭转了人们对传统建筑的刻板印象（见图 6.1）。但是随着数字媒介的产生，美学元素在工程学的其他分支领域里变得更为常见了。

图 6.1 出于美学原因，巴黎蓬皮杜艺术中心展示了建筑的机械功能。这座建筑由理查德·罗杰斯、伦佐·皮亚诺、吉安方克·法兰锲尼和他们的团队设计，并于 1977 年向公众开放。（图片由瑞恩拉姆提供，并获得 CC BY-SA 3.0 授权。图片来自 https://en.wikipara.org/w/index.php?curid=37297406。）

与任何工艺一样，对数字媒介的掌握也会对工程项目的结果产生巨大影响。然而，掌握一种工艺与技术上的创新是完全不相关的。

当然，创新可以在既定范式的框架内发生。但真正改变游戏规则的创新，如冯·诺依曼的存储程序式计算机或者伯纳斯·李的万维网等，更像是库恩所述的范式转换，而不是仅仅在一个范式中的实践。这些创新改变了许多后来的工程师在常态工程中的实践活动。我接下来要讨论的问题是，是什么导致了这些范式的转换。事实证明，工程领域的范式转换与科学领域的情况大不相同。

### 6.2 Crisis and Failure

Kuhn claims that scientific revolutions occur only after an accretion of anomalous observations made under the old paradigm creates a crisis and only when a new paradigm emerges to replace the old. These are not the forces that drive paradigm shifts in technology.

Paradigm shifts in technology occur for at least three reasons. First, the complexity of systems being engineered overwhelms our human ability to understand or control these systems. For example, programming languages emerged because writing correct machine or assembly code became impossibly difficult. Second, it becomes possible to do something that nobody imagined was possible before. For example, Google and other search engines enable nearly instantaneous search over everything humans have ever published. Third, complex social, political, and business forces can drive paradigm shifts in technology. Military needs, for example, essentially created aviation, nuclear weapons, and many other technologies, and military budgets provided most of the funding for the early development of computing.

Figure 6.2

In scientific revolutions, according to Kuhn, new paradigms typically replace old paradigms. In technology revolutions, new paradigms may be built on top of old paradigms, not replacing them so much as hiding them behind a layer of abstraction. (Photos of a sign with the likeness of Charles Darwin on Santa Cruz Island in the Galapagos.)

In section 3.2 , Complexity Simplified , I pointed out that one source of complexity is a large number of parts. Even simple parts with simple functions, such as transistors acting as switches, when there are enough of them, enable enormously complex functionality. Digital technology, rooted in these transistors, has been an enormous source of complexity-driven paradigm shifts for several decades.

In 1965, Gordon Moore, cofounder of Intel, 1 famously predicted that the number of components (transistors, resistors, diodes, and capacitors) in an integrated circuit would double every year for at least the next ten years. In 1975, he revised the forecast rate to double approximately every two years. This prediction, widely known as「Moore's law,」has been a guiding principle for the semiconductor industry ever since.

In practice, until around 2015, Moore's prediction held steady. The Intel 8080 was a single-chip microcomputer introduced in 1974 with approximately 4,400 transistors. According to Moore's law, therefore, a single-chip microcomputer in 2014 should contain 4,400 × 2 (2014−1974)/2 ≈ 4,610,000,000 transistors, which is remarkably close to the 5.56 billion transistors on the Intel Xeon Haswell-E5, introduced in 2014. Although the demise of Moore's law has been predicted many times, most industry observers seem to agree that as of 2015, it has finally significantly slowed.

This rapid acceleration of the capabilities of digital technology has created a steady stream of crises, where inevitably the models and mechanisms used to design and program systems repeatedly break down under the crush of additional capability. As far back as 1972, Edsger Dijkstra, wrote,

To put it quite bluntly: as long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem. (Dijkstra, 1972)

And that was just 1972! If it was a gigantic problem then, then there is no word for what it is now.

Moore's law refers only to individual computers on individual silicon chips. Today, we find an extraordinary rise in the number of computing devices that are interconnected through networks. Around 1980, Robert Metcalfe, cofounder of 3Com and coinventor of Ethernet, the most widely used wired networking technology today, is said to have postulated what is now known as Metcalfe's law. This law states that the value of a network is proportional to the square of the number of compatible communicating devices on the network. So, for example, if a single isolated device is worth $1, then a network with 10 connected devices is worth

$1 × 10 2 = $100.

A network with 100 devices would be similarly worth $10,000, and with 1000 devices, $1,000,000. I'll let you calculate Metcalfe's assessment of the worth of the Internet today, which has roughly six billion connected devices.

And the number of connected devices is growing fast. Today, industry leaders breathlessly predict some 50 billion connected devices by 2020, due to the rise of the so-called Internet of Things (IoT). The IoT connects devices that are not first and foremost computers, such as thermostats, cars, door locks, climate control systems, and so on. As they give such predictions, you can almost see the visions of dollars dancing in their eyes.

Because value presumably follows from capability, I assume that Metcalfe would conclude that the capability of a network, not just the value, is also proportional to the square of the number of connected devices. Because complexity typically also follows from capability, we should not expect any slowing of crises over the next few years, even if Moore's law grinds to a halt. The crush of increasing complexity does not appear to be slowing.

In any technology, increasing complexity can create crises in two ways. First, designing reliable systems becomes harder. The tools and models that worked well at lower complexity strain until they break as the complexity increases. Second, perhaps as a consequence of the first, the likelihood that a design project will fail increases.

When I worked at Bell Labs in the early 1980s, a large project called AIS/Net 1000 intended to provide bridges between the disparate computing systems that existed at the time. At that time, interconnecting computers through networks was quite a new phenomenon, and as evidenced by Metcalfe's law, the value of such interconnections was recognized, at least by Metcalfe.

But these interconnections exposed many incompatibilities between the computer systems, particularly computer systems from different vendors. These systems had been designed to work in isolation. The binary bit patterns used to represent numbers and text, for example, were different. The order in which bits were arranged in memory differed. The protocols and speeds used to communicate differed. These differences meant that one computer often could not directly communicate with another, and even if it could, it would not interpret the bit patterns produced by the other correctly. In effect, each computer operated within its own paradigm, and the paradigms were incommensurable.

As computers became networked, these incompatibilities triggered a crisis. AIS/Net 1000 aimed to solve this problem by performing translations within the network, permitting the disparate paradigms to persist. When one computer sends a message to another, the message would be automatically translated during transport. Nobody would have to change how they did things, and AT&T would sell the glue that enabled interoperability. This was to be the Babel fish of networks. 2

The project failed. AT&T wrote off more than $1 billion of development effort. It turns out that few customers were actually willing to pay for this service. Instead, the paradigms saw a Darwinian consolidation. Competing species were unable to coexist within the same ecosystem of networked computers.

Instead of a Babel fish, the Internet emerged. A key enabler for the Internet was the acceptance of the so-called Open Systems Interconnection (OSI) model. OSI is a layering of modeling paradigms, sketched in figure 6.3 . Like the layers in figure 3.3 , each level of the OSI model provides a conceptual framework for communication between computers. The lowest level, called the physical layer, is concerned with transporting sequences of bits from one place to another without concern for what the bits mean. The layers above this assign more meaning to the bits. For example, layer 6, the presentation layer, may treat a collection of, say, one million bits as an encoding of an image in a particular format, such as the standardized JPEG format widely used in digital cameras and on the web.

Kuhn talks about paradigms being incommensurable. In the OSI model, the terms「frame,」「packet,」「segment,」and「session」all refer to a finite collection of bits, but they all have different meanings at different levels. Understanding these different meanings is one of the most confusing parts about working with low-level networking software, in my experience. It is much easier to work at exactly one of these levels and not try to cross layers.

Calling the layers of the OSI model「paradigms」is perhaps a bit odd because they differ significantly from Kuhn's scientific paradigms. Like Kuhn's paradigms, they do provide a mental model for humans to understand how a system operates. For example, it is a different mental model to visualize one computer sending an image, a photograph, to another, versus visualizing one computer sending a stream of one million bits. But unlike Kuhn's paradigms, for these layers to work, their definition must be made absolutely precise. A misinterpretation of a single bit among one million bits may render an image unreadable. Kuhn's paradigms are much more robust; they can tolerate a certain amount of creative misunderstanding, which can sometimes form the engine for innovation or even paradigm shifts.

Figure 6.3

The OSI model for communication between computers.

It is not easy to make the OSI model layers precise. For computers on the Internet to reliably communicate, they all need to agree on precise meanings at every layer, down to the interpretations of each individual bit. The process of building the standards that codify this agreement can be a messy, political, and bureaucratic morass of conflicting national and business interests.

As a case in point, it may be helpful to understand how the OSI model came about. The OSI model is a joint effort of two standardization bodies, the International Organization for Standardization (ISO) and the Telecommunication Standardization Sector of the International Telecommunication Union (ITU-T, formerly CCITT), which had each separately developed similar models for communicating computers in the late 1970s. But similar models are not enough to get computers to communicate. The models have to be identical. Hence, these two bodies got together to publish a joint document, a process that no doubt involved considerable bickering over minutia.

To get a sense of all the competing interests that get involved, it may be helpful to understand how these standardization bodies are organized. The ISO is composed of representatives of various national standardization agencies from some 162 countries. The ITU-T, a United Nations agency, coordinates standards for telecommunications. In addition to representatives from many governments, these bodies include representatives from competing businesses, some of which will have already sunk considerable investments into the technologies being standardized. As a consequence, the battles that can emerge over standards development can be prolonged and painful, and the ensuing compromises can sometimes undermine the effectiveness of the resulting standards.

JPEG, one of the most commonly used encodings for photographs and a level-6 (presentation layer) standard in the OSI model, is an acronym for the Joint Photographic Experts Group, which created the standard. This group is a committee overlapping ISO/IEC JTC1 and ITU-T, the same organizations involved in the OSI model, except for the addition of the International Electrotechnical Commission (IEC). The IEC is a nongovernmental international standards organization that develops standards for electrical, electronic, and related technologies. Surely by now, from the barrage of acronyms, you see how bureaucratic all of this is.

As with many such standards, one of the complexities that arose with JPEG concerned intellectual property. A major challenge in establishing such international standards is to ensure that anyone can legally use the standard without infringing on the rights of someone else. There can be quite a bit of posturing during the development of a standard, where businesses will attempt to ensure that the use of the standard requires license payments to them for patents that they hold or where patents that they hold will give them a competitive advantage when implementing the standard. Organizations can even be quite sneaky about this, concealing their business interests from the standards body until it is too late to change the standard. As a result, standards often do not reflect the best technical solutions to a problem.

In the case of JPEG, after publication of the standard, several companies asserted that the standard infringed on patents that they held. In a collection of notable cases starting around 2007, a patent holding company called Global Patent Holdings, LLC, claimed that the act of downloading a JPEG image from a website or sending it through email infringed a patent that it held, U.S. Patent 5,253,341 by Rozmanith and Berinson (1993). A messy set of suits, countersuits, and threats ensued. According to a Wikipedia article on JPEG,

Global Patent Holdings had also used the '341 patent to sue or threaten outspoken critics of broad software patents. (https://en.wikipedia.org/wiki/JPEG , retrieved April 26, 2016)

After extensive battles, the U.S. Patent and Trademark Office issued a Reexamination Certificate in 2009 canceling all claims of the patent, asserting that prior art invalidated the claims. By this time, many organizations had wasted enormous amounts of money on completely nonproductive fights over intellectual property.

A patent holding company is a corporation that does not manufacture or sell products but just acquires and holds patents for the purpose of extracting royalty payments from companies that do sell products. Such companies are often called「patent trolls,」after the troll in the Norwegian fairy tale「Three Billy Goats Gruff」who eats anyone who tries to cross the bridge under which it lives.

The emergence of patent trolls has significantly changed the business climate for technology companies in the United States. An organization that produces a product and also owns a patent portfolio may be hesitant to sue another organization that also owns a patent portfolio because that other organization may countersue for patent infringement. But an organization that does not produce any products is much less vulnerable to countersuits. These organizations exist only for the purpose of siphoning money from organizations that produce products. In my opinion, they are parasites.

But I digress. The main point is that it is not only creativity that determines the nature of the layers of paradigms in a technology, but that complex business and political interests can also intervene. These layers, therefore, are not in any sense objective truths. They are the result of flawed and human processes.

AIS/Net 1000 failed to solve the crises created when incompatible computers started to become networked. That crisis has since been largely resolved through the emergence of the Internet, which depends on standardization of all levels of the OSI model. The so-called Internet Protocol (IP, not to be confused with intellectual property) is at level 3 in the OSI model, the network layer. All traffic in the Internet uses IP. A layer above, at the transport layer, is the widely used Transmission Control Protocol (TCP), which overlays on IP the concept of reliable transmission. Specifically, a TCP/IP packet sent to a computer must be acknowledged by that computer. The sending computer will repeatedly send the packet until it receives an acknowledgment. As a consequence, barring catastrophic failure in the network or in the sending or receiving computer, every sent packet is eventually received. TCP also ensures that packets sent in order are received in the same order. TCP is essential to email, among many other services.

Email, in turn, relies on another protocol called SMTP, for Simple Mail Transfer Protocol, a level-5 protocol (session layer). At this level, a sequence of TCP/IP packets is collected into a unit, an email message. The design of this protocol is greatly simplified by being able to assume the properties of the lower layers, specifically that packets are delivered reliably and in order. If you send a JPEG image by email, then the level-6 (presentation layer) protocol for JPEG image encoding defines the interpretation of the bits contained in the packets as an image.

The OSI model provides separation of concerns, where routing of packets, reliable delivery and ordering, email addressing and content, and encoding of the email payload are all separated. Each of the protocols involved is much easier to design and understand because it uses the properties of the layers below, and it avoids providing capabilities that will be provided by a layer above.

AIS/Net 1000 offered a solution to a crisis but it turned out not to be the solution that prevailed. I believe a key reason is the lack of separation of concerns. AIS/Net 1000 was to be the single solution to interconnectedness, whereas the OSI model made it possible for many competing solutions at each layer to fight it out, creating a Darwinian ecosystem with distinct niches within which solutions could compete.

There are other spectacular technology failures with similar reasons. Ed Cone blogs about the failure of the Federal Aviation Administration's (FAA's) Advanced Automation System (AAS) project, conceived in 1981 and terminated in 1994 (Cone, 2002). In this project, the FAA contracted IBM Federal Systems, a division of IBM later acquired by Lockheed Martin, to replace the nation's air traffic control system with a completely new modern design. According to Cone,「the FAA ultimately declared that $1.5 billion worth of hardware and software of the $2.6 billion spent was useless.」

The historical backdrop of the project is telling. In 1981, the air traffic controllers went on strike, and President Ronald Reagan summarily fired all 11,345 of them. This accentuated a crisis already under way created by an aging inflexible air traffic control system. Part of the solution was to be a system that was more automated, requiring fewer controllers to manage more planes.

Robert Britcher, who worked on the project at IBM Federal Systems, wrote about it in his book, The Limits of Software: People, Projects, and Perspectives , where he states that it「may have been the greatest debacle in the history of organized work」(Britcher, 1999, p. 163).

Why did it fail? Cone quotes Pete Marish, a senior analyst at the General Accounting Office:

It was basically a Big Bang approach, gigantic programs that would revolutionize overnight how [the] FAA did its work. (Cone, 2002)

Cone further quotes Bill Krampf, who worked on the project at IBM Federal Systems:

We entered [the] software phase without the requirements phase completed. (Cone, 2002)

Krampf's observation, however, is probably a misdiagnosis of the problem. I seriously doubt that completing the requirements phase before starting the work on software would have solved the problem. The idea of completing requirements before engaging in detailed design goes against the grain of one of today's most popular software engineering strategies, called「agile development.」In an agile process, requirements are developed along with the software through a series of incremental「sprints,」short development efforts with modest partial goals toward the overall project objective. An agile process directly involves the customer and expects requirements to evolve as the project evolves. This way of managing complexity is more realistic than doing specification before design.

Marish's diagnosis is likely more accurate. Wholesale technology replacements generally require too many concurrent paradigm shifts. Indeed, Cone attributes the failure to enormous optimism about emerging immature technology paradigms, including object-oriented design, distributed computing, and the Ada programming language. Cone writes,

AAS was supposed to be a showcase for Unix-based distributed computing and for development in Ada, a programming language created by the Air Force that became the state-sponsored religion in object-oriented technology, itself a relatively young methodology for writing code in self-contained, reusable chunks. (Cone, 2002)

I find the words「state-sponsored religion」fascinating; they reflect the dogmatic fervor that I frequently encounter in computer scientists who espouse devotion to one or another programming language.

Another dramatic failure of a large engineering project was the U.S. Army's Future Combat Systems (FCS) program. Although there were many reasons that this program failed, one was similar to the reason for the failure of the FAA's AAS program: the program was too ambitious about replacing too many systems all at once. According to a 2012 report by the RAND Corporation,

Compared to more traditional acquisition strategies, the [systems-of-systems] approach significantly increased both the complexity of the organizations needed to execute the FCS program and the technical challenges associated with system engineering, software engineering, and system integration. (Pernin et al., 2012)

The FCS program was launched in 2003 with an estimated cost of $92 billion (including the projected cost of a fleet of war-fighting vehicles). By 2009, when Defense Secretary Robert Gates announced that he wanted to scrap the core of the program, the suite of combat vehicles, the estimated cost had ballooned to some $200 billion.

AIS/Net 1000, the FAA's AAS program, and Army's FCS program were all attempting to solve enormously complex problems. When complexity becomes unmanageable, a crisis in the prevailing paradigms becomes apparent. AIS/Net 1000 had as its goal to ameliorate the crisis without inducing a paradigm shift. But that's not how it played out. Instead, the Internet emerged. The other two projects failed in part because they attempted to address crises with a wholesale replacement of many existing paradigms all at once. But technology paradigms grow more organically, more bottom up than top down. They are not imposed on engineers so much as discovered, nurtured, and grown by engineers.

Wholesale simultaneous replacement of several paradigms at once fails because each individual replacement has poor prospects of success. The fact is that most technology innovations fail. We tend to remember only the ones that succeed. It is often impossible to predict which of several competing technologies will eventually prevail, even when it seems clear which technology is more fit.

The layering of paradigms offers a fundamentally creative way to deal with a crisis of complexity. One solution is not to fix a broken paradigm, replacing it with a new one, but rather to build an entirely new paradigm on the scaffolding of the old. We build platforms on top of platforms. Because of separation of concerns, a layer in a paradigm can change, and the effects will only be felt one layer up. In the Internet, for example, at layer 3 of the OSI model, the world is in the midst of migrating from IP version 4 to IP version 6 (no version 5 was ever deployed).

The new version IPv6 changes quite a few fundamental things, including the addresses used to identify nodes in the Internet. IPv4, it turns out, provides only four billion distinct addresses. Given that there are already six billion devices on the Internet, this obviously creates problems. Considerable cleverness is required to reuse addresses without creating ambiguities. IPv6 increases the number of addresses to

2 128 = 340,282,366,920,938,463,463,374,607,431,768,211,456.

It simply would not be possible to make such a fundamental change without the separation of concerns offered by the OSI model. This change has no effect, for example, on the JPEG encoding of images transmitted over the Internet.

Similarly, the layers of digital technology in figure 3.3 provide separation of concerns that permits independent simultaneous evolution at all levels. For example, the shift to using FinFETs for transistors had no effect on the design of instruction set architectures, except perhaps to offer opportunities through added capability. I examine this question of opportunity next.

6.2 危机与失败

库恩认为，科学革命只有在旧范式下的异常情况积累到一定程度后才会发生，而且只有在出现新范式取代旧范式的时候才会发生。但是，这些并不是推动技术范式转换的驱动力。

导致技术范式转换的原因至少有三个。首先，正在设计的系统的复杂性超出我们人类理解或控制这些系统的能力。例如，编程语言的出现是因为编写正确的机器码或汇编代码变得异常困难。其次，去做一些以前没有人想象得到的事情开始变得可能。例如，谷歌和其他搜索引擎几乎可以即时搜索到人类发布过的任何内容。第三，复杂的社会、政治和商业力量可以推动技术范式的转换。例如，军事需求根本性地创造了航空、核武器和许多其他技术，同时军事预算也为计算机技术的早期发展提供了大部分资金。

在 3.2 节「简化的复杂性」中，我指出复杂性的一个来源是使用了大量的组件。即使是那些具有简单功能的简单组件，例如充当开关的晶体管，只要有足够的数量，也能实现极其复杂的功能。几十年来，植根于这些晶体管的数字技术一直是推动由复杂性所驱动的范式转换的主要原动力。

1965 年，英特尔的联合创始人戈登·摩尔做出一个著名的预测，集成电路中的元件（晶体管、电阻、二极管和电容）的数量将在未来，至少是 10 年内以每年翻一番的速度增长。1975 年，他又将预测的增长率修正为每两年翻一番。这就是人们熟知的「摩尔定律」。自提出之日起，该定律就一直是半导体行业的指导原则。

图 6.2 库恩认为，在科学革命中，新范式通常会取代旧范式。在技术革命中，新范式可能建立在旧范式的基础之上，将旧范式隐藏在一个抽象层之后，而不是彻底取代它们。（在加拉帕戈斯群岛的圣克鲁斯岛上，一幅带有查尔斯·达尔文肖像的标语照片。)

实际上，直到 2015 年左右，摩尔的预测一直保持稳定。英特尔 8080 是 1974 年推出的一款单片计算机（简称单片机），有大约 4 400 个晶体管。因此，根据摩尔定律，2014 年生产的一台单片机应该包含如下数量的晶体管：

4 400×2(2014-1974)/2

≈ 4 610 000 000

这个数值与 2014 年推出的英特尔 Xeon Haswell-E5 处理器上的 55.6 亿个晶体管的数量非常接近。尽管许多人都预言了摩尔定律的消亡，但是，大多数行业观察人士似乎都认为，直到 2015 年，摩尔定律所预测的增长速度才显著放缓。

数字技术能力的快速加速已经引发了一系列持续的危机。用于设计和编程系统的模型与机制在额外能力的挤压下会不可避免地反复崩溃。早在 1972 年，艾兹格·迪杰斯特拉曾写道：

坦率地说，只要没有机器，编程就根本不是问题；当我们有了几台功能较弱的计算机时，编程会成为微小的问题；而现在，我们有了强大的计算机，编程也随之变成了一个同样巨大的问题。（迪杰斯特拉，1972）

迪杰斯特拉说上面这段话的时候还只是 1972 年！如果这在当时已经是一个巨大的问题，就没有文字可以描述其现在达到的程度了。

摩尔定律只适用于基于单个硅芯片的单个计算机。今天，我们看到通过网络互连的计算设备的数量在急剧增长。大约在 1980 年，3Com 公司的联合创始人之一、以太网（以太网是当今使用最广泛的有线网络技术）的共同发明人罗伯特·梅特卡夫就提出了一个现在被称为梅特卡夫定律的推定。该定律认为，网络的价值与网络上兼容的通信设备的数量平方成正比。例如，如果单台设备的价值为 1 美元，那么一个连接 10 台设备的网络的价值就是：

1 美元 × 102

=100 美元

以此类推，一个拥有 100 台设备的网络价值将会是 1 万美元，而一个有 1 000 台设备的网络价值为 100 万美元。现在我想让你计算一下梅特卡夫对今天的互联网估值，今天的互联网有大约 60 亿台连接的设备。

连接设备的数量还在快速增长。今天，由于所谓物联网（IoT）的兴起，行业领袖们预测到 2020 年将会有 500 亿台联网设备。物联网连接的并不是最重要的计算机设备，例如恒温器、汽车、门锁、空调等等。当他们给出这样的预测时，你几乎都可以看到美元在他们眼中舞动的样子。

因为价值可能来自容量，所以我假设梅特卡夫会得出这样的结论 —— 网络容量，不仅是网络的价值，也与所连接设备数量的平方成正比。由于复杂性通常也与容量相伴而生，因此，我们不应指望未来几年危机会有所减缓。即便是摩尔定律逐渐失效，日益增长的复杂性所引发的压力也不会减缓。

在任何技术中，日益增加的复杂性都会以两种方式导致危机。首先，设计可靠的系统变得更加困难。在复杂性较低时运行良好的工具和模型会随着复杂性的增加而不断受压，直至崩溃。其次，也许是前一个原因的结果，一个设计项目失败的可能性增加了。

20 世纪 80 年代初，当我在贝尔实验室工作时，有一个名为 AIS/Net 1000 的大型项目，该项目旨在为当时存在的不同计算系统提供连接的桥梁。当时，通过网络连接计算机还是一种相当新颖的现象，而且，正如梅特卡夫定律所表明的，这种互联的价值得到了认可。至少梅特卡夫认为如此。

但是，这种互联暴露了不同计算机系统之间的许多不兼容性，特别是来自不同供应商的计算机系统。原因在于这些系统都被设计成是独立工作的，例如用来表示数字和文本的二进制模式是各不相同的，这些比特在存储器中的排列顺序不同，用于通信的协议和速度不同，等等。这些差异意味着一台计算机通常无法与另一台计算机直接进行通信。即使可以，其也无法正确地解释由另一台计算机生成的比特模式。换句话说，每台计算机都在它自己的范式中运行，而这些范式是不可通约的。

随着计算机的网络化，这种不兼容引发了一场危机。AIS/Net 1000 项目旨在通过在网络中执行不同范式转换的机制来解决这个问题，从而允许不同的范式继续存在。当一台计算机向另一台计算机发送消息时，该消息将在传输过程中被自动转换。这样任何操作计算机的人都不需要改变自己的工作方式。这就是网络中的巴别鱼。然而，这个项目最终还是失败了。美国电话电报公司取消了超过 10 亿美元的开发经费。事实上，项目失败的主要原因在于，很少有客户愿意为这项服务付费。相反，这些不同的计算机系统的范式正好见证了达尔文主义的稳固。相互竞争的范式无法在同一个联网计算机的生态系统中共存。

小说中的巴别鱼并没有出现，而互联网出现了。开放系统互连（OSI）模型被接受是促成互联网诞生的一个关键因素。

OSI 是一个分层的建模范式，如图 6.3 所示。与图 3.3 所示的各层一样，OSI 模型的每一层都为计算机之间的通信提供了一个概念框架。最低层被称为物理层，负责将比特序列从一个位置传输到另一个位置，而不关心这些比特的具体含义。最低层之上的层赋予这些比特更多的意义。举例来说，第 6 层，即表示层，可以将 100 万比特的集合当作一种特定格式的图像编码，如在数码相机和网络上广泛使用的标准化 JPEG 格式。

库恩认为不同的范式之间是不可通约的。在 OSI 模型中，「帧」、「包」、「段」和「会话」都是指一个有限的比特集合，但它们都在不同的层次上，且具有不同的含义。以我个人的经验来说，理解这些不同的含义是使用低层次网络软件时最容易混淆的部分之一。在其中的一个具体层次上工作而不尝试跨越多个层次通常要容易很多。

把 OSI 模型的各个层称为「范式」也许令人感到奇怪，因为它们与库恩所说的科学范式有很大的不同。就像库恩提到的范式一样，它们也确实为人类理解系统如何运行提供了一个心智模型。例如，想象一台计算机向另一台计算机发送一幅图像（照片），是与想象一台计算机发送 100 万比特的数据流不同的心智模型。但与库恩的范式不同，要使这些层发挥作用，就必然要对它们进行绝对精确的定义。如果这 100 万比特中的一个被误读，那么也许会导致图像变得不可读。然而，库恩的范式要更具鲁棒性，它们能够容忍一定程度的创造性误读，这有时会形成创新的引擎，甚至会引起范式的转换。

7. 应用层

应用程序直接使用的网络服务

6. 表示层

将比特模式解释为文本、图像、数字等

5. 会话层

将多个来回的数据父换视为一个单兀

4. 传输层

数据段的可靠传输

3. 网络层

在一个多结点网络对比特构成的数据包进行路由

2. 数据链路层

在两个结点之间传输一个比特帧

1. 物理层

有线或无线的比特流

图 6.3 用于计算机间通信的 OSI 模型。

要使得 OSI 模型的这些层都非常精确并不是一件容易的事情。为了使互联网上的计算机能够可靠地通信，OSI 模型需要在每一层上就精确的含义达成一致，向下一直到对每个比特的解释。然而，制定这份标准协议的过程可能会牵扯太多的因素，它很可能会陷入一个混乱的、政治的、官僚主义的泥潭，因为这一协议与涉及的每个国家以及每个商业集团的利益都息息相关，这常常会引发利益上的冲突。

下面这个例子可能有助于我们了解 OSI 模型是如何产生的。OSI 模型是两个标准化机构共同努力的结果，即国际标准化组织（ISO）和国际电信联盟电信标准化部门（ITU-T，前身是国际电报电话咨询委员会 CCITT），它们在 20 世纪 70 年代末分别开发了类似的计算机通信模型。但是，类似的模型还不足以让计算机进行通信。模型必须是相同的，才能实现不同计算机之间的通信。因此，这两个机构联合发布了一份共同文件。很显然，这一过程无疑会涉及多个细节问题上的激烈争论。

为了了解所涉及的所有竞争利益，我们很有必要先了解一下这些标准化机构是如何组织的。国际标准化组织由来自 162 个国家的标准化机构的代表组成。ITU-T 是一个联合国机构，它主要负责协调电信标准。除了许多国家政府的代表，这些机构还包括来自竞争行业的代表，其中一些企业已经对正在标准化的技术投入了大量资金。因此，标准的制定是一个旷日持久且令人心力交瘁的过程，而随后的相互妥协有时会破坏所形成标准的效力。

JPEG 是最常用的图像编码，也是 OSI 模型第 6 层（表示层）中常用的一个标准，它是创建该标准的联合摄影专家组（Joint Photographic ExpertsGroup）的首字母缩略词。这个小组是一个由 ISO/IEC JTC1 和 ITU-T 交叉组成的委员会，除了加入国际电工委员会（IEC），这些组织也都是 OSI 模型的成员。IEC 是一个非政府性的国际标准组织，主要为电气、电子和相关技术制定标准。当然，看着这一连串的缩略词，我想读者们也能感受到这一切是多么官僚。

与许多此类的国际标准一样，JPEG 所带来的复杂性之一与知识产权有关。建立这种国际标准要面对的一个主要挑战就是，如何确保任何人在不侵犯他人权利的情况下合法地使用该标准。在制定一个标准的过程中，可能会出现相当多的不同立场，获得专利权的企业将试图确保这个标准的使用要为它们所拥有的专利支付许可费用，或者在实施标准时，使它们所拥有的专利具有竞争优势。有些组织甚至会在暗中这样做，并且会向标准机构隐瞒其商业利益，等到标准形成后就不好再改变了。因此，标准往往不能反映问题的最佳技术解决方案。

现在，我们来看看 JPEG 的情况，该标准发布后，一些公司声称该标准侵犯了它们持有的专利。在从 2007 年开始的一系列著名诉讼案件中，全球专利控股有限责任公司声称，从网站下载 JPEG 图像或通过电子邮件发送图像的行为侵犯了其所持有的一项美国专利，专利编号为 5253341，该专利是由罗茨马尼特和贝林森在 1993 年申请的。同一时期，一连串的诉讼、反诉讼和威胁接踵而至。维基百科上有如下关于 JPEG 的一篇文章：

全球专利控股有限责任公司还利用 341 这项专利起诉或者威胁那些对大量软件专利进行直言不讳的批评的人。（https://en.wikipedia.org/wiki/JPEG，2016 年 4 月 26 日检索。）

经过广泛而持久的斗争，2009 年，美国专利和商标局颁发了一份专利复审证书，撤销了该专利的所有权利主张，并声明现有技术已使得这些主张无效。至此，许多组织已经为这项完全没有成效的知识产权斗争浪费了大量的资金。

一家专利控股公司是指并不制造或销售产品，只是为了从销售产品的公司收取专利使用费而获取和持有专利的公司。这类公司通常被称为「专利巨魔」，这个名字取自挪威童话故事《三只山羊嘎啦嘎啦》里的巨魔，它住在一座大桥的下面，会吃掉任何试图经过那里的动物。

专利巨魔的出现极大地改变了美国科技公司的商业环境。一个生产产品并拥有专利组合的组织可能犹豫是否起诉另一个拥有专利组合的组织，因为被起诉的组织可能会对专利侵权提起反诉。但是，一个不生产任何产品的组织更不容易受到反诉。这些组织的存在只是为了从生产产品的组织那里攫取资金。在我看来，它们其实就是寄生虫。

我似乎又离题了。我要表达的一个主要观点是 —— 技术中那些范式层的性质不仅取决于创造力，而且会受到复杂的商业和政治利益因素的干扰。因此，可以这么说，这些层在任何意义上都不是什么客观真理，它们是有缺陷的，也是人为过程的结果。

AIS/Net 1000 项目没能解决彼此不兼容的计算机联网时所引发的危机。这场危机在很大程度上以互联网的出现而得到解决，互联网 OSI 模型中所有层的标准化。所谓互联网协议（Internet Protocol，缩写为 IP，不要与知识产权混淆）在 OSI 模型中处于第 3 层，即网络层。互联网上的所有流量都要使用该协议。网络层之上是传输层，该层中广泛使用传输控制协议（TCP），其在 IP 之上增加了可靠传输的概念。具体而言，发送到接收方计算机的 TCP/IP 数据包必须得到该计算机的确认。发送方计算机将重复发送该数据包，直到它接收到一个应答为止。因此，除非网络或者发送方、接收方计算机发生了灾难性故障，否则每个发送的数据包最终都会被接收。TCP 还确保按顺序发送的数据包将以相同的顺序被接收。TCP 对于电子邮件和许多其他服务来说是必不可少的。

而电子邮件则依赖于另一种 SMTP，即简单邮件传输协议。该协议位于第 5 层（会话层）。在这一层，TCP/IP 数据包序列被汇集到一个单元，即一个电子邮件消息。由于能够假定下面这些层的特性，特别是数据包是被可靠且有序地分发的，因此该协议的设计被大大简化了。如果你通过电子邮件发送 JPEG 图像，那么，用于 JPEG 图像编码的第 6 层（表示层）协议将会把包中包含的比特数据定义为图像。

OSI 模型对这些关注点进行了分离，其中，数据包的路由、可靠传输与排序、电子邮件地址和内容以及电子邮件内容的编码都是分开的。所涉及的每个协议都更易于设计和理解，因为它们使用下面各层的属性，并且避免了提供上面一层所应提供的功能。

AIS/Net 1000 项目为网络危机提供了一个解决方案，但事实证明，它并不是一个非常有优势的解决方案。我认为一个关键的原因是，它没有实现有效的分层。AIS/Net 1000 是实现互联的单一解决方案，而 OSI 模型使每一层的许多解决方案能够相互竞争，实现优胜劣汰，从而创造出一个类似于达尔文式的生态系统。在这个生态系统中，解决方案可以互相竞争。

还有其他一些引人注目的技术也有类似的失败原因。埃德·科恩在他的博客里讲述了美国联邦航空管理局（FAA）高级自动化系统（AAS）项目失败的原因。该项目于 1981 年开始构思，1994 年被迫终止（科恩，2002）。在这个项目中，联邦航空管理局与 IBM 联邦系统公司签约，以一种全新的现代设计来替代美国的空中交通控制系统。IBM 联邦系统公司是 IBM 的一个部门，后来被洛克希德·马丁公司收购。根据科恩的说法，「联邦航空管理局最终宣布，之前花费的 26 亿美元中，有大约 15 亿美元的硬件和软件是无用的」。

这个项目的背景很能说明问题。1981 年，美国空中交通管制管理人员举行大罢工，罗纳德·里根总统立即解雇了他们中的 11 345 人。这反而加剧了一场原本由老旧、僵化的空中交通管制系统引发的危机。所以，解决这场危机的方案之一是建立一个自动化程度更高的管制系统，以雇用更少的管制人员来管理更多的飞机。

罗伯特·布里彻曾经在 IBM 联邦系统公司参与过这个项目，他在《软件的局限性：人员、项目和视角》一书中写道，这「可能是有组织工作历史上最大的失败」（布里彻，1999:163）。

该项目为什么会失败？科恩引用了下面这段美国审计总署资深分析师皮特·马里什的话：

这基本上是一个大爆炸式的方法，如此庞大的项目将在一夜之间彻底改变美国联邦航空管理局的工作方式。（科恩，2002）

科恩还引用了在 IBM 联邦系统公司参加该项目工作的比尔·克兰普夫的一段话：

我们没有完成需求阶段，一下子就进入了软件阶段。（科恩，2002）

然而，我认为克兰普夫的观点可能是对这个问题的错误诊断。我非常怀疑在开始软件设计工作之前完成需求阶段的工作是否真的能解决问题。在进行详细设计之前完成需求的想法，与当今最流行的软件工程策略之一的「敏捷开发」理念背道而驰。在敏捷开发过程中，需求与软件是一起通过一系列的增量式「冲刺」开发形成的，这些冲刺是短暂的开发工作，且仅有部分目标是针对整个项目目标的。敏捷开发的过程会直接涉及客户，并期望需求随着项目的发展而不断得到完善。这种管理复杂性的方法比在设计之前制定规范更加现实。

我认为马里什对问题的诊断可能更为准确。大规模的技术替代通常需要同时对太多的范式进行转换。事实上，科恩将该项目的失败归因于对新兴的不成熟技术范式的过分乐观，其中包括面向对象设计、分布式计算和 Ada 编程语言等。科恩写道：

AAS 被认为是基于 Unix 操作系统的分布式计算以及采用 Ada 语言进行开发的一个展示窗口，Ada 是由美国空军创建的一种编程语言，后来成为由国家资助的面向对象技术的「教义」，其本身是一种相对年轻的方法，用于以自包含的、可重用的块来编写代码。（科恩，2002）

我觉得「国家资助的教义」这个说法很有趣，它反映了我常常在计算机科学家中见到的教条主义狂热。这些科学家忘我地献身于一种或另一种编程语言。

另一个大型工程项目的重大失败，是美国陆军的未来作战系统（FCS）计划。虽然这个工程项目失败的原因有很多，但其中有一个类似于联邦航空管理局 AAS 项目失败的原因：该计划过于雄心勃勃，想要一次更换太多的系统。兰德公司 2012 年的一份报告有如下描述：

与更为传统的获取策略相比，系统的系统（systems-of-systems）这一方法显著地增加了执行 FCS 计划所需组织的复杂性，同时也加大了与系统工程、软件工程和系统集成相关的技术挑战。（佩尔南等，2012）

FCS 计划于 2003 年启动，预计耗资 920 亿美元（包括一支作战车队的预算费用）。到了 2009 年，当国防部长罗伯特·盖茨宣布他想要废除这个计划的核心部分 —— 这支作战车队时，估计成本已经高达 2 000 亿美元了。

AIS/Net 1000 项目、联邦航空管理局的 AAS 项目以及美国陆军的 FCS 项目最初都雄心勃勃地想要解决极其复杂的问题。但是，当复杂性变得难以控制的时候，主流范式中的危机就变得显而易见了。AIS/Net 1000 的目标是在不引发范式转换的情况下缓解危机。但是，事情并未像预想的那样发展。相反，互联网出现了。另外两个项目的失败在某种程度上是因为它们都试图用大规模替换现有范式的方法来应对危机。然而，技术范式的发展更为有机，更自下而上，而非自上而下。与其说技术范式是强加给工程师的，不如说其是由工程师发现、培育并成长起来的。

大规模地同时替换多个范式肯定会导致失败，因为单个范式的转换前景很难立即成功。事实上，大多数技术创新都以失败告终。可是，我们往往只记得那些成功的例子。在现实中，即使有哪种技术看似更合适，其发展的结果也是很难预料的，人们往往无法预测几种相互竞争的技术中究竟哪种最终会占上风。

范式的分层提供了处理复杂性危机的基本的创造性方法。一种解决方案是，不去修复一个有缺陷的范式，而是以一个新范式取而代之，在旧范式的基础上建立一个全新的范式。也就是说，我们在平台之上构建平台。由于每个层分别关注各自的主要问题，范式中的一个层就可以发生变化，而且其影响只会被它上面的一层感知。例如，在互联网中，OSI 模型第 3 层的网络协议正处于从 IPv4 迁移到 IPv6（从来没有部署过 IPv5）的过程中。

新版本 IPv6 改变了很多基本的东西，包括用于识别互联网中结点的地址。事实证明，IPv4 只能提供 40 亿个不同的地址。考虑到现在互联网上已经有 60 亿台设备，这显然会引发问题。因为在不产生歧义的情况下，重用地址需要相当聪明的方法。IPv6 中，地址数目将会增加到如下数量：

2128

=340 282 366 920 938 463 463 374 607 431 768 211 456。

如果没有 OSI 模型提供的分离式设计，就不可能做出这样的根本性改变。例如，这一变化对通过互联网传输图像的 JPEG 编码没有产生任何影响。

同样，图 3.3 中的数字技术层也提供了分离式设计，这允许在所有层次上同时进行独立的演化。例如，转向采用 FinFETs 作为晶体管的这一改变，不但不会对指令集体系架构的设计产生任何影响，还可能通过增加的功能提供更多的机遇。接下来，我就要研究一下机遇的问题。

### 6.3 Crisis and Opportunity

According to Kuhn, observations made in the course of normal science may reveal anomalies, inconsistencies with the prevailing paradigm that governs the normal science. These anomalies can create a crisis that leads to a paradigm shift. In engineering, it is not usually scientific observations that create a crisis. We have already seen that increasing complexity can trigger a crisis. A second trigger is opportunity.

Consider the introduction of the iPhone in 2007. At the time, two of the dominant producers of cell phones were Nokia, a Finnish company, and Research in Motion (RIM), a Canadian company, maker of the Blackberry. These two companies have drastically reduced visibility in the cell phone market today. I've already pointed out that the iPhone introduced no new technology. So why was it such a revolution, summarily overthrowing the old regime?

The「crisis」in the paradigm overthrown by the iPhone was not a crisis of complexity. It was a crisis of opportunity. At the time, cell phones were starting to be used for things other than making phone calls. The Blackberry had captured business markets with its built-in keyboard and email capability. Nokia phones were routinely used, primarily by young people, to send text messages, despite the incredible awkwardness of typing text on a 12-key numeric keypad. At the time, there were even contests for speed texting on such keypads because it required quite a bit of skill.

In 2007, wireless networks had modest ability to carry data, although the emphasis was still on carrying voice signals. That capability was exploited by the Blackberry and by the texting function on other phones, but phones were still primarily for making voice calls. Today, the ability to make voice calls seems like an incidental feature of a smartphone. When I want a voice call with my 20-year-old daughter, I need to exchange several text messages with her to arrange it. She acquiesces only in deference to my age. Her other communications are likely more through Snapchat and other services I've never heard of.

The iPhone came about through a realization of what was possible with the technology of the time. But the real revolution was not replacing the phones of the time with better phones. It was the introduction of a whole new platform, a new layer in the stack of layers of paradigms. Specifically, the real revolution was the introduction of the app development platform. With the introduction of the iPhone, Apple published the specifications that enabled millions of creative programmers around the world to develop applications for the phone and in 2008 launched the App Store to broker the sales of apps to customers.

The nature of a revolution is that its consequences usually cannot be anticipated, but after the revolution its consequences seem inevitable. It is easy to forget today that in 2007, most of us had never heard of apps and app stores, although, as usual with technology innovations, variants of the concept had existed at least since the 1990s. But Apple really made the concept take off. Apple's model has been emulated by every cell phone vendor that still has any significance, down to the details of copying the patented look of the phone, a move that has resulted in an endless string of patent infringement lawsuits and countersuits.

I am quite sure that if we could time travel back to 2007 and assemble the smartest, most creative experts worldwide in a room, they would not be able to anticipate even 10% of the functions that we routinely carry around in our pockets today: instant traffic reports worldwide (go ahead: check the traffic in Budapest right now), airline reservations, banking transactions (even check deposits), tide charts, worldwide weather forecasts, up-to-the-minute mass transit timetables, remote monitoring of our homes, a taxi service, restaurant reviews, a library with millions of books and journals, and many creative games. In addition to all those functions that never before existed, the device replaces several other devices that we previously would have to carry separately, including the phone, the music player, a flashlight, the keys to our house, the video entertainment device (remember the portable DVD player?), a compass, a calculator, an address book, our calendar, a camera, a radio, a notepad, and an alarm clock. Oh yes, and it also sends text messages and email.

The smartphone was not a consequence of a crisis of complexity, it was a consequence of opportunity enabled by millions of transistors on a chip, good digital radios, touch-screen interfaces, and the Internet, all preexisting technologies. The key to the revolution, the decisive battle that won the war, was the app development platform and the app store.

In recent years, we have seen an astonishing number of similarly disruptive revolutions. Amazon put thousands of bookstores out of business and is in the process of threatening the rest of retail. Uber and Lyft have undermined the taxi business. Lulu and other print-on-demand services are threatening the publishing industry. E-books are threatening Lulu and the rest of the printing industry. Libraries are increasingly irrelevant. Travel agencies have almost entirely vanished.

Each of these revolutions entails a paradigm shift. But paradigm shifts do not come easily to people, lending some stability and inertia. Even disruptive changes take some time to play out. According to Statista, an online statistics company, there were still about 28,000 bookstores in the United States in 2012. Although this number is down significantly from more than 38,000 in 2004, it is still a significant number.

Some paradigm shifts replace prior paradigms. Even for taxi services, for example, we are now more likely to summon them using a smartphone app or a web page than via a phone call. But all of these paradigm shifts also build on prior paradigms, leaving them unchanged. Smartphone technology, for example, relies heavily on Internet technology, the latter of which has hardly changed in response to this revolution. To be sure, there are small changes, such as better support at OSI levels 6 and 7 for small screens, but these changes are tiny. The prior paradigm provides a platform for the new paradigm, a situation rarely seen in the scientific paradigm shifts that Kuhn talks about. The transitivity of models makes this possible.

Many examples of failed paradigm shifts also exist. In the 1980s, for example, several university research projects and startup companies were going to disrupt the computer industry with dataflow computers, which presented an entirely different way to define an instruction set architecture (Arvind et al., 1991). These all failed. Perhaps a more curious example is the repeated failure of artificial intelligence (AI) as a field. AI has survived several boom and bust cycles, where unbridled enthusiasm is followed by disillusionment and collapse of investment. Starting in the late 1980s, AI experienced what some researchers in the field called an「AI winter,」an allusion to a nuclear winter, and had only fully recovered by about 2010. Perhaps dataflow computers will be similarly resurrected. Such failures fade quickly from our memory (except, of course, for the people most directly involved in the failures).

But failures are a normal and healthy part of intellectual inquiry. The rapid advances of digital technology provide a healthy, thriving ecosystem for mutation, adaptation, and extinction of paradigms. There need not be anything fundamentally wrong with a new paradigm that fails. Unlike scientific paradigms, technology paradigms are not held up to a standard of truth or concurrence with observations of the physical world. Their survival instead depends on many intangibles, including, perhaps most important, the readiness of the public and even the engineers to assimilate the paradigm.

6.3 危机与机遇

库恩认为，我们在常态科学中观察到的异常现象可能会揭示异常，其与常态科学的主流范式不一致。这些异常可能会导致一个范式转换的危机。在工程学中，通常并不是科学观察导致危机的产生。我们已经看到，日益增加的复杂性可能会引发一场危机。第二个触发危机的因素则是机遇。

让我们再来考虑 2007 年推出苹果手机时的情形。当时，手机的两个主要制造商分别是芬兰的诺基亚公司和加拿大的黑莓公司。今天这两家公司在手机市场上的知名度已经大不如前了。正如我已经指出的那样，苹果手机并没有引入任何新技术。那么，为什么它会成为一场革命，一下子就推翻了旧的格局呢？

被苹果手机颠覆的范式中的「危机」并不是一场由复杂性引发的危机，这是一场机遇危机。当时，除了打电话，手机也开始被用作他途 —— 这正是它能够快速发展的新机遇。黑莓手机凭借其内置的键盘和电子邮件功能占领了商业市场。尽管在 12 键数字键盘上输入文本令人觉得很不方便，诺基亚手机还是经常被用来发送短信，主要是年轻人在使用。在当时，甚至出现了在这种键盘上快速发送短信的竞赛，因为这需要相当多的技巧。

2007 年，无线网络具有了一定的数据传输能力，但主要是传输语音信号。黑莓和其他手机的短信功能都充分利用了这一功能，但手机仍然主要用于语音通话。今天，语音通话似乎只是智能手机的一个附带功能。当我想和我 20 岁的女儿通话时，我需要和她交换几条短信来安排这件事。她只是出于对我年龄的尊重而默许这样的安排。她的其他通信方式可能更多是 Snapchat（照片分享应用程序）和其他我从未听说过的应用程序。

苹果手机的出现，是因为人们意识到在当时的技术条件下什么是可能的。但是，真正的革命并不是用更好的手机取代当时的手机。真正的革命是指引入一个全新的平台，并在范式层的堆叠中形成一个新的范式层。具体来说，真正的革命是引入应用程序开发平台。随着苹果手机的推出，苹果公司发布了一系列规范，这使得全球数百万富有创造力的程序员能够为苹果手机开发各种应用程序。

2008 年，苹果公司推出了应用商店（App Store），从而向客户代理销售应用程序。

革命的本质在于，其后果通常是无法预料的，但在革命发生之后，其后果似乎又是不可避免的。今天我们也许已经淡忘了曾经发生的一些事情。

2007 年，我们中的大多数人从未听说过应用程序和应用商店。其实，这些概念与其他技术创新一样，其雏形概念至少在 20 世纪 90 年代就已经存在了。但是，苹果公司确实让这些概念迅速得到普及。并且，苹果公司的模式已经为每一个仍有影响力的手机厂商所效仿，甚至是复制手机外观专利的细节。此举导致了无穷无尽的专利侵权诉讼和反诉讼。

我可以肯定地说，如果我们回到 2007 年，把世界上最聪明、最具创造力的专家聚集在一个房间里，他们甚至都无法预测我们今天日常随身携带的手机中的 10% 的功能：全球即时交通报告（现在我们就可以查一下布达佩斯的交通状况）、机票预订、银行交易（甚至是查询银行存款余额）、潮汐图、全球天气预报、最新的公共交通时刻表，还有对自己家的远程监控、出租车服务、餐馆评论、一个拥有数百万本图书和期刊的数字图书馆，以及许多富有创意的游戏，等等。除了这些以前从未存在过的功能，这种设备还取代了我们以前必须单独携带的其他一些设备，包括电话、音乐播放器、手电筒、家里的钥匙、视频娱乐设备（还记得便携式 DVD 播放机吗）、指南针、计算器、地址簿、日历、照相机、收音机、记事本和闹钟。哦，是的，它还可以发送短信和电子邮件呢。

智能手机并不是一场复杂性危机的结果，而是芯片上的数以百万计的晶体管、良好的数字收音机、触摸屏接口和互联网等这些预先就已存在的技术所带来机遇的产物。这场科学革命赢得决定性胜利的关键，是应用程序开发平台和应用商店。

近年来，我们已经见证了数量惊人的类似颠覆性革命。亚马逊令数千家书店停业，并正在对其他零售业务构成威胁。优步（Uber）和来福车（Lyft）已经削弱了出租车业务。

Lulu 和其他的按需印刷服务正威胁着出版业的发展。而电子书又在威胁着 Lulu 和印刷业中的其他行业。昔日的图书馆变得越来越无关紧要。旅行社几乎就要消失了。

每发生一次这样的革命都需要经历一次范式的转换。但是，人们不再那么容易接受范式的转换，因为他们习惯于原来的范式，原来的范式具有一定的稳定性，并给人们带来一定的惰性。即使是颠覆性的转换，也需要一段时间才能完成，因为接受需要时间。根据 Statista（全球领先的数据统计互联网公司）的数据，2012 年美国仍有 2.8 万家书店。虽然这一数字大大低于 2004 年的 3.8 万家，但仍然是一个很大的数字。

一些范式的转换取代了以前的范式。以出租车服务为例，我们现在更有可能通过智能手机的应用程序或网页预约出租车，而不是通过打电话。但是，所有这些范式的转换都建立在先前的范式之上，并没有改变那些旧范式。例如，智能手机技术很大程度上依赖于互联网技术，然而，互联网技术几乎没有随着这场技术革命发生改变。当然，也出现了一些小的变化，例如 OSI 模型第 6 层和第 7 层现在可以更好地支持小屏幕，但是这些变化是很小的。先前的范式为新范式提供了一个平台，这种情况在库恩谈到的科学范式转换中很少出现。模型的传递性使这一切成为可能。

然而，也存在一些失败的范式转换。例如，在 20 世纪 80 年代，一些大学研究项目和初创公司尝试用数据流计算机颠覆当时的计算机行业，其中，数据流计算机提供了一种完全不同的方法来定义指令集体系架构（阿尔温德等，1991）。这些尝试都以失败告终。而人工智能（AI）作为一个领域的反复失败也许是一个更奇怪的案例。人工智能历经了好几个繁荣和萧条的发展周期，在这几个周期里，人们对它的狂热总是伴随着幻想的破灭和投资的失败。从 20 世纪 80 年代末开始，人工智能领域就经历了被该领域的一些研究人员称为「人工智能的寒冬」的阶段，它暗示了一个核冬天，直到 2010 年前后人工智能的发展才完全恢复。也许数据流计算机会以同样的方式复活。这样的失败很快就会从我们的记忆中消失（当然，那些与失败直接相关的人除外）。

然而，失败却是人工智能知识探索中一个正常和良性的过程。数字技术的迅速发展为范式的突变、适应和消亡提供了一个健康而繁荣的生态系统。一个新技术范式的失败并不是因为它有什么根本性错误。与科学范式不同，技术范式所秉承的并非真理式的标准，或是与物理世界的观察相一致的标准。相反，它们的存在往往取决于许多无形的因素，其中最重要的或许是，公众甚至工程师是否准备好接受这种范式。

### 6.4 Models in Crisis

Paradigm shifts in engineering are triggered primarily by crises of complexity and opportunity. These have been relentlessly driven for the last 50 years by the staggering advances in digital technology. They continue to be driven by the increasing interconnectedness of digital devices and penetration of computers into everything we use.

So where are the most pressing crises today? For this question, I can only speculate because I cannot see the future any better than anyone else. But I do see at least two substantial crises looming.

Let me start with a crisis of opportunity. With increasing interconnectedness comes rapidly increasing volumes of data about the state of the world, society, and individuals in society. For example, credit card companies already carefully track most of our purchases, missing only the ones where we pay cash. These companies construct models of our behavior and use those models for various purposes, including to disallow transactions that appear to be anomalous and hence may be fraudulent. For example, if you don't travel much, and a store in China tries to charge a purchase to your credit card, then the transaction is likely to be denied by the credit card company's computers. If you travel a lot, as I do, however, then this same transaction is more likely to be allowed. If you normally buy expensive scotch at boutique stores, then a purchase of Rotgut Moonshine at a store called Payroll Loans & Liquor is also likely to be denied.

These decisions are not made by humans; they are made by computers, the ones that dream (see section 5.6 ). The computers are running machine-learning algorithms that build models of your behavior by observing your transactions and then classify each subsequent transaction as anomalous or normal based on the probability that the model would generate such behavior.

The credit card example exhibits a contradiction that is common in big data applications. Although we probably appreciate that the credit card company attempts to prevent fraudulent use of our card, many of us find it creepy that the company has built a probabilistic model of our behavior. Similarly, we may like that a map app on our smartphone tells us about nearby restaurants, but we are likely not thrilled to learn that, as a consequence of using the app, Google's computers know where we are.

Now imagine that the computers in your car begin to communicate with the outside world. Some insurance programs already use such communicated information to vary your insurance bill based on usage and style of driving. What if the insurance company sells the information they get from your car to your credit card company? Metcalfe's law is based in part on the observation that aggregated data is more valuable than isolated data. The credit card company may now verify that your car is indeed parked at Payroll Loans & Liquor and either allow the transaction or report your car stolen.

The data being gathered about us by various organizations is growing at a staggering rate. In the United States, privacy laws intended to protect us from misuse of that data are ineffective because these laws have simply resulted in a barrage of small print that every organization is now required to throw at you, knowing that you will not read it. In fact, the U.S. government has exhibited a distinctly double standard, simultaneously trying to strengthen privacy laws and prevent encrypted data communication. Encryption, the government says, interferes with its ability to detect and prevent potential terrorist attacks. Indeed, it no doubt does. Again, we are faced with contradictory requirements.

Many organizations today are collecting but not effectively using vast amounts of data. Consulting and market research company Gartner calls「dark data」the「information assets that organizations collect, process and store in the course of their regular business activity, but generally fail to use for other purposes.」The subtext is that those same businesses are missing an opportunity. They should be mining the data. The data has value.

The research and consulting firm Forrester defines「perishable insights」as「urgent business situations (risks and opportunities) that firms can only detect and act on at a moment's notice.」Fraud detection for credit cards is just one example of such perishable insights. Once the transaction is allowed, the damage is done. We also saw another example of a perishable insight in chapter 1 in the Wikipedia vandalism detection algorithm, although that one has less privacy cost. More dramatically, dark data in health care and medicine could be much better used to get (literally) perishable insights.

I believe a crisis of opportunity exists today in converting data feeds into insights in time to make effective use of them while either ensuring privacy or at least preserving public trust that the loss of privacy will not be abused. Clearly, this problem is not just technical.

Contradictory requirements demand innovation. Consider that thermostats, door locks, television sets, watches, running shoes, football helmets, books, and, in fact, nearly everything around us is going online. And many devices are acquiring the ability to listen for spoken words and react to those words. And when you read an electronic book, it reads you back. Connecting those devices to the network is likely to deliver real value to us, including, for example, reducing our carbon footprint and vulnerability to terrorists. These potential benefits cannot be ignored. Neither can the risks. The situation is crying for a paradigm shift.

The second crisis I see looming is a crisis of complexity. This crisis is not just about increasing numbers of components but rather about the conjoining of engineered systems that have traditionally used different kinds of models to manage their own complexity.

Consider a modern commercial airplane such as the Airbus A350 or the Boeing 787. These systems are software-intensive with hundreds of microprocessors managing functions that include translating pilot commands into rudder movements, controlling the landing gear, managing cabin pressurization and airflow, managing electric power generation and distribution, and operating the passenger entertainment system. Such an aircraft is a much more complex system than, say, a data center handling Facebook pages. The latter only has to deal with bits and the heat generated by processing those bits. A data center is an information-processing system that can operate almost entirely within the models and paradigms of computer science. But an airplane design conjoins models of aeronautical, mechanical, electrical, and civil engineering, as well as those of computer science. In such a system, the structures of civil engineering interact with the flight dynamics of aeronautical engineering under the control of a software system (computer science) running a feedback control system (electrical engineering). The silos of specialization that are standard in engineering today become an obstacle because the models and paradigms developed in each of these disciplines are incommensurable.

Despite the enormous complexity and challenges of crossing so many silos, Boeing and Airbus both manage to make astonishingly safe and reliable airplanes. How? Today, their design processes and methods are extremely conservative, and regulatory oversight is heavy. Many rules must be followed to get an aircraft certified to carry civilian passengers.

However, these processes reveal some fundamental flaws in the engineering models and methodologies that are available today to aircraft engineers. A symptom of such flaws is a story I first heard from an engineer who had worked on the Boeing 777. This was Boeing's first fly-by-wire airliner, meaning that pilot controls are mediated by a computer. The 777 first entered into service in 1995. According to this engineer, as of the early 1990s, Boeing expected this model of aircraft to be in production for perhaps 50 years, to 2045. The engineer told me that in the early 1990s, Boeing purchased a 50-year supply of the microprocessors for the flight control system so they could use the same microprocessors for the entire production run of the aircraft.

Recall in chapter 4 my observation that hardware is ephemeral. In fact, any particular silicon chip is unlikely to remain in production for more than a few years. It becomes obsolete quickly under the pressure of Moore's law. And when a fab gets updated to leverage a new technology, such as the FinFET, it becomes impossible to produce an identical chip.

But why does Boeing need the chip to be identical? The whole point of the layering of paradigms of figure 3.3 is so that software designs are isolated from changes in the hardware. It should be enough to use any chip that can correctly execute the software for the control system.

But the flaw is in the notion of correctness. Starting at the instruction set architecture layer in figure 3.3 , and for all layers above that, what it means to correctly execute software has nothing to do with how long it takes to do anything. Timing of actions is not part of the programming paradigms used today.

But in a flight control system, the software is directly controlling physical actuators, and in the physical world, timing is important. In fact, in just about every model that an aeronautical, mechanical, or electrical engineer will use, timing of actions is central to the model. The paradigms used by these engineers are incommensurable with the paradigms used by computer scientists.

As a consequence, the layering of figure 3.3 fails to provide adequate abstractions, and hence fails to provide separation of concerns. Boeing is forced to operate without any layering, and hence has to ensure that every airplane carries exactly the same design all the way down to the semiconductor physics layer.

I have subsequently heard similar stories from engineers at Airbus, which has been making fly-by-wire aircraft for longer than Boeing. The engineers at Airbus tell me that they store the microprocessors in liquid nitrogen in an attempt to extend their shelf life by slowing down the natural diffusion processes of the dopants in the silicon.

The complexity of aircraft designs keeps increasing. A key objective in aircraft design is to decrease weight because this reduces fuel consumption, extends range, and improves the carbon footprint. One way to decrease weight is to use more advanced materials in the airframe and more flexible structures. But flexible structures require more tightly coordinated control systems. Timing discrepancies in control systems can create disastrous stresses on airframe structures.

Another way to decrease weight is to reduce the amount of wiring and hydraulic piping. This can be accomplished with more advanced networking, but essentially all of the layers in the OSI model of figure 6.3 above the physical layer also ignore timing, just like the software layers in figure 3.3 . As a consequence, aircraft manufacturers are unable to benefit from most of the advances in networking of the last 40 years.

Even with a fixed microprocessor, the fact that timing is irrelevant to correctness from the ISA up means that in software design, aircraft manufacturers also cannot use most of the innovations of computer science from the last 40 years. The FAA prohibits use of object-oriented languages, for example, in safety-critical software. Even interrupts, the standard way that all modern microprocessors get data in and send data out to the outside world, are prohibited. To be sure, interrupts create many subtle software problems. As far back as 1972, Edsger Dijkstra lamented,

[I]n one or two respects modern machinery is basically more difficult to handle than the old machinery. Firstly, we have got the interrupts, occurring at unpredictable and irreproducible moments; compared with the old sequential machine that pretended to be a fully deterministic automaton, this has been a dramatic change, and many a systems programmer's grey hair bears witness to the fact that we should not talk lightly about the logical problems created by that feature. (Dijkstra, 1972)

Despite this lament, to this day, interrupts remain the primary method for I/O and are central to every modern operating system design. Aircraft manufacturers, therefore, are also excluded from the last 40 years of advances in operating systems.

In view of the failure of most computer science innovations of the last 40 years to address the needs of aircraft designers, I am astonished at their remarkable safety track record. I have enormous respect for the engineers who design these planes. They are stuck with the prototype-and-test style of design that was used by Edison, unable to leverage the transitivity of models, and their prototypes are much more complex than Edison's prototypes.

Figure 6.4 shows a prototype of an Airbus A350, their newest model. Airbus calls this prototype an「iron wing.」The prototype includes all parts of an A350 except the airframe, cabin, and engines. This is why it doesn't look like an airplane. It is the guts without the skeleton or skin. The wiring is all exactly the same length as on the real aircraft. The hydraulic tubes are bent as on the real aircraft to get around the (missing) airframe structure. When running tests using this prototype, the same generators that are driven by the engines on the real aircraft are driven by artificial engines so that the prototype runs on its own power. This prototype is obviously much more complicated than Edison's lightbulbs, but it is exactly the same sort of concrete prototype.

Figure 6.4

An Airbus「iron wing」prototype of an A350.

Aircraft manufacturers are not alone in facing this problem. Modern cars are mostly「drive-by-wire」today, where the driver commands (pushing on the accelerator and brakes and turning the steering wheel) are mediated by a computer before going to the wheels or engine. Automotive designers face the same problems but with far fewer regulatory constraints. Their problems are only going to get worse as automation increases (lane keeping, automated accident prevention, and fully automated driving).

It doesn't stop there. Any modern factory is computer controlled and is similarly a safety-critical system where the timing of actions is essential to safety. Trains are computer controlled. Ventilation, lighting, and fire mitigation systems in buildings are computer controlled. Modern electric power grids, water distribution systems, and communication systems are computer controlled.

In 2006, Helen Gill of the U.S. National Science Foundation coined the term「cyberphysical systems」(CPS) for such systems that combine computing, networking, and physical dynamics. There is clearly today a crisis of complexity for such systems. She launched a major NSF initiative to fund research to address precisely the problems that I have indicated. This program continues, and progress is being made, albeit still primarily in research labs and not yet in industrial production. Gill recognized that conjoining the「cyber」world with the physical world created an enormous crisis of complexity, one that the existing paradigms were poorly equipped to deal with.

What is the origin of the term「cyber」in CPS? The related term「cyberspace」is attributed to William Gibson, who used the term in the novel Neuromancer , but the roots of the term CPS are older and deeper. It would be more accurate to view the terms「cyberspace」and「cyberphysical systems」as stemming from the same root,「cybernetics,」which was coined by Norbert Wiener (Wiener, 1948), an American mathematician who had a huge impact on the development of control systems theory. During World War II, Wiener pioneered technology for the automatic aiming and firing of anti-aircraft guns. Although the mechanisms he used did not involve digital computers, the principles are similar to those used today in computer-based feedback control systems. His control logic was effectively a computation, albeit one carried out with analog circuits and mechanical parts, and therefore cybernetics is the conjunction of physical processes, computation, and communication. Wiener derived the term from the Greek word for helmsman, governor, pilot, or rudder. The metaphor is apt for control systems.

The term CPS is sometimes confused with「cybersecurity,」which concerns the confidentiality, integrity, and availability of data and has no intrinsic connection with physical processes. The term「cybersecurity,」therefore, is about the security of cyberspace and is thus only indirectly connected to cybernetics. CPS certainly involves many challenging security and privacy concerns, but these are by no means the only concerns.

My own research at Berkeley includes some examples of NSF-funded research projects under the CPS program. One project that concluded in 2015, called the PRET project (for Performance with Repeatable Timing), designed an instruction set architecture that explicitly includes timing within its programming paradigm. In effect, this project reopened decisions that Fred Brooks made all the way back in the 1960s when he designed the System/360 ISA without any explicit control over timing. The project concluded with a demonstration that it is possible to achieve precise control over timing with no loss of performance and modest cost in hardware. If this architectural approach is adopted in industry, it will enable separation of concerns in the layering of figure 3.3 for cyberphysical systems.

A second example from my research group is a project we called PTIDES, which also concluded in 2015. This project addressed software that is distributed across networks, using the OSI model of figure 6.3 , but modifying the paradigms to explicitly control timing. But I will spare you the details.

Despite progress in the research labs, the crisis of complexity remains for cyberphysical systems, and the crisis of opportunity remains for data science. In the remaining chapters, I examine just how far we can push the layering of models. All the techniques we know of today have limitations, and understanding these limitations is essential to a complete understanding of technology revolutions. These limitations hint at opportunities for innovation.

6.4 危机中的模型

工程领域的范式转换主要是由复杂性和机遇引发的。在过去的 50 年里，数字技术的惊人进步极大地推动了范式的转换。数字设备日益互联的发展趋势以及计算机越来越渗透到我们日常生活中的现实，都成为工程领域范式转换的巨大驱动力。

那么，今天最紧迫的危机又在哪里？对于这个问题，我只能试着加以推测，因为在这个问题上，我的眼光并不比其他人更为独到。但是，我确实看到了至少有两场重大危机即将来临。

让我先从危机谈起。随着互联性的不断增长，有关世界、社会以及社会中个人状态的数据会迅速增加。例如，除了无法追踪我们以现金方式消费的信息，信用卡公司已经能够详细了解我们的大部分消费行为了。这些公司为我们的消费行为建立了模型，并将这些模型用于各种目的，包括可以禁止疑似欺诈的异常交易。例如，如果你不经常旅行，可是外国的一家商店试图从你的信用卡中扣除购物费用，那么信用卡公司的计算机可能会拒绝这笔交易。相反，如果你像我一样经常出差，那么同样的交易更有可能被允许。如果你经常在精品店购买昂贵的威士忌，那么你要是在 Payroll Loans &Liquor 这样的商店购买劣质酒的话，这笔交易同样有可能被拒绝。

上述这些决定不是由人类做出的，而是由那些会做梦的计算机做出的（详见 5.6 节）。计算机正在运行机器学习算法，它们通过观察你的交易状况构建你的行为模型，然后根据模型生成此类行为的概率，将每个后续的交易归类为异常和正常两类。

信用卡的例子展示了大数据应用中常见的矛盾。虽然我们非常理解信用卡公司这样做是为了防止客户的信用卡被盗刷，但是，当我们中的许多人发现，信用卡公司已经建立了我们行为的概率模型时，这将令人有些毛骨悚然。同样，我们可能会喜欢智能手机上的应用程序，它能够告诉我们附近的餐馆，但是，由于使用了该类应用程序，谷歌的计算机就会知道我们的具体位置。我想多数人在了解到这一点之后，一定不会觉得特别开心。

现在想象一下，你车里的电脑可以与外界进行通信。实际上，一些保险计划已经通过这些信息归纳出你的驾车风格和用车状况，从而改变你的保单。如果保险公司把从你的汽车中得到的相关信息卖给了你的信用卡公司，又会怎样呢？梅特卡夫定律部分地基于这样一种观点，即聚合数据比孤立数据更有价值。信用卡公司现在可能会核实你的车是否就停在 Payroll Loans & Liquor 商店外面，之后要么允许这笔买酒的交易，要么向警察局报告你的车被盗了。

各类机构都在以惊人的增长速度收集着与我们个人有关的各项数据。在美国，原本旨在保护我们私人数据不被滥用的隐私法变得徒劳无效了，因为这类法律只不过导致了大量印着密密麻麻小字体的法律文件的产生。现在，政府要求每个机构都必须向你出示这样的文件，当然，也很清楚你并不会去阅读这些文件。事实上，现在美国政府表现出明显的双重标准。它一方面试图加强隐私法的力度，同时又阻止加密的数据通信。政府表示，加密会干扰其侦测和防止潜在的恐怖袭击的能力。的确如此，这一点毫无疑问。

同样，我们再次面临相互矛盾的要求。

今天，许多机构都在收集大量的数据，但并没有有效地使用它们。咨询和市场研究公司高德纳将「机构在日常商业活动过程中收集、处理和存储的，但通常又不能用于其他目的信息资产」称为「暗数据」。其潜台词是这些企业正在错失一个机遇。它们应该好好挖掘这些数据，因为这些数据是非常有价值的。

从事研究和咨询的公司弗雷斯特将「稍纵即逝的洞察力」定义为「企业只有在得到通知后才能察觉和采取行动的各种紧急的商业情况（如风险和机会）」。信用卡欺诈检测只是这种稍纵即逝的洞察力的一个例子。交易一旦被允许，损失就会形成。我们在第 1 章还看到另一个稍纵即逝的洞察力的例子，是维基百科关于恶意破坏的检测算法，尽管这个例子的隐私成本更低。更引人注目的是，医疗和医药领域的暗数据可以被更好地用来获得（实实在在的）稍纵即逝的洞察力。

我相信，在及时将输入的数据转化为洞察力以有效利用它们的同时，确保隐私或者至少是保持公众的信任（隐私的丧失不会被滥用），是一个存在着机遇的危机，显然这个问题不仅仅是技术性的。

这些矛盾的要求呼唤着创新。让我们想想看，恒温器、门锁、电视机、手表、跑鞋、橄榄球头盔、书籍等等，我们周围几乎所有东西都被接入互联网。许多设备正在获得听取口语指令并对其迅速做出反应的能力。当你读一本电子书时，它也在阅读你，了解你的个人信息。将这些设备接入网络可能会给我们带来真正的价值，例如，减少我们的碳排放量和相对于恐怖分子的脆弱性。这些潜在的好处不容忽视。当然，其中的风险也不可小觑。这种情况迫切需要范式的转换。

我看到的第二个危机是一场复杂性的危机。这场危机不仅关系到组件数量的增加，而且关系到传统上使用不同模型来管理自身复杂性的工程系统的结合。

以现代商用飞机如空中客车 A350 或波音 787 为例，这些系统都是软件密集型的，拥有数百个微处理器以实现不同的功能，包括将飞行员的指令转换为舵机运动、控制起落架、管理机舱的增压和气流、管理发电和配电以及运行乘客娱乐系统等等。这样的飞机系统比处理脸书页面的数据中心要复杂得多。后者只需要处理比特数据以及因处理这些数据而产生的热量。数据中心是一个信息处理系统，它几乎完全可以在计算机科学的模型和范式中运行。但是，一架飞机的设计结合了航空、机械、电气和土木工程的模型，以及计算机科学的模型。在这样一个系统中，土木工程的结构与航空工程的飞行动力学在运行反馈控制系统（电气工程）的软件系统（计算机科学）控制下相互作用。在今天的工程体系中，专业化的「竖井」是标准的，这已成为一个障碍，因为每个学科开发的模型和范式是不可通约的。

尽管跨越如此多的「竖井」存在着巨大的复杂性和挑战，但波音公司和空客公司都设法成功地制造了安全可靠的飞机。它们是怎样做到这一点的呢？今天，它们的设计过程和方法都非常保密，并且监管很严格。想要获得运载民用乘客的许可证，必须严格遵守许多规则。

然而，这些过程揭示了工程模型和方法上的一些根本缺陷，这些缺陷对今天的飞机设计工程师而言仍然存在。这些缺陷中的一个问题是我从一位工程师所讲的故事中第一次得知的，这位工程师曾参与了波音 777 的设计工作。波音 777 是波音公司研制的第一架电传控制飞机，这意味着飞行的控制是由计算机实现的。波音 777 于 1995 年首次投入使用。根据这位工程师的描述，早在 20 世纪 90 年代初，波音公司就预计这种型号的飞机将生产 50 年，直至 2045 年。这位工程师告诉我，20 世纪 90 年代初，波音公司为飞行控制系统的制造购买了可用 50 年的微处理器，这样就可以在飞机的整个生产过程中使用相同的微处理器了。

请大家回想一下，我曾经在第 4 章提到硬件是短暂的。事实上，任何一种硅片的生产都不可能持续几年以上。在摩尔定律的压力下，它很快就会过时。当一家晶圆厂利用一项新技术（如 FinFET）更新其产品时，它就不可能再生产相同的芯片了。

但是，为什么波音公司需要相同的芯片呢？图 3.3 所示的范式分层的全部要点在于，使软件设计与硬件的变化分离开来。这样的话，使用任何能够正确执行控制系统软件的芯片就足够了。

但是，缺陷存在于正确性的概念上。从图 3.3 所示的指令集体系架构层开始，对于上面的所有层而言，正确执行软件的意义与其所要执行的动作所需的时间无关。动作的定时特性并未被包含在今天所使用的编程范式中。

但是在飞行控制系统中，软件直接控制物理执行器，而在物理世界，时间是很重要的因素。事实上，在航空、机械或电气工程师所要使用的每个模型中，动作的执行时间都是模型的核心。然而，这些工程师使用的范式与计算机科学家使用的范式是不可通约的。

因此，图 3.3 所示的分层未能提供足够的抽象逻辑，由此也就无法提供对这些关注点的分离。波音公司被迫在没有任何分层的情况下运作，因此就必须确保每架飞机的设计直到半导体物理层的设计都是完全相同的。

后来，我从空客公司的工程师那里又听到了类似的故事。空客公司制造电传控制飞机的时间比波音公司还要长。空客的工程师告诉我，他们把微处理器放在液氮中储存，试图让含有掺杂物在硅的自然扩散过程中能够延长保质期。

飞机设计的复杂性不断增加。飞机设计的一个关键目标是减少重量，因为这可以降低燃料的消耗，延长航程，并减少碳排放量。减轻重量的一种方法是在机身上使用更先进的材料和更灵活的结构。但是，灵活的结构需要更紧致的协调控制系统。控制系统中的定时差异会对机身结构形成巨大压力。

另一种减轻重量的方法是减少线缆和液压管道的数量。这可以通过更高级的网络来实现。然而，实际上，就像图 3.3 中的软件层一样，图 6.3 OSI 所示模型中物理层以上的所有层都忽略了定时的问题。因此，飞机制造商无法从过去 40 年的网络发展中获益。

即使是使用一款相同的微处理器，时间与指令集体系架构的正确性无关这一事实，也意味着在软件设计中，飞机制造商不能使用过去 40 年来计算机科学的大部分创新成果。联邦航空管理局禁止在安全关键软件中使用面向对象的语言。甚至作为所有现代微处理器获取数据并将数据发送到外部世界的标准方式，中断这一机制也是被禁止的。可以确定的是，中断会产生许多棘手的软件问题。早在 1972 年，艾兹格·迪杰斯特拉就感慨道：

从一两个方面来看，现代机器基本上要比老式机器更难操作。首先，我们拥有中断，其往往发生在不可预测和不可复制的时刻；与以往那些看上去像是确定性自动机的老式连续型机器相比，这是一个巨大的变化，而且，许多系统程序员头顶的白发都证明了这样一个事实，即我们不应该轻率地谈论由该特性所产生的逻辑问题。（迪杰斯特拉，1972）

尽管如此，直到今天中断仍然是输入 / 输出系统的主要方法，并且也是每个现代操作系统设计的核心。因此，飞机制造商也被排除在过去 40 年操作系统的进步之外了。

鉴于过去 40 年大多数计算机科学创新都未能满足飞机设计者的需求这一事实，我对其卓越的安全航行记录感到非常惊讶。我非常崇敬设计这些飞机的工程师。他们受困于爱迪生所使用的原型 — 测试设计风格，无法利用模型的传递性，而且他们的原型要比爱迪生的原型复杂得多。

图 6.4 给出了空客公司最新型号 A350 飞机的原型。空客公司称它为「铁鸟」。该原型包含了一架 A350 飞机除机身、机舱和发动机之外的所有部件。这就是它看起来不像一架飞机的原因，它是没有结构和蒙皮的一些内部装置。其内部的线缆长度与实际的飞机完全相同。液压管也像在实际飞机上那样弯曲着，以绕过（缺失的）机身结构。当使用这个原型进行测试时，在实际飞机上由发动机驱动的发电机由人工发动机驱动，因此这个原型可以依靠自己的动力运行。显然，这个原型要比爱迪生的灯泡复杂得多，但它是相同类型的具体原型。

面临这一难题的不仅仅是飞机制造商。现代汽车大多也是「线传控制」的，驾驶员的指令（踩油门、踩刹车和转动方向盘等）在进入车轮或发动机之前都是由计算机控制的。汽车设计师面临着同样的问题，但受到的监管约束要少很多。随着汽车自动化程度的提高（如车道保持、自动事故预防和全自动驾驶等），汽车设计师面临的问题只会变得更加糟糕。

图 6.4 空中客车 A350 的「铁鸟」原型。

我们所面临的问题不止于此。所有的现代工厂都是由计算机控制的，是类似的安全关键系统，在这种系统里，动作的定时特性对系统的可靠安全性而言尤为重要。火车是由计算机控制的，建筑物的通风、照明和防火系统是由计算机控制的，现代的电网、供水系统和污水处理系统也是由计算机控制的。

2006 年，美国国家科学基金会的海伦·吉尔创造了「信息物理系统」（Cyber-Physical System，简写为 CPS）一词，指的是那些将计算、网络和物理动力学结合在一起的系统。如今，这类系统显然面临着一场复杂性的危机。海伦·吉尔发起了一项美国国家科学基金会的重大提案，资助相关研究，以解决我之前已指出的问题。这一计划仍在继续，而且正在取得进展，尽管这些进展主要还集中在研究实验室里，而不是工业生产中。吉尔认识到，将「赛博」（cyber，此处也译为信息）世界与物理世界结合在一起，导致了巨大的复杂性危机，而现有的范式很难应对这一危机。

CPS 中「赛博」一词的起源是什么？与此相关的术语「赛博空间」（cyberspace，也译为网络空间）是威廉·吉布森在小说《神经漫游者》中使用的词语，但 CPS 这个词的词根有着更深的渊源。

更准确的说法是，「赛博空间」和「信息物理系统」源自同一个词根，即由美国数学家诺伯特·维纳（维纳，1948）创造的「控制论」（cybernetics）。维纳对控制系统理论的发展有着巨大的影响。在第二次世界大战期间，他发明了高射炮自动瞄准和射击技术。虽然他所使用的设计机制不涉及数字计算机，但其原理与今天在基于计算机的反馈控制系统中所使用的原理相似。他的控制逻辑实际上就是一种计算，尽管它是用模拟电路和机械部件实现的，因此，控制论是物理过程、计算和通信的结合。维纳是由希腊语中的舵手、总督、驾驶员或方向等词得出这个名词的。这个比喻很适合控制系统。

CPS 一词有时会与「赛博安全」（cybersecurity，也译为网络空间安全）相混淆，后者涉及数据的保密性、完整性和可用性，与物理过程没有内在的联系。因此，「赛博安全」一词是关于网络空间的安全，只是间接地与控制论有关联。当然，CPS 涉及许多具有挑战性的安全和隐私问题，但这并不是我们唯一要关注的。

我在伯克利大学的研究包括一些由美国国家科学基金会资助的 CPS 研究项目。2015 年，我完成了一个名为 PRET 的项目（for Performance withRepeatable Timing，简写为 PRET，致力于实现可重复定时的性能）。该项目设计了指令集体系架构，其编程范式中明确包括了定时功能。实际上，这个项目重新开启了弗雷德·布鲁克斯在 20 世纪 60 年代所做的决策，当时他设计了 System/360 ISA 系统，该系统没有任何明确的定时控制。该项目最后进行了一次演示，表明在不损失性能和硬件成本适中的情况下，可以实现对定时的精确控制。如果在工业领域中采用这种体系结构方法，它将能够为信息物理系统进行图 3.3 所示分层中的关注点分离。

第二个例子是我们课题组的 PTIDES 项目，也是在 2015 年完成的。该项目使用图 6.3 所示的 OSI 模型，解决了跨网络的分布式软件问题。但是，该项目修改了范式，以显式地控制定时。具体细节不再赘述。

尽管相关研究已在实验室里取得了一定的进展，但信息物理系统中的复杂性危机仍然存在，而数据科学的机遇危机也依然存在。在后续的章节中，我将进一步探究我们能够在多大程度上推进模型的分层。我们今天所掌握的所有技术都有其局限性，了解这些局限性对于全面理解技术革命至关重要，因为这些局限性也蕴含了创新的机会。

__________

1 Moore was one of the「traitorous eight」who left the Shockley Semiconductor Laboratory to found Fairchild Semiconductor and start Silicon Valley.

2 Douglas Adams described the Babel fish in The Hitchhiker's Guide to the Galaxy . According to Adams,「The Babel fish is small, yellow, leech-like, and probably the oddest thing in the universe. It feeds on brain wave energy, absorbing all unconscious frequencies and then excreting telepathically a matrix formed from the conscious frequencies and nerve signals picked up from the speech centres of the brain, the practical upshot of which is that if you stick one in your ear, you can instantly understand anything said to you in any form of language: the speech you hear decodes the brain wave matrix.」
