BEYOND ENGINEERING

A ccording to legend, the thirteenth-century scientist and monk Roger Bacon was a dabbler in black magic, and once constructed a talking mechanical head. It is said that he wanted to defend England from invaders by building a wall around the kingdom, and he constructed the head in order to ask its advice about how to build the wall. Bacon fashioned the head out of brass, replicating the design of a human head in every detail. He heated it over a fire while uttering magical incantations — a process that went on for days. Eventually, the head awoke and began to talk. Unfortunately, Bacon was by that time so exhausted from casting spells that he had fallen asleep. His young assistant was unwilling to awaken the master for the mere ramblings of a brass head, and the head exploded over the fire before Bacon could ask it any questions.

The Bacon legend has elements in common with stories about other conjurers who constructed an artificial intelligence: Dedalus, Pygmalion, Albertus Magnus, the Rabbi of Prague. A theme common to many of these stories is that some form of cooking or ripening is necessary to make something start to think. In the days before computing machines, few imagined that a process as complex as thinking could ever be broken down into operations that could be implemented by mechanisms. Instead, the assumption was that if an intelligence were ever to be created, it would be by an emergent process — that is, by a process in which the complex behavior emerges as a global consequence of billions of tiny local interactions. It was assumed that what the conjurer needed was not the correct wiring diagram but the correct recipe, according to which the ingredients would organize themselves into an intelligence. Such a process would allow an intelligence to be created without the conjurer's understanding exactly how the process — or the intelligence itself — worked.

Oddly enough, I am in basic agreement with this prescientific notion: I believe that we may be able create an artificial intelligence long before we understand natural intelligence, and I suspect that the creation process will be one in which we arrange for intelligence to emerge from a complex series of interactions that we do not understand in detail — that is, a process less like engineering a machine and more like baking a cake or growing a garden. We will not engineer an artificial intelligence; rather, we will set up the right conditions under which an intelligence can emerge. The greatest achievement of our technology may well be the creation of tools that allow us to go beyond engineering — that allow us to create more than we can understand.

Before discussing how this emergent design process might work, let us consider our best example of intelligence: the human brain. Since the brain itself was「designed」 — by the emergent process of Darwinian evolution — it may be usefully compared with the engineered designs that we have considered so far.

THE BRAIN

The human brain has about 10 12 neurons, and each neuron has, on average, 10 5 connections. The brain is to some degree a self-organizing system, but it would be wrong to think of it as a homogeneous mass. It contains hundreds of different types of neurons, many of which occur only in particular regions. Studies of brain tissue show that the patterns of neuronal connection, too, differ in the various regions of the brain: there are some fifty areas in which the pattern is recognizably different, and there are probably many more in which the differences in neural anatomy are too subtle for us to distinguish.

Each area of the brain is apparently specialized for a particular type of function, such as recognizing color in visual images, producing intonation in speech, or keeping track of the names of things. We know this because when specific areas are damaged by an accident or a stroke there is also a corresponding loss of function. For example, damage to areas 44 and 45 on the left side of the frontal lobe — together they are called Broca's area — often robs someone of the ability to produce grammatical speech. People so afflicted may still pronounce words clearly and they may understand the speech of others, but they will be unable to construct grammatical sentences. Damage to an area known as the annular gyrus, located a little farther toward the back of the head, causes difficulties in reading and writing; damage to yet other areas results in an inability to recall the names of familiar objects or recognize familiar faces.

It would be wrong to assume that the various areas of the brain are analogous to the functional building blocks of a computer. For one thing, damage to most areas will not cause a well-defined loss of function: removal of most of the right frontal lobe, for example, sometimes causes indefinable changes in personality and sometimes causes no noticeable change at all. Even in those cases in which the loss of function is well-defined, it is not at all evident that the function was performed entirely by the damaged area; it may be that the area just provided some minor element of support necessary for the function. An automobile with a dead battery will not be able to move, but we don't therefore assume that the battery is responsible for propelling the car.

There are certain areas of the brain — in particular, areas near the back of the head associated with visual processing — where we can actually make some sense of the pattern of connections: for instance, those involved in receiving the inputs from the left and right eye to create the sense of depth in stereo vision. But in most of the brain, the「wiring pattern」remains a mystery. Even the notion that most of the brain is hardwired for specific functions may turn out to be incorrect. Language, for instance, seems to be processed mostly on the left side, whereas spatial recognition, such as the ability to understand a map, seems to be performed primarily on the right. Yet under a microscope the pattern of left-brain and right-brain tissue looks pretty much the same. If there is a systematic difference between the wiring patterns in the brain's two hemispheres, it is too subtle for us to discern.

It may be that brain functions are learned in some sort of self-organizing process that changes the strength of various synaptic connections in order to fit an area for a certain kind of function. This is surely true to a degree. We know, for instance, that a monkey with a missing finger will continue to use the area of its brain that normally processes information from that finger: the idle neurons are recruited to perform processing for the animal's other fingers. Human beings probably rearrange the functions of the brain in a similar manner as they recover from a stroke. A stroke victim may initially have trouble with a specific function, such as recognizing faces, and then relearn the function with time. Since damaged neurons cannot regenerate themselves, the patient presumably relearns the function by recruiting neurons in a different part of the brain.

If functions like recognizing faces and understanding language are learned in different parts of the brain, then there must also be some sense in which these functions are already built in from the beginning. Newborn babies are particularly interested in faces in the first few days of life, and they learn to recognize them long before they learn to distinguish between much simpler shapes, such as letters. Similarly, babies seem predisposed to pay attention to certain kinds of patterns in speech that allow them to learn words and grammar. The functions that process language and recognize faces end up in different parts of the brain because, presumably, those parts of the brain are somehow primed to perform those dissimilar functions.

Even in those portions of the brain where functions seem to be hardwired, the pattern of wiring bears little resemblance to the hierarchical structure of functional blocks within a computer: there is no simple pattern of inputs going to outputs. Instead the connections are often bidirectional, with one set of neurons connecting in one direction and a complementary set connecting in reverse. Figure 29 shows the wiring diagram of the visual cortex of the macaque monkey, as best as can be determined by tracing the connections. Each of the lines in the diagram represents a bundle of many thousands of neurons, along with a complementary bundle in the reverse direction. At first glance, it appears as if everything is connected to everything else — unlike the neat, hierarchical circuit diagram of an engineered computer.

The important point here is that the brain is not only very complicated but also very different in structure from an engineered machine. That does not mean that we cannot ever engineer a machine to perform the functions of the human brain, but it does mean that we cannot expect to understand an intelligence by taking it apart and analyzing it as if it were a hierarchically designed machine.

It is possible that a satisfactory description of what the brain does will be almost as complex as a description of the structure of the brain — in which case, there is no meaningful sense in which we can understand it. In engineering, the way we deal with complexity is to break it into parts. Once we understand each part separately, we can understand the interactions between the parts. The way we understand each of the parts is to apply the engineering process recursively, breaking each part into a subpart, and so on. The design of an electronic computer, along with all its software, is impressive testimony to how far this process can be pushed. As long as the function of each part is carefully specified and implemented, and as long as the interactions between the parts are controlled and predictable, this system of「divide and conquer」works very well, but an evolved object like the brain does not necessarily have this kind of hierarchical structure.

FIGURE 29

Block diagram of the macaque visual cortex

THE PROBLEM WITH MODULARITY

The reliance on a strict hierarchical structure is the Achilles heel of the engineering process, since of necessity it creates the kind of adamant inflexibility we associate with machines. As discussed in chapter 6 , hierarchical systems are fragile in the sense that they are prone to catastrophic failure. Products of engineering are inherently fragile, because each part of an engineered system must meet the design specifications of how it should interact with other parts. These specifications serve as a kind of contract between components. If one of the components breaks its part of the contract, the design assumptions of the systems are invalid, and the system breaks down in an unpredictable way. The failure of a single low-level component can percolate through the system with catastrophic effects. Of course, complex systems like computers and airplanes are engineered to avoid these so-called single-point failures, through the methods of redundancy described in chapter 6 , but such techniques can guard the system only against anticipated failures. All the potential consequences of a particular failure must be predicted and understood — a task that becomes increasingly difficult as the machine becomes more and more complex.

The problem goes beyond the failure of individual components. In a complicated system, even correctly functioning parts can produce unexpected behaviors when they interact. Often when a large software system malfunctions, the programmers responsible for each of the parts can convincingly argue that each of their respective subroutines is doing the right thing. Often they are all correct, in the sense that each subroutine is correctly implementing its own specified function. The flaw lies in the specifications of what the parts are supposed to do and how they are supposed to interact. Such specifications are difficult to write correctly without anticipating all possible interactions. Large complex systems, like computer operating systems or telephone networks, often exhibit puzzling and unanticipated behaviors even when every part is functioning as designed. You may recall that a few years ago the long-distance telephone lines of the eastern United States stopped routing calls for several hours. The system used a sophisticated fault-tolerant design, based on redundancy. All its components were functioning correctly, but an unanticipated interaction between two versions of the software running at different switching stations caused the entire system to fail.

It is amazing to me that the engineering process works as well as it does. Designing something as complicated as a computer or an operating system can require thousands of people. If the system is sufficiently complicated, no one person can have a complete view of the system. This situation generally leads to mistakes stemming from misunderstandings of interfaces and inefficiencies of design. Again, such interface difficulties get worse as the system becomes more complex.

It is important to note that the problems outlined above are not inherent weakness of machines or of software per se. They are weaknesses of the engineering design process. We know that not everything that is complex is fragile. The brain is much more complicated than a computer, yet it is much less prone to catastrophic failure. The contrast in reliability between the brain and the computer illustrates the difference between the products of evolution and those of engineering. A single error in a computer's program can cause it to crash, but the brain is usually able to tolerate bad ideas and incorrect information and even malfunctioning components. Individual neurons in the brain are constantly dying, and are never replaced; unless the damage is severe, the brain manages to adapt and compensate for these failures. (Ironically, as I was writing this chapter, my computer crashed and required rebooting.) Humans rarely crash.

SIMULATED EVOLUTION

So, in creating an artificial intelligence, what is the alternative to engineering? One approach is to mimic within the computer the process of biological evolution. Simulated evolution gives us a different way to design complicated hardware and software — a way that avoids many of the problems of engineering. To understand how simulated evolution works, let's look at a specific example. Say that we want to design a piece of software that sorts numbers into descending order. The standard engineering approach would be to write such a program using one of the sorting algorithms discussed in chapter 5 , but let's consider how we might instead「evolve」the software.

The first step is to generate a「population」of random programs. We can create this population using a pseudorandom number generator to produce random sequences of instructions (see chapter 4 ). To speed up the process, we can use only those instructions useful for sorting, such as comparison and exchange instructions. Each of these random sequences of instructions is a program: the random population will contain, say, 10,000 such programs, each one a few hundred instructions long.

The next step is to test the population to find which programs are the most successful. This requires us to run each of the programs to see whether or not it can sort a test sequence correctly. Of course, since the programs are random, none are likely to pass the test — but by sheer luck some will come closer to a correct sorting than others. For instance, by chance, a program may move low numbers to the back of the sequence. By testing each program on a few different number sequences, we can assign a fitness score to each program.

The next step is to create new populations descended from the high-scoring programs. To accomplish this, programs with less than average scores are deleted; only the fittest programs survive. The new population is created by making copies of the surviving programs with minor random variations, a process analogous to asexual reproduction with mutation. Alternatively, we can「breed」new programs by pairing survivors in the previous generation — a process analogous to sexual reproduction. We accomplish this by combining instruction sequences from each of the「parent」programs to produce a「child.」The parents presumably survived because they contained useful instruction sequences, and there is a good chance that the child will inherit the most useful traits from each of the parents.

When the new generation of programs is produced, it is again subjected to the same testing and selection procedure, so that once again the fittest programs survive and reproduce. A parallel computer will produce a new generation every few seconds, so the selection and variation processes can feasibly be repeated many thousands of times. With each generation, the average fitness of the population tends to increase — that is, the programs get better and better at sorting. After a few thousand generations, the programs will sort perfectly.

I have used simulated evolution to evolve a program to solve specific sorting problems, so I know that the process works as described. In my experiments, I also favored the programs that sorted the test sequences quickly, so that faster programs were more likely to survive. This evolutionary process created very fast sorting programs. For the problems I was interested in, the programs that evolved were actually slightly faster than any of the algorithms described in chapter 5  — and, in fact, they were faster at sorting numbers than any program I could have written myself.

One of the interesting things about the sorting programs that evolved in my experiment is that I do not understand how they work. I have carefully examined their instruction sequences, but I do not understand them: I have no simpler explanation of how the programs work than the instruction sequences themselves. It may be that the programs are not understandable — that there is no way to break the operation of the program into a hierarchy of understandable parts. If this is true — if evolution can produce something as simple as a sorting program which is fundamentally incomprehensible — it does not bode well for our prospects of ever understanding the human brain.

I have used mathematical tests to prove that the evolved sorting programs are flawless sorters, but I have even more faith in the process that produced them than in the mathematical tests. This is because I know that each of the evolved sorting programs descends from a long line of programs whose survival depended on being able to sort.

The fact that evolved software cannot always be understood makes some people nervous about using it in real applications, but I think this nervousness is founded on false assumptions. One of the assumptions is that engineered systems are always well understood, but this is true only of relatively simple systems. As noted, no single person completely understands a complex operating system. The second false assumption is that systems are less trustworthy if they cannot be explained. Given the choice of flying in an airplane operated by an engineered computer program or one flown by a human pilot, I would pick the human pilot. And I would do so even though I don't understand how the human pilot works. I prefer to put my faith in the process that produced the pilot. As with the sorting programs, I know that a pilot is descended from a long line of survivors. If the safety of the airplane depended on sorting numbers correctly, I would rather depend on an evolved sorting program than on one written by a team of programmers.

EVOLVING A THINKING MACHINE

Simulated evolution is not in itself a solution to the problem of making a thinking machine, but it points us in the right direction. The key idea is to shift the burden of complexity away from the hierarchy of design and onto the combinatorial power of the computer. Essentially, simulated evolution is a kind of heuristic search technique that searches the space of possible designs. The heuristics it uses to search the space are Try a design similar to the best designs you have found so far and Combine elements of two successful designs. Both heuristics work well.

Simulated evolution is a good way to create novel structures, but it is an inefficient way to tune an existing design. Its weaknesses as well as its strengths stem from evolution's inherent blindness to the「Why」of a design. Unlike the feedback systems described in the last chapter, where specific changes were made to correct specific failures, evolution chooses variations blindly, without taking into account how the changes will affect the outcome.

The human brain takes advantage of both mechanisms: it is as much a product of learning as it is of evolution. Evolution paints the broad strokes, and the development of the individual in interaction with its environment completes the picture. In fact, the product of evolution is not so much a design for a brain as the design for a process that generates a brain — not so much a blueprint as a recipe. Thus, there are multiple levels of emergent processes operating at once. An evolutionary process creates a recipe for growing a brain, and the developmental process interacts with the environment to wire the brain. The developmental process includes both the internally driven processes of morphogenesis and the externally driven processes of learning. The maturational forces of morphogenesis cause nerve cells to grow in the right patterns, and the process of learning fine-tunes the connections. The ultimate stage in the brain's learning is a cultural process, in which knowledge acquired by other individuals over many generations is transferred into it.

I have described each of these emergent mechanisms (evolution, morphogenesis, learning) as if they were discrete processes, but in reality they are synergistically intertwined. There is no hard line between the maturational forces of morphogenesis and the instructional processes of culture. When a mother coos baby talk to her newborn child, this is both an instruction process and an aid in the maturation of the infant brain. The process of morphogenesis is itself an adaptive process, in which each cell develops in constant interaction with the rest of the cells in the organism, in a complex feedback process that tends to correct errors and keep the development of the organism on track.

There are also synergistic interactions between the evolutionary processes that create the species and the developmental processes that create the individual. The clearest example of the interaction between development and evolution is known as the Baldwin effect, first described by the evolutionary biologist James Baldwin in 1896 and rediscovered by the computer scientist Geoffrey Hinton almost a century later. The basic idea of the Baldwin effect is that when you combine evolution with development, evolution can happen faster; the adaptive processes of development can fix the flaws in an imperfect evolutionary design.

To understand the Baldwin effect, one must first appreciate the difficulty of evolving traits that require multiple mutations to occur together. Consider the evolution of the instinct for nest-building behavior in a bird. It is reasonable to assume that building a nest requires a few dozen individual steps, such as locating a twig, picking it up with the beak, carrying the twig back to the nest, and so on. Let's also assume, for the sake of the example, that each of these steps requires a different mutation and that the benefit to the bird (in the form of a completed nest) requires the complete set of mutations. In other words, if even a single step is missing, the nest will not get built at all, and therefore the bird will be no more fit than its peers and will derive no evolutionary advantage. Obviously, the problem with evolving such a trait is that evolution will select for one of its component mutations only if all the others are present: the simultaneous occurrence of all these mutations within a single individual is a highly improbable event. Since no single step is beneficial by itself, it is difficult to imagine how a behavior such as nest building could possibly evolve.

The Baldwin effect is synergistic interaction between evolution and learning. This interaction helps to solve this problem by offering the bird partial credit for a mutation that produces a single step of the task. A bird that is born knowing how to do some of the steps will have an advantage over a bird that does not, since it will have fewer steps to learn, so it's more likely to arrive at succesful nest-building behavior. Each single step that the bird is born with contributes to the possibility of learning, and therefore is valuable in itself. Viewed this way, each individual mutation will be favored independently, so that nest-building behavior will result from steps that are added to the bird's instinctual repertoire gradually, and in less time than it would take for a probabilistic fluke that produces the mutations all at once in a single individual In effect, the fact that the bird can learn makes the evolution happen faster. The Baldwin effect applies not just to learning but to any adaptive mechanism in the development of the individual.

Part of the reason that I'm optimistic about the prospects of evolving a thinking machine is that we do not have to start from scratch. We can「prime」the initial population of machines with patterns of structure that we observe in the brain. We can also start with whatever patterns of development and learning we observe in natural systems, even if we do not have a complete understanding of them. This should help even if our guesses are not quite right, since starting our search somewhere near a solution is probably much better than starting at random. By including some model of development in this process, the evolution of a thinking machine could take advantage of the Baldwin effect.

Another effect that radically reduces the time required to develop a complex behavior is instruction. A human baby develops intelligence at least in part because it has other humans to learn from. Part of this learning is acquired by sheer imitation, and part through explicit instruction. Human language is a spectacular mechanism for transferring ideas from one mind to another, allowing us to accumulate useful knowledge and behavior over many generations at a rate that far outpaces biological evolution. The「recipe」for human intelligence lies as much in human culture as it does in the human genome.

However, even starting with everything we know, I would not expect us to be able to evolve high-level artificial intelligence in a single step. Here is a rough outline of how the sequence of stages might progress. We would begin by evolving a design of a machine with the intelligence of, say, an insect by creating a simple environment in which insectlike intelligence would be favored, and by starting with an initial population predisposed through its developmental mechanisms to develop the kinds of neural structures we see in insects. Through a sequence of successively richer simulated environments, we might eventually evolve our insect intelligence into the intelligence of a frog, a mouse, and so on. Even going this far would doubtless take decades of work and involve many dead ends and false starts, but eventually this course of research could lead to the evolution of an artificial intelligence with the complexity and flexibility of the primate brain.

Should we ever manage to evolve a machine that can understand language, we would be able to skip ahead rapidly, by taking advantage of human culture. I imagine that we would need to teach an intelligent machine by much the same process that we would teach a human child, with the same mixture of skills, facts, morals, and stories. Since we would be incorporating human culture into the machine's recipe for intelligence, the resulting machine would not be an entirely artificial intelligence but rather a human intelligence supported by an artificial mind. For this reason, I expect that we would get along with it just fine.

I am aware, of course, that building such a machine will create a tangle of moral issues. For instance, once such a machine has been created, will it be immoral to turn it off? I would guess that turning it off would be wrong, but I do not pretend to be certain of the moral status of an intelligent artifact. Fortunately, we will have many years to work such questions out.

Most people are interested in not so much the practical moral questions of a hypothetical future as the philosophical issues that the mere possibility an artificial intelligence raises about ourselves. Most of us do not appreciate being likened to machines. This is understandable: we ought to be insulted to be likened to stupid machines, such as toasters and automobiles, or even to today's computers. Saying that the mind is a relative of a current-generation computer is as demeaning as saying that a human being is related to a snail. Yet both statements are true, and both can be helpful. Just as we can learn something about ourselves by studying the neural structure of the snail, we can learn something about ourselves by studying the simple caricature of thought within today's computers. We may be animals, but in a sense our brain is a kind of machine.

Many of my religious friends are shocked that I see the human brain as a machine and the mind as computation. On the other hand, my scientific friends accuse me of being a mystic because I believe that we may never achieve a complete understanding of the phenomenon of thought. Yet I remain convinced that neither religion nor science has everything figured out. I suspect that consciousness is a consequence of the action of normal physical laws, and a manifestation of a complex computation, but to me this makes consciousness no less mysterious and wonderful — if anything, it makes it more so. Between the signals of our neurons and the sensations of our thoughts lies a gap so great that it may never be bridged by human understanding. So when I say that the brain is a machine, it is meant not as an insult to the mind but as an acknowledgment of the potential of a machine. I do not believe that a human mind is less than what we imagine it to be, but rather that a machine can be much, much more.