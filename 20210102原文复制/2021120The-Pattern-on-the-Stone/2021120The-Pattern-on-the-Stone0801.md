COMPUTERS THAT LEARN AND ADAPT

T he computer programs I have so far described operate according to fixed rules supplied by the programmer. They have no way of inventing new rules themselves, or of improving the ones they are given. The chess-playing programs, if their programmers do not tinker with them, will keep making the same mistakes over and over, no matter how many games of chess they play. In this sense, computers are completely predictable; it is in this sense that computers can「do only what they are programmed to do」 — a point often raised by the defenders of humankind in the「man vs. machine」debate.

But not all software is this inflexible. It is possible to write programs that improve with experience. When they operate with such programs, computers can learn from their mistakes and correct their own errors. They accomplish this by making use of feedback. Any system based on feedback needs three kinds of information:

1 What is the desired state (the goal )?

2 What is the difference between the current state and the desired state (the error )?

3 What actions will reduce the difference between the current state and the goal state (the response ).

The feedback system adjusts the response, according to the error, to achieve the goal. The simplest and most familiar examples of feedback systems are not learning systems but control systems; the household thermostat is a good example. This feedback system recognizes only two possible errors and produces only two possible responses. The goal is to maintain a particular temperature, and the possible errors are that the temperature is either too hot or too cold. The responses are predetermined: if the temperature is too cold, the response is to turn on the furnace; if the temperature is too hot, the response is to turn the furnace off. Since the thermostat can only turn the furnace on or off, the response has nothing to do with the magnitude of the error. (This is a fact that I have repeatedly tried to explain to various members of my household, who insist on turning the thermostat all the way up to 90 degrees whenever the house is too cold, in hopes that somehow things will warm up faster; this tactic does not help. The thermostat can only turn the furnace on; it cannot turn it up.)

In principle, however, there is no reason why a home heating thermostat could not respond in proportion to the error. Such a system would require a method of adjusting the output of the furnace, rather than just turning it on or off. The apparatus would doubtless be more complicated and expensive, but it would ensure that the temperature was more precisely maintained. Such proportional control thermostats are used today in systems that control delicate industrial processes. Some household appliances — certain models of Japanese washing machines, for example — also use proportional control (or an approximation of it), a feature often advertised as fuzzy logic.

Another example of a system that uses proportional control is the automatic-pilot system that guides an airplane. The goal in this case is to keep the plane pointed in a given direction. A direction finder, such as a compass, measures the error in the direction the plane is traveling. The autopilot responds by making an adjustment in the position of the rudder of the plane in proportion to the size and direction of the error. Thus a slight deviation in course will result in only a slight change in the rudder position, but a major deviation, such as the swerving produced by a shift in the wind, will result in a large change in the setting of the rudder. If the automatic pilot system did not use proportional control but instead pushed the rudder all the way to the left or all the way to the right in the fashion of the home thermostat, the airplane would oscillate back and forth in an uncomfortable and probably dangerous manner.

In all of these feedback systems, the connection between error and response is fixed. The sensitivity of the response is predetermined by the design of the control system. But it is also possible to design an even more flexible feedback system, in which the response of the system adapts with time. In this case, the parameters of an initial feedback system are adjusted by a second feedback system. If the second feedback system adapts and improves over time, the system can be said to have「learned」the parameters of control.

Consider, for example, how a human pilot learns to fly an airplane. Typically, the student pilot oversteers at first — that is, overcorrects for every error. The pilot is using something like the system the thermostat uses to control the heat: if the plane is too far to the left, turn right; if it is too far to the right, turn left. Since there is a delay between turning the control rudder and the response of the plane, the system begins to oscillate. The pilot needs to learn how to move the rudder in proportion to the error, and this requires gauging the sensitivity of the response. The pilot learns this parameter through another feedback system; in this case, the goal of the feedback is to keep the plane on the correct course without oscillations, and the error is the degree of oscillation. The response is to adjust the response of the primary feedback system — that is, to adjust the amount that the control rudder is moved in order to correct a given erroneous angle in the plane's heading. Whenever the pilot's first feedback system is oscillating, he reduces its responsiveness. He increases its responsiveness if the plane begins to drift off course. Once the pilot learns the correct sensitivity, he can keep the plane on course without any oscillation.

It would be possible to build an automatic pilot that uses a second feedback system to adjust its own parameters, as described here. In this case, the autopilot could be said to have「learned」to fly the plane, in the same way that the human pilot learns. As far as I know, such adaptive autopilot systems are not used in real airplanes, but if they were they would have certain advantages. If the airplane sustained damage that caused a change in the responsiveness of the plane, such as a partly broken rudder, the autopilot would be able to adapt to this new situation. It might even be able to adapt if the connections to the rudder's control motor were accidentally reversed, so that the signal that normally turns the airplane right instead turned it left. Like a human pilot, the autopilot would require a fair amount of time to adjust to such a radical change in circumstances.

TRAINING THE COMPUTER

This basic notion of feedback is central to all learning systems, although it often takes a more complicated form than the self-adjusting automatic pilot. Often, feedback in computer programs is provided by training with the help of examples. The trainer (usually a human being) plays the role of a teacher, and the program becomes a student. A classical example of a trained learning system is a program written by the AI pioneer Patrick Winston, which learns the definition of concepts like「arch」from a series of positive and negative examples provided by an instructor. Winston's program learns new concepts by looking at simple line drawings of piles of blocks. The program is able to analyze such drawings and generate symbolic descriptions of the piles of blocks: for example,「Two touching cubes, supporting a wedge.」The trainer shows the program some examples of block configurations that form arches and another set of examples that do not, telling the program which are examples of「arch」and which are not. Initially, the program has no definition for the concept of「arch,」but as it is shown these positive and negative examples, it begins to formulate a working definition. Each time the program is shown a new example, it tests its working definition against the new example. If the definition sufficiently describes a positive example, or rules out a negative example, the program does not modify the definition. If the definition is in error, it is modified to fit the example.

Here is a scenario of how the program learns the definition of「arch」from a few examples. Assume that the first example the program is shown is a positive example: example A in Figure 25 , two upright rectangular blocks supporting a triangle. To start, the program will have to make an initial guess at formulating the definition of an arch. This initial guess does not need to be accurate, because it will be refined by future examples. Let's assume that the program uses the shapes of the blocks as its initial guess at a definition:「An arch is two rectangular blocks and a triangular block.」The second example the program is shown might be the same blocks, all lying down (example C in Figure 25 ). This is a negative example — that is, an example of something that is not an arch. Since the program's initial working definition mistakenly identifies this negative example as an arch, it will modify its definition to exclude the example. The program does this by identifying differences between the definition and the example and using them to add restrictions to the definition. In this case, the difference is in the relationships of the blocks, so an improved definition will include these relationships:「An arch is two upright rectangular blocks supporting a triangular block.」Now let's say the trainer supplies another positive example (B in Figure 25 ). This example uses a rectangular block at the top, instead of a triangular one. Since the program's working definition is not broad enough to include this positive example, the program will generalize its definition of「arch」to allow other shapes.

FIGURE 25

Positive and negative examples of arches

After being shown these examples and a few others, the program will converge on the following definition of an arch:「A prismatic body supported by two upright blocks that do not touch one another.」Each element of the definition has been learned by making some kind of mistake, and the definition has been adjusted accordingly. Once the program converges on the right definition, it stops making mistakes and leaves its definition unchanged. It can then correctly identify as an arch any arch it is shown, even if it has never seen that particular set of blocks before. It has learned the concept of「arch.」

NEURAL NETWORKS

Winston's program learns the concept of「arch,」but concepts like「touching,」「triangular block,」and「support」have been built into it from the beginning. Its representation of the world is specifically designed for piles of blocks. The search for a more general, universal representation scheme has led many researchers to computing systems with structures analogous to connected nets of biological neurons, such as occur in the brain. Such a system is called an artificial neural network.

A neural network is a simulated network of artificial neurons. This simulation may be performed on any kind of computer, but because the artificial neurons can operate concurrently, a parallel computer is the most natural place to execute it. Each artificial neuron has one output and a large number of inputs, perhaps hundreds or thousands. In the most common type of neural network, the signals between the neurons are binary — that is, either 1 or 0. The output of one neuron can be connected to the inputs of many others. Each input has a number associated with it, called its weight , which determines how much of an effect the input has upon the neuron's single output. This weight can be any number, positive or negative. The neuron's output is thus determined by a vote of the signals coming into its inputs, adjusted by the weights of the inputs. The neuron computes its output by multiplying each input signal by the input weight and summing the results; in other words, it adds up the weight of all the inputs that receive a 1 signal. If the weighted sum reaches a specific threshold, the output is 1; otherwise, the output is 0.

The function of an artificial neuron corresponds, very roughly, to the function of some types of real neurons in the brain. Real neurons also have one output and many inputs, and the input connections, called synapses , have different strengths (corresponding to the different input weights). A signal can either enhance or inhibit the firing of the neuron (corresponding to positive and negative weights), and the neuron will fire when the combined stimulation of the inputs is equal to or above some threshold. These are the senses in which an artificial neuron is analogous to a real one. There are also many ways in which a real neuron is much more complicated than an artificial one, but this simple artificial neuron is sufficient for building a system capable of learning.

The first thing to notice about artificial neurons is that they can be used to carry out the And, Or , and Invert operations. A neuron implements the Or function if the threshold is 1 and each of the input weights is equal to or greater than 1. A neuron with a threshold equal to the sum of the weights will implement the And function. Neurons with a single, negatively weighted input and a threshold of 0 will implement the Invert function. Since any logical function can be constructed by combining the And, Or , and Invert functions, a network of neurons can implement any Boolean function. Artificial neurons are universal building blocks.

We don't know very much about how the human brain works, but in some parts of the brain it seems that new information is learned by modifying the strength of the synapses that connect the neurons. This is certainly the case in the lower organisms on which we perform experiments — for example, sea snails. Sea snails can be taught certain conditioned responses, and it can be shown that they learn the response by changing the strength of the synaptic connections between neurons. Assuming that human learning works the same way, you are (I hope) adjusting the connections in your brain as you read this book.

A network of artificial neurons can「learn」by changing the weights of its connections. A good example is a very simple type of neural network called a perceptron , which can learn to recognize patterns. The way perceptrons learn is indicative of how most neural networks operate. A perceptron is a network with two layers of neurons and a single output. Each input in the first layer is connected to a sensing device like a light detector, which measures the brightness of one spot on an image. Each input of the second layer is connected to an output from the first layer, as shown in Figure 26.

Imagine that we are trying to teach the perception to recognize the letter A, which we will accomplish by showing it a large number of positive and negative examples of an A. The goal is for the perceptron to adjust the weights of the second layer so that its output will be 1 if, and only if, it is shown the image of an A. It accomplishes this by adjusting those weights whenever it makes an error. Each neuron in the first layer of the perceptron looks at a small patch of whatever example is being presented. Each of these first-layer neurons is programmed to recognize a specific local feature, such as a particular corner or a line at a particular orientation; it does so by means of the fixed weights of its own inputs. For example, here is a pattern of negative and positive input weights for the receptive field of a first-layer neuron programmed to recognize a corner, such as the point at the top of a capital A:

The first layer of the perception contains thousands of such feature-detecting neurons, each one programmed to recognize a particular kind of local feature in a particular part of the receptive field. This first layer of neurons detects features in the image which are useful for distinguishing between any letters; serifs are easy to detect, so they make letters more recognizable to the perceptron, just as they make a particular letter easier for the human eye to identify.

FIGURE 26

Perceptron

The local-feature detectors in the first layer provide the evidence, and the weights of the second layer determine how to weigh this evidence. For example, a corner pointing upward in the upper part of the image is evidence in favor of an A, while a corner pointing downward in the middle is evidence against. The perceptron learns by adjusting the weights on the inputs to the second layer. The learning algorithm is very simple: whenever the trainer indicates that the perceptron has made a mistake, the perceptron will adjust all of the weights of all the inputs that voted in favor of the mistake in such a way as to make future mistakes less likely. For instance, if the perceptron incorrectly identifies an image as an A, the weights of all the inputs that voted in favor of the false conclusion will be decreased. If the perceptron fails to identify a real A, then the inputs that voted in favor of the A will be increased. If the perceptron has enough feature detectors of the right type, this training method will eventually cause the perceptron to learn to recognize A's.

The learning procedure of the perceptron is another example of feedback. The goal is to set the weights correctly, the errors are misidentifications of the training examples, and the response is to adjust the weights. Notice that perceptrons, like Winston's arch program, learn only by making mistakes. This is a characteristic of all feedback-based learning systems. Given enough training, this particular procedure will always converge upon a correct choice of weights, assuming that there is a set of weights that does the job. This makes the perceptron seem like the perfect pattern-recognition machine, but the catch is the assumption that there exists some correct pattern of weights that will accomplish the task. To recognize the letter A in various sizes, fonts, and positions, the perceptron needs a very rich set of feature detectors in the first layer.

FIGURE 27

Perceptron spiral

Perceptrons can learn to recognize any letter if they are given enough features to work with, but there are some types of patterns, more complex than letters, that cannot be recognized by summing together local features in any way. For example, simply by summing up the evidence of local patches, a perceptron cannot tell whether or not all the dark spots in an image are connected, because connectedness is a global property; no local feature, by itself, can serve as evidence for or against connectedness. Figure 27 , adapted from Marvin Minsky and Seymour Papert's book Perceptrons , demonstrates that connectedness cannot always be assessed just by looking at local features.

For these and other reasons, two-layer perceptrons are not the most practical neural networks for recognizing most types of patterns. More general neural networks, with more layers, are able to recognize more complicated patterns. Such networks use similar procedures for learning. Trained neural networks of this type are often used for tasks like image recognition and speech recognition — tasks that are difficult to specify by a fixed set of rules. For instance, the simple word-recognition systems that are built into many children's toys today are based on neural networks.

SELF-ORGANIZING SYSTEMS

The disadvantage of a learning system based on positive and negative examples is that it requires a trainer to classify the examples. There is another type of neural network, which does not require a trainer — or, to put it another way, there are networks in which the training signals are generated by the network itself. Such a self-training network is a self-organizing system. Self-organizing systems have been studied for years (Alan Turing published important work in this area), but there has been a recent renewal of research activity in such systems, and even some new progress, partly because of the availability of faster computers. Like trained neural networks, self-organizing systems are a natural fit for parallel computers.

As an example of a self-organizing system that works, consider the problem of transmitting an image from the eye to the brain (see Figure 28 ). The retina, on which the image is projected, is a two-dimensional sheet of light-sensitive neurons. The image on the retina is transformed into a similar projected image in the brain by a bundle of neurons that transfers the image. If this bundle is wired imperfectly, then the projected image will be slightly scrambled, with each pixel in slightly the wrong place. I will describe a self-organizing artificial neural network that can learn to unscramble such a picture, restoring each pixel to its proper position. The unscrambler consists of a single layer of neurons arranged in a two-dimensional array. The outputs of these neurons form the corrected image. If the picture is scrambled only slightly, then each pixel in the scrambled image will be in the general neighborhood of its correct position. Each neuron's inputs look at a neighborhood of pixels in the scrambled image, and the neuron learns which of these pixels should be connected to the output in order to produce the unscrambled image. The neuron forms the connection by setting the weight of the correct input to 1 and the weight of its other inputs to 0.

FIGURE 28

Eye, with scrambled nerve bundle and unscrambler

The training algorithm for the unscrambler is based on the fact that images have a nonrandom structure. As discussed earlier, real images are not just random arrays of dots but pictures of the world, so nearby areas of the image tend to look the same. The unscrambler turns this statement around, by assuming that pixels that tend to look the same ought therefore to be near one another. The neurons in the unscrambler work by measuring the correlation of each of their inputs with the outputs of the neighboring neurons during exposure to a series of images. Whenever a neuron makes an「error」by firing differently from its neighbors, the neuron increases the weight of the inputs that match the outputs of its neighbors and decreases the weights of the other inputs. Of course, its neighbors are also learning their connections at the same time, so in the beginning it is a case of (so to speak) the blind leading the blind, but eventually some of the unscrambler neurons begin locking onto their correct inputs and thus become effective trainers for their neighbors. Again, the only neurons that are adjusted are those that have made mistakes. As the neurons train one another, an unscrambled image begins to emerge in the outputs, and eventually the network organizes itself to produce an image of perfect clarity.

The self-adjusting autopilot, Patrick Winston's arch program, the perception, and the unscrambler are just a few examples of systems that learn. All these systems are based on either external or internal feedbacks, and all learn by correcting their mistakes. The design of each of these systems was inspired by a biological system of similar function. In harvesting these products of evolution, we are like the fool in Aesop's fable,「The Goose That Laid the Golden Egg,」who chooses the eggs instead of the goose. In the next chapter, we shall discuss the goose.

CHAPTER 9

