## 记忆时间

## 卡片

### 0101. 反常识卡——

这本书的主题核心，就是最大的反常识卡，并且注意时间脉络。

常识：

反常识：

知识来源：比如提出者，如何演化成型的；书或专栏具体出现的地方。

例子。

### 0201. 术语卡——

根据反常识，再补充三个证据——就产生三张术语卡。

例子。

### 0202. 术语卡——

### 0203. 术语卡——

### 0301. 人名卡——

根据这些证据和案例，找出源头和提出术语的人是谁——产生一张人名卡，并且分析他为什么牛，有哪些作品，生平经历是什么。

维基百科链接：有的话。

#### 01. 出生日期

用一句话描述你对这个大牛的印象。

#### 02. 贡献及经历

#### 03. 论文及书籍

#### 04. 演讲汇总

找一个他的 TED 演讲，有的话。

### 0401. 金句卡——

最后根据他写的非常震撼的话语——产生一张金句卡。

### 0501. 行动卡——

行动卡是能够指导自己的行动的卡。

### 0601. 任意卡——

最后还有一张任意卡，记录个人阅读感想。

## 模板

### 1. 逻辑脉络

用自己的话总结主题，梳理逻辑脉络，也就是在这个专栏的整个地图里，这一章节所在的节点。

### 2. 摘录及评论

1『自己的观点』

2『行动指南』

3『与其他知识的连接』

## 20191027Can-marketplace-science-be-trusted

Henry rejected the efforts to remove Silliman. More importantly, he resolved to expand the NAS membership; new members were to be judged on the basis of their research, not on the source of their income 1. By the 1870s, it was already clear that industry relied on science.

1『非但没开出 Silliman，还借此扩员，揭示了工业与科学的结合。』

The Silliman-Whitney controversy marked a watershed in the relationship between science and industry. For US scientists, as well as many in Britain and Europe, private companies had become valuable patrons, supplying both funds for research and problems to be researched, and were gainful employers who provided short-term commissions. Likewise, companies regarded scientists and their findings as profitable to the development of their respective industries.

Over the next 150 years, relations between science and industry continued to evolve — in four significant stages. 

1. Consultancy (1820–80)

2. Industry (1880–1940)

3. Military (1940–80)

4. Outsourcing (1980 on)

The profession of scientific consulting goes back to the early nineteenth century, when individuals or groups of capitalists occasionally commissioned scientists to examine prospects in farming, mining, transportation (canals and railroads) and manufacturing. These fee-for-expertise engagements were short term and advisory. By the 1870s, changes in US commercial law (similar to those in British and European law) allowed the formation of limited-liability, joint-stock companies. These businesses, with their large pools of funds and numerous shareholders looking for investment assurances, regularly consulted scientists. As the engagements became both more routine (continuous testing and analysing of existing products and processes) and more investigative, scientists began to receive lucrative contracts and retainers 1.

In the United States, geologists were among the most active consultants during the Gilded Age, a period of rapid economic growth from the 1870s to the 1890s, especially in precious-metal mining in the area west of the Mississippi River. In Britain and Germany, the most prolific consultants were chemists, because of their essential expertise in new products such as acids, soaps, paints and especially synthetic dyes, including mauve and alizarin. Consulting chemists also found themselves in prominent public roles as expert witnesses in sensational patent cases. Witness-box quarrelling among chemists made good newspaper copy, and it highlighted profound developments in the chemical industries. Changes in patent law in the United States, Britain and Germany allowed inventors to claim those new chemical products and processes as their intellectual property (IP) instead of judging them to be scientific discoveries, which were, by definition, unpatentable.

At the turn of the twentieth century, the independent consulting scientist was replaced by the salaried researcher in new industrial laboratories. These labs represented the incorporation of applied science; that is, the creation of a separate place within the organization for ‘research and development’ — a phrase that entered the lexicon at this time.

In Germany, the largest dye companies, such as Bayer, Hoechst and BASF, were the first to establish dedicated labs for chemical research. In the United States, the prototype for the industrial research lab appeared in the electrical industry, when inventor Thomas Edison set up an ‘invention factory’ in Menlo Park, New Jersey, in 1876. 

It was the First World War and the embargo on all German products, especially chemicals, that was the catalyst to the golden age of ‘industrial research’, a neologism of the 1920s. Between 1919 and 1936, US corporations established more than 1,100 labs in nearly all industries — petroleum, pharmaceuticals, cars, steel — thereby dominating the world’s industrial research. In 1921, these employed roughly 3,000 engineers and scientists; by 1940, there were more than 27,000 researchers. At the end of the Second World War, the figure was nearly 46,0005.

This remarkable proliferation reflected the massive scale of vertically integrated corporations that controlled nearly all areas of their respective industries, from natural resources through R&D to mass production and mass marketing. Industrial research was also fuelled by radical changes in US patent law that allowed these behemoths to claim the IP of their employees. The inventor was now the corporation.

1『重点观点：Having research in thrall to industry raised the alarm, again, that capitalism corrupted science. 』

In short, national standing and international acclaim seemed to confirm that science done under the auspices of industry was equal to science in universities or governments. Still, industrial labs of the 1920s and 1930s were not simply universities without students. As institutions of applied science, they always needed to show corporate headquarters their value in terms of profitable products and processes.

The Second World War transformed the relationship between science and industry, along with the very terms — and even the history — of those relations.

Here, then, was a new argument. As many commentators at the time and since have pointed out, it did not reflect either the experience of the war years (during which multifunctional teams worked on military projects such as the atomic bomb or radar) or of the previous decades (in which multifunctional teams worked in R&D labs on corporate projects such as the light bulb). Science — The Endless Frontier thus propounded a different idea for developing new technologies, both military and commercial. In time, this became known as the linear model of innovation 9.

The theory posits a conveyor belt, beginning with basic science and moving smoothly along to development, then to manufacturing and production, and culminating with technology or innovation. Increase the amount of basic science and the (alleged) result would be more technology, innovation and overall economic growth. Theoretically, basic research was to be centred in universities (and military funding did transform US universities and their science departments accordingly). But corporate R&D labs were also contracting with the military, as they had been during the war. With these military contracts, as well as enlarged funding from corporate headquarters (business leaders also bought into the linear model), industrial labs were redirected away from applied science and towards basic research 10.

These leading corporate laboratories — Bell Labs, IBM, Westinghouse, DuPont, RCA (Princeton), Xerox Palo Alto Research Center (PARC, 1970) — became powerhouses of basic science. Between 1956 and 1987, 12 corporate scientists won Nobel prizes. Bell Labs alone has collected eight in physics and one in chemistry since the Second World War, including one for its most famous technology, the transistor, in 1956. In the early 1960s, corporate researchers authored 70% of papers appearing in Physics Abstracts. By 1980, Xerox PARC matched the world’s leading universities on citation impact 6,8.

With its emphasis on basic science as the necessary prerequisite to any future technological progress, the linear model was a break with the past. It prompted a new interpretation of the historical relations of science and industry. 

In the 1950s and 1960s, economists, historians and other scholars began to re-examine the latter half of the nineteenth century, and claimed to have discovered a ‘Second Industrial Revolution’. Characterized by the chemical and electrical industries, this revolution involved replacing the old trial-and-error methods of invention used in the dirty industries of the ‘First Industrial Revolution’ (textile factories, coal mines and iron foundries) with science-based methods. In this revisionist history, glamorous synthetic dyes and bright electric bulbs sprang directly from the pure science of organic chemistry and electromagnetic physics. History thus seemed to provide definitive evidence for the necessity of continued funding of basic science, as well as a ready explanation for why US and Western European corporations had dominated the world’s economy for more than a century 13.

It was not to last.

In response, US corporations began to restructure and downsize. Business leaders and shareholders decided that the multi-division conglomerate had become too unwieldy to compete. A new, leaner corporation was required. One way to restructure was outsourcing, replacing internal suppliers with external ones. Corporations began to relocate their manufacturing, once the backbone of the industrial economy, to plants in lower-cost and less-regulated countries. (The pace has only accelerated, especially after 2001, when China joined the World Trade Organization.)

Another way to downsize was divestiture, selling off subsidiaries unrelated to the core business. To shareholders seeking quick profits, long-term corporate research looked like a financial liability. The central laboratory became a prime target. In 1988, RCA sold off its Princeton lab as an independent business, Sarnoff Corporation. 

Accompanying globalized competitive markets, liberalized free trade and shareholder short-termism, the US military began to cut back funding for basic science at corporate labs. With the exception of a few years in the early 1980s (US president Ronald Reagan’s Strategic Defense Initiative, the ‘Star Wars’ programme), the US government steadily reallocated research funds to universities and other non-profit organizations, particularly towards medical schools and research hospitals through the National Institutes of Health (NIH). With continuous funding, new fields (molecular biology, biochemistry and biotechnology, for instance) surged past the diminished physical sciences. By 1988, only about 10% of basic research articles in physics were authored by industrial scientists; by 2005, the number had plummeted to less than 3% 15.

The demise of the corporate research lab heralded the death of the linear-model idea. Many scholars concluded that it was too simplistic. The pathway from science to technology was neither straight nor singular, and perhaps not even one way (technological advances can also lead to scientific discoveries). For corporate executives, investment in basic science did not seem to pay off. DuPont discovered no new nylons; Kodak failed to produce a revolution in photography; RCA lost its edge in consumer electronics; IBM ignored the personal computer; and Xerox PARC let slip the graphical user interface.

In the late 1960s and 1970s, small firms such as Intel, Microsoft, Apple, Sun Microsystems and Cisco Systems did commercialize the basic research being done at the larger corporations. Without establishing traditional research labs of their own, these players came to dominate the new information technology (IT) industry. In 1991, for example, when Microsoft created Microsoft Research — one of the largest industrial labs of its generation — its declared mission was not basic science, but innovation. In a more extreme case, Apple co-founder Steve Jobs shut down a fledgling research lab in 1998 in the belief that innovation would not require any investment in R&D.

Until 2010 and the emergence of machine learning, artificial intelligence (AI) and the Internet of Things, most technology companies ignored basic research. In 2012, following Jobs’s death, Apple began investing in R&D again, particularly in AI. Likewise, Amazon, Google, Facebook and Uber began to recruit AI researchers from academia. This brain drain has become so serious that universities have begun to worry about their ability to train future AI researchers.

Twenty-first-century corporations value science (particularly, patentable discoveries) and still think that basic research can lead to invention and innovation. They would just prefer that someone else do it (and pay for it). In business terms, they optimize their ‘supply-chain management’, a phrase that gained currency in the 1990s, by replacing stable in-house labs (warehouses of scientists and engineers) with flexible contract research. Their ability to do so was greatly facilitated by the US government and the loosening of antitrust enforcement. The settlement of the monopoly case against Microsoft in 2001, for example, stands in stark contrast to the forced break-up of AT&T in 1984.

Universities have traditionally been the home of basic science. In the twenty-first century they have also become the source of innovation and entrepreneurship, in part because of sweeping changes in US patent law. In 1980, the US Supreme Court (in Diamond v. Chakrabarty) significantly expanded what could be patented to include new life forms. That same year, the US Congress passed the Bayh–Dole Act, permitting universities to patent the results of research funded by the NIH or other federal agencies and conducted on their campuses by faculty members, students and employees. Universities started filing for patents at an increasing rate — from 2,266 in 1996 to 5,990 in 2014. The university is now an inventor 16.

1『重点观点：The pathway from science to technology was neither straight nor singular, and perhaps not even one way. 』

1『礼来的胰岛素。』

The emergence of biotech represented both a new business plan (entrepreneurial scientists partnering with venture capitalists to sell their research) and a new model of innovation. Here, industry shifted from a single internal or closed source of research to multiple external or open sources18. In this model, academic entrepreneurs, commercialized universities, globalized contract-research institutes and numerous small research start-ups supply the science and the IP. Larger, more established firms then develop and commercialize these into new products and processes.

The emergence of biotech represented both a new business plan (entrepreneurial scientists partnering with venture capitalists to sell their research) and a new model of innovation. Here, industry shifted from a single internal or closed source of research to multiple external or open sources18. In this model, academic entrepreneurs, commercialized universities, globalized contract-research institutes and numerous small research start-ups supply the science and the IP. Larger, more established firms then develop and commercialize these into new products and processes.

According to some economists and business scholars, open innovation characterizes a ‘Third Industrial Revolution’ 19. From their perspective, the university professor seeking to patent the results of federally funded research to form a start-up, with seed money from venture capitalists, is the direct descendant of the consulting chemist of the nineteenth century. In this ecosystem, a population of nimble researchers and small firms has displaced a pack of lumbering corporate labs 20. To critics and less-sanguine academics, the twenty-first-century relations of science and industry illustrate the commodification of university research and the corruption of the pursuit of knowledge by the profit motive 21.

Today, a complex innovation web has replaced the old conveyor belt. This is another new model — global commercialization. Supply-chain science is premised on the belief that research is a fungible commodity to be bought on demand and sold by the lowest-cost lab. In some ways, twenty-first-century contract research is reminiscent of nineteenth-century consulting science. In both cases, the question remains: is marketplace science trustworthy?




