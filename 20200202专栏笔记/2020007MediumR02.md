# 2020007MediumR02

## 记忆时间

## 卡片

### 0101. 反常识卡——

这本书的主题核心，就是最大的反常识卡，并且注意时间脉络。

#### 01. 常识

#### 02. 反常识

#### 03. 知识来源

比如提出者，如何演化成型的；书或专栏具体出现的地方。

#### 04. 例子

### 0201. 术语卡——

根据反常识，再补充三个证据——就产生三张术语卡。

例子。

### 0202. 术语卡——

### 0203. 术语卡——

### 0301. 人名卡——

根据这些证据和案例，找出源头和提出术语的人是谁——产生一张人名卡，并且分析他为什么牛，有哪些作品，生平经历是什么。

维基百科链接：有的话。

#### 01. 出生日期

用一句话描述你对这个大牛的印象。

#### 02. 贡献及经历

#### 03. 论文及书籍

#### 04. 演讲汇总

找一个他的 TED 演讲，有的话。

### 0401. 金句卡——

最后根据他写的非常震撼的话语——产生一张金句卡。

### 0501. 行动卡——

行动卡是能够指导自己的行动的卡。

### 0601. 任意卡——

最后还有一张任意卡，记录个人阅读感想。

## 20200212SQL_and_Pandas.md

Both of these tools are really useful. I recommend learning both. The combination will give you the ability to do a broad range of data analysis and manipulation efficiently. Soon, you won’t have to deal with Excel crashing on you anymore. Note: If you are working with really large data sets, you can use Dask which is built on Pandas specifically for big data. I may do a write-up on Dask basics in the future.

As I mentioned in my previous post, my technical experience has almost exclusively been in SQL. While SQL is awesome and can do some really cool things, it has its limitations — these limitations are in large part why I decided to acquire Data Science superpowers at Lambda School. In my previous data roles, I’ve needed to analyze data files from external sources and the tools I had access to either limited the amount of data it could process or took an exorbitant amount of time, making the task so mundane as to be almost impossible to complete thoroughly.

In all the positions I’ve held there has been a data pipeline that goes a little something like this: We received data from an outside source which needed to be analyzed for quality and understanding of ETL requirements. We used excel to do this, but anyone who has tried to use excel for large files knows what a nightmare it is. So we’d write excel macros, but since each file is so different they weren’t always very helpful. We could have spent money on tools built for data analysis, but those cost money, and are a hard sell when those paying the bill don’t directly feel or understand the pain of the data life.

Enter: Python’s Pandas library. My mind was blown. How could I not have known about this incredibly useful, FREE tool? It would have made my life so much easier! How, you ask? Well, let me tell you.

Unlike SQL, Pandas has built-in functions that help when you don’t even know what the data looks like. This is especially useful when the data is already in a file format (.csv, .txt, .tsv, etc). Pandas also allows you to work on data sets without impacting database resources. I’ll explain and show some examples of a few of the functions that I really like:

pandas.read_csv(). First you need to pull the data into a dataframe. Once you’ve set it to a variable name (‘df’ below), you can use the other functions to analyze and manipulate the data. I used the ‘index_col’ parameter when loading the data into a dataframe. This parameter is setting the first column (index = 0) as the row labels for the dataframe. You can find other helpful parameters here. Sometimes you have play around with parameters before it’s in the correct format. This function won’t return an output if it is set to a variable, but once set you can use the next function to view the data.

1『read_csv() 里，‘index_col’ 参数设置可以把第一列作为行名。经验证，也适用于 read_excel() 函数。』

pandas.head(). The head function is really helpful in just previewing what the dataframe looks like after you have loaded it up. The default is to show the first 5 rows, but you can adjust that by typing .head(10). This is a great place to start. We can see that there is a combination of strings, ints, floats, and that some columns have NaN values.

    df.head(10)

pandas.info(). The info function will give a breakdown of the dataframe columns and how many non-null entries each have. It also tells you what the data type is for each column and how many total entries are in the dataframe.

    df.info()

pandas.describe(). The describe function is really useful to see the distribution of your data, particularly numerical fields like ints and floats. As you can see below, it returns a dataframe with the mean, min, max, standard deviation, etc for each column. In order to see all columns, not just numeric, you’ll have to use the include parameter shown below. Notice that ‘unique’, ‘top’, and ‘freq’ have been added. These are only shown for non-numeric data types and NaN for numberic. The other breakdowns from above are NaN for these new columns.

```
df.describe()
df.describe(include='all')
```

The isna function on it’s own isn’t particularly useful since it will return the whole dataframe with either False if the field is populated or True if it is a NaN or NULL value. If you include.sum() with isna(), then you’ll get an output like the one below with a count of NaN or NULL fields for each column.

    df.isna().sum()

pandas.plot(). Pandas plot function is really useful to just get a quick visualization of your data. This function uses matplotlib for visualizations, so if you are familiar with that library, this will be easy to understand. You can find all the different parameters you can use for this function here.

When to use SQL vs. Pandas. Which tool to use depends on where your data is, what you want to do with it, and what your own strengths are. If your data is already in a file format, there is no real need to use SQL for anything. If your data is coming from a database, then you should go through the following questions to understand how much you should use SQL.

How much access do you have to the DB?

If you only have access to write a query and someone else runs it for you, you won’t be able to really look at your data. This is a time where you should just pull all the data you think you might be needing and export into a csv to use pandas. Another consideration: if a query you will need to run for your data is going to take up a lot of database resources and you know that your database admin wouldn’t allow it or like it, then just pull the data and do the work outside of the database with pandas. Avoid SELECT * in your queries, especially when you aren’t sure how much data could be in a table.

How are you wanting to transform/join your data?

If you already know some of the things you want to do with the data like filter out certain values, join to another table, combine certain fields in a calculation or concatenation, etc, it’s going to be easier to run SQL to pull the data as you want it and then export into a csv for any data analysis or data science work.

What are your strengths?

The biggest question is where your strengths are. If you feel more comfortable in one or the other, then stick with that language to do your data manipulation.

## 20200212An_Overview_of_Pythons_Datatable_package.md

The datatable module definitely speeds up the execution as compared to the default pandas and this definitely is a boon when working on large datasets. However, datatable lags behind pandas in terms of the functionalities. But since datatable is still undergoing active development, we might see some major additions to the library in the future.

If you are an R user, chances are that you have already been using the data.table package. Data.table is an extension of the data.frame package in R. It’s also the go-to package for R users when it comes to the fast aggregation of large data (including 100GB in RAM).

The R’s data.table package is a very versatile and a high-performance package due to its ease of use, convenience and programming speed. It is a fairly famous package in the R community with over 400k downloads per month and almost 650 CRAN and Bioconductor packages using it(source).

So, what is in it for the Python users? Well, the good news is that there also exists a Python counterpart to thedata.table package called datatable which has a clear focus on big data support, high performance, both in-memory and out-of-memory datasets, and multi-threaded algorithms. In a way, it can be called as data.table’s younger sibling.

Datatable. Modern machine learning applications need to process a humongous amount of data and generate multiple features. This is necessary in order to build models with greater accuracy. Python’s datatable module was created to address this issue. It is a toolkit for performing big data (up to 100GB) operations on a single-node machine, at the maximum possible speed. The development of datatable is sponsored by H2O.ai and the first user of datatable was Driverless.ai.

This toolkit resembles pandas very closely but is more focussed on speed and big data support. Python’s datatable also strives to achieve good user experience, helpful error messages, and a powerful API. In this article, we shall see how we can use datatable and how it scores over pandas when it comes to large datasets.

Installation. On MacOS, datatable can be easily installed with pip:

    pip install datatable

On Linux, installation is achieved with a binary distribution as follows:

Currently, datatable does not work on Windows but work is being done to add support for Windows also. For more information see [Installation — datatable 0.10.1 documentation](https://datatable.readthedocs.io/en/latest/install.html). The code for this article can be accessed from the associated [parulnith/An-Overview-of-Python-Datatable-package: Python library for efficient multi-threaded data processing, with the support for out-of-memory datasets.](https://github.com/parulnith/An-Overview-of-Python-Datatable-package) or can be viewed on my binder by clicking the image below.

3『

作者的 GitHub：[parulnith (Parul Pandey)](https://github.com/parulnith)

[parulnith/10-Simple-hacks-to-speed-up-your-Data-Analysis-in-Python: Some useful Tips and Tricks to speed up the data analysis process in Python.](https://github.com/parulnith/10-Simple-hacks-to-speed-up-your-Data-Analysis-in-Python)

』

Reading the Data. The dataset being used has been taken from Kaggle and belongs to the Lending Club Loan Data Dataset. The dataset consists of complete loan data for all loans issued through the 2007–2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file consists of 2.26 Million rows and 145 columns. The data size is ideal to demonstrate the capabilities of the datatable library.

```
# Importing necessary Libraries

import numpy as np
import pandas as pd
import datatable as dt
```

Let’s load in the data into the Frame object. The fundamental unit of analysis in datatable is a Frame. It is the same notion as a pandas DataFrame or SQL table: data arranged in a two-dimensional array with rows and columns.

With datatable. The fread() function above is both powerful and extremely fast. It can automatically detect and parse parameters for the majority of text files, load data from .zip archives or URLs, read Excel files, and much more. Additionally, the datatable parser : 1) Can automatically detect separators, headers, column types, quoting rules, etc. 2) Can read data from multiple sources including file, URL, shell, raw text, archives and glob. 3) Provides multi-threaded file reading for maximum speed. 4) Includes a progress indicator when reading large files. 5) Can read both RFC4180-compliant and non-compliant files.

With pandas. Now, let us calculate the time taken by pandas to read the same file. The results show that datatable clearly outperforms pandas when reading large datasets. Whereas pandas take more than a minute, datatable only takes seconds for the same.

Frame Conversion. The existing Frame can also be converted into a numpy or pandas dataframe as follows:

```
numpy_df = datatable_df.to_numpy()
pandas_df = datatable_df.to_pandas()
```

Let’s convert our existing frame into a pandas dataframe object and compare the time taken. It appears that reading a file as a datatable frame and then converting it to pandas dataframe takes less time than reading through pandas dataframe. Thus, it might be a good idea to import a large data file through datatable and then convert it to pandas dataframe.

1『不错的办法，处理大数据时先用 datatable 导入，接着转成 pandas 的 dataframe，这样比直接从 pandas 导入数据还要快。』

Basic Frame Properties. Let’s look at some of the basic properties of a datatable frame which are similar to the pandas’ properties:

We can also use the head command to output the top ‘n’ rows.

    datatable_df.head(10)

The colour signifies the datatype where red denotes string, green denotes int and blue stands for float.

Summary Statistics. Calculating the summary stats in pandas is a memory consuming process but not anymore with datatable. We can compute the following per-column summary stats using datatable. Let’s calculate the mean of the columns using both datatable and pandas to measure the time difference. The above command cannot be completed in pandas as it starts throwing memory error.

Data Manipulation. Data Tables like dataframes are columnar data structures. In datatable, the primary vehicle for all these operations is the square-bracket notation inspired by traditional matrix indexing but with more functionalities. The same DT[i, j] notation is used in mathematics when indexing matrices, in C/C++, in R, in pandas, in numpy, etc. Let’s see how we can perform common data manipulation activities using datatable:

Selecting Subsets of Rows/Columns. The following code selects all rows and the funded_amnt column from the dataset.

    datatable_df[:,'funded_amnt']

Here is how we can select the first 5 rows and 3 columns

    datatable_df[:5,:3]

Sorting the Frame. Sorting the frame by a particular column can be accomplished by datatable as follows. Notice the substantial time difference between datable and pandas.

Deleting Rows/Columns. Here is how we can delete the column named member_id:

    del datatable_df[:, 'member_id']

GroupBy. Just like in pandas, datatable also has the groupby functionalities. Let’s see how we can get the mean of funded_amount column grouped by the grade column. What does .f stand for? f stands for frame proxy, and provides a simple way to refer to the Frame that we are currently operating upon. In the case of our example, dt.f simply stands for dt_df.

Filtering Rows. The syntax for filtering rows is pretty similar to that of GroupBy. Let us filter those rows of loan_amntfor which the values of loan_amnt are greater than funded_amnt.

    datatable_df[dt.f.loan_amnt>dt.f.funded_amnt,"loan_amnt"]

Saving the Frame. It is also possible to write the Frame’s content into a csv file so that it can be used in future.

    datatable_df.to_csv('output.csv')

For more data manipulation functions, refer to the documentation page.

