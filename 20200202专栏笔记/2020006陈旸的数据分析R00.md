## 记忆时间

## 卡片

### 0101. 反常识卡——

这本书的主题核心，就是最大的反常识卡，并且注意时间脉络。

#### 01. 常识

#### 02. 反常识

#### 03. 知识来源

比如提出者，如何演化成型的；书或专栏具体出现的地方。

#### 04. 例子

### 0201. 术语卡——

根据反常识，再补充三个证据——就产生三张术语卡。

例子。

### 0202. 术语卡——

### 0203. 术语卡——

### 0301. 人名卡——

根据这些证据和案例，找出源头和提出术语的人是谁——产生一张人名卡，并且分析他为什么牛，有哪些作品，生平经历是什么。

维基百科链接：有的话。

#### 01. 出生日期

用一句话描述你对这个大牛的印象。

#### 02. 贡献及经历

#### 03. 论文及书籍

#### 04. 演讲汇总

找一个他的 TED 演讲，有的话。

### 0401. 金句卡——

最后根据他写的非常震撼的话语——产生一张金句卡。

### 0501. 行动卡——

行动卡是能够指导自己的行动的卡。

### 0601. 任意卡——

最后还有一张任意卡，记录个人阅读感想。

## 发刊词

清华有一门课，叫数据挖掘，正是通过这门课，我学会了如何从海量的数据中找到关联关系，以及如何进行价值挖掘。那时候感觉自己掌握了一门利器，就特别想找到一个钉子，来试试自己手里的这把锤子。当时恰好赶上 2009 年微博的热潮。我用 3 个月的时间就积累了 4 万粉丝，一年的时间积累了上百万粉丝。这是怎么做到的呢？通过数据采集，我收集了每天的微博热点，然后对热点进行抓取、去广告，再让机器定时自动进行发布。同时我让账号每天都去关注明星的粉丝列表，这样可以获得 15% 的回粉概率。久而久之，就会有源源不断的粉丝。

你看，其实就是数据分析帮我做到了微博的自动化运营。这还只是一个小例子，数据分析的影响已经渗透到了我们工作生活的方方面面。通过数据分析，我们可以更好地了解用户画像，为企业做留存率、流失率等指标分析，进而精细化产品运营。如果你关注比特币，数据分析可以帮助你预测比特币的走势。面对生活中遇到的种种麻烦，数据分析也可以提供解决方案，比如信用卡反欺诈，自动屏蔽垃圾邮件等。可以说，我们生活在数据驱动一切的时代，数据挖掘和数据分析就是这个时代的「淘金」，从国家、企业、组织到个人，都一定会关注各种数据，从这些数据中得到价值。

也正是这个原因，数据分析人才成了香饽饽，不管是数据分析师，数据分析工程师，还是数据产品经理，有数据思维的运营人员，都变得越来越抢手。你是不是也已经摩拳擦掌，做好了了解这一领域的准备呢？我想在接下来的 15 周时间里，把自己在清华学习数据挖掘的体会和工作实践中对数据分析的理解，重新梳理整合呈现给你，和你一起在数据分析这个领域来一场急行军。

我也知道数据分析能力很重要，但是数据分析是不是很难？到底该怎么学呢？其实这里有一些误区，数据分析并非遥不可及，它不难，掌握高效的学习方法很重要；但是它也不简单，需要你耐下性子，跟我一起来慢慢掌握数据分析的核心知识点和工具操作。

了高效的学习方法，我把它称为 MAS 方法：Multi-Dimension：想要掌握一个事物，就要从多个角度去认识它；Ask：不懂就问，程序员大多都很羞涩，突破这一点，不懂就问最重要；Sharing：最好的学习就是分享。用自己的语言讲出来，是对知识的进一步梳理。

怎么和数据分析建立多维度连接呢？我特意把内容分成了三个大类：第一类是基础概念；第二类是工具。这个部分可以很好地锻炼你的实操能力；第三类是题库。题库的作用是帮你查漏补缺，在这个过程中，你会情不自禁地进行思考。

这个连接的过程，也是我们从「思维」到「工具」再到「实践」的一个突破过程。如果说重要性，一定是「思维」最重要，因为思维是底层逻辑和框架，可以让我们一通百通，举一反三，但是思维修炼也是最难的。所以，我强调把学习重心放在工具和实践上，即学即用，不断积累成就感，思维也就慢慢养成了。说到底，学习数据分析的核心就是培养数据思维，掌握挖掘工具，熟练实践并积累经验。为了能带给你更好的学习效果，我在专栏里设计了五大模块。

预习篇。我会给你介绍数据分析的全景图，和你进一步探讨最佳的学习路径。我还专门准备了 3 篇 Python 入门内容；基础篇。我会带你修炼数据思维，从数据分析的基础概念，到数据采集、数据处理以及数据可视化；算法篇。算法是数据挖掘的精华所在，我精选了 10 大算法，包括分类、聚类和预测三大类型。每个算法我们都从原理和案例两个维度来理解，达到即学即用的目的；实战篇。项目实战是我们学习的一个重要关卡。我准备了 5 个项目带你真实体验。比如在金融行业中，如何使用数据分析算法对信用卡违约率进行分析？现在的互联网产品都进入到千人千面的人工智能阶段，如何针对一个视频网站搭建视频推荐算法；工作篇。我选择了几个大家最关心的职场问题，比如面试时注意什么，职位晋升路径是怎样的等等。

1『数据分析技能培养方法论，一是数据分析思维，二是数据分析的工具，三是数据分析的实战。思维的培养，可以通过在学习工具、实战过程中提炼升华，也可以通过读书学习大牛的思维，还可以学习其他学科相关的知识连接来完善；分析工具里包括基本概念和算法，基本概念比如数据采集、数据处理和数据可视化。算法比如分类、聚类和预测模型；实战要求多做具体的项目。从「思维」到「工具」再到「实践」，再回到「思维」形成闭环。』

通过这个专栏你将收获：数据和算法思维。这不仅是在技术上的思维模式，更是我们平时看待问题解决问题的思维方式。如果你将数据视为财富，将数据分析视为获得财富的工具，那么在大数据时代，你将获得更宽广的视野；工具。用好工具，你将拥有收集数据、处理数据、得到结果的能力，它会让你在工作中游刃有余；更好的工作机会和价值。无论是当前火爆的人工智能，还是数据算法工程师的市场，都很看重数据分析和数据处理的能力。从「思维」到「工具」再到「实践」，沿着这个路径拓展自己的能力边界，拥有更强的竞争力。

### 黑板墙

业务洞察是分析数据的前提，分析数据是理解数据的前提，理解数据是数据挖掘的前提。从业务到数据再到挖掘，每一步环环相扣，相辅相成。业务千变万化，规律亘古不变。

作者回复：有关数据分析思维的书籍，《思维简史：从丛林到宇宙》；有关数据处理的书籍，《数据挖掘：概念与技术》、《Pentaho Kettle 解决方案》、《精益数据分析》、《Small Data》、《利用 Python 进行数据分析》。

2『已下载书籍「2018146思维简史」、「2020037数据挖掘概念与技术3Ed/2020037Data_Mining_Concepts_and_Techniques3Ed」、「2020035Pentaho_Kettle解决方案/2020035Pentaho_Kettle_Solutions」、「2019064精益数据分析」、「2019065利用Python进行数据分析2Ed」、「2020034Small_Data」、「2020036Data_Mining_for_Business_Analytics_Python」。』

1）首先从爬虫开始是不错的，这样你能感受到成长的过程。2）数据挖掘算法，如果你想了解十大算法的话，理论部分你需要花一些功夫。当然这些在 Python 中都有类库可以使用。做练习的话，你也可以把这些算法都用一遍，然后看下哪个算法模型的结果更好。3）网上这方面的资源确实比较多，他们大多讲的是理论原理。我认为你更注重的在于实战，因为做项目不仅更有成就感，还能更好的让你理解这些算法、爬虫的原理。我会在专栏里给你做个「专属题库」，对应爬虫、数据挖掘这些的题目，你可以做个评测，不明白的地方，我也会给你做讲解。4）资料比较多，但其实不用每个都看一遍。尤其是理论的部分，看一遍就可以了，关键是把它抽出来做个思维导图。

对数据挖掘 / 机器学习很感兴趣，自学有段时间了，也接触了不少工具，但遇到具体问题还是很盲目，有下面几个方面的困惑：如何做好「特征工程」，没有思路，也没有思考方向，看了不少博客，所谓的技巧也都知道了，但遇到问题还是用不好；对于样本类别不均衡的问题，不会处理，尝试过下采样或过采样，但似乎改变了样本原本的分布，效果不太好；对各种机器模型输出的结果没有把控能力，搞不懂为何有时效果好，有时效果却很差。因为没有人带，自学感觉很迷茫，希望能跟随这门课程提升自己应用能力。另外，想请教一下老师，为了能更好地掌握经典的机器学习算法，有没有必要自己实现一遍？

作者回复：我认为非常有必要自己使用这些机器学习算法来解决实际问题。当然原理可以采用伪代码的方式，把流程画出来即可。项目中，很多时候都是直接使用类库，所以你更应该关注的机器学习的效率和结果。很多时候，我们在选择模型的时候，都要试，一次会用多种模型，然后看训练结果的好坏，再决定采用哪个模型。特征工程，以及调试的过程其实就是经验积累的过程，很多时候调参数的时间，比你写程序的时间还要长。但是这个积累过程还是挺重要的，当你有了更多经验之后，这个「试」的效率就会提升！

1『反复提到的一个思路，用多个模型尝试，看结果反馈来判断模型的好坏。』

数据就是这个时代的石油，确实是这样子的，在读研的时候深有体会，实验室的很多科研，项目都需要用到数据分析的思维和能力，工作之后也在为现在的公司处理数据帮助运营人员进行精准营销，无论是传统行业还是互联网行业，这都是一门重要的能力。作者回复：同意你说的，传统行业和互联网行业，不论是运营岗，还是营销岗，都需要数据分析能力和思维。

我是一个想转商业数据分析与挖掘的生物学（生物信息方向）硕士研究生，很需要有一门课大概能告诉我一个算法或者数学模型适用于哪些商业或者运营的情景，这是我现在急需的，也是对课程的期望，哪些东西可以解决哪些问题，也希望作者能推荐一些类似的书。作者回复：我上大学的时候，也了解一些生物信息学的情况，非常能理解你的心情和想转到商业数据分析的决心。我觉得需要从两个方面来下手：1）工具角度：课程里讲的算法，你可以帮他当做是个工具。他的诞生是从数学原理开始，形成的理论模型。这些模型都有自己的特点和适用范围。但总的来说，还是工具。2）商业角度：工作或应用中，首先都是从商业角度出发的，尤其是哪些是高频使用的，或者离「钱」更近的地方，也就是决策价值更大的地方。当然从工具使用到商业价值的转换，还需要你有自己的思维和建模能力。商业相关书籍推荐：《洛克菲勒留给儿子的 38 封信》、《商业冒险：华尔街的 12 个经典故事》、《从 0 到 1：开启商业与未来的秘密》、《商业的本质》。数据分析相关书籍：参考之前的信息。

## 010数据分析基础篇

### 1. 逻辑脉络

1、数据分析基础技能里包含 3 大块，数据采集、数据挖掘和数据可视化。数据分析修炼牢记两条原则：不重复造轮子和工具决定效率。

2、数据挖掘的知识清单，分别是数据挖掘的基本流程、十大算法和数学原理。挖掘的基本流程：理解商业、理解数据、准备数据、建立模型、评估模型和发布上线。数据挖掘的十大算法：分类算法、聚类算法、关联分析和连接分析。数据挖掘的数学原理：概率论与数理统计、线代、图论和最优化方法。

3、Python 基础知识介绍。

4、numpy 的基础知识，比如 ndarray 对象的概念、创建结构数组、ufunc 运算、创建连续数组、算术运算、统计函数以及 NumPy 排序。

5、pandas 的基础知识，比如其数据结构、数据的导入和输出、数据清洗操作（删除元素、重命令行名列名、去重复值、格式的转换）、数据统计函数、数据表的合并以及 SQL 和 pandas 的连接。



### 2. 摘录及评论

### 0101数据分析全景图及修炼指南.md

数据分析分成三个重要的组成部分：数据采集。它是我们的原材料，也是最「接地气」的部分，因为任何分析都要有数据源；数据挖掘。它可以说是最「高大上」的部分，也是整个商业价值所在。之所以要进行数据分析，就是要找到其中的规律，来指导我们的业务。因此数据挖掘的核心是挖掘数据的商业价值，也就是我们所谈的商业智能 BI；数据可视化。它可以说是数据领域中万金油的技能，可以让我们直观地了解到数据分析的结果。

在数据采集部分中，你通常会和数据源打交道，然后使用工具进行采集。在专栏里，我会告诉你都有哪些常用的数据源，以及如何获取它们。另外在工具使用中，你也将掌握「八爪鱼」这个自动抓取的神器，它可以帮你抓取 99% 的页面源。当然我也会教你如何编写 Python 爬虫。掌握 Python 爬虫的乐趣是无穷的。它不仅能让你获取微博上的热点评论，自动下载例如「王祖贤」的海报，还能自动给微博加粉丝，让你掌握自动化的快感。

数据挖掘。知识型的工程，相当于整个专栏中的「算法」部分。首先你要知道它的基本流程、十大算法、以及背后的数学基础。这一部分我们会接触到一些概念，比如关联分析，Adaboost 算法等等，你可能对这些概念还是一知半解，没有关系，我会详细为你介绍这些「朋友」。每讲完一个算法原理，我都会带你做一个项目的实战，我精选了一些典型的、有趣的项目，比如对泰坦尼克号乘客进行生存预测、对文档进行自动分类、以及导演是如何选择演员的等等。掌握了数据挖掘，就好比手握水晶球一样，它会通过历史数据，告诉你未来会发生什么。当然它也会告诉你这件事发生的置信度是怎样的，置信度这个词你先记住就可以了，后面我们来学习它具体代表什么。

数据可视化。这是一个非常重要的步骤，也是我们特别感兴趣的一个步骤。数据往往是隐性的，尤其是当数据量大的时候很难感知，可视化可以帮我们很好地理解这些数据的结构，以及分析结果的呈现。如何进行数据可视化呢？有两种方法。第一种就是使用 Python。在 Python 对数据进行清洗、挖掘的过程中，我们可以使用 Matplotlib、Seaborn 等第三方库进行呈现。第二种就是使用第三方工具。如果你已经生成了 csv 格式文件，想要采用所见即所得的方式进行呈现，可以采用微图、DataV、Data GIF Maker 等第三方工具，它们可以很方便地对数据进行处理，还可以帮你制作呈现的效果。数据采集和数据可视化的原理简单，容易理解。这两个部分注重的是工具的掌握，所以我会把重点放在讲解工具以及应用实战上。虽然这些理论我会给你一一讲解，但纸上得来终觉浅，绝知此事要躬行。手拿地图，我们知道要去哪里，但是怎么去呢？我认为学习数据分析最好的方法是：在工具中灵活运用，在项目中加深理解。

修炼指南。刚才我们讲了数据分析全景图，包括数据采集、数据挖掘、数据可视化这三个部分。你可能觉得东西很多，无从下手，或者感觉数据挖掘涉及好多算法，有点「高深莫测」，掌握起来是不是会吃力。其实这些都是不必要的烦恼。借用傅盛的话来说，人与人最大的差别在于「认知」，所谓成长就是认知的升级。很多人存在对「认知」的误解，认为认知不就是概念么？那么你有没有想过，针对同一个概念，为什么不同的人掌握的程度是不一样的呢？我们只有把知识转化为自己的语言，它才真正变成了我们自己的东西。这个转换的过程，就是认知的过程。

那么如何提升自己的学习吸收能力呢？简单地说，就是要「知行合一」。如果说认知是大脑，那么工具就好比我们的双手，数据工程师和算法科学家每天打交道最多的就是工具。如果你开始做数据分析的项目，你脑海中已经思考好了数据挖掘的算法模型，请牢记下面这两点原则。

不重复造轮子。举个数据采集的例子，我见过很多公司，都有数据采集的需求，他们认为某些工具不能满足他们个性化的需求，因此决定招人专门做这项工作。而结果怎样呢？做了 1 年多的实践，工资投入几十万，结果发现 Bug 一大堆，最后还是选择了第三方工具。耗时耗力，还没什么成效。一个模型是否有相关的类库可以使用 —— 这几乎是每个程序员入行被告知的第一条准则。我也会对新人反复灌输这个概念。大部分情况下你都能找到类库来完成你的想法。

工具决定效率。「不要重复造轮子」意味着首先需要找到一个可以用的轮子，也就是工具。我们该如何选择呢？这取决于你要做的工作，工具没有好坏之分，只有适合与否。除去研究型的工作，大部分情况下，工程师会选择使用者最多的工具。因为：Bug 少、文档全、案例多。比如 Python 在处理数据挖掘上就有很多第三方库，这些库都有大量的用户和帮助文档可以帮助你来上手。选择好工具之后，你要做的就是积累「资产」了。我们很难记住大段的知识点，也背不下来工具的指令，但是我们通常能记住故事、做过的项目、做过的题目。这些题目和项目是你最先行的「资产」。如何快速积累这些「资产」呢？这里我送你三个字：熟练度。把题目完成只是第一步，关键在于训练我们工具使用的「熟练度」。

高中的时候，有一次我做「八皇后」的问题，第一次解答花了一个小时的时间。当时老师明确告诉我必须在 20 分钟内完成，我不敢相信，从解题、思考、动手，最后完成，1 个小时不算慢。但是后来我调整了思考的结构。最后我 6 分钟就可以完成那道题。当熟练度增加的时候，你的思考认知模型也在逐渐提升。所以专栏中，我给你做了一个「专属题库」，在专属题库中你可以进行自我评测，当然我也会对这些练习题进行讲解。在工作篇中，我也会和你一起分享面试技巧、探讨职场上的晋升之路。

认知三步曲，从认知到工具，再到实战。记录下你每天的认知。尤其是每次课程后，对知识点的自我理解。这些认知对应工具的哪些操作。用工具来表达你对知识点的掌握，并用自己的语言记录下这些操作笔记。做更多练习来巩固你的认知。我们学习的内容对于大部分外人来说，就像「开车」一样，很酷。我们学习的内容，对于要掌握的人来说，也像「开车」一样，其实并不难，而且很多人已经上路了。你需要的就是更多的练习。

### 黑板墙

作者回复：知行合一，并不是个先后的顺序过程，就像你刚才说的：实战其实就是很好的学习，能让你理解工具使用和知识点。刚才有个同学分享了：自己写 Demo 是个非常好的体验。

1『Demo 应该指原型，待确认。』

对于软件技术的学习，我认为目标驱动法，或者叫项目驱动法效率比较高。当然这也是建立在具有明确目标的前提下，知道要做什么了。不知道具体目标的时候，能做一个 Demo，实践一下比较好。一个项目对于这个领域基础知识的需求可能并不全面，也就是说你做完一个项目，可能仅仅用了一部分知识，其余知识没有用到。但是有一点比较好，就是项目从启动到结束，对于这个领域的知识，是从浅到深的过程，或者说可以体会到理论如何运用到实践，这个会了以后，就可以举一反三，进而学习这个领域的其余知识，运用到其余项目。我不适合记忆，记不住，最近一段时间在想，如何能更有效的搜索以前的知识点。比如记录到有道云笔记上的知识，但我发现有道云笔记的搜索不好用，还不如 QQ 邮箱的搜索好用。我是「理解」型的学习方法，学习的过程中形成连贯的思维，然后等到需要用的时候，再去搜索。但有的时候脑海中的关键字和以前记的电子笔记不一样，所以如何贯通这个关键字，我正在考虑如何解决这个问题。比如每次记笔记的时候，旁边写上关键字。我还感受到过一种学习方法，就是「内心恐惧」学习法，这个时候会有强大的动力将其学会。

作者回复：掌握分 3 个阶段：第一个阶段，会用就可以了；第二个阶段，你发现还有很多可以调整的地方，开始探索算法的原理，各个参数如何优化；第三个阶段，自己会有新的想法，在原有的基础上进行创新。比如，前人在 ID3 算法基础上提出了 C4.5 算法，就是个很好的例子。所以说呢，第一阶段不需要太多数学基础，而随着阶段的提升，数学的价值也越来越重要。Anyway 慢慢修炼，先用起来。

作者回复：我觉得你的实战经验一定不错。也很感谢你对 MAS 方法的认可，就像你说的：授人予鱼，不如授人予渔。我觉得你可以换个维度，从工作场景和业务需求出发，尤其很多突破性的工作还是在于数据挖掘，或者说人工智能的部分。很明显，AI 这两年的热度要明显高于大数据，也说明这方面的突破比以往更快。所以，我建议你多了解不同行业的业务需求，在工具上可以更关注算法层面。

### 0102学习数据挖掘的最佳路径是什么.md

什么是数据挖掘呢？想象一下，茫茫的大海上，孤零零地屹立着钻井，想要从大海中开采出宝贵的石油。对于普通人来说，大海是很难感知的，就更不用说找到宝藏了。但对于熟练的石油开采人员来说，大海是有坐标的。他们对地质做勘探，分析地质构造，从而发现哪些地方更可能有石油。然后用开采工具，进行深度挖掘，直到打到石油为止。大海、地质信息、石油对开采人员来说就是数据源、地理位置、以及分析得到的结果。而我们要做的数据挖掘工作，就好像这个钻井一样，通过分析这些数据，从庞大的数据中发现规律，找到宝藏。

数据挖掘的知识清单，分别是数据挖掘的基本流程、十大算法和数学原理。

挖掘的基本流程。数据挖掘的过程可以分成以下 6 个步骤。1）商业理解：数据挖掘不是我们的目的，我们的目的是更好地帮助业务，所以第一步我们要从商业的角度理解项目需求，在这个基础上，再对数据挖掘的目标进行定义。2）数据理解：尝试收集部分数据，然后对数据进行探索，包括数据描述、数据质量验证等。这有助于你对收集的数据有个初步的认知。3）数据准备：开始收集数据，并对数据进行清洗、数据集成等操作，完成数据挖掘前的准备工作。4）模型建立：选择和应用各种数据挖掘模型，并进行优化，以便得到更好的分类结果。5）模型评估：对模型进行评价，并检查构建模型的每个步骤，确认模型是否实现了预定的商业目标。6）上线发布：模型的作用是从数据中找到金矿，也就是我们所说的「知识」，获得的知识需要转化成用户可以使用的方式，呈现的形式可以是一份报告，也可以是实现一个比较复杂的、可重复的数据挖掘过程。数据挖掘结果如果是日常运营的一部分，那么后续的监控和维护就会变得重要。

数据挖掘的十大算法。为了进行数据挖掘任务，数据科学家们提出了各种模型，在众多的数据挖掘模型中，国际权威的学术组织 ICDM （the IEEE International Conference on Data Mining）评选出了十大经典的算法。按照不同的目的，我可以将这些算法分成四类，以便你更好的理解。1）分类算法：C4.5，朴素贝叶斯（Naive Bayes），SVM，KNN，Adaboost，CART。2）聚类算法：K-Means，EM。3）关联分析：Apriori。4）连接分析：PageRank

1）C4.5 算法是得票最高的算法，可以说是十大算法之首。C4.5 是决策树的算法，它创造性地在决策树构造过程中就进行了剪枝，并且可以处理连续的属性，也能对不完整的数据进行处理。它可以说是决策树分类中，具有里程碑式意义的算法。2）朴素贝叶斯模型是基于概率论的原理，它的思想是这样的：对于给出的未知物体想要进行分类，就需要求解在这个未知物体出现的条件下各个类别出现的概率，哪个最大，就认为这个未知物体属于哪个分类。3）SVM 的中文叫支持向量机，英文是 Support Vector Machine，简称 SVM。SVM 在训练中建立了一个超平面的分类模型。如果你对超平面不理解，没有关系，我在后面的算法篇会给你进行介绍。4）KNN 也叫 K 最近邻算法，英文是 K-Nearest Neighbor。所谓 K 近邻，就是每个样本都可以用它最接近的 K 个邻居来代表。如果一个样本，它的 K 个最接近的邻居都属于分类 A，那么这个样本也属于分类 A。

5）Adaboost 在训练中建立了一个联合的分类模型。boost 在英文中代表提升的意思，所以 Adaboost 是个构建分类器的提升算法。它可以让我们多个弱的分类器组成一个强的分类器，所以 Adaboost 也是一个常用的分类算法。6）CART 代表分类和回归树，英文是 Classification and Regression Trees。像英文一样，它构建了两棵树：一棵是分类树，另一个是回归树。和 C4.5 一样，它是一个决策树学习方法。7）Apriori 是一种挖掘关联规则（association rules）的算法，它通过挖掘频繁项集（frequent item sets）来揭示物品之间的关联关系，被广泛应用到商业挖掘和网络安全等领域中。频繁项集是指经常出现在一起的物品的集合，关联规则暗示着两种物品之间可能存在很强的关系。

8）K-Means 算法是一个聚类算法。你可以这么理解，最终我想把物体划分成 K 类。假设每个类别里面，都有个「中心点」，即意见领袖，它是这个类别的核心。现在我有一个新点要归类，这时候就只要计算这个新点与 K 个中心点的距离，距离哪个中心点近，就变成了哪个类别。9）EM 算法也叫最大期望算法，是求参数的最大似然估计的一种方法。原理是这样的：假设我们想要评估参数 A 和参数 B，在开始状态下二者都是未知的，并且知道了 A 的信息就可以得到 B 的信息，反过来知道了 B 也就得到了 A。可以考虑首先赋予 A 某个初值，以此得到 B 的估值，然后从 B 的估值出发，重新估计 A 的取值，这个过程一直持续到收敛为止。EM 算法经常用于聚类和机器学习领域中。10）PageRank 起源于论文影响力的计算方式，如果一篇文论被引入的次数越多，就代表这篇论文的影响力越强。同样 PageRank 被 Google 创造性地应用到了网页权重的计算中：当一个页面链出的页面越多，说明这个页面的「参考文献」越多，当这个页面被链入的频率越高，说明这个页面被引用的次数越高。基于这个原理，我们可以得到网站的权重划分。

算法可以说是数据挖掘的灵魂，也是最精华的部分。这 10 个经典算法在整个数据挖掘领域中的得票最高的，后面的一些其他算法也基本上都是在这个基础上进行改进和创新。今天你先对十大算法有一个初步的了解，你只需要做到心中有数就可以了，具体内容不理解没有关系。

数据挖掘的数学原理。说了这么多数据挖掘中的经典算法，但是如果你不了解概率论和数理统计，还是很难掌握算法的本质；如果你不懂线性代数，就很难理解矩阵和向量运作在数据挖掘中的价值；如果你没有最优化方法的概念，就对迭代收敛理解不深。所以说，想要更深刻地理解数据挖掘的方法，就非常有必要了解它后背的数学原理。

1）概率论与数理统计。概率论在我们上大学的时候，基本上都学过，不过大学里老师教的内容，偏概率的多一些，统计部分讲得比较少。在数据挖掘里使用到概率论的地方就比较多了。比如条件概率、独立性的概念，以及随机变量、多维随机变量的概念。很多算法的本质都与概率论相关，所以说概率论与数理统计是数据挖掘的重要数学基础。2）线性代数。向量和矩阵是线性代数中的重要知识点，它被广泛应用到数据挖掘中，比如我们经常会把对象抽象为矩阵的表示，一幅图像就可以抽象出来是一个矩阵，我们也经常计算特征值和特征向量，用特征向量来近似代表物体的特征。这个是大数据降维的基本思路。基于矩阵的各种运算，以及基于矩阵的理论成熟，可以帮我们解决很多实际问题，比如 PCA 方法、SVD 方法，以及 MF、NMF 方法等在数据挖掘中都有广泛的应用。

3）图论。社交网络的兴起，让图论的应用也越来越广。人与人的关系，可以用图论上的两个节点来进行连接，节点的度可以理解为一个人的朋友数。我们都听说过人脉的六度理论，在 Facebook 上被证明平均一个人与另一个人的连接，只需要 3.57 个人。当然图论对于网络结构的分析非常有效，同时图论也在关系挖掘和图像分割中有重要的作用。4）最优化方法。最优化方法相当于机器学习中自我学习的过程，当机器知道了目标，训练后与结果存在偏差就需要迭代调整，那么最优化就是这个调整的过程。一般来说，这个学习和迭代的过程是漫长、随机的。最优化方法的提出就是用更短的时间得到收敛，取得更好的效果。

### 黑板墙

课后思考：对于思考题，我想到了沃尔玛「啤酒和尿布」经典案例。沃尔玛正是将 Apriori 算法引入到 POS 机数据分析中，从而获得了营销上奇迹。简单说来就是在一个数据集中，找到经常出现的商品组合。当然 Apriori 算法的计算量很大，当商品数据量大时效率低，FP-Tree 算法优化了该算法。

1）数据挖掘学习方法体会：有了知识清单，相当于有了一个系统思维在那，对快速识别问题的确很有帮助～很好的方法方便实践，就像巴菲特和芒格的投资是使用的公司尽调清单一样，MECE 的解决问题。2）基于电商商品的关联进行推荐从而提高销售的话，个人认为是 Apriori 算法，其为了提取频繁项集和一定置信度的关联规则，即用户购买了 X 产品有多大概率去买 Y，根据置信度高的原则推荐。

### 0103Python基础语法.md

最好掌握 Python 语言。首先，在一份关于开发语言的调查中，使用过 Python 的开发者，80% 都会把 Python 作为自己的主要语言。Python 已经成为发展最快的主流编程语言，从众多开发语言中脱颖而出，深受开发者喜爱。其次，在数据分析领域中，使用 Python 的开发者是最多的，远超其他语言之和。最后，Python 语言简洁，有大量的第三方库，功能强大，能解决数据分析的大部分问题，这一点我下面具体来说。

Python 语言最大的优点是简洁，它虽然是 C 语言写的，但是摒弃了 C 语言的指针，这就让代码非常简洁明了。同样的一行 Python 代码，甚至相当于 5 行 Java 代码。我们读 Python 代码就像是读英文一样直观，这就能让程序员更好地专注在问题解决上，而不是在语言本身。当然除了 Python 自身的特点，Python 还有强大的开发者工具。在数据科学领域，Python 有许多非常著名的工具库：比如科学计算工具 NumPy 和 Pandas 库，深度学习工具 Keras 和 TensorFlow，以及机器学习工具 Scikit-learn，使用率都非常高。

大部分 Python 库都同时支持 Python 2.7.x 和 3.x 版本。虽然官方称 Python2.7 只维护到 2020 年，但是我想告诉你的是：千万不要忽视 Python2.7，它的寿命远不止到 2020 年，而且这两年 Python2.7 还是占据着 Python 版本的统治地位。一份调查显示：在 2017 年的商业项目中 2.7 版本依然是主流，占到了 63.7%，即使这两年 Python3.x 版本使用的增速较快，但实际上 Python3.x 在 2008 年就已经有了。

这两个版本该如何选择呢？版本选择的标准就是看你的项目是否会依赖于 Python2.7 的包，如果有依赖的就只能使用 Python2.7，否则你可以用 Python 3.x 开始全新的项目。

for 循环是一种迭代循环机制，迭代即重复相同的逻辑操作。如果规定循环的次数，我们可以使用 range 函数，它在 for 循环中比较常用。range (11) 代表从 0 到 10，不包括 11，也相当于 range (0,11)，range 里面还可以增加步长，比如 range (1,11,2) 代表的是 [1,3,5,7,9]。

1 到 10 的求和也可以用 while 循环来写，这里 while 控制了循环的次数。while 循环是条件循环，在 while 循环中对于变量的计算方式更加灵活。因此 while 循环适合循环次数不确定的循环，而 for 循环的条件相对确定，适合固定次数的循环。

注释在 python 中使用 #，如果注释中有中文，一般会在代码前添加 # -\*- coding: utf-8 -*。如果是多行注释，使用三个单引号，或者三个双引号。

1『原来注释里含有中文的情况下才需要加「# -\*- coding: utf-8 -*」』

Python 语言中 import 的使用很简单，直接使用 import module_name 语句导入即可。这里 import 的本质是什么呢？import 的本质是路径搜索。import 引用可以是模块 module，或者包 package。针对 module，实际上是引用一个.py 文件。而针对 package，可以采用 from … import … 的方式，这里实际上是从一个目录中引用模块，这时目录结构中必须带有一个 \__init__.py 文件。

上面的讲的这些基础语法，我们可以用 sumlime text 编辑器运行 Python 代码。另外，告诉你一个相当高效的方法，你可以充分利用一个刷题进阶的网址：[ZOJ](https://zoj.pintia.cn/home)，这是浙江大学 ACM 的 OnlineJudge。

2『跟力库一样去刷。』

### 0104Python科学计算用NumPy快速处理数据.md

它不仅是 Python 中使用最多的第三方库，而且还是 SciPy、Pandas 等数据科学的基础库。它所提供的数据结构比 Python 自身的「更高级、更高效」，可以这么说，NumPy 所提供的数据结构是 Python 数据分析的基础。上次讲到了 Python 数组结构中的列表 list，它实际上相当于一个数组的结构。而 NumPy 中一个关键数据类型就是关于数组的，那为什么还存在这样一个第三方的数组结构呢？

实际上，标准的 Python 中，用列表 list 保存数组的数值。由于列表中的元素可以是任意的对象，所以列表中 list 保存的是对象的指针。虽然在 Python 编程中隐去了指针的概念，但是数组有指针，Python 的列表 list 其实就是数组。这样如果我要保存一个简单的数组 [0,1,2]，就需要有 3 个指针和 3 个整数的对象，这样对于 Python 来说是非常不经济的，浪费了内存和计算时间。

为什么要用 NumPy 数组结构而不是 Python 本身的列表 list？这是因为列表 list 的元素在系统内存中是分散存储的，而 NumPy 数组存储在一个均匀连续的内存块中。这样数组计算遍历所有的元素，不像列表 list 还需要对内存地址进行查找，从而节省了计算资源。另外在内存访问模式中，缓存会直接把字节块从 RAM 加载到 CPU 寄存器中。因为数据连续的存储在内存中，NumPy 直接利用现代 CPU 的矢量化指令计算，加载寄存器中的多个连续浮点数。另外 NumPy 中的矩阵计算可以采用多线程的方式，充分利用多核 CPU 计算资源，大大提升了计算效率。

当然除了使用 NumPy 外，你还需要一些技巧来提升内存和提高计算资源的利用率。一个重要的规则就是：避免采用隐式拷贝，而是采用就地操作的方式。举个例子，如果我想让一个数值 x 是原来的两倍，可以直接写成 x\*=2，而不要写成 y=x*2。这样速度能快到 2 倍甚至更多。既然 NumPy 这么厉害，你该从哪儿入手学习呢？在 NumPy 里有两个重要的对象：ndarray（N-dimensional array object）解决了多维数组问题，而 ufunc（universal function object）则是解决对数组进行处理的函数。下面，我就带你一一来看。

ndarray 实际上是多维数组的含义。在 NumPy 数组中，维数称为秩（rank），一维数组的秩为 1，二维数组的秩为 2，以此类推。在 NumPy 中，每一个线性的数组称为一个轴（axes），其实秩就是描述轴的数量。

创建数组前，你需要引用 NumPy 库，可以直接通过 array 函数创建数组，如果是多重数组，比如示例里的 b，那么该怎么做呢？你可以先把一个数组作为一个元素，然后嵌套起来，比如示例 b 中的 [1,2,3] 就是一个元素，然后 [4,5,6][7,8,9] 也是作为元素，然后把三个元素再放到 [] 数组里，赋值给变量 b。

当然数组也是有属性的，比如你可以通过函数 shape 属性获得数组的大小，通过 dtype 获得元素的属性。如果你想对数组里的数值进行修改的话，直接赋值即可，注意下标是从 0 开始计的，所以如果你想对 b 数组，九宫格里的中间元素进行修改的话，下标应该是 [1,1]。

你看下这个例子，首先在 NumPy 中是用 dtype 定义的结构类型，然后在定义数组的时候，用 array 中指定了结构数组的类型 dtype=persontype，这样你就可以自由地使用自定义的 persontype 了。比如想知道每个人的语文成绩，就可以用 chineses = peoples [:][‘chinese’]，当然 NumPy 中还有一些自带的数学运算，比如计算平均值使用 np.mean。

ufunc 是 universal function 的缩写，是不是听起来就感觉功能非常强大？确如其名，它能对数组中每个元素进行函数操作。NumPy 中很多 ufunc 函数计算速度非常快，因为都是采用 C 语言实现的。

NumPy 可以很方便地创建连续数组，比如我使用 arange 或 linspace 函数进行创建。np.arange 和 np.linspace 起到的作用是一样的，都是创建等差数组。这两个数组的结果 x1,x2 都是 [1 3 5 7 9]。结果相同，但是你能看出来创建的方式是不同的。arange () 类似内置函数 range ()，通过指定初始值、终值、步长来创建等差数列的一维数组，默认是不包括终值的。linspace 是 linear space 的缩写，代表线性等分向量的含义。linspace () 通过指定初始值、终值、元素个数来创建等差数列的一维数组，默认是包括终值的。

通过 NumPy 可以自由地创建等差数组，同时也可以进行加、减、乘、除、求 n 次方和取余数。我还以 x1, x2 数组为例，求这两个数组之间的加、减、乘、除、求 n 次方和取余数。在 n 次方中，x2 数组中的元素实际上是次方的次数，x1 数组的元素为基数。在取余函数里，你既可以用 np.remainder (x1, x2)，也可以用 np.mod (x1, x2)，结果是一样的。

统计函数。如果你想要对一堆数据有更清晰的认识，就需要对这些数据进行描述性的统计分析，比如了解这些数据中的最大值、最小值、平均值，是否符合正态分布，方差、标准差多少等等。它们可以让你更清楚地对这组数据有认知。

计数组 / 矩阵中的最大值函数 amax ()，最小值函数 amin ()。amin () 用于计算数组中的元素沿指定轴的最小值。对于一个二维数组 a，amin (a) 指的是数组中全部元素的最小值，amin (a,0) 是延着 axis=0 轴的最小值，axis=0 轴是把元素看成了 [1,4,7], [2,5,8], [3,6,9] 三个元素，所以最小值为 [1,2,3]，amin (a,1) 是延着 axis=1 轴的最小值，axis=1 轴是把元素看成了 [1,2,3], [4,5,6], [7,8,9] 三个元素，所以最小值为 [1,4,7]。同理 amax () 是计算数组中元素沿指定轴的最大值。

1『axis=0 是一列一列的，axis=1 是一行一行的。axis 即为轴或者秩。』

统计最大值与最小值之差 ptp ()。对于相同的数组 a，np.ptp (a) 可以统计数组中最大值与最小值的差，即 9-1=8。同样 ptp (a,0) 统计的是沿着 axis=0 轴的最大值与最小值之差，即 7-1=6（当然 8-2=6,9-3=6，第三行减去第一行的 ptp 差均为 6），ptp (a,1) 统计的是沿着 axis=1 轴的最大值与最小值之差，即 3-1=2（当然 6-4=2, 9-7=2，即第三列与第一列的 ptp 差均为 2）。

统计数组的百分位数 percentile()。percentile () 代表着第 p 个百分位数，这里 p 的取值范围是 0-100，如果 p=0，那么就是求最小值，如果 p=50 就是求平均值，如果 p=100 就是求最大值。同样你也可以求得在 axis=0 和 axis=1 两个轴上的 p% 的百分位数。你可以用 median () 和 mean () 求数组的中位数、平均值，同样也可以求得在 axis=0 和 1 两个轴上的中位数、平均值。

average () 函数可以求加权平均，加权平均的意思就是每个元素可以设置个权重，默认情况下每个元素的权重是相同的，所以 np.average (a)=(1+2+3+4)/4=2.5，你也可以指定权重数组 wts=[1,2,3,4]，这样加权平均 np.average (a,weights=wts)=(1\*1+2\*2+3\*3+4*4)/(1+2+3+4)=3.0。

统计数组中的标准差 std ()、方差 var ()。方差的计算是指每个数值与平均值之差的平方求和的平均值，即 mean ((x - x.mean ())** 2)。标准差是方差的算术平方根。在数学意义上，代表的是一组数据离平均值的分散程度。所以 np.var (a)=1.25, np.std (a)=1.118033988749895。

NumPy 排序。排序是算法中使用频率最高的一种，也是在数据分析工作中常用的方法，计算机专业的同学会在大学期间的算法课中学习。那么这些排序算法在 NumPy 中实现起来其实非常简单，一条语句就可以搞定。这里你可以使用 sort 函数，sort (a, axis=-1, kind=‘quicksort’, order=None)，默认情况下使用的是快速排序；在 kind 里，可以指定 quicksort、mergesort、heapsort 分别表示快速排序、合并排序、堆排序。同样 axis 默认是 -1，即沿着数组的最后一个轴进行排序，也可以取不同的 axis 轴，或者 axis=None 代表采用扁平化的方式作为一个向量进行排序。另外 order 字段，对于结构化的数组可以指定按照某个字段进行排序。

在 NumPy 学习中，你重点要掌握的就是对数组的使用，因为这是 NumPy 和标准 Python 最大的区别。在 NumPy 中重新对数组进行了定义，同时提供了算术和统计运算，你也可以使用 NumPy 自带的排序功能，一句话就搞定各种排序算法。当然要理解 NumPy 提供的数据结构为什么比 Python 自身的「更高级、更高效」，要从对数据指针的引用角度进行理解。

## 0105Python科学计算Pandas.md

和 NumPy 一样，Pandas 有两个非常重要的数据结构：Series 和 DataFrame。使用 Pandas 可以直接从 csv 或 xlsx 等文件中导入数据，以及最终输出到 excel 表中。重点介绍了数据清洗中的操作，当然 Pandas 中同样提供了多种数据统计的函数。最后我们介绍了如何将数据表进行合并，以及在 Pandas 中使用 SQL 对数据表更方便地进行操作。Pandas 包与 NumPy 工具库配合使用可以发挥巨大的威力，正是有了 Pandas 工具，Python 做数据挖掘才具有优势。

在数据分析工作中，Pandas 的使用频率是很高的，一方面是因为 Pandas 提供的基础数据结构 DataFrame 与 json 的契合度很高，转换起来就很方便。另一方面，如果我们日常的数据清理工作不是很复杂的话，你通常用几句 Pandas 代码就可以对数据进行规整。Pandas 可以说是基于 NumPy 构建的含有更高级数据结构和分析能力的工具包。在 NumPy 中数据结构是围绕 ndarray 展开的，那么在 Pandas 中的核心数据结构是什么呢？Series 和 DataFrame 这两个核心数据结构，分别代表着一维的序列和二维的表结构。基于这两种数据结构，Pandas 可以对数据进行导入、清洗、处理、统计和输出。

Series 是个定长的字典序列。说是定长是因为在存储的时候，相当于两个 ndarray，这也是和字典结构最大的不同。因为在字典的结构里，元素的个数是不固定的。Series 有两个基本属性：index 和 values。在 Series 结构中，index 默认是 0,1,2,…… 递增的整数序列，当然我们也可以自己来指定索引，比如 index=[‘a’, ‘b’, ‘c’, ‘d’]。

这个例子中，x1 中的 index 采用的是默认值，x2 中 index 进行了指定。我们也可以采用字典的方式来创建 Series，比如；DataFrame 类型数据结构类似数据库表。它包括了行索引和列索引，我们可以将 DataFrame 看成是由相同索引的 Series 组成的字典类型。

1『Series 只有行索引，DataFrame 有行索引和列索引。』

我们虚构一个王者荣耀考试的场景，想要输出几位英雄的考试成绩；在后面的案例中，我一般会用 df, df1, df2 这些作为 DataFrame 数据类型的变量名，我们以例子中的 df2 为例，列索引是 [‘English’, ‘Math’, ‘Chinese’]，行索引是 [‘ZhangFei’, ‘GuanYu’, ‘ZhaoYun’, ‘HuangZhong’, ‘DianWei’]，所以 df2 的输出是；在了解了 Series 和 DataFrame 这两个数据结构后，我们就从数据处理的流程角度，来看下他们的使用方法。

数据导入和输出。Pandas 允许直接从 xlsx，csv 等文件中导入数据，也可以输出到 xlsx, csv 等文件，非常方便。需要说明的是，在运行的过程可能会存在缺少 xlrd 和 openpyxl 包的情况，到时候如果缺少了，可以在命令行模式下使用「pip install」命令来进行安装。

数据清洗。数据清洗是数据准备过程中必不可少的环节，Pandas 也为我们提供了数据清洗的工具，在后面数据清洗的章节中会给你做详细的介绍，这里简单介绍下 Pandas 在数据清洗中的使用方法。删除 DataFrame 中的不必要的列或行。Pandas 提供了一个便捷的方法 drop () 函数来删除我们不想要的列或行。比如我们想把「语文」这列删掉；想把「张飞」这行删掉；重命名列名 columns，让列表名更容易识别。如果你想对 DataFrame 中的 columns 进行重命名，可以直接使用 rename (columns=new_names, inplace=True) 函数，比如我把列名 Chinese 改成 YuWen，English 改成 YingYu；去重复的值。数据采集可能存在重复的行，这时只要使用 drop_duplicates () 就会自动把重复的行去掉。

```
df2 = df2.drop(columns=['Chinese'])
df2 = df2.drop(index=['ZhangFei'])

df2.rename(columns={'Chinese': 'YuWen', 'English': 'Yingyu'}, inplace = True)

df = df.drop_duplicates () #去除重复行
```

更改数据格式。这是个比较常用的操作，因为很多时候数据格式不规范，我们可以使用 astype 函数来规范数据格式，比如我们把 Chinese 字段的值改成 str 类型，或者 int64 可以这么写；数据间的空格。有时候我们先把格式转成了 str 类型，是为了方便对数据进行操作，这时想要删除数据间的空格，我们就可以使用 strip 函数；如果数据里有某个特殊的符号，我们想要删除怎么办？同样可以使用 strip 函数，比如 Chinese 字段里有美元符号，我们想把这个删掉，可以这么写。

```
df2['Chinese'].astype('str') 
df2['Chinese'].astype(np.int64) 

#删除左右两边空格
df2['Chinese']=df2['Chinese'].map(str.strip)
#删除左边空格
df2['Chinese']=df2['Chinese'].map(str.lstrip)
#删除右边空格
df2['Chinese']=df2['Chinese'].map(str.rstrip)

df2['Chinese']=df2['Chinese'].str.strip('$')
```

大小写转换。大小写是个比较常见的操作，比如人名、城市名等的统一都可能用到大小写的转换，在 Python 里直接使用 upper (), lower (), title () 函数，方法如下；查找空值。数据量大的情况下，有些字段存在空值 NaN 的可能，这时就需要使用 Pandas 中的 isnull 函数进行查找。比如，我们输入一个数据表如下；如果我们想看下哪个地方存在空值 NaN，可以针对数据表 df 进行 df.isnull ()，结果如下；如果我想知道哪列存在空值，可以使用 df.isnull ().any ()，结果如下：

```
#全部大写
df2.columns = df2.columns.str.upper()
#全部小写
df2.columns = df2.columns.str.lower()
#首字母大写
df2.columns = df2.columns.str.title()
```

使用 apply 函数对数据进行清洗。apply 函数是 Pandas 中自由度非常高的函数，使用频率也非常高。比如我们想对 name 列的数值都进行大写转化可以用；我们也可以定义个函数，在 apply 中进行使用。比如定义 double_df 函数是将原来的数值 \*2 进行返回。然后对 df1 中的「语文」列的数值进行 *2 处理，可以写成；我们也可以定义更复杂的函数，比如对于 DataFrame，我们新增两列，其中 ’new1’ 列是「语文」和「英语」成绩之和的 m 倍，'new2’ 列是「语文」和「英语」成绩之和的 n 倍，我们可以这样写；其中 axis=1 代表按照列为轴进行操作，axis=0 代表按照行为轴进行操作，args 是传递的两个参数，即 n=2, m=3，在 plus 函数中使用到了 n 和 m，从而生成新的 df。

```
df['name'] = df['name'].apply(str.upper)

def double_df(x):
           return 2*x
df1[u'语文'] = df1[u'语文'].apply(double_df)

def plus(df,n,m):
    df['new1'] = (df[u'语文']+df[u'英语']) * m
    df['new2'] = (df[u'语文']+df[u'英语']) * n
    return df
df1 = df1.apply(plus,axis=1,args=(2,3,))
```

数据统计。在数据清洗后，我们就要对数据进行统计了。Pandas 和 NumPy 一样，都有常用的统计函数，如果遇到空值 NaN，会自动排除。常用的统计函数包括；表格中有一个 describe () 函数，统计函数千千万，describe () 函数最简便。它是个统计大礼包，可以快速让我们对数据有个全面的了解。下面我直接使用 df1.descirbe () 输出结果为：

```
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
print df1.describe()
```

数据表合并。有时候我们需要将多个渠道源的多个数据表进行合并，一个 DataFrame 相当于一个数据库的数据表，那么多个 DataFrame 数据表的合并就相当于多个数据库的表合并。比如我要创建两个 DataFrame；两个 DataFrame 数据表的合并使用的是 merge () 函数，有下面 5 种形式：1）比如我们可以基于 name 这列进行连接。2）inner 内链接是 merge 合并的默认情况，inner 内连接其实也就是键的交集，在这里 df1, df2 相同的键是 name，所以是基于 name 字段做的连接。3）左连接是以第一个 DataFrame 为主进行的连接，第二个 DataFrame 作为补充。4）右连接是以第二个 DataFrame 为主进行的连接，第一个 DataFrame 作为补充。5）外连接相当于求两个 DataFrame 的并集。

```
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
df2 = DataFrame({'name':['ZhangFei', 'GuanYu', 'A', 'B', 'C'], 'data2':range(5)})

df3 = pd.merge(df1, df2, on='name')
df3 = pd.merge(df1, df2, how='inner')
df3 = pd.merge(df1, df2, how='left')
df3 = pd.merge(df1, df2, how='right')
df3 = pd.merge(df1, df2, how='outer')
```

如何用 SQL 方式打开 Pandas。Pandas 的 DataFrame 数据类型可以让我们像处理数据表一样进行操作，比如数据表的增删改查，都可以用 Pandas 工具来完成。不过也会有很多人记不住这些 Pandas 的命令，相比之下还是用 SQL 语句更熟练，用 SQL 对数据表进行操作是最方便的，它的语句描述形式更接近我们的自然语言。事实上，在 Python 里可以直接使用 SQL 语句来操作 Pandas。

这里给你介绍个工具：pandasql。pandasql 中的主要函数是 sqldf，它接收两个参数：一个 SQL 查询语句，还有一组环境变量 globals () 或 locals ()。这样我们就可以在 Python 里，直接用 SQL 语句中对 DataFrame 进行操作，举个例子：

```
import pandas as pd
from pandas import DataFrame
from pandasql import sqldf, load_meat, load_births
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
pysqldf = lambda sql: sqldf(sql, globals())
sql = "select * from df1 where name ='ZhangFei'"
print pysqldf(sql)
```

上面这个例子中，我们是对「name='ZhangFei」的行进行了输出。当然你会看到我们用到了 lambda，lambda 在 python 中算是使用频率很高的，那 lambda 是用来做什么的呢？它实际上是用来定义一个匿名函数的，具体的使用形式为：

     lambda argument_list: expression

这里 argument_list 是参数列表，expression 是关于参数的表达式，会根据 expression 表达式计算结果进行输出返回。在上面的代码中，我们定义了：

    pysqldf = lambda sql: sqldf(sql, globals())

在这个例子里，输入的参数是 sql，返回的结果是 sqldf 对 sql 的运行结果，当然 sqldf 中也输入了 globals 全局参数，因为在 sql 中有对全局参数 df1 的使用。


























