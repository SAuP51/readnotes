## 记忆时间

## 20200304Top-3-Selenium-Functions-.md

get(). The get command launches a new browser and opens the given URL in your Chrome Webdriver. It simply takes the string as your specified URL and opens it for testing purposes. If you are using Selenium IDE, it is similar to open command.

    driver.get("https://google.com");

The ‘driver’ is your Chrome Webdriver on which you are going to perform all actions and it looks like this after executing the command above:

find\_element(). This function is crucial when you want to access elements on the page. Let’s say we want to access the「Google search」button in order to perform a search. There are many ways to access elements, but my preferred one is to find XPath of the element. XPath is the element's ultimate location on the web page.

By clicking F12 you will inspect the page and get the background information about the page you are at. By clicking the selection tool, you will be able to select elements. After finding the button, click on the right at the blue marked section and Copy element’s「Full Xpath」.

    self.driver.find_element_by_xpath('/html/body/div/div[4]/form/div[2]/div[1]/div[3]/center/input[1]')

That is the full command in order to find the specific element. I prefer full XPath over the regular XPath because regular one can be changed if the element in a new session changes and then next time you perform your script it does not work.

send_keys() and click(). I added a bonus function for you so we can follow through with this example and make it fully work. Send_keys function is used for typing the text into a field to you selected by using the find\_element function. Let’s say we want to type「plate」into google and perform a search. We already have our「Google search」button, now we just need to type in the text and click the button with the help of click function.

```
google_tray = self.driver.find_element_by_xpath('/html/body/div/div[4]/form/div[2]/div[1]/div[1]/div/div[2]/input')
google_tray.send_keys("plate")
google_search = self.driver.find_element_by_xpath('/html/body/div/div[4]/form/div[2]/div[1]/div[3]/center/input[1]')
google_search.click()
```

## 20200304Web-scraping-in-5-minutes.md

### 01. Introduction

we can face the web-scraping-challenge by using just a few Python public libraries:

1. ‘requests’ calling ‘import requests’

2. ‘BeautifulSoup’ calling ‘from bs4 import BeautifulSoup’

3. We may also want to use also ‘tqdm\_notebook’ tool calling ‘from tqdm import tqdm\_notebook’ in case we need to iterate through a site with several pages. For example, suppose we’re trying to extract some information from all the job posts in https://indeed.co.uk after searching ‘data scientist’. We’ll probably have several result pages. The ‘bs4’ library will allow us to go all over them in a very simple way. We could do this also using just a simple for loop, but the ‘tqdm_notebook’ tool provides visualization about the evolution of the process, what meshes quite nicely if we’re scraping hundreds or thousands of pages from a site in one run.

But anyway, let’s go step by step. For this article, we’ll be scraping Penguin’s list of the 100 must-read classic books ([100 Must-Read Classic Books, As Chosen By Our Readers | Fiction, Novels & More](https://www.penguin.co.uk/articles/2018/100-must-read-classic-books/)). And we’ll try to get only the title and the author for each one of them. This simple notebook and others are available in my [gonzaferreiro/Simple_web_scraper](https://github.com/gonzaferreiro/Simple_web_scraper) (including the entire project about scraping indeed and running a classification model).

2『已经 forked 作者的源码「2020010Simple_web_scraper」。』

1『

因为网页刷不出来，试着输出内容，放到自己的服务器里看。

```
import requests 
from bs4 import BeautifulSoup
import re

url = 'https://www.penguin.co.uk/articles/2018/100-must-read-classic-books/'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')

## 试着输出内容，放到自己的服务器里看
with open('result.txt', 'w') as f:
    f.write(r.text)
```

』

### 02. Hands-on application

In this case, the text seems to be contained inside ‘\<div class=”cmp-text text”>’. The \<div> tag is nothing more than a container unit that encapsulates other page elements and divides the HTML document into sections. We may find our data under this or another tag. For example, the \<li> tag is used to represent an item in a list, and the \<h2> tag represents a level 2 heading in an HTML document (HTML includes 6 levels of headings, which are ranked by importance).

In this case, we didn’t have any special parameter in our URL. But let’s go back to our previous example of scraping Indeed.co.uk. In that scenario, as we mentioned before, we’d have to use tqdm_notebook. This tool works as a for loop iterating through the different pages:

```
for start in tqdm_notebook(range(0, 1000 10)):
    url = “https://www.indeed.co.uk/jobs?q=datascientist&start{}".format(start)
    r = requests.get(url)
    soup = BeautifulSoup(r.text,’html.parser’)
```

Pay attention to how we’re specifying to go from page 0 to 100, jumping from 10 to 10, and then inserting the ‘start’ parameter into the url by using start={}”.format(start). Amazing! We already have our HTML code in a nice format. Now let’s obtain our data! There are several ways of doing this, but for me, the neatest way is to write a simple function to be executed just after creating our BeautifulSoup object.

Previously we saw that our data (the title + author info) was in the following location: ‘\<div class=”cmp-text text”>’ . So what we’ll have to do is dismantle that code sequence, in order to find all the ‘cmp-text text’ elements in the code. For that, we’ll use our BeautifulSoup object:

    for book in soup.find_all('div', attrs={'class':'cmp-text text'})

In this way, our soup object will go through all the code, finding all the ‘cmp-text text’ elements and giving us the content of each one of them. A good idea to understand what we’re getting on each iteration is to print the text calling ‘book.text’. In this case, we’d see the following:

What we can see here, is that the text under our ‘cmp-text text’ element contains the title + author, but also the brief description of the book. We can solve this in several ways. For the sake of this example, I used regex calling ‘import re’. Regex is a vast topic by its own, but in this case, I used a very simple tool that always comes in handy and you can learn easily.

    pattern = re.compile(r'(?<=. ).+(?=\n)')

Using our ‘re’ object, we can create a pattern using the regex lookahead and lookbehind tools. Specifying (?<=behind_text) we can look for whatever follows our ‘match\_text’. And with (?=ahead_text) we are indicating for anything followed by ‘match_text’, ’. Once regex found our code, we can tell it to give us anything that’s a number, only letters, special characters, some specific words or letter, or even a combination of all this. In our example, the ‘.+’ expression in the middle basically means ‘bring me anything’. So we’ll find everything in between ‘. ‘ (the point and the space after the book’s position in the list) and a ‘\n’, that’s a jump to the next line. Now we only have to pass our ‘re’ object the pattern, and the text:

    appender = re.findall(pattern,text)

This will give us a list, for example, with the following content:

    [‘The Great Gatsby by F. Scott Fitzgerald’]

Easy peasy to solve calling the text in the list and splitting it at ‘ by ‘, to after store title and author in different lists. Let’s put everything together now in a function as we said before:

```
def extract_books_from_result(soup):
    returner = {'books': [], 'authors': []}
    for book in soup.find_all('div', attrs={'class':'cmp-text text'}):
        try:
            text = book.text
            pattern = re.compile(r'(?<=. ).+(?=\n)')
            appender = re.findall(pattern,text)[0].split(' by')
            # Including a condition just to avoid anything that’s not a book Each book should consist in a list with two elements: 
            # [0] = title and [1] = author
            if len(appender) > 1:
                returner['books'].append(appender[0])
                returner['authors'].append(appender[1])
                
        except:
            None
            
    returner_df = pd.DataFrame(returner, columns=['books','authors']) 
    
    return returner_df
```
And finally, let’s run all together:

```
url = 'https://www.penguin.co.uk/articles/2018/100-must-read-classic-books/'
r = requests.get(url)
soup = BeautifulSoup(r.text,'html.parser')
results = extract_books_from_result(soup)
```

Unfortunately, web scraping is not always that easy, since there are several web sites that don’t want us sneaking around. In some cases, they will have a very intricated HTML to avoid rookies, but in many, they will be blocking good people like us trying to obtain some decent data. There are several ways of not being detected, but sadly, this article has gone away from me and my promise of learning web scraping in five minutes. But don’t worry, I’ll be writing more about this in the near future.

## 20200318My-top-10-Python-packages-for-data-science.md

### 01. Data processing

pandas. Developed by Wes McKinney more than a decade ago, this package offers powerful data processing capabilities. For people with a SAS background, it is a bit like SAS data steps. You can do sorting, merging, filtering, etc. The key difference is in pandas, you call a function to perform these tasks. By the way, I was really amazed to know that Wes McKinney was able to develop pandas after only a few years of Python experience. Some people are just really gifted! His book Python for Data Analysis is highly recommended if you are just starting out your Python data science journey.

3『就是之前看过的书籍「利用 Python 进行数据分析」。』

numpy. Pandas builds on top of this package numpy. So when you will often rely on this package for basic data manipulations. For example, when you need to create a new column based on the age of the customer, you need to do something like:

    df['isRetired'] = np.where(df['age']>=65, 'yes', 'no')

qgrid. An amazing package that allows you to sort, filter, and edit DataFrames in Jupyter Notebooks.

### 02. Graphing

The next three packages are all to do with graphing — which is a key step in exploratory data analysis.

matplotlib. This package allows you to do all sorts of graphs. If you are using it in a Jupyter Notebook, remember to run this line of code to enable the display of the graphs:

    %matplotlib inline

seaborn. With the help of this package, you can make matplotlib graphs looking much more attractive.

plotly. Nowadays we come across interactive graphs everywhere. They really offer a much better user experience. When we hover the mouse over a line plot we expect some text to pop up. When we select a line, we expect it to stand out from the other lines. Sometimes we would like to zoom into parts of the graph. These are the areas where interactive graphs shine. plotly allows you to build interactive graphs easily with a Jupyter Notebook. Over time, I see a lot of potential in sending Jupyter Notebooks with beautiful plotly graphs to the stakeholders.

### 03. Modeling

statsmodels. This package allows you to build Generalized Linear Models (GLMs) which are still widely used by actuaries today. It also offers time series analysis and other statistical modelling capabilities.

scikit-learn. This is the main machine learning package allowing you to complete most machine learning tasks, including classification, regression, clustering, and dimensionality reduction. I also use model selection and pre-processing functions. From k-fold cross-validation to scaling data and encoding categorical features, it has so much to offer.

lightgbm. This is one of my favorite machine learning packages for Gradient Boost Machine (GBM). I gave a talk in the 2018 Data Analytics seminar about this package. For a fraction of the time and effort needed to build GLMs, you could run a GBM, look at the importance matrix to find out the most important features for your model and have a good initial understanding of the problem. This can be a stand-alone step, or a quick first step before building a full GLM that’s more readily accepted by the stakeholders.

lime. Model interpretation still remains a challenge for machine learning models like GBM. When stakeholders don’t understand a model, they can’t trust it, and as a result, there’s no adoption.

However, I feel model interpretation packages like lime is starting to change this. It allows you to examine each model prediction and work out what’s driving the prediction.

### 20200318Replacing-Excel-with-Python.md

GitHub repo link: [ank0409/Ditching-Excel-for-Python: Functionalities in Excel translated to Python](https://github.com/ank0409/Ditching-Excel-for-Python)

2『已经 forked 项目「2020011Ditching-Excel-for-Python」。』

Importing Excel Files into a Pandas DataFrame. Though read_excel method includes million arguments but I will make you familiarise with the most common ones that will come very handy in day to day operations. I’ll be using the Iris sample dataset which is freely available online for educational purpose. Please follow the below link to download the dataset and ensure to save it in the same folder where you are saving your python file.

1『感觉后面的内容太零碎，没看了。』

## 20200318Productivity-tips-for-Jupyter.md

I’ve been very busy working on my MRes project in recent weeks, having very little sleep. This made me seek ways to improve my workflow in the most important tool of my work: Jupyter Notebook/Jupyter Lab. I collected all the hacks & tips in this piece, hoping that other researchers may find those useful:

Note: To make it easy-to-use, I collected the snippets presented below into a Python3 package (jupyter-helpers). You can get it with:

    pip install jupyter_helpers

It will work out of the box, but if you wish the best experience, these dependencies are highly recommended:

```
pip install ipywidgets
jupyter labextension install @jupyter-widgets/jupyterlab-manager
```

1『jupyter labextension install @jupyter-widgets/jupyterlab-manager，提示没有该安装命令。』

### 06. Selectively import from other notebooks

For some time I was trying to follow data/methods/results separation, having three Jupyter notebooks for each larger analysis: data.ipynb, methods.ipynb and results.ipynb. To save time on useless re-computation of some stuff I wanted to have selective import from data and methods notebooks for use in the results notebook. This was described in this SO thread, and I still hope to see some suggestions.

[python - Selectively import from another Jupyter Notebook - Stack Overflow](https://stackoverflow.com/questions/54317381/selectively-import-from-another-jupyter-notebook)

### 07. Scroll to the recently executed cell

You may have noticed that previously shown Notifications class made the notebook scroll down on exception to the offending cell (Figure 1). This can be disabled by passing scroll_to_exceptions=False.

If you conversely, want more auto-scrolling, you could use the underlying helper function to mark the cell that you ended working with at night to quickly open your notebook back on it in the morning:

```
from jupyter_helpers.utilities import scroll_to_current_cell
scroll_to_current_cell(preserve=True)
```

## 20200319Jupyter-Superpower.md

