## 记忆时间

## 卡片

### 0101. 反常识卡——

这本书的主题核心，就是最大的反常识卡，并且注意时间脉络。

#### 01. 常识

#### 02. 反常识

#### 03. 知识来源

比如提出者，如何演化成型的；书或专栏具体出现的地方。

#### 04. 例子

### 0201. 术语卡——

根据反常识，再补充三个证据——就产生三张术语卡。

例子。

### 0202. 术语卡——

### 0203. 术语卡——

### 0301. 人名卡——

根据这些证据和案例，找出源头和提出术语的人是谁——产生一张人名卡，并且分析他为什么牛，有哪些作品，生平经历是什么。

维基百科链接：有的话。

#### 01. 出生日期

用一句话描述你对这个大牛的印象。

#### 02. 贡献及经历

#### 03. 论文及书籍

#### 04. 演讲汇总

找一个他的 TED 演讲，有的话。

### 0401. 金句卡——

最后根据他写的非常震撼的话语——产生一张金句卡。

### 0501. 行动卡——

行动卡是能够指导自己的行动的卡。

### 0601. 任意卡——

最后还有一张任意卡，记录个人阅读感想。

## 模板

### 1. 逻辑脉络

用自己的话总结主题，梳理逻辑脉络，也就是在这个专栏的整个地图里，这一章节所在的节点。

### 2. 摘录及评论

1『自己的观点』

2『行动指南』

3『与其他知识的连接』

## 前言

当您运行 scrapy runspider somefile.py 命令时，Scrapy 尝试从该文件中查找 Spider 的定义，并且在爬取引擎中运行它。Scrapy 首先读取定义在 start_urls 属性中的 URL (在本示例中，就是 StackOverflow 的 top question 页面的 URL)，创建请求，并且将接收到的 response 作为参数调用默认的回调函数 parse ，来启动爬取。在回调函数 parse 中，我们使用 CSS Selector 来提取链接。接着，我们产生 (yield) 更多的请求，注册 parse_question 作为这些请求完成时的回调函数。

这里，您可以注意到 Scrapy 的一个最主要的优势：请求（request）是被异步调度和处理的 。这意味着，Scrapy 并不需要等待一个请求（request）完成及处理，在此同时，也发送其他请求或者做些其他事情。这也意味着，当有些请求失败或者处理过程中出现错误时，其他的请求也能继续处理。

在允许您可以以非常快的速度进行爬取时（以容忍错误的方式同时发送多个 request），Scrapy 也通过一些设置来允许您控制其爬取的方式。例如，您可以为两个 request 之间设置下载延迟，限制单域名（domain）或单个 IP 的并发请求量，甚至可以使用自动限制插件来自动处理这些问题。最终，parse_question 回调函数从每个页面中爬取到问题（question）的数据并产生了一个 dict，Scrapy 收集并按照终端（command line）的要求将这些结果写入到了 JSON 文件中。

注解：这里使用了 feed exports 来创建了 JSON 文件，您可以很容易的改变导出的格式（比如 XML 或 CSV）或者存储后端（例如 FTP 或者 Amazon S3）。您也可以编写 item pipeline 来将 item 存储到数据库中。

您已经了解了如何通过 Scrapy 提取存储网页中的信息，但这仅仅只是冰山一角。Scrapy 提供了很多强大的特性来使得爬取更为简单高效，例如：1）对 HTML，XML 源数据「选择及提取」的内置支持，提供了 CSS 选择器（selector）以及 XPath 表达式进行处理，以及一些帮助函数 (helper method) 来使用正则表达式来提取数据。2）提供交互式 shell 终端，为您测试 CSS 及 XPath 表达式，编写和调试爬虫提供了极大的方便。3）通过 feed 导出提供了多格式 (JSON、CSV、XML)，多存储后端 (FTP、S3、本地文件系统) 的内置支持。4）提供了一系列在 spider 之间共享的可复用的过滤器 (即 Item Loaders)，对智能处理爬取数据提供了内置支持。5）针对非英语语系中不标准或者错误的编码声明，提供了自动检测以及健壮的编码支持。6）高扩展性。您可以通过使用 signals，设计好的 API (中间件，extensions, pipelines) 来定制实现您的功能。7）内置的中间件及扩展为下列功能提供了支持: * cookies and session 处理 * HTTP 压缩 * HTTP 认证 * HTTP 缓存 * user-agent 模拟 * robots.txt * 爬取深度限制 * 其他。8）内置 Telnet 终端 ，通过在 Scrapy 进程中钩入 Python 终端，使您可以查看并且调试爬虫。9）以及其他一些特性，例如可重用的，从 Sitemaps 及 XML/CSV feeds 中爬取网站的爬虫、 可以 自动下载 爬取到的数据中的图片 (或者其他资源) 的 media pipeline、 带缓存的 DNS 解析器，以及更多的特性。

## 0101Scrapy入门教程.md

下列任务：1）创建一个 Scrapy 项目；2）定义提取的 Item；3）编写爬取网站的 spider 并提取 Item；4）编写 Item Pipeline 来存储提取到的 Item (即数据)。

在开始爬取之前，您必须创建一个新的 Scrapy 项目。进入您打算存储代码的目录中，运行下列命令：scrapy startproject tutorial。该命令将会创建包含下列内容的 tutorial 目录。

定义 Item。Item 是保存爬取到的数据的容器；其使用方法和 python 字典类似。虽然您也可以在 Scrapy 中直接使用 dict，但是 Item 提供了额外保护机制来避免拼写错误导致的未定义字段错误。They can also be used with Item Loaders, a mechanism with helpers to conveniently populate Items. 类似在 ORM 中做的一样，您可以通过创建一个 scrapy.Item 类，并且定义类型为 scrapy.Field 的类属性来定义一个 Item。首先根据需要从 dmoz.org 获取到的数据对 item 进行建模。我们需要从 dmoz 中获取名字，url，以及网站的描述。对此，在 item 中定义相应的字段。编辑 tutorial 目录中的 items.py 文件：

```
import scrapy

class DmozItem(scrapy.Item):
    title = scrapy.Field()
    link = scrapy.Field()
    desc = scrapy.Field()
```

一开始这看起来可能有点复杂，但是通过定义 item，您可以很方便的使用 Scrapy 的其他方法。而这些方法需要知道您的 item 的定义。

Spider 是用户编写用于从单个网站 (或者一些网站) 爬取数据的类。其包含了一个用于下载的初始 URL，如何跟进网页中的链接以及如何分析页面中的内容，提取生成 item 的方法。为了创建一个 Spider，您必须继承 scrapy.Spider 类，且定义一些属性：1）name：用于区别 Spider。该名字必须是唯一的，您不可以为不同的 Spider 设定相同的名字。2）start_urls：包含了 Spider 在启动时进行爬取的 url 列表。因此，第一个被获取到的页面将是其中之一。后续的 URL 则从初始的 URL 获取到的数据中提取。3）parse () 是 spider 的一个方法。被调用时，每个初始 URL 完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。该方法负责解析返回的数据 (response data)，提取数据 (生成 item) 以及生成需要进一步处理的 URL 的 Request 对象。

以下为我们的第一个 Spider 代码，保存在 tutorial/spiders 目录下的 dmoz_spider.py 文件中：

```
import scrapy

class DmozSpider(scrapy.Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def parse(self, response):
        filename = response.url.split("/")[-2] + '.html'
        with open(filename, 'wb') as f:
            f.write(response.body)
```

爬取。进入项目的根目录，执行下列命令启动 spider：scrapy crawl dmoz。该命令启动了我们刚刚添加的 dmoz spider，向 dmoz.org 发送一些请求。您将会得到类似的输出；注解：最后你可以看到有一行 log 包含定义在 start_urls 的初始 URL，并且与 spider 中是一一对应的。在 log 中可以看到其没有指向其他页面 (referer: None)。现在，查看当前目录，您将会注意到有两个包含 url 所对应的内容的文件被创建了：Book , Resources，正如我们的 parse 方法里做的一样。

1『启动命令里的对象 dmoz，是在类 Spider 里定义的 name 属性。』

Scrapy 为 Spider 的 start_urls 属性中的每个 URL 创建了 scrapy.Request 对象，并将 parse 方法作为回调函数 (callback) 赋值给了 Request。Request 对象经过调度，执行生成 scrapy.http.Response 对象并送回给 spider 的 parse () 方法。

Selectors 选择器简介。从网页中提取数据有很多方法。Scrapy 使用了一种基于 XPath 和 CSS 表达式机制：Scrapy Selectors 。关于 selector 和其他提取机制的信息请参考[Selector 文档](https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/selectors.html#topics-selectors)。

这里给出 XPath 表达式的例子及对应的含义：1）/html/head/title：选择 HTML 文档中 <head> 标签内的 <title> 元素。2）/html/head/title/text ()：选择上面提到的 <title> 元素的文字。3）//td：选择所有的 <td> 元素。4）//div [@class="mine"]：选择所有具有 class="mine" 属性的 div 元素。

上边仅仅是几个简单的 XPath 例子，XPath 实际上要比这远远强大的多。如果您想了解的更多，我们推荐通过这些例子来学习 XPath, 以及这篇教程学习：[A very brief primer to thinking in XPath // plasmasturm.org](http://plasmasturm.org/log/xpath101/)。注解：CSS vs XPath，您可以仅仅使用 CSS Selector 来从网页中提取数据。不过，XPath 提供了更强大的功能。其不仅仅能指明数据所在的路径，还能查看数据。比如，您可以这么进行选择：包含文字 ‘Next Page’ 的链接 。正因为如此，即使您已经了解如何使用 CSS selector，我们仍推荐您使用 XPath。

为了配合 CSS 与 XPath，Scrapy 除了提供了 Selector 之外，还提供了方法来避免每次从 response 中提取数据时生成 selector 的麻烦。Selector 有四个基本的方法（点击相应的方法可以看到详细的 API 文档）：1）xpath ()：传入 xpath 表达式，返回该表达式所对应的所有节点的 selector list 列表。2）css ()：传入 CSS 表达式，返回该表达式所对应的所有节点的 selector list 列表。3）extract ()：序列化该节点为 unicode 字符串并返回 list。4）re ()：根据传入的正则表达式对数据进行提取，返回 unicode 字符串 list 列表。

在 Shell 中尝试 Selector 选择器。为了介绍 Selector 的使用方法，接下来我们将要使用内置的 Scrapy shell 。Scrapy Shell 需要您预装好 IPython（一个扩展的 Python 终端）。您需要进入项目的根目录，执行下列命令来启动 shell：

    scrapy shell "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"

注解：当您在终端运行 Scrapy 时，请一定记得给 url 地址加上引号，否则包含参数的 url (例如 & 字符) 会导致 Scrapy 运行失败。shell 的输出类似。当 shell 载入后，您将得到一个包含 response 数据的本地 response 变量。输入 response.body 将输出 response 的包体，输出 response.headers 可以看到 response 的包头。

\#TODO... 更为重要的是，response 拥有一个 selector 属性，该属性是以该特定 response 初始化的类 Selector 的对象。您可以通过使用 response.selector.xpath () 或 response.selector.css () 来对 response 进行查询。此外，scrapy 也对 response.selector.xpath () 及 response.selector.css () 提供了一些快捷方式，例如 response.xpath () 或 response.css () 。同时，shell 根据 response 提前初始化了变量 sel 。该 selector 根据 response 的类型自动选择最合适的分析规则 (XML vs HTML)。让我们来试试:

```
In [1]: response.xpath('//title')
Out[1]: [<Selector xpath='//title' data=u'<title>Open Directory - Computers: Progr'>]

In [2]: response.xpath('//title').extract()
Out[2]: [u'<title>Open Directory - Computers: Programming: Languages: Python: Books</title>']

In [3]: response.xpath('//title/text()')
Out[3]: [<Selector xpath='//title/text()' data=u'Open Directory - Computers: Programming:'>]

In [4]: response.xpath('//title/text()').extract()
Out[4]: [u'Open Directory - Computers: Programming: Languages: Python: Books']

In [5]: response.xpath('//title/text()').re('(\w+):')
Out[5]: [u'Computers', u'Programming', u'Languages', u'Python']
```









