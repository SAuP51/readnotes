
Nine


			Centuries to Millennia Before





Let’s start with a seeming digression. Parts of chapters 4 and 7 have debunked some supposed sex differences concerning the brain, hormones, and behavior. One difference, however, is persistent. It’s far from issues that concern this book, but bear with me.

			A remarkably consistent finding, starting with elementary school students, is that males are better at math than females. While the difference is minor when it comes to considering average scores, there is a huge difference when it comes to math stars at the upper extreme of the distribution. For example, in 1983, for every girl scoring in the highest percentile on the math SAT, there were eleven boys.

			Why the difference? There have always been suggestions that testosterone is central. During development, testosterone fuels the growth of a brain region involved in mathematical thinking, and giving adults testosterone enhances some math skills. Oh, okay, it’s biological.

			But consider a paper published in Science in 2008.1 The authors examined the relationship between math scores and sexual equality in forty countries (based on economic, educational, and political indices of gender equality; the worst was Turkey, the United States was middling, and, naturally, the Scandinavians were tops). Lo and behold, the more gender equal the country, the less of a discrepancy in math scores. By the time you get to the Scandinavian countries, it’s statistically insignificant. And by the time you examine the most gender-equal country on earth at the time, Iceland, girls are better at math than boys.*



					L. Guiso et al., “Culture, Gender, and Math,” Sci 320 (2008): 1164.

					Visit bit.ly/2o88s4O for a larger version of this graph.



			In other words, while you can never be certain, the Afghan girl pictured on top, on the next page, seated next to her husband, is less likely than the Swedish girl pictured below her to solve the Erdös-Hajnal conjecture in graph theory.

			In other, other words, culture matters. We carry it with us wherever we go. As one example, the level of corruption—a government’s lack of transparency regarding use of power and finances—in UN diplomats’ home countries predicts their likelihood of racking up unpaid parking tickets in Manhattan. Culture leaves long-lasting residues—Shiites and Sunnis slaughter each other over a succession issue fourteen centuries old; across thirty-three countries population density in the year 1500 significantly predicts how authoritarian the government was in 2000; over the course of millennia, earlier adoption of the hoe over the plow predicts gender equality today.2



			And in other, other, other words, when we contemplate our iconic acts—the pulling of a trigger, the touching of an arm—and want to explain why they happened using a biological framework, culture better be on our list of explanatory factors.

			Thus, the goals of this chapter:


Look at systematic patterns of cultural variation as they pertain to the best and worst of our behaviors.

				Explore how different types of brains produce different culture and different types of culture produce different brains. In other words, how culture and biology coevolve.3

				See the role of ecology in shaping culture.





DEFINITIONS, SIMILARITIES, AND DIFFERENCES


			“Culture,” naturally, has been defined various ways. One influential definition comes from Edward Tylor, a distinguished nineteenth-century cultural anthropologist. For him culture is “that complex whole which includes knowledge, belief, art, morals, law, custom, and any other capabilities and habits acquired by man [sic] as a member of society.”4

			This definition, obviously, is oriented toward something that is specific to humans. Jane Goodall blew off everyone’s socks in the 1960s by reporting the now-iconic fact that chimps make tools. Her study subjects modified twigs by stripping off the leaves and pushing them into termite mounds; termites would bite the twig, still holding on when it was pulled out, yielding a snack for the chimps.

			This was just the start. Chimps were subsequently found to use various tools—wood or rock anvils for cracking open nuts, wads of chewed leaves to sponge up hard-to-reach water, and, in a real shocker, sharpened sticks for spearing bush babies.5 Different populations make different tools; new techniques spread across social networks (among chimps who hang with one another); kids learn the ropes by watching their moms; techniques spread from one group to another when someone emigrates; chimp nut-cracking tools in excess of four thousand years old have been excavated. And in my favorite example, floating between tool use and accessorizing, a female in Zambia got it into her head to go around with a strawlike blade of grass in her ear. The action had no obvious function; apparently she just liked having a piece of grass sticking out of her ear. So sue her. She did it for years, and over that time the practice spread throughout her group. A fashionista.

			In the decades since Goodall’s discovery, tool use has been observed in apes and monkeys, elephants, sea otters, mongoose.6 Dolphins use sea sponges to dig up fish burrowed into the sea floor. Birds use tools for nest building or food acquisition—jays and crows, for example, use twigs to fish for insects, much as chimps do. And there’s tool use in cephalopods, reptiles, and fish.



					E. van Leeuwen et al., “A Group-Specific Arbitrary Tradition in Chimpanzees (Pan troglodytes),” Animal Cog 17 (2014): 1421.



			All this is mighty impressive. Nonetheless, such cultural transmission doesn’t show progression—this year’s chimp nut-cracking tool is pretty much the same as that of four thousand years ago. And with few exceptions (more later), nonhuman culture is solely about material culture (versus, say, social organization).

			So the classical definition of culture isn’t specific to humans.7 Most cultural anthropologists weren’t thrilled with Goodall’s revolution—great, next the zoologists will report that Rafiki persuaded Simba to become the Lion King—and now often emphasize definitions of culture that cut chimps and other hoi polloi out of the party. There’s a fondness for the thinking of Alfred Kroeber, Clyde Kluckhohn, and Clifford Geertz, three heavyweight social anthropologists who focused on how culture is about ideas and symbols, rather than the mere behaviors in which they instantiate, or material products like flint blades or iPhones. Contemporary anthropologists like Richard Shweder have emphasized a more affective but still human-centric view of culture as being about moral and visceral versions of right and wrong. And of course these views have been critiqued by postmodernists for reasons I can’t begin to follow.

			Basically, I don’t want to go anywhere near these debates. For our purposes we’ll rely on an intuitive definition of culture that has been emphasized by Frans de Waal: “culture” is how we do and think about things, transmitted by nongenetic means.

			Working with that broad definition, is the most striking thing about the array of human cultures the similarities or the differences? Depends on your taste.

			If the similarities seem most interesting, there are plenty—after all, multiple groups of humans independently invented agriculture, writing, pottery, embalming, astronomy, and coinage. At the extreme of similarities are human universals, and numerous scholars have proposed lists of them. One of the lengthiest and most cited comes from the anthropologist Donald Brown.8 Here’s a partial list of his proposed cultural universals: the existence of and concern with aesthetics, magic, males and females seen as having different natures, baby talk, gods, induction of altered states, marriage, body adornment, murder, prohibition of some type of murder, kinship terms, numbers, cooking, private sex, names, dance, play, distinctions between right and wrong, nepotism, prohibitions on certain types of sex, empathy, reciprocity, rituals, concepts of fairness, myths about afterlife, music, color terms, prohibitions, gossip, binary sex terms, in-group favoritism, language, humor, lying, symbolism, the linguistic concept of “and,” tools, trade, and toilet training. And that’s a partial list.

			For the purposes of this chapter, the staggeringly large cultural differences in how life is experienced, in resources and privileges available, in opportunities and trajectories, are most interesting. Just to start with some breathtaking demographic statistics born of cultural differences: a girl born in Monaco has a ninety-three-year life expectancy; one in Angola, thirty-nine. Latvia has 99.9 percent literacy; Niger, 19 percent. More than 10 percent of children in Afghanistan die in their first year, about 0.2 percent in Iceland. Per-capita GDP is $137,000 in Qatar, $609 in the Central African Republic. A woman in South Sudan is roughly a thousand times more likely to die in childbirth than a woman in Estonia.9

			The experience of violence also varies enormously by culture. Someone in Honduras is 450 times more likely to be murdered than someone in Singapore. 65 percent of women experience intimate-partner violence in Central Africa, 16 percent in East Asia. A South African woman is more than one hundred times more likely to be raped than one in Japan. Be a school kid in Romania, Bulgaria, or Ukraine, and you’re about ten times more likely to be chronically bullied than a kid in Sweden, Iceland, or Denmark (stay tuned for a closer look at this).10

			Of course, there are the well-known gender-related cultural differences. There are the Scandinavian countries approaching total gender equality and Rwanda, with 63 percent of its lower-house parliamentary seats filled by women, compared with Saudi Arabia, where women are not allowed outside the house unless accompanied by a male guardian, and Yemen, Qatar, and Tonga, with 0 percent female legislators (and with the United States running around 20 percent).11

			Then there’s the Philippines, where 93 percent of people say they feel happy and loved, versus 29 percent of Armenians. In economic games, people in Greece and Oman are more likely to spend resources to punish overly generous players than to punish those who are cheaters, whereas among Australians such “antisocial punishment” is nonexistent. And there are wildly different criteria for prosocial behavior. In a study of employees throughout the world working for the same multinational bank, what was the most important reason cited to help someone? Among Americans it was that the person had previously helped them; for Chinese it was that the person was higher ranking; in Spain, that they were a friend or acquaintance.12

			Your life will be unrecognizably different, depending on which culture the stork deposited you into. In wading through this variability, there are some pertinent patterns, contrasts, and dichotomies.





COLLECTIVIST VERSUS INDIVIDUALIST CULTURES


			As introduced in chapter 7, a large percentage of cross-cultural psychology studies compare collectivist with individualist cultures. This almost always means comparisons between subjects from collectivist East Asian cultures and Americans, coming from that mother of all individualist cultures.* As defined, collectivist cultures are about harmony, interdependence, conformity, and having the needs of the group guiding behavior, whereas individualist cultures are about autonomy, personal achievement, uniqueness, and the needs and rights of the individual. Just to be a wee bit caustic, individualist culture can be summarized by that classic American concept of “looking out for number one”; collectivist culture can be summarized by the archetypical experience of American Peace Corps teachers in such countries—pose your students a math question, and no one will volunteer the correct answer because they don’t want to stand out and shame their classmates.

			Individualist/collectivist contrasts are striking. In individualist cultures, people more frequently seek uniqueness and personal accomplishment, use first-person singular pronouns more often, define themselves in terms that are personal (“I’m a contractor”) rather than relational (“I’m a parent”), attribute their successes to intrinsic attributes (“I’m really good at X”) rather than to situational ones (“I was in the right place at the right time”). The past is more likely to be remembered via events (“That’s the summer I learned to swim”) rather than social interactions (“That’s the summer we became friends”). Motivation and satisfaction are gained from self- rather than group-derived effort (reflecting the extent to which American individualism is about noncooperation, rather than nonconformity). Competitive drive is about getting ahead of everyone else. When asked to draw a “sociogram”—a diagram of their social network, with circles representing themselves and their friends, connected by lines—Americans tend to place the circle representing themselves in the middle of the page and make it the largest.13

			In contrast, those from collectivist cultures show more social comprehension; some reports suggest that they are better at Theory of Mind tasks, more accurate in understanding someone else’s perspective—with “perspective” ranging from the other person’s abstract thoughts to how objects appear from where she is sitting. There is more blame of the group when someone violates a norm due to peer pressure, and a greater tendency to give situational explanations for behavior. Competitive drive is about not falling behind everyone else. And when drawing sociograms, the circle representing “yourself” is far from the center, and far from the biggest.

			Naturally, these cultural differences have biological correlates. For example, subjects from individualist cultures strongly activate the (emotional) mPFC when looking at a picture of themselves, compared to looking at a picture of a relative or friend; in contrast, the activation is far less for East Asian subjects.* Another example is a favorite demonstration of mine of cross-cultural differences in psychological stress—when asked in free recall, Americans are more likely than East Asians to remember times in which they influenced someone; conversely, East Asians are more likely to remember times when someone influenced them. Force Americans to talk at length about a time someone influenced them, or force East Asians to detail their influencing someone, and both secrete glucocorticoids from the stressfulness of having to recount this discomfiting event. And work by my Stanford colleagues and friends Jeanne Tsai and Brian Knutson shows that mesolimbic dopamine systems activate in European Americans when looking at excited facial expressions; in Chinese, when looking at calm expressions.

			As we will see in chapter 13, these cultural differences produce different moral systems. In the most traditional of collectivist societies, conformity and morality are virtually synonymous and norm enforcement is more about shame (“What will people think if I did that?”) than guilt (“How could I live with myself?”). Collectivist cultures foster more utilitarian and consequentialist moral stances (for example, a greater willingness for an innocent person to be jailed in order to prevent a riot). The tremendous collectivist emphasis on the group produces a greater degree of in-group bias than among individualist culture members. In one study, for example, Korean and European American subjects observed pictures of either in- or out-group members in pain. All subjects reported more subjective empathy and showed more activation of Theory of Mind brain regions (i.e., the temporoparietal junction) when observing in-group members, but the bias was significantly greater among Korean subjects. In addition, subjects from both individualist and collectivist cultures denigrate out-group members, but only the former inflate assessments of their own group. In other words, East Asians, unlike Americans, don’t have to puff up their own group to view others as inferior.14

			What is fascinating is the direction that some of these differences take, as shown in approaches pioneered by one of the giants in this field, Richard Nisbett of the University of Michigan. Westerners problem-solve in a more linear fashion, with more reliance on linguistic rather than spatial coding. When asked to explain the movement of a ball, East Asians are more likely to invoke relational explanations built around the interactions of the ball with its environment—friction—while Westerners focus on intrinsic properties like weight and density. Westerners are more accurate at estimating length in absolute terms (“How long is that line?”) while East Asians are better with relational estimates (“How much longer is this line than that?”). Or how’s this one: Consider a monkey, a bear, and a banana. Which two go together? Westerners think categorically and choose the monkey and bear—they’re both animals. East Asians think relationally and link the monkey and banana—if you’re thinking of a monkey, also think of food it will need.15

			Remarkably, the cultural differences extend to sensory processing, where Westerners process information in a more focused manner, East Asians in a more holistic one.16 Show a picture of a person standing in the middle of a complex scene; East Asians will be more accurate at remembering the scene, the context, while Westerners remember the person in the middle. Remarkably, this is even observed on the level of eye tracking—typically Westerners’ eyes first look at a picture’s center, while East Asians scan the overall scene. Moreover, force Westerners to focus on the holistic context of a picture, or East Asians on the central subject, and the frontal cortex works harder, activating more.

			As covered in chapter 7, cultural values are first inculcated early in life. So it’s no surprise that culture shapes our attitudes about success, morality, happiness, love, and so on. But what is startling to me is how these cultural differences also shape where your eyes focus on a picture or how you think about monkeys and bananas or the physics of a ball’s trajectory. Culture’s impact is enormous.

			Naturally, there are various caveats concerning collectivist/individualist comparisons:


The most obvious is the perpetual “on the average”—there are plenty of Westerners, for example, who are more collectivist than plenty of East Asians. In general, people who are most individualist by various personality measures are most individualist in neuroimaging studies.17

				Cultures change over time. For example, levels of conformity in East Asian cultures are declining (one study, for example, shows increased rates of babies in Japan receiving unique names). Moreover, one’s degree of inculcation into one’s culture can be altered rapidly. For example, priming someone beforehand with individualist or collectivist cultural cues shifts how holistically he processes a picture. This is especially true for bicultural individuals.18

				We will soon see about some genetic differences between collectivist and individualist populations. There is nothing resembling genetic destiny about this—the best evidence for this conclusion comes from one of the control groups in many of these studies, namely East Asian Americans. In general, it takes about a generation for the descendants of East Asian immigrants to America to be as individualist as European Americans.19

				Obviously, “East Asians” and “Westerners” are not monolithic entities. Just ask someone from Beijing versus the Tibetan steppes. Or stick three people from Berkeley, Brooklyn, and Biloxi in a stalled elevator for a few hours and see what happens. As we will see, there is striking variation within cultures.



			Why should people in one part of the globe have developed collectivist cultures, while others went individualist? The United States is the individualism poster child for at least two reasons. First there’s immigration. Currently, 12 percent of Americans are immigrants, another 12 percent are (like me) children of immigrants, and everyone else except for the 0.9 percent pure Native Americans descend from people who emigrated within the last five hundred years.20 And who were the immigrants? Those in the settled world who were cranks, malcontents, restless, heretical, black sheep, hyperactive, hypomanic, misanthropic, itchy, unconventional, yearning to be free, yearning to be rich, yearning to be out of their damn boring repressive little hamlet, yearning. Couple that with the second reason—for the majority of its colonial and independent history, America has had a moving frontier luring those whose extreme prickly optimism made merely booking passage to the New World insufficiently novel—and you’ve got America the individualistic.

			Why has East Asia provided textbook examples of collectivism?21 The key is how culture is shaped by the way people traditionally made a living, which in turn is shaped by ecology. And in East Asia it’s all about rice. Rice, which was domesticated there roughly ten thousand years ago, requires massive amounts of communal work. Not just backbreaking planting and harvesting, which are done in rotation because the entire village is needed to harvest each family’s rice.* Collective labor is first needed to transform the ecosystem—terracing mountains and building and maintaining irrigation systems for controlled flooding of paddies. And there’s the issue of dividing up water fairly—in Bali, religious authority regulates water access, symbolized by iconic water temples. How’s this for amazing—the Dujiuangyan irrigation system irrigates more than five thousand square kilometers of rice farms near Changdu, China, and it is more than two thousand years old. The roots of collectivism, like those of rice, run deep in East Asia.*



			A fascinating 2014 Science paper strengthens the rice/collectivism connection by exploring an exception.22 In parts of northern China it’s difficult to grow rice, and instead people have grown wheat for millennia; this involves individual rather than collective farming. And by the standard tests of individualist versus collectivist cultures (e.g., draw a sociogram, which two are most similar of a rabbit, dog, and carrot?)—they look like Westerners. The region has two other markers of individualism, namely higher rates of divorce and of inventiveness—patent filings—than in rice-growing regions. The roots of individualism, likes those of wheat, run deep in northern China.

			The links between ecology, mode of production, and culture are shown in a rare collectivist/individualist study not comparing Asians and Westerners.23 The authors studied a Turkish region on the Black Sea, where mountains hug the coastline. There, in close proximity, people live by fishing, by farming the narrow ribbon of land between the sea and the mountains, or as mountain shepherds. All three groups had the same language, religion, and gene stock.

			Herding is solitary; while Turkish farmers and fishermen (and women) were no Chinese rice farmers, they at least worked their fields in groups and manned their fishing boats in crews. Herders thought less holistically than farmers or fishermen—the former were better at judging absolute length of lines, the other two at relative judgments; when shown a glove, a scarf, and a hand, herders grouped gloves and scarves categorically, while the others grouped relationally, pairing gloves and hands. In the authors’ words, “social interdependence fosters holistic thinking.”

			This theme appears in another study, comparing Jewish boys from either observant Orthodox homes (dominated by endless shared rules about beliefs and behaviors) with ones from far more individualist secular homes. Visual processing was more holistic in the Orthodox, more focused in the secular.24

			The East Asian/Western collectivist/individualist dichotomy has a fascinating genetic correlate.25 Recall from the last chapter dopamine and DRD4, the gene for the D4 receptor. It’s extraordinarily variable, with at least twenty-five human variants (with lesser variability in other primates). Moreover, the variation isn’t random, inconsequential drift of DNA sequences; instead there has been strong positive selection for variants. Most common is the 4R variant, occurring in about half of East Asians and European Americans. There’s also the 7R variant, producing a receptor less responsive to dopamine in the cortex, associated with novelty seeking, extroversion, and impulsivity. It predates modern humans but became dramatically more common ten to twenty thousand years ago. The 7R variant occurs in about 23 percent of Europeans and European Americans. And in East Asians? 1 percent.



					Y. Ding et al., “Evidence of Positive Selection Acting at the Human Dopamine Receptor D4 Gene Locus,” PNAS 99 (2002): 309.

					Visit bit.ly/2nsuHz9 for a larger version of this graph.



			So which came first, 7R frequency or cultural style? The 4R and 7R variants, along with the 2R, occur worldwide, implying they already existed when humans radiated out of Africa 60,000 to 130,000 years ago. Classic work by Kenneth Kidd of Yale, examining the distribution of 7R, shows something remarkable.

			Starting at the left of the figure above, there’s roughly a 10 to 25 percent incidence of 7R in various African, European, and Middle Eastern populations. Jumping to the right side of the figure, there’s a slightly higher incidence among the descendants of those who started island-hopping from mainland Asia to Malaysia and New Guinea. The same for folks whose ancestors migrated to North America via the Bering land bridge about fifteen thousand years ago—the Muskoke, Cheyenne, and Pima tribes of Native Americans. Then the Maya in Central America—up to around 40 percent. Then the Guihiba and Quechua of the northern parts of South America, at around 55 percent. Finally there are the descendants of folks who made it all the way to the Amazon basin—the Ticuna, Surui, and Karitiana—with a roughly 70 percent incidence of 7R, the highest in the world. In other words, the descendants of people who, having made it to the future downtown Anchorage, decided to just keep going for another six thousand miles.* A high incidence of 7R, associated with impulsivity and novelty seeking, is the legacy of humans who made the greatest migrations in human history.

			And then in the middle of the chart is the near-zero incidence of 7R in China, Cambodia, Japan, and Taiwan (among the Ami and Atayal). When East Asians domesticated rice and invented collectivist society, there was massive selection against the 7R variant; in Kidd’s words, it was “nearly lost” in these populations.* Maybe the bearers of 7R broke their necks inventing hang gliding or got antsy and tried to walk to Alaska but drowned because there was no longer a Bering land bridge. Maybe they were less desirable mates. Regardless of the cause, East Asian cultural collectivism coevolved with selection against the 7R variant.*

			Thus, in this most studied of cultural contrasts, we see clustering of ecological factors, modes of production, cultural differences, and differences in endocrinology, neurobiology, and gene frequencies.* The cultural contrasts appear in likely ways—e.g., morality, empathy, child-rearing practices, competition, cooperation, definitions of happiness—but also in unexpected ones—e.g., where, within milliseconds, your eyes look at a picture, or you’re thinking about bunnies and carrots.





PASTORALISTS AND SOUTHERNERS


			Another important link among ecology, mode of production, and culture is seen in dry, hardscrabble, wide-open environments too tough for agriculture. This is the world of nomadic pastoralism—people wandering the desert, steppes, or tundra with their herds.

			There are Bedouins in Arabia, Tuareg in North Africa, Somalis and Maasai in East Africa, Sami of northern Scandinavia, Gujjars in India, Yörük in Turkey, Tuvans of Mongolia, Aymara in the Andes. There are herds of sheep, goats, cows, llamas, camels, yaks, horses, or reindeer, with the pastoralists living off their animals’ meat, milk, and blood and trading their wool and hides.

			Anthropologists have long noted similarities in pastoralist cultures born of their tough environments and the typically minimal impact of centralized government and the rule of law. In that isolated toughness stands a central fact of pastoralism: thieves can’t steal the crops on someone’s farm or the hundreds of edible plants eaten by hunter-gatherers, but they can steal someone’s herd. This is the vulnerability of pastoralism, a world of rustlers and raiders.

			This generates various correlates of pastoralism:26


Militarism abounds. Pastoralists, particularly in deserts, with their far-flung members tending the herds, are a spawning ground for warrior classes. And with them typically come (a) military trophies as stepping-stones to societal status; (b) death in battle as a guarantee of a glorious afterlife; (c) high rates of economic polygamy and mistreatment of women; and (d) authoritarian parenting. It is rare for pastoralists to be pastoral, in the sense of Beethoven’s Sixth Symphony.


Worldwide, monotheism is relatively rare; to the extent that it does occur, it is disproportionately likely among desert pastoralists (while rain forest dwellers are atypically likely to be polytheistic). This makes sense. Deserts teach tough, singular things, a world reduced to simple, desiccated, furnace-blasted basics that are approached with a deep fatalism. “I am the Lord your God” and “There is but one god and his name is Allah” and “There will be no gods before me”—dictates like these proliferate. As implied in the final quote, desert monotheism does not always come with only one supernatural being—monotheistic religions are replete with angels and djinns and devils. But they sure come with a hierarchy, minor deities paling before the Omnipotent One, who tends to be highly interventionist both in the heavens and on earth. In contrast, think of tropical rain forest, teeming with life, where you can find more species of ants on a single tree than in all of Britain. Letting a hundred deities bloom in equilibrium must seem the most natural thing in the world.


Pastoralism fosters cultures of honor. As introduced in chapter 7, these are about rules of civility, courtesy, and hospitality, especially to the weary traveler because, after all, aren’t all herders often weary travelers? Even more so, cultures of honor are about taking retribution after affronts to self, family, or clan, and reputational consequences for failing to do so. If they take your camel today and you do nothing, tomorrow they will take the rest of your herd, plus your wives and daughters.*

			Few of humanity’s low or high points are due to the culturally based actions of, say, Sami wandering the north of Finland with their reindeer, or Maasai cow herders in the Serengeti. Instead the most pertinent cultures of honor are ones in more Westernized settings. “Culture of honor” has been used to describe the workings of the Mafia in Sicily, the patterns of violence in rural nineteenth-century Ireland, and the causes and consequences of retributive killings by inner-city gangs. All occur in circumstances of resource competition (including the singular resource of being the last side to do a retributive killing in a vendetta), of a power vacuum provided by the minimal presence of the rule of law, and where prestige is ruinously lost if challenges are left unanswered and where the answer is typically a violent one. Amid those, the most famous example of a Westernized culture of honor is the American South, the subject of books, academic journals, conferences, and Southern studies majors in universities. Much of this work was pioneered by Nisbett.27

			Hospitality, chivalry toward women, and emphasis on social decorum and etiquette are long associated with the South.28 In addition, the South traditionally emphasizes legacy, long cultural memory, and continuity of family—in rural Kentucky in the 1940s, for example, 70 percent of men had the same first name as their father, far more than in the North. When coupled with lesser mobility in the South, honor in need of defense readily extends to family, clan, and place. For example, by the time the Hatfields and McCoys famously began their nearly thirty-year feud in 1863,* they had lived in the same region of the West Virginia/Kentucky border for nearly a century. The Southern sense of honor in place is also seen in Robert E. Lee; he opposed Southern secession, even made some ambiguous statements that could be viewed as opposed to slavery. Yet when offered the command of the Union Army by Lincoln, Lee wrote, “I wish to live under no other government and there is no sacrifice I am not ready to make for the preservation of the Union save that of honor.” When Virginia chose secession, he regretfully fulfilled his sense of honor to his home and led the Confederate Army of Northern Virginia.

			In the South, defense of honor was, above all, an act of self-reliance.29 The Southerner Andrew Jackson was advised by his dying mother to never seek redress from the law for any grievances, to instead be a man and take things into his own hands. That he certainly did, with a history of dueling (even fatally) and brawling; on his final day as president, he articulated two regrets in leaving office—that he “had been unable to shoot Henry Clay or to hang John C. Calhoun.” Carrying out justice personally was viewed as a requirement in the absence of a functional legal system. At best, legal justice and individual justice were in uneasy equilibrium in the nineteenth-century South; in the words of the Southern historian Bertram Wyatt-Brown, “Common law and lynch law were ethically compatible. The first enabled the legal profession to present traditional order, and the second conferred upon ordinary men the prerogative of ensuring that community values held ultimate sovereignty.”

			The core of retribution for honor violations was, of course, violence. Sticks and stones might break your bones, but names will cause you to break the offender’s bones. Dueling was commonplace, the point being not that you were willing to kill but that you were willing to die for your honor. Many a Confederate boy went off to war advised by his mother that better he come back in a coffin than as a coward who fled.

			The result of this all is a long, still-extant history of high rates of violence in the South. But crucially, violence of a particular type. I once heard it summarized by a Southern studies scholar describing the weirdness of leaving the rural South to start grad school in a strange place, Cambridge, Massachusetts, where families would get together at Fourth of July picnics and no one would shoot each other. Nisbett and Dov Cohen have shown that the high rates of violence, particularly of murder, by white Southern males are not features of large cities or about attempts to gain material goods—we’re not talking liquor store stickups. Instead the violence is disproportionately rural, among people who know each other, and concerns slights to honor (that sleazebag cousin thought it was okay to flirt with your wife at the family reunion, so you shot him). Moreover, Southern juries are atypically forgiving of such acts.30



					R. Nisbett and D. Cohen, Culture of Honor: The Psychology of Violence in the South (Boulder, CO: Westview Press, 1996).

					Visit bit.ly/2neHKTg for a larger version of this graph.





					Southern, but not Northern, college studies show strong physiological responses to a social provocation.

					Visit bit.ly/2mNCQ4g for a larger version of this graph.



			Southern violence was explored in one of the all-time coolest psychology studies, involving the use of a word rare in science journals, conducted by Nisbett and Cohen. Undergraduate male subjects had a blood sample taken. They then filled out a questionnaire about something and were then supposed to drop it off down the hall. It was in the narrow hallway, filled with file cabinets, that the experiment happened. Half the subjects traversed the corridor uneventfully. But with half, a confederate (get it? ha-ha) of the psychologists, a big beefy guy, approached from the opposite direction. As the subject and the plant squeezed by each other, the latter would jostle the subject and, in an irritated voice, say the magic word—“asshole”—and march on. Subject would continue down the hall to drop off the questionnaire.

			What was the response to this insult? It depended. Subjects from the South, but not from elsewhere, showed massive increases in levels of testosterone and glucocorticoids—anger, rage, stress. Subjects were then told a scenario where a guy observes a male acquaintance making a pass at his fiancée—what happens next in the story? In control subjects, Southerners were a bit more likely than Northerners to imagine a violent outcome. And after being insulted? No change in Northerners and a massive boost in imagined violence among Southerners.

			Where do these Westernized cultures of honor come from? Violence between the Crips and the Bloods in LA is not readily traced to combatants’ mind-sets from growing up herding yak. Nonetheless, pastoralist roots have been suggested to explain the Southern culture of honor. The theory as first propounded by historian David Hackett Fischer in 1989: Early American regionalism arose from colonists in different parts of America coming from different places.31 There were the Pilgrims from East Anglia in New England. Quakers from North Midlands going to Pennsylvania and Delaware. Southern English indentured servants to Virginia. And the rest of the South? Disproportionately herders from Scotland, Ireland, and northern England.

			Naturally, the idea has some problems. Pastoralists from the British Isles mostly settled in the hill country of the South, whereas the honor culture is stronger in the Southern lowlands. Others have suggested that the Southern ethos of retributive violence was born from the white Southern nightmare scenario of slave uprisings. But most historians have found a lot of validity in Fischer’s idea.





Violence Turned Inward


			Culture-of-honor violence is not just about outside threat—the camel rustlers from the next tribe, the jerk at the roadhouse who came on to some guy’s girlfriend. Instead it is equally defined by its role when honor is threatened from within. Chapter 11 examines when norm violations by members of your own group provoke cover-ups, excuses, or leniency, and when they provoke severe public punishment. The latter is when “you’ve dishonored us in front of everyone,” a culture-of-honor specialty. Which raises the issue of honor killings.

			What constitutes an honor killing? Someone does something considered to tarnish the reputation of the family. A family member then kills the despoiler, often publicly, thereby regaining face. Mind-boggling.

			Some characteristics of honor killings:


While they have been widespread historically, contemporary ones are mostly restricted to traditional Muslim, Hindu, and Sikh communities.

				Victims are usually young women.

				Their most common crimes? Refusing an arranged marriage. Seeking to divorce an abusive spouse and/or a spouse to whom they were forcibly married as a child. Seeking education. Resisting constraining religious orthodoxy, such as covering their head. Marrying, living with, dating, interacting with, or speaking to an unapproved male. Infidelity. Religious conversion. In other words, a woman resisting being the property of her male relatives. And also, stunningly, staggeringly, a frequent cause of honor killings is being raped.

				In the rare instances of men being subject to honor killings, the typical cause is homosexuality.



			There has been debate as to whether honor killings are “just” domestic violence, and whether morbid Western fascination with them reflects anti-Muslim bias;32 if some Baptist guy in Alabama murders his wife because she wants a divorce, no one frames it as a “Christian honor killing” reflecting deep religious barbarity. But honor killings typically differ from garden-variety domestic violence in several ways: (a) The latter is usually committed by a male partner; the former are usually committed by male blood relatives, often with the approval of and facilitation by female relatives. (b) The former is rarely an act of spontaneous passion but instead is often planned with the approval of family members. (c) Honor killings are often rationalized on religious grounds, presented without remorse, and approved by religious leaders. (d) Honor killings are carried out openly—after all, how else can “honor” be regained for the family?—and the chosen perpetrator is often an underage relative (e.g., a younger brother), to minimize the extent of sentencing for the act.

			By some pretty meaningful criteria, this is not “just” domestic violence. According to estimates by the UN and advocacy groups, five to twenty thousand honor killings occur annually. And they are not restricted to far-off, alien lands. Instead they occur throughout the West, where patriarchs expect their daughters to be untouched by the world they moved them to, where a daughter’s successful assimilation into this world proclaims the irrelevance of that patriarch.



					Left to right, starting top column: Shafilea Ahmed, England, killed by father and mother after resisting an arranged marriage; age 17. Anooshe Sediq Ghulam, Norway, married at 13; killed by husband after requesting a divorce; age 22. Palestina Isa, USA, killed by parents for dating someone outside the faith, listening to American music, and secretly getting a part-time job; age 16. Aqsa Parvez, Canada, killed by father and brother for refusing to wear a hajib; age 16. Ghazala Khan, Denmark, killed by nine family members for refusing an arranged marriage; age 19. Fadime Sahindal, Sweden, killed by father for refusing an arranged marriage; age 27. Hatun Surucu Kird, Germany, killed by brother after divorcing the cousin she was forced to marry at age 16; age 23. Hina Salem, Italy, killed by father for refusing an arranged marriage; age 20. Amina and Sarah Said, USA, both sisters killed by parents who perceived them as becoming too Westernized; ages 18 and 17.





STRATIFIED VERSUS EGALITARIAN CULTURES


			Another meaningful way to think about cross-cultural variation concerns how unequally resources (e.g., land, food, material goods, power, or prestige) are distributed.33 Hunter-gatherer societies have typically been egalitarian, as we’ll soon see, throughout hominin history. Inequality emerged when “stuff”—things to possess and accumulate—was invented following animal domestication and the development of agriculture. The more stuff, reflecting surplus, job specialization, and technological sophistication, the greater the potential inequality. Moreover, inequality expands enormously when cultures invent inheritance within families. Once invented, inequality became pervasive. Among traditional pastoralist or small-scale agricultural societies, levels of wealth inequality match or exceed those in the most unequal industrialized societies.

			Why have stratified cultures dominated the planet, generally replacing more egalitarian ones? For population biologist Peter Turchin, the answer is that stratified cultures are ideally suited to being conquerors—they come with chains of command.34 Both empirical and theoretical work suggests that in addition, in unstable environments stratified societies are “better able to survive resource shortages [than egalitarian cultures] by sequestering mortality in the lower classes.” In other words, when times are tough, the unequal access to wealth becomes the unequal distribution of misery and death. Notably, though, stratification is not the only solution to such instability—this is where hunter-gatherers benefit from being able to pick up and move.

			A score of millennia after the invention of inequality, Westernized societies at the extremes of the inequality continuum differ strikingly.

			One difference concerns “social capital.” Economic capital is the collective quantity of goods, services, and financial resources. Social capital is the collective quantity of resources such as trust, reciprocity, and cooperation. You learn a ton about a community’s social capital with two simple questions. First: “Can people usually be trusted?” A community in which most people answer yes is one with fewer locks, with people watching out for one another’s kids and intervening in situations where one could easily look away. The second question is how many organizations someone participates in—from the purely recreational (e.g., a bowling league) to the vital (e.g., unions, tenant groups, co-op banks). A community with high levels of such participation is one where people feel efficacious, where institutions work transparently enough that people believe they can effect change. People who feel helpless don’t join organizations.

			Put simply, cultures with more income inequality have less social capital.35 Trust requires reciprocity, and reciprocity requires equality, whereas hierarchy is about domination and asymmetry. Moreover, a culture highly unequal in material resources is almost always also unequal in the ability to pull the strings of power, to have efficacy, to be visible. (For example, as income inequality grows, the percentage of people who bother voting generally declines.) Almost by definition, you can’t have a society with both dramatic income inequality and plentiful social capital. Or translated from social science–ese, marked inequality makes people crummier to one another.

			This can be shown in various ways, studied on the levels of Westernized countries, states, provinces, cities, and towns. The more income inequality, the less likely people are to help someone (in an experimental setting) and the less generous and cooperative they are in economic games. Early in the chapter, I discussed cross-cultural rates of bullying and of “antisocial punishment,” where people in economic games punish overly generous players more than they punish cheaters.* Studies of these phenomena show that high levels of inequality and/or low levels of social capital in a country predict high rates of bullying and of antisocial punishment.36

			Chapter 11 examines the psychology with which we think about people of different socioeconomic status; no surprise, in unequal societies, people on top generate justifications for their status.37 And the more inequality, the more the powerful adhere to myths about the hidden blessings of subordination—“They may be poor, but at least they’re happy/honest/loved.” In the words of the authors of one paper, “Unequal societies may need ambivalence for system stability: Income inequality compensates groups with partially positive social images.”

			Thus unequal cultures make people less kind. Inequality also makes people less healthy. This helps explain a hugely important phenomenon in public health, namely the “socioeconomic status (SES)/health gradient”—as noted, in culture after culture, the poorer you are, the worse your health, the higher the incidence and impact of numerous diseases, and the shorter your life expectancy.38

			Extensive research has examined the SES/health gradient. Four quick rule-outs: (a) The gradient isn’t due to poor health driving down people’s SES. Instead low SES, beginning in childhood, predicts subsequent poor health in adulthood. (b) It’s not that the poor have lousy health and everyone else is equally healthy. Instead, for every step down the SES ladder, starting from the top, average health worsens. (c) The gradient isn’t due to less health-care access for the poor; it occurs in countries with universal health care, is unrelated to utilization of health-care systems, and occurs for diseases unrelated to health-care access (e.g., juvenile diabetes, where having five checkups a day wouldn’t change its incidence). (d) Only about a third of the gradient is explained by lower SES equaling more health risk factors (e.g., lead in your water, nearby toxic waste dump, more smoking and drinking) and fewer protective factors (e.g., everything from better mattresses for overworked backs to health club memberships).

			What then is the principal cause of the gradient? Key work by Nancy Adler at UCSF showed that it’s not so much being poor that predicts poor health. It’s feeling poor—someone’s subjective SES (e.g., the answer to “How do you feel you’re doing financially when you compare yourself with other people?”) is at least as good a predictor of health as is objective SES.

			Crucial work by the social epidemiologist Richard Wilkinson of the University of Nottingham added to this picture: it’s not so much that poverty predicts poor health; it’s poverty amid plenty—income inequality. The surest way to make someone feel poor is to rub their nose in what they don’t have.

			Why should high degrees of income inequality (independent of absolute levels of poverty) make the poor unhealthy? Two overlapping pathways:


A psychosocial explanation has been championed by Ichiro Kawachi of Harvard. When social capital decreases (thanks to inequality), up goes psychological stress. A mammoth amount of literature explores how such stress—lack of control, predictability, outlets for frustration, and social support—chronically activates the stress response, which, as we saw in chapter 4, corrodes health in numerous ways.


A neomaterialist explanation has been offered by Robert Evans of the University of British Columbia and George Kaplan of the University of Michigan. If you want to improve health and quality of life for the average person in a society, you spend money on public goods—better public transit, safer streets, cleaner water, better public schools, universal health care. But the more income inequality, the greater the financial distance between the wealthy and the average and thus the less direct benefit the wealthy feel from improving public goods. Instead they benefit more from dodging taxes and spending on their private good—a chauffeur, a gated community, bottled water, private schools, private health insurance. As Evans writes, “The more unequal are incomes in a society, the more pronounced will be the disadvantages to its better-off members from public expenditure, and the more resources will those members have [available to them] to mount effective political opposition” (e.g., lobbying). Evans notes how this “secession of the wealthy” promotes “private affluence and public squalor.” Meaning worse health for the have-nots.39

			The inequality/health link paves the way for understanding how inequality also makes for more crime and violence. I could copy and paste the previous stretch of writing, replacing “poor health” with “high crime,” and I’d be set. Poverty is not a predictor of crime as much as poverty amid plenty is. For example, extent of income inequality is a major predictor of rates of violent crime across American states and across industrialized nations.40

			Why does income inequality lead to more crime? Again, there’s the psychosocial angle—inequality means less social capital, less trust, cooperation, and people watching out for one another. And there’s the neomaterialist angle—inequality means more secession of the wealthy from contributing to the public good. Kaplan has shown, for example, that states with more income inequality spend proportionately less money on that key crime-fighting tool, education. As with inequality and health, the psychosocial and neomaterial routes synergize.

			A final depressing point about inequality and violence. As we’ve seen, a rat being shocked activates a stress response. But a rat being shocked who can then bite the hell out of another rat has less of a stress response. Likewise with baboons—if you are low ranking, a reliable way to reduce glucocorticoid secretion is to displace aggression onto those even lower in the pecking order. It’s something similar here—despite the conservative nightmare of class warfare, of the poor rising up to slaughter the wealthy, when inequality fuels violence, it is mostly the poor preying on the poor.

			This point is made with a great metaphor for the consequences of societal inequality.41 The frequency of “air rage”—a passenger majorly, disruptively, dangerously losing it over something on a flight—has been increasing. Turns out there’s a substantial predictor of it: if the plane has a first-class section, there’s almost a fourfold increase in the odds of a coach passenger having air rage. Force coach passengers to walk through first class when boarding, and you more than double the chances further. Nothing like starting a flight by being reminded of where you fit into the class hierarchy. And completing the parallel with violent crime, when air rage is boosted in coach by reminders of inequality, the result is not a crazed coach passenger sprinting into first class to shout Marxist slogans. It’s the guy being awful to the old woman sitting next to him, or to the flight attendant.*





POPULATION SIZE, POPULATION DENSITY, POPULATION HETEROGENEITY


			The year 2008 marked a human milestone, a transition point nine thousand years in the making: for the first time, the majority of humans lived in cities.

			The human trajectory from semipermanent settlements to the megalopolis has been beneficial. In the developed world, when compared with rural populations, city dwellers are typically healthier and wealthier; larger social networks facilitate innovation; because of economies of scale, cities leave a smaller per-capita ecological footprint.42

			Urban living makes for a different sort of brain. This was shown in a 2011 study of subjects from a range of cities, towns, and rural settings who underwent an experimental social stressor while being brain-scanned. The key finding was that the larger the population where someone lived, the more reactive their amygdala was during that stressor.*43

			Most important for our purposes, urbanized humans do something completely unprecedented among primates—regularly encountering strangers who are never seen again, fostering the invention of the anonymous act. After all, it wasn’t until nineteenth-century urbanization that crime fiction was invented, typically set in cities—in traditional settings there’s no whodunit, since everyone knows what everyone dun.

			Growing cultures had to invent mechanisms for norm enforcement among strangers. For example, across numerous traditional cultures, the larger the group, the greater the punishment for norm violations and the more cultural emphases on equitable treatment of strangers. Moreover, larger groups evolved “third-party punishment” (stay tuned for more in the next chapter)—rather than victims punishing norm violators, punishment is meted out by objective third parties, such as police and courts. At an extreme, a crime not only victimizes its victim but also is an affront to the collective population—hence “The People Versus Joe Blow.”*44

			Finally, life in larger populations fosters the ultimate third-party punisher. As documented by Ara Norenzayan of the University of British Columbia, it is only when societies grow large enough that people regularly encounter strangers that “Big Gods” emerge—deities who are concerned with human morality and punish our transgressions.45 Societies with frequent anonymous interactions tend to outsource punishment to gods.* In contrast, hunter-gatherers’ gods are less likely than chance to care whether we’ve been naughty or nice. Moreover, in further work across a range of traditional cultures, Norenzayan has shown that the more informed and punitive people consider their moralistic gods to be, the more generous they are to coreligionist strangers in a financial allocation game.

			Separate from the size of a population, how about its density? One study surveying thirty-three developed countries characterized each nation’s “tightness”—the extent to which the government is autocratic, dissent suppressed, behavior monitored, transgressions punished, life regulated by religious orthodoxy, citizens viewing various behaviors as inappropriate (e.g., singing in an elevator, cursing at a job interview).46 Higher population density predicted tighter cultures—both high density in the present and, remarkably, historically, in the year 1500.

			The issue of population density’s effects on behavior gave rise to a well-known phenomenon, mostly well known incorrectly.

			In the 1950s John Calhoun at the National Institute of Mental Health asked what happens to rat behavior at higher population densities, research prompted by America’s ever-growing cities.47 And in papers for both scientists and the lay public, Calhoun gave a clear answer: high-density living produced “deviant” behavior and “social pathology.” Rats became violent; adults killed and cannibalized one another; females were aggressive to their infants; there was indiscriminate hypersexuality among males (e.g., trying to mate with females who weren’t in estrus).

			The writing about the subject, starting with Calhoun, was colorful. The bloodless description of “high-density living” was replaced with “crowding.” Aggressive males were described as “going berserk,” aggressive females as “Amazons.” Rats living in these “rat slums” became “social dropouts,” “autistic,” or “juvenile delinquents.” One expert on rat behavior, A. S. Parkes, described Calhoun’s rats as “unmaternal mothers, homosexuals and zombies” (quite the trio you’d invite to dinner in the 1950s).48

			The work was hugely influential, taught to psychologists, architects, and urban planners; more than a million reprints were requested of Calhoun’s original Scientific American report; sociologists, journalists, and politicians explicitly compared residents of particular housing projects and Calhoun’s rats. The take-home message sent ripples through the American heartland destined for the chaotic sixties: inner cities breed violence, pathology, and social deviance.

			Calhoun’s rats were more complicated than this (something underemphasized in his lay writing). High-density living doesn’t make rats more aggressive. Instead it makes aggressive rats more aggressive. (This echoes the findings that neither testosterone, nor alcohol, nor media violence uniformly increases violence. Instead they make violent individuals more sensitive to violence-evoking social cues.) In contrast, crowding makes unaggressive individuals more timid. In other words, it exaggerates preexisting social tendencies.

			Calhoun’s erroneous conclusions about rats don’t even hold for humans. In some cities—Chicago, for example, circa 1970—higher population density in neighborhoods does indeed predict more violence. Nevertheless, some of the highest-density places on earth—Hong Kong, Singapore, and Tokyo—have miniscule rates of violence. High-density living is not synonymous with aggression in rats or humans.

			—

			The preceding sections examined the effects of living with lots of people, and in close quarters. How about the effects of living with different kinds of people? Diversity. Heterogeneity. Admixture. Mosaicism.

			Two opposite narratives come to mind:


Mister Rogers’ neighborhood: When people of differing ethnicities, races, or religions live together, they experience the similarities rather than the differences and view one another as individuals, transcending stereotypes. Trade flows, fostering fairness and mutuality. Inevitably, dichotomies dissolve with intermarriage, and soon you’re happily watching your grandkid in the school play on “their” side of town. Just visualize whirled peas.


Sharks versus the Jets: Differing sorts of people living in close proximity rub, and thus abrade, elbows regularly. One side’s act of proud cultural identification feels like a hostile dig to the other side, public spaces become proving grounds for turf battles, commons become tragedies.

			Surprise: both outcomes occur; the final chapter explores circumstances where intergroup contact leads to one rather than the other. Most interesting at this juncture is the importance of the spatial qualities of the heterogeneity. Consider a region filled with people from Elbonia and Kerplakistan, two hostile groups, each providing half the population. At one extreme, the land is split down the middle, each group occupying one side, producing a single boundary between the two. At the other extreme is a microcheckerboard of alternating ethnicities, where each square on the grid is one person large, producing a vast quantity of boundaries between Elbonians and Kerplakis.

			Intuitively, both scenarios should bias against conflict. In the condition of maximal separation, each group has a critical mass to be locally sovereign, and the total length of border, and thus of the amount of intergroup elbow rubbing, is minimized. In the scenario of maximal mixing, no patch of ethnic homogeneity is big enough to foster a self-identity that can dominate a public space—big deal if someone raises a flag between their feet and declares their square meter to be the Elbonian Empire or a Kerplakistani Republic.

			But in the real world things are always in between the two extremes, and with variation in the average size of each “ethnic patch.” Does patch size, and thus amount of border, influence relations?

			This was explored in a fascinating paper from the aptly named New England Complex Systems Institute, down the block from MIT.49 The authors first constructed an Elbonian/Kerplaki mixture, with individuals randomly distributed as pixels on a grid. Pixels were given a certain degree of mobility plus a tendency to assort with other pixels of the same type. As self-assortment progresses, something emerges—islands and peninsulas of Elbonians amid a sea of Kerplakis, or the reverse, a condition that intuitively seems rife with potential intergroup violence. As self-assortment continues, the number of such isolated islands and peninsulas declines. The intermediate stage that maximizes the number of islands and peninsulas maximizes the number of people living within a surrounded enclave.*

			The authors then considered a balkanized region, namely the Balkans, ex-Yugoslavia, in 1990. This was just before Serbians, Bosnians, Croatians, and Albanians commenced Europe’s worst war since World War II, the war that taught us the names of places like Srebrenica and people like Slobodan Milošević. Using a similar analysis, with ethnic island size varying from roughly twenty to sixty kilometers in diameter, they identified the spots theoretically most rife for violence; remarkably, this predicted the sites of major fighting and massacres in the war.

			In the words of the authors, violence can arise “due to the structure of boundaries between groups rather than as a result of inherent conflicts between the groups themselves.” They then showed that the clarity of borders matters as well. Good, clear-cut fences—e.g., mountain ranges or rivers between groups—make for good neighbors. “Peace does not depend on integrated coexistence, but rather on well defined topographical and political boundaries separating groups, allowing for partial autonomy within a single country,” the authors concluded.

			Thus, not only do size, density, and heterogeneity of populations help explain intergroup violence, but patterns and clarity of fragmentation do as well. These issues will be revisited in the final chapter.





THE RESIDUES OF CULTURAL CRISES


			In times of crisis—the London Blitz, New York after 9/11, San Francisco after the 1989 Loma Prieta earthquake—people pull together.* That’s cool. But in contrast, chronic, pervasive, corrosive menace doesn’t necessarily do the same to people or cultures.

			The primal menace of hunger has left historical marks. Back to that study of differences between countries’ tightness (where “tight” countries were characterized by autocracy, suppression of dissent, and omnipresence and enforcement of behavior norms).50 What sorts of countries are tighter?* In addition to the high population-density correlates mentioned earlier, there are also more historical food shortages, lower food intake, and lower levels of protein and fat in the diet. In other words, these are cultures chronically menaced by empty stomachs.

			Cultural tightness was also predicted by environmental degradation—less available farmland or clean water, more pollution. Similarly, habitat degradation and depletion of animal populations worsens conflict in cultures dependent on bush meat. And a major theme of Jared Diamond’s magisterial Collapse: How Societies Choose to Fail or Succeed is how environmental degradation explains the violent collapse of many civilizations.

			Then there’s disease. In chapter 15 we’ll touch on “behavioral immunity,” the ability of numerous species to detect cues of illness in other individuals; as we’ll see, implicit cues about infectious disease make people more xenophobic. Similarly, historical prevalence of infectious disease predicts a culture’s openness to outsiders. Moreover, other predictors of cultural tightness include having high historical incidence of pandemics, of high infant and child mortality rates, and of higher cumulative average number of years lost to communicable disease.

			Obviously, weather influences the incidence of organized violence—consider the centuries of European wars taking a hiatus during the worst of winter and the growing season.51 Even broader is the capacity of weather and climate to shape culture. The Kenyan historian Ali Mazrui has suggested that one reason for Europe’s historical success, relative to Africa, has been the weather—Western-style planning ahead arose from the annual reality of winter coming.* Larger-scale changes in weather are known to be consequential. In the tightness study, cultural tightness was also predicted by a history of floods, droughts, and cyclones. Another pertinent aspect of weather concerns the Southern Oscillation, known as El Niño, the multiyear fluctuation of average water temperatures in the equatorial Pacific Ocean. El Niños, occurring about every dozen years, involve warmer, drier weather (with the opposite during La Niña years) and are associated in many developing countries with droughts and food shortages. Over the last fifty years El Niños have roughly doubled the likelihood of civil conflict, mostly by stoking the fires of preexisting conflicts.

			The relationship between drought and violence is tricky. The civil conflict referred to in the previous paragraph concerned deaths caused by battle between governmental and nongovernmental forces (i.e., civil wars or insurgencies). Thus, rather than fighting over a watering hole or a field for grazing, this was fighting for modern perks of power. But in traditional settings drought may mean spending more time foraging or hauling water for your crops. Raiding to steal the other group’s women isn’t a high priority, and why rustle someone else’s cows when you can’t even feed your own? Conflict declines.

			Interestingly, something similar occurs in baboons. Normally, baboons in rich ecosystems like the Serengeti need forage only a few hours a day. Part of what endears baboons to primatologists is that this leaves them about nine hours daily to devote to social machinations—trysting and jousting and backbiting. In 1984 there was a devastating drought in East Africa. Among baboons, while there was still sufficient food, it took every waking moment to get enough calories; aggression decreased.52

			So ecological duress can increase or decrease aggression. This raises the key issue of what global warming will do to our best and worst behaviors. There will definitely be some upsides. Some regions will have longer growing seasons, increasing the food supply and reducing tensions. Some people will eschew conflict, being preoccupied with saving their homes from the encroaching ocean or growing pineapples in the Arctic. But amid squabbling about the details in predictive models, the consensus is that global warming won’t do good things to global conflict. For starters, warmer temperatures rile people up—in cities during the summers, for every three degree increase in temperature, there was a 4 percent increase in interpersonal violence and 14 percent in group violence. But global warming’s bad news is more global—desertification, loss of arable land due to rising seas, more droughts. One influential meta-analysis projected 16 percent and 50 percent increases in interpersonal and group violence, respectively, in some regions by 2050.53





OH, WHY NOT: RELIGION


			Time for a quick hit-and-run about religion before considering it in the final chapter.

			Theories abound as to why humans keep inventing religions. It’s more than a human pull toward the supernatural; as stated in one review, “Mickey Mouse has supernatural powers, but no one worships or would fight—or kill—for him. Our social brains may help explain why children the world over are attracted to talking teacups, but religion is much more than that.” Why does religion arise? Because it makes in-groups more cooperative and viable (stay tuned for more in the next chapter). Because humans need personification and to see agency and causality when facing the unknown. Or maybe inventing deities is an emergent by-product of the architecture of our social brains.54

			Amid these speculations, far more boggling is the variety of the thousands of religions we’ve invented. They vary as to number and gender of deities; whether there’s an afterlife, what it’s like, and what it takes to enter; whether deities judge or interfere with humans; whether we are born sinful or pure and whether sexuality changes those states; whether the myth of a religion’s founder is of sacredness from the start (so much so that, say, wise men visit the infant founder) or of a sybarite who reforms (e.g., Siddhārtha’s transition from palace life to being the Buddha); whether the religion’s goal is attracting new followers (say, with exciting news—e.g., an angel visited me in Manchester, New York, and gave me golden plates) or retaining members (we’ve got a covenant with God, so stick with us). On and on.

			There are some pertinent patterns amid this variation. As noted, desert cultures are prone toward monotheistic religions; rain forest dwellers, polytheistic ones. Nomadic pastoralists’ deities tend to value war and valor in battle as an entrée to a good afterlife. Agriculturalists invent gods who alter the weather. As noted, once cultures get large enough that anonymous acts are possible, they start inventing moralizing gods. Gods and religious orthodoxy dominate more in cultures with frequent threats (war, natural disasters), inequality, and high infant mortality rates.

			Before turfing this subject to the final chapter, three obvious points: (a) a religion reflects the values of the culture that invented or adopted it, and very effectively transmits those values; (b) religion fosters the best and worst of our behaviors; (c) it’s complicated.

			—

			We’ve now looked at various cultural factors—collectivism versus individualism, egalitarian versus hierarchical distribution of resources, and so on. While there are others to consider, it’s time to shift to the chapter’s final topic. This is one that has generated shit storms of debate as old as the weathered layers of Olduvai Gorge and as fresh as a newborn baby’s tush, a topic that has scientists who study peace at one another’s throats.





HOBBES OR ROUSSEAU


			Yes, those guys.

			To invoke some estimates, anatomically modern humans emerged about 200,000 years ago, and behaviorally modern ones about 40,000 to 50,000 years ago; animal domestication is 10,000 to 20,000 years old, agriculture around 12,000. After plant domestication, it was roughly 5,000 more years until “history” began with civilizations in Egypt, the Mideast, China, and the New World. When in this arc of history was war invented? Does material culture lessen or worsen tendencies toward war? Do successful warriors leave more copies of their genes? Has the centralization of authority by civilization actually civilized us, providing a veneer of socially contractual restraint? Have humans become more or less decent to one another over the course of history? Yes, it’s short/nasty/brutish versus noble savage.

			In contrast to the centuries of food fights among philosophers, contemporary Hobbes-versus-Rousseau is about actual data. Some of it is archaeological, where researchers have sought to determine the prevalence and antiquity of warfare from the archaeological record.

			Predictably, half of each conference on the subject consists of definitional squabbles. Is “war” solely organized and sustained violence between groups? Does it require weapons? A standing army (even if only seasonally)? An army with hierarchy and chain of command? If fighting is mostly along lines of relatedness, is it a vendetta or clan feud instead of a war?





Fractured Bones


			For most archaeologists the operational definition has been streamlined to numerous people simultaneously meeting violent deaths. In 1996 the archaeologist Lawrence Keeley of the University of Illinois synthesized the existing literature in his highly influential War Before Civilization: The Myth of the Peaceful Savage, ostensibly showing that the archaeological evidence for war is broad and ancient.55

			A similar conclusion comes in the 2011 book The Better Angels of Our Nature: Why Violence Has Declined, by Harvard’s Steven Pinker.56 Cliché police be damned, you can’t mention this book without calling it “monumental.” In this monumental work Pinker argued that (a) violence and the worst horrors of inhumanity have been declining for the last half millennium, thanks to the constraining forces of civilization; and (b) the warfare and barbarity preceding that transition are as old as the human species.

			Keeley and Pinker document savagery galore in prehistoric tribal societies—mass graves filled with skeletons bearing multiple fractures, caved-in skulls, “parrying” fractures (which arise from raising your arm to fend off a blow), stone projectiles embedded in bone. Some sites suggest the outcome of battle—a preponderance of young adult male skeletons. Others suggest indiscriminate massacre—butchered skeletons of both sexes and all ages. Other sites suggest cannibalism of the vanquished.

			In their separate surveys of the literature, Keeley and Pinker present evidence of prestate tribal violence comes from sites in Ukraine, France, Sweden, Niger, India, and numerous precontact American locations.57 This collection of sites includes the oldest such massacre, the 12,000- to 14,000-year-old Jebel Sahaba site along the Nile in northern Sudan, a cemetery of fifty-nine men, women, and children, nearly half of whom have stone projectiles embedded in their bones. And it includes the largest massacre site, the 700-year-old Crow Creek in South Dakota, a mass grave of more than four hundred skeletons, with 60 percent showing evidence of violent deaths. Across the twenty-one sites surveyed, about 15 percent of skeletons showed evidence of “death in warfare.” One can, of course, be killed in war in a way that doesn’t leave fractures or projectiles embedded in bone, suggesting that the percentage of deaths due to warfare was higher.



					Otzi, in his current state (left), and in an artist’s reconstruction (right). Note: his killer, still at large, probably looked pretty much the same.



			Keeley and Pinker also document how prehistoric settlements frequently protected themselves with defensive palisades and fortifications. And, of course, as the poster child for prehistoric violence, there is Otzi, the 5,300-year-old Tyrolean “iceman” found in a melting glacier in 1991 on the Italian/Austrian border. In his shoulder was a freshly embedded arrowhead.

			Thus, Keeley and Pinker document mass casualties of warfare long predating civilization. Just as important, both (starting with Keeley’s subtitle) suggest a hidden agenda among archaeologists to ignore that evidence. Why had there been, to use Keeley’s phrase, “pacification of the past”? In chapter 7 we saw how World War II produced a generation of social scientists trying to understand the roots of fascism. In Keeley’s view, the post–World War II generations of archaeologists recoiled from the trauma of the war by recoiling from the evidence that humans had been prepping a long time for World War II. For Pinker, writing from a younger generation’s perspective, the current whitewashing of prehistoric violence has the flavor of today’s archaeological graybeards being nostalgic about getting stoned in high school and listening to John Lennon’s “Imagine.”

			Keeley and Pinker generated a strong backlash among many notable archaeologists, who charged them with “war-ifying the past.” Most vocal has been R. Brian Ferguson of Rutgers University, with publications with titles like “Pinker’s List: Exaggerating Prehistoric War Mortality.” Keeley and Pinker are criticized for numerous reasons:58


Some of the sites supposedly presenting evidence for warfare actually contain only a single case of violent death, suggesting homicide, not war.

				The criteria for inferring violent death include skeletons in close proximity to arrowheads. However, many such artifacts were actually tools used for other purposes, or simply chips and flakes. For example, Fred Wendorf, who excavated Jebel Sahaba, considered most of the projectiles associated with skeletons to have been mere debris.59

				Many fractured bones were actually healed. Instead of reflecting war, they might indicate the ritualized club fighting seen in many tribal societies.

				Proving that a human bone was gnawed on by a fellow human instead of another carnivore is tough. One tour-de-force paper demonstrated cannibalism in a Pueblo village from around the year 1100—human feces there contained the human version of the muscle-specific protein myoglobin.60 In other words, those humans had been eating human meat. Nonetheless, even when cannibalism is clearly documented, it doesn’t indicate whether there was exo- or endocannibalism (i.e., eating vanquished enemies versus deceased relatives, as is done in some tribal cultures).

				Most important, Keeley and Pinker are accused of cherry-picking their data, discussing only sites of putative war deaths, rather than the entire literature.* When you survey the thousands of prehistoric skeletal remains from hundreds of sites worldwide, rates of violent deaths are far lower than 15 percent. Moreover, there are regions and periods lacking any evidence of warlike violence. The glee in refuting the broadest conclusions of Keeley and Pinker is unmistakable (e.g., Ferguson in the previously cited work: “For 10,000 years in the Southern Levant, there is not one single instance where it can be said with confidence, ‘war was there.’ [his emphasis] Am I wrong? Name the place.”). Thus these critics conclude that wars were rare prior to human civilizations. Supporters of Keeley and Pinker retort that you can’t ignore bloodbaths like Crow Creek or Jebel Sahaba and that the absence of proof (of early war in so many of these sites) is not proof of absence.



			This suggests a second strategy for contemporary Hobbes-versus-Rousseau debates, namely to study contemporary humans in prestate tribal societies. How frequently do they war?





Prehistorians in the Flesh


			Well, if researchers endlessly argue about who or what gnawed on a ten-thousand-year-old human bone, imagine the disagreements about actual living humans.

			Keeley and Pinker, along with Samuel Bowles of the Santa Fe Institute, conclude that warfare is nearly universal in contemporary nonstate societies. This is the world of headhunters in New Guinea and Borneo, Maasai and Zulu warriors in Africa, Amazonians on raiding parties in the rain forest. Keeley estimates that, in the absence of pacification enforced by outside forces such as a government, 90 to 95 percent of tribal societies engage in warfare, many constantly, and a much higher percentage are at war at any time than is the case for state societies. For Keeley the rare peaceful tribal societies are usually so because they have been defeated and dominated by a neighboring tribe. Keeley charges that there has been systematic underreporting of violence by contemporary anthropologists intent on pacifying living relics of the past.



					Clockwise, top left: New Guinea, Masai, Amazonian, Zulu



			Keeley also tries to debunk the view that tribal violence is mostly ritualistic—an arrow in someone’s thigh, a head or two bashed with a war club, and time to call it a day. Instead violence in nonstate cultures is lethal. Keeley seems to take pride in this, documenting how various cultures use weapons designed for warfare, meant to cause festering damage. He often has an almost testy, offended tone about those pacifying anthropologists who think indigenous groups lack the organization, self-discipline, and Puritan work ethic to inflict bloodbaths. He writes about the superiority of tribal warriors to Westernized armies, e.g., describing how in the Anglo-Zulu War, Zulu spears were more accurate than nineteenth-century British guns, and how the Brits won the war not because they were superior fighters but because their logistical sophistication allowed them to fight prolonged wars.

			Like Keeley, Pinker concludes that warfare is nearly ubiquitous in traditional cultures, reporting 10 to 30 percent of deaths as being war related in New Guinea tribes such as the Gebusi and Mae Enga, and a 35 to 60 percent range for Waorani and Jivaro tribes in the Amazon. Pinker estimates rates of death due to violence. Europe currently is in the range of 1 death per 100,000 people per year. During the crime waves of the 1970s and 1980s, the United States approached 10; Detroit was around 45. Germany and Russia, during their twentieth-century wars, averaged 144 and 135, respectively. In contrast, the twenty-seven nonstate societies surveyed by Pinker average 524 deaths. There are the Grand Valley Dani of New Guinea, the Piegan Blackfoot of the American Great Plains, and the Dinka of Sudan, all of whom in their prime approached 1,000 deaths, roughly equivalent to losing one acquaintance per year. Taking the gold are the Kato, a California tribe that in the 1840s crossed the finish line near 1,500 deaths per 100,000 people per year.

			No tour of violence in indigenous cultures is complete without the Yanomamö, a tribe living in the Brazilian and Venezuelan Amazon. According to dogma, there is almost always raiding between villages; 30 percent of adult male deaths are due to violence; nearly 70 percent of adults have had a close relative killed by violence; 44 percent of men have murdered.61 Fun folks.

			The Yanomamö are renowned because of Napoleon Chagnon, one of the most famous and controversial anthropologists, a tough, pugnacious, no-holds-barred academic brawler who first studied them in the 1960s. He established the Yanomamös’ street cred with his 1967 monograph Yanomamo: The Fierce People, an anthropology classic. Thanks to his publications and his ethnographic films about Yanomamö violence, both their fierceness and his are well-known tropes in anthropology.*

			A central concept in the next chapter is that evolution is about passing copies of your genes into the next generation. In 1988 Chagnon published the remarkable report that Yanomamö men who were killers had more wives and offspring than average—thus passing on more copies of their genes. This suggested that if you excel at waging it, war can do wonders for your genetic legacy.

			Thus, among these nonstate tribal cultures standing in for our prehistoric past, nearly all have histories of lethal warfare, some virtually nonstop, and those who excel at killing are more evolutionarily successful. Pretty grim.

			And numerous anthropologists object strenuously to every aspect of that picture:62


Again with the cherry-picking. In Pinker’s analysis of violence among hunter-horticulturalists and other tribal groups, all but one of his examples come from the Amazon or the New Guinea highlands. Global surveys yield much lower rates of warfare and violence.

				Pinker had foreseen this criticism by playing the Keeley pacification-of-the-past card, questioning those lower rates. In particular he has leveled this charge against the anthropologists (whom he somewhat pejoratively calls “anthropologists of peace,” somewhat akin to “believers in the Easter Bunny”) who have reported on the remarkably nonviolent Semai people of Malaysia. This produced a testy letter to Science from this group that, in addition to saying that they are “peace anthropologists,” not “anthropologists of peace,”* stated that they are objective scientists who studied the Semai without preconceived notions, rather than a gaggle of hippies (they even felt obliged to declare that most of them are not pacifists). Pinker’s response was “It is encouraging that ‘anthropologists of peace’ now see their discipline as empirical rather than ideological, a welcome change from the days when many anthropologists signed manifestos on which their position on violence was ‘correct,’ and censured, shut down, or spread libelous rumors about colleagues who disagreed.” Whoof, accusing your academic adversaries of signing manifestos is like a sharp knee to the groin.63

				Other anthropologists have studied the Yanomamö, and no one else reports violence like Chagnon has.64 Moreover, his report of increased reproductive success among more murderous Yanomamö has been demolished by the anthropologist Douglas Fry of the University of Alabama at Birmingham, who showed that Chagnon’s finding is an artifact of poor data analysis: Chagnon compared the number of descendants of older men who had killed people in battle with those who had not, finding significantly more kids among the former. However: (a) Chagnon did not control for age differences—the killers happened to be an average of more than a decade older than the nonkillers, meaning more than a decade more time to accumulate descendants. (b) More important, this was the wrong analysis to answer the question posed—the issue isn’t the reproductive success of elderly men who had been killers in their youth. You need to consider the reproductive success of all killers, including the many who were themselves killed as young warriors, distinctly curtailing their reproductive success. Not doing so is like concluding that war is not lethal, based solely on studies of war veterans.

				Moreover, Chagnon’s finding does not generalize—at least three studies of other cultures fail to find a violence/reproductive success link. For example, a study by Luke Glowacki and Richard Wrangham of Harvard examined a nomadic pastoralist tribe, the Nyangatom of southern Ethiopia. Like other pastoralists in their region, the Nyangatom regularly raid one another for cattle.65 The authors found that frequent participation in large, open battle raiding did not predict increased lifelong reproductive success. Instead such success was predicted by frequent participation in “stealth raiding,” where a small group furtively steals cows from the enemy at night. In other words, in this culture being a warrior on ’roids does not predict amply passing on your genes; being a low-down sneaky varmint of a cattle rustler does.

				These indigenous groups are not stand-ins for our prehistoric past. For one thing, many have obtained weapons more lethal than those of prehistory (a damning criticism of Chagnon is that he often traded axes, machetes, and shotguns to Yanomamö for their cooperation in his studies). For another, these groups often live in degraded habitats that increase resource competition, thanks to being increasingly hemmed in by the outside world. And outside contact can be catastrophic. Pinker cites research showing high rates of violence among the Amazonian Aché and Hiwi tribes. However, in examining the original reports, Fry found that all of the Aché and Hiwi deaths were due to killings by frontier ranchers intent on forcing them off their land.66 This tells nothing about our prehistoric past.



			Both sides in these debates see much at stake. Near the end of his book, Keeley airs a pretty weird worry: “The doctrines of the pacified past unequivocally imply that the only answer to the ‘mighty scourge of war’ is a return to tribal conditions and the destruction of all civilization.” In other words, unless this tomfoolery of archaeologists pacifying the past stops, people will throw away their antibiotics and microwaves, do some scarification rituals, and switch to loincloths—and where will that leave us?

			Critics on the other side of these debates have deeper worries. For one thing, the false picture of, say, Amazonian tribes as ceaselessly violent has been used to justify stealing their land. According to Stephen Corry of Survival International, a human-rights organization that advocates for indigenous tribal peoples, “Pinker is promoting a fictitious, colonialist image of a backward ‘Brutal Savage’, which pushes the debate back over a century and is still used to destroy tribes.”67

			—

			Amid these roiling debates, let’s keep sight of what got us to this point. A behavior has occurred that is good, bad, or ambiguous. How have cultural factors stretching back to the origins of humans contributed to that behavior? And rustling cattle on a moonless night; or setting aside tending your cassava garden to raid your Amazonian neighbors; or building fortifications; or butchering every man, woman, and child in a village is irrelevant to that question. That’s because all these study subjects are pastoralists, agriculturalists, or horticulturalists, lifestyles that emerged only in the last ten thousand to fourteen thousand years, after the domestication of plants and animals. In the context of hominin history stretching back hundreds of thousands of years, being a camel herder or farmer is nearly as newfangled as being a lobbyist advocating for legal rights for robots. For most of history, humans have been hunter-gatherers, a whole different kettle of fish.





War and Hunter-Gatherers, Past and Present


			Roughly 95 to 99 percent of hominin history has been spent in small, nomadic bands that foraged for edible plants and hunted cooperatively. What is known about hunter-gatherer (for sanity’s sake, henceforth HG) violence?

			Given that prehistoric HGs didn’t have lots of material possessions that have lasted tens of thousands of years, they haven’t left much of an archaeological record. Insight into their minds and lifestyle comes from cave paintings dating back as much as forty thousand years. Though paintings from around the world show humans hunting, hardly any unambiguously depict interhuman violence.

			The paleontological record is even sparser. To date, there has been discovered one site of an HG massacre, dating back ten thousand years in northern Kenya; this will be discussed later.

			What to do with this void of information? One approach is comparative, inferring the nature of our distant ancestors by comparing them with extant nonhuman primates. Early versions of this approach were the writings of Konrad Lorenz and of Robert Ardrey, who argued in his 1966 best seller The Territorial Imperative that human origins are rooted in violent territoriality.68 The most influential modern incarnation comes from Richard Wrangham, particularly in his 1997 book (with Dale Peterson) Demonic Males: Apes and the Origins of Human Violence. For Wrangham chimps provide the clearest guide to the behavior of earliest humans, and the picture is a bloody one. He essentially leapfrogs HGs entirely: “So we come back to the Yanomamo. Do they suggest to us that chimpanzee violence is linked to human war? Clearly they do.” Wrangham summarizes his stance:


The mysterious history before history, the blank slate of knowledge about ourselves before Jericho, has licensed our collective imagination and authorized the creation of primitive Edens for some, forgotten matriarchies for others. It is good to dream, but a sober, waking rationality suggests that if we start with ancestors like chimpanzees and end up with modern humans building walls and fighting platforms, the 5-million-year-long trail to our modern selves was lined, along its full stretch, by a male aggression that structured our ancestors’ social lives and technology and minds.

			It’s Hobbes all the way down, plus Keeley-esque contempt for pacification-of-the-past dreamers.

			This view has been strongly criticized: (a) We’re neither chimps nor their descendants; they’ve been evolving at nearly the same pace as humans since our ancestral split. (b) Wrangham picks and chooses in his cross-species linkages; for example, he argues that the human evolutionary legacy of violence is rooted not only in our close relationship to chimps but also in our nearly-as-close kinship with gorillas, who practice competitive infanticide. The problem is that, overall, gorillas display minimal aggression, something Wrangham ignores in linking human violence to gorillas. (c) As the most significant species cherry-picking, Wrangham pretty much ignores bonobos, with their far lower levels of violence than chimps, female social dominance, and absence of hostile territoriality. Crucially, humans share as much of their genes with bonobos as with chimps, something unknown when Demonic Males was published (and, notably, Wrangham has since softened his views).

			For most in the field, most insights into the behavior of our HG ancestors come from study of contemporary HGs.

			Once, the world of humans consisted of nothing but HGs; today the remnants of that world are in the few remaining pockets of peoples who live pure HG lives. These include the Hadza of northern Tanzania, Mbuti “Pygmies” in the Congo, Batwa in Rwanda, Gunwinggu of the Australian outback, Andaman Islanders in India, Batak in the Philippines, Semang in Malaysia, and various Inuit cultures in northern Canada.

			To start, it was once assumed that among HGs, women do the gathering while men supply most of the calories by hunting. In actuality, the majority of calories come from foraging; men spend lots of time talking about how awesome they were in the last hunt and how much awesomer they’ll be in the next—among some Hadza, maternal grandmothers supply more calories to families than do the Man the Hunter men.69

			The arc of human history is readily equated with an arc of progress, and key to the latter is the view that agriculture was the best thing humans ever invented; I’ll rant about that later. A cornerstone of the agriculture lobby is the idea that primordial HGs were half starved. In reality, HGs typically work fewer hours for their daily bread than do traditional farmers and are longer-lived and healthier. In the words of anthropologist Marshall Sahlins, HGs were the original affluent society.



					Clockwise from top: Hadza; Mbuti, Andaman, Semang



			There are some demographic themes shared among contemporary HGs.70 Dogma used to be that HG bands had fairly stable group membership, producing considerable in-group relatedness. More recent work suggests less relatedness than thought, reflecting fluid fusion/fission groupings in nomadic HGs. The Hadza show one consequence of such fluidity, namely that particularly cooperative hunters find one another and work together. More on this in the next chapter.

			What about our best and worst behaviors in contemporary HGs? Up into the 1970s, the clear answer was that HGs are peaceful, cooperative, and egalitarian. Interband fluidity serves as a safety valve preventing individual violence (i.e., when people are at each other’s throats, someone moves to another group), and nomadicism as a safety valve preventing intergroup violence (i.e., rather than warring with the neighboring band, just hunt in a different valley from them).

			The standard-bearers for HG grooviness were the Kalahari !Kung.*71 The title of an early monograph about them—Elizabeth Marshall Thomas’s 1959 The Harmless People—says it all.* !Kung are to the Yanomamö as Joan Baez is to Sid Vicious and the Sex Pistols.

			Naturally, this picture of the !Kung in particular and HGs in general was ripe for revisionism. This occurred when field studies were sufficiently long term to document HGs killing one another, as summarized in an influential 1978 publication by Carol Ember of Yale.72 Basically, if you’re observing a band of thirty people, it will take a long time to see that, on a per-capita basis, they have murder rates approximating Detroit’s (the standard comparison always made). Admitting that HGs were violent was seen as a purging of sixties anthropological romanticism, a bracing slap in the face for anthropologists who had jettisoned objectivity in order to dance with wolves.

			By the time of Pinker’s synthesis, HG violence was established, and the percentage of their deaths attributed to warfare averaged around 15 percent, way more than in modern Western societies. Contemporary HG violence constitutes a big vote for the Hobbesian view of warfare and violence permeating all of human history.



					Kalahari !Kung hunter-gatherers



			Time for the criticisms:73


Mislabeling—some HGs cited by Pinker, Keeley, and Bowles are, in fact, hunter-horticulturalists.

				Many instances of supposed HG warfare, on closer inspection, were actually singular homicides.

				Some violent Great Plains HG cultures were untraditional in the sense of using something crucial that didn’t exist in the Pleistocene—domesticated horses ridden into battle.

				Like non-Western agriculturalists or pastoralists, contemporary HG are not equivalent to our ancestors. Weapons invented in the last ten thousand years have been introduced through trade; most HG cultures have spent millennia being displaced by agriculturalists and pastoralists, pushed into ever tougher, resource-sparse ecosystems.

				Once again, the cherry-picking issue, i.e., failure to cite cases of peaceful HGs.

				Most crucially, there’s more than one type of HG. Nomadic HGs are the original brand, stretching back hundreds of thousands of years.74 But in addition to HG 2.0 equestrians, there are “complex HGs,” who are different—violent, not particularly egalitarian, and sedentary, typically because they’re sitting on a rich food source that they defend from outsiders. In other words, a transitional form from pure HGs. And many of the cultures cited by Ember, Keeley, and Pinker are complex HGs. This difference is relevant to Nataruk, that northern Kenyan site of a ten-thousand-year-old massacre—skeletons of twenty-seven unburied people, killed by clubbing, stabbing, or stone projectiles. The victims were sedentary HGs, living alongside a shallow bay on Lake Turkana, prime beachfront property with easy fishing and plentiful game animals coming to the water to drink. Just the sort of real estate that someone else would try to muscle in on.



			The most thoughtful and insightful analyses of HG violence come from Fry and from Christopher Boehm of the University of Southern California. They paint a complex picture.



					D. P. Fry and P. Söderberg, “Lethal Aggression in Mobile Forager Bands and Implications for the Origins of War,” Sci 341 (2013): 270.

					Visit bit.ly/2oeg96t for a larger version of this graph.



			Fry has provided what I consider the cleanest assessment of warfare in such cultures. In a notable 2013 Science paper, he and Finnish anthropologist Patrik Söderberg reviewed all cases of lethal violence in the ethnographic literature in “pure” nomadic HGs (i.e., well studied before extensive contact with outsiders and living in a stable ecosystem). The sample consisted of twenty-one such groups from around the world. Fry and Söderberg observed what might be called warfare (defined by the fairly unstringent criterion of conflict producing multiple casualties) in only a minority of the cultures. Not exactly widespread. This is probably the best approximation we’ll ever get about warfare in our HG ancestors. Nonetheless, these pure HGs are no tie-dyed pacifists; 86 percent of the cultures experienced lethal violence. What are their causes?

			In his 2012 book Moral Origins: The Evolution of Virtue, Altruism, and Shame, Boehm also surveys the literature, using slightly less stringent criteria than Fry uses, producing a list of about fifty relatively “pure” nomadic HG cultures (heavily skewed toward Inuit groups from the Arctic).75 As expected, violence is mostly committed by men. Most common is killing related to women—two men fighting over a particular woman, or attempts to kidnap a woman from a neighboring group. Naturally, there are men killing their wives, usually over accusations of adultery. There’s female infanticide and killing arising from accusations of witchcraft. There are occasional killings over garden-variety stealing of food or refusals to share food. And lots of revenge killings by relatives of someone killed.

			Both Fry and Boehm report killings akin to capital punishment for severe norm violations. What norms do nomadic HGs value most? Fairness, indirect reciprocity, and avoidance of despotism.

			Fairness. As noted, HGs pioneered human cooperative hunting and sharing among nonrelatives.76 This is most striking with meat. It’s typically shared by successful hunters with unsuccessful ones (and their families); individuals playing dominant roles in hunts don’t necessarily get much more meat than everyone else; crucially, the most successful hunter rarely decides how the meat is divided—instead this is typically done by a third party. There are fascinating hints about the antiquity of this. Big-game hunting by hominins 400,000 years ago has been documented; bones from animals butchered then show cut marks that are chaotic, overlapping at different angles, suggesting a free-for-all. But by 200,000 years ago the contemporary HG pattern is there—cut marks are evenly spaced and parallel, suggesting that single individuals butchered and dispensed the meat.

			This does not mean, though, that sharing is effortless for pure HGs. Boehm notes how, for example, the !Kung perpetually kvetch about being shortchanged on meat. It’s the background hum of social regulation.

			Indirect reciprocity. The next chapter discusses reciprocal altruism between pairs of individuals. Boehm emphasizes how nomadic HGs specialize, instead, in indirect reciprocity. Person A is altruistic to B; B’s social obligation now isn’t necessarily as much being altruistic to A as paying the altruism forward to C. C pays it forward to D, etc. . . . This stabilizing cooperation is ideal for big-game hunters, where two rules hold: (a) your hunts are usually unsuccessful; and (b) when they are successful, you typically have more meat than your family can consume, so you might as well share it around. As has been said, an HG’s best investment against future hunger is to put meat in other people’s stomachs now.

			Avoidance of despotism. As also covered in the next chapter, there’s considerable evolutionary pressure for detecting cheating (when someone reneges on their half of a reciprocal relationship). For nomadic HGs, policing covert cheating is less of a concern than overt evidence of intimidation and powermongering. HGs are constantly on guard against bullies throwing their weight around.

			HG societies expend lots of collective effort on enforcing fairness, indirect reciprocity, and avoidance of despotism. This is accomplished with that terrific norm-enforcement mechanism, gossip. HGs gossip endlessly, and as studied by Polly Wiessner of the University of Utah, it’s mostly about the usual: norm violation by high-status individuals.77 People magazine around the campfire.* Gossiping serves numerous purposes. It helps for reality testing (“Is it just me, or was he being a total jerk?”), passing news (“Two guesses who just happened to get a foot cramp during the hairiest part of the hunt today”), and building consensus (“Something needs to be done about this guy”). Gossip is the weapon of norm enforcement.

			HG cultures take similar actions—collectively subjecting miscreants to criticism, shaming and mockery, ostracizing and shunning, refusing to share meat, nonlethal physical punishment, expulsion from the group, or, as a last resort, killing the person (done either by the whole group or by a designated executioner).

			Boehm documents such judicial killings in nearly half the pure HG cultures. What transgressions merit them? Murder, attempts at grabbing power, use of malicious sorcery, stealing, refusal to share, betrayal of the group to outsiders, and of course breaking of sexual taboos. All typically punished this way after other interventions have failed repeatedly.

			—

			So, Hobbes or Rousseau? Well, a mixture of the two, I say unhelpfully. This lengthy section makes clear that you have to make some careful distinctions: (a) HGs versus other traditional ways of making a living; (b) nomadic HGs versus sedentary ones; (c) data sets that canvass an entire literature versus those that concentrate on extreme examples; (d) members of traditional societies killing one another versus members being killed by gun-toting, land-grabbing outsiders; (e) chimps as our cousins versus chimps erroneously viewed as our ancestors; (f) chimps as our closest ancestors versus chimps and bonobos as our closest ancestors; (g) warfare versus homicide, where lots of the former can decrease the latter in the name of in-group cooperation; (h) contemporary HGs living in stable, resource-filled habitats with minimal interactions with the outside world versus contemporary HGs pushed into marginal habitats and interacting with non-HGs. Once you’ve done that, I think a pretty clear answer emerges. The HGs who peopled earth for hundreds of thousands of years were probably no angels, being perfectly capable of murder. However, “war”—both in the sense that haunts our modern world and in the stripped-down sense that haunted our ancestors—seems to have been rare until most humans abandoned the nomadic HG lifestyle. Our history as a species has not been soaked in escalated conflict. And ironically Keeley tacitly concludes the same—he estimates that 90 to 95 percent of societies engage in war. And whom does he note as the exceptions? Nomadic HGs.

			Which brings us to agriculture. I won’t pull any punches—I think that its invention was one of the all-time human blunders, up there with, say, the New Coke debacle and the Edsel. Agriculture makes people dependent on a few domesticated crops and animals instead of hundreds of wild food sources, creating vulnerability to droughts and blights and zoonotic diseases. Agriculture makes for sedentary living, leading humans to do something that no primate with a concern for hygiene and public health would ever do, namely living in close proximity to their feces. Agriculture makes for surplus and thus almost inevitably the unequal distribution of surplus, generating socioeconomic status differences that dwarf anything that other primates cook up with their hierarchies. And from there it’s just a hop, skip, and a jump until we’ve got Mr. McGregor persecuting Peter Rabbit and people incessantly singing “Oklahoma.”

			Maybe this is a bit over the top. Nonetheless, I do think it is reasonably clear that it wasn’t until humans began the massive transformation of life that came from domesticating teosinte and wild tubers, aurochs and einkorn, and of course wolves, that it became possible to let loose the dogs of war.





SOME CONCLUSIONS


			The first half of the chapter explored where we are; the second, how we most likely got here.

			“Where we are” is awash in cultural variation. From our biological perspective, the most fascinating point is how brains shape cultures, which shape brains, which shape . . . That’s why it’s called coevolution. We’ve seen some evidence of coevolution in the technical sense—where there are significant differences between different cultures in the distribution of gene variants pertinent to behavior. But those influences are pretty small. Instead what is most consequential is childhood, the time when cultures inculcate individuals into further propagating their culture. In that regard, probably the most important fact about genetics and culture is the delayed maturation of the frontal cortex—the genetic programming for the young frontal cortex to be freer from genes than other brain regions, to be sculpted instead by environment, to sop up cultural norms. To hark back to a theme from the first pages of this book, it doesn’t take a particularly fancy brain to learn how to motorically, say, throw a punch. But it takes a fancy, environmentally malleable frontal cortex to learn culture-specific rules about when it’s okay to throw punches.

			In another theme from the first half, cultural differences manifest themselves in monumentally important, expected ways—say, whom it is okay to kill (an enemy soldier, a cheating spouse, a newborn of the “wrong” sex, an elderly parent too old to hunt, a teenage daughter who is absorbing the culture around her rather than the culture her parents departed). But the manifestations can occur in unlikely places—e.g., where your eyes look within milliseconds of seeing a picture, or whether thinking of a rabbit prompts you to think of other animals or of what rabbits eat.

			Another key theme is the paradoxical influence of ecology. Ecosystems majorly shape culture—but then that culture can be exported and persist in radically different places for millennia. Stated most straightforwardly, most of earth’s humans have inherited their beliefs about the nature of birth and death and everything in between and thereafter from preliterate Middle Eastern pastoralists.

			The second half of the chapter, just concluded, addresses the key issue of how we got here—has it been hundreds of thousands of years of Hobbes or of Rousseau? Your answer to that question greatly shapes what you’ll make of something we’ll consider in the final chapter, namely that over the last half millennium people have arguably gotten a lot less awful to one another.




