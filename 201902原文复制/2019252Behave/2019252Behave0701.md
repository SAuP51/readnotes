

Seven


			Back to the Crib, Back to the Womb





After journeying to Planet Adolescence, we resume our basic approach. Our behavior—good, bad, or ambiguous—has occurred. Why? When seeking the roots of behavior, long before neurons or hormones come to mind, we typically look first at childhood.





COMPLEXIFICATION


			Childhood is obviously about increasing complexity in every realm of behavior, thought, and emotion. Crucially, such increasing complexity typically emerges in stereotypical, universal sequences of stages. Most child behavioral development research is implicitly stage oriented, concerning: (a) the sequence with which stages emerge; (b) how experience influences the speed and surety with which that sequential tape of maturation unreels; and (c) how this helps create the adult a child ultimately becomes. We start by examining the neurobiology of the “stage” nature of development.





A BRIEF TOUR OF BRAIN DEVELOPMENT


			The stages of human brain development make sense. A few weeks after conception, a wave of neurons are born and migrate to their correction locations. Around twenty weeks, there is a burst of synapse formation—neurons start talking to one another. And then axons start being wrapped in myelin, the glial cell insulation (forming “white matter”) that speeds up action.

			Neuron formation, migration, and synaptogenesis are mostly prenatal in humans.1 In contrast, there is little myelin at birth, particularly in evolutionarily newer brain regions; as we’ve seen, myelination proceeds for a quarter century. The stages of myelination and consequent functional development are stereotypical. For example, the cortical region central to language comprehension myelinates a few months earlier than that for language production—kids understand language before producing it.

			Myelination is most consequential when enwrapping the longest axons, in neurons that communicate the greatest distances. Thus myelination particularly facilitates brain regions talking to one another. No brain region is an island, and the formation of circuits connecting far-flung brain regions is crucial—how else can the frontal cortex use its few myelinated neurons to talk to neurons in the brain’s subbasement to make you toilet trained?2

			As we saw, mammalian fetuses overproduce neurons and synapses; ineffective or unessential synapses and neurons are pruned, producing leaner, meaner, more efficient circuitry. To reiterate a theme from the last chapter, the later a particular brain region matures, the less it is shaped by genes and the more by environment.3





STAGES


			What stages of child development help explain the good/bad/in-between adult behavior that got the ball rolling in chapter 1?

			The mother of all developmental stage theories was supplied in 1923, pioneered by Jean Piaget’s clever, elegant experiments revealing four stages of cognitive development:4


Sensorimotor stage (birth to ~24 months). Thought concerns only what the child can directly sense and explore. During this stage, typically at around 8 months, children develop “object permanence,” understanding that even if they can’t see an object, it still exists—the infant can generate a mental image of something no longer there.*

				Preoperational stage (~2 to 7 years). The child can maintain ideas about how the world works without explicit examples in front of him. Thoughts are increasingly symbolic; imaginary play abounds. However, reasoning is intuitive—no logic, no cause and effect. This is when kids can’t yet demonstrate “conservation of volume.” Identical beakers A and B are filled with equal amounts of water. Pour the contents of beaker B into beaker C, which is taller and thinner. Ask the child, “Which has more water, A or C?” Kids in the preoperational stage use incorrect folk intuition—the water line in C is higher than that in A; it must contain more water.

				Concrete operational stage (7 to 12 years). Kids think logically, no longer falling for that different-shaped-beakers nonsense. However, generalizing logic from specific cases is iffy. As is abstract thinking—for example, proverbs are interpreted literally (“‘Birds of a feather flock together’ means that similar birds form flocks”).

				Formal operational stage (adolescence onward). Approaching adult levels of abstraction, reasoning, and metacognition.





					Kid playing hide-and-seek while in the “If I can’t see you (or even if I can’t see you as easily as usual), then you can’t see me” stage.



			Other aspects of cognitive development are also conceptualized in stages. An early stage occurs when toddlers form ego boundaries—“There is a ‘me,’ separate from everyone else.” A lack of ego boundaries is shown when a toddler isn’t all that solid on where he ends and Mommy starts—she’s cut her finger, and he claims his finger hurts.5

			Next comes the stage of realizing that other individuals have different information than you do. Nine-month-olds look where someone points (as can other apes and dogs), knowing the pointer has information that they don’t. This is fueled by motivation: Where is that toy? Where’s she looking? Older kids understand more broadly that other people have different thoughts, beliefs, and knowledge than they, the landmark of achieving Theory of Mind (ToM).6

			Here’s what not having ToM looks like. A two-year-old and an adult see a cookie placed in box A. The adult leaves, and the researcher switches the cookie to box B. Ask the child, “When that person comes back, where will he look for the cookie?” Box B—the child knows it’s there and thus everyone knows. Around age three or four the child can reason, “They’ll think it’s in A, even though I know it’s in B.” Shazam: ToM.

			Mastering such “false belief” tests is a major developmental landmark. ToM then progresses to fancier insightfulness—e.g., grasping irony, perspective taking, or secondary ToM (understanding person A’s ToM about person B).7

			Various cortical regions mediate ToM: parts of the medial PFC (surprise!) and some new players, including the precuneus, the superior temporal sulcus, and the temporoparietal junction (TPJ). This is shown with neuroimaging; by ToM deficits if these regions are damaged (autistic individuals, who have limited ToM, have decreased gray matter and activity in the superior temporal sulcus); and by the fact that if you temporarily inactivate the TPJ, people don’t consider someone’s intentions when judging them morally.8

			Thus there are stages of gaze following, followed by primary ToM, then secondary ToM, then perspective taking, with the speed of transitions influenced by experience (e.g., kids with older siblings achieve ToM earlier than average).9

			Naturally, there are criticisms of stage approaches to cognitive development. One is at the heart of this book: a Piagetian framework sits in a “cognition” bucket, ignoring the impact of social and emotional factors.

			One example to be discussed in chapter 12 concerns preverbal infants, who sure don’t grasp transitivity (if A > B, and B > C, then A > C). Show a violation of transitivity in interactions between shapes on a screen (shape A should knock over shape C, but the opposite occurs), and the kid is unbothered, doesn’t look for long. But personify the shapes with eyes and a mouth, and now heart rate increases, the kid looks longer—“Whoa, character C is supposed to move out of character A’s way, not the reverse.” Humans understand logical operations between individuals earlier than between objects.10

			Social and motivational state can shift cognitive stage as well. Rudiments of ToM are more demonstrable in chimps who are interacting with another chimp (versus a human) and if there is something motivating—food—involved.*11

			Emotion and affect can alter cognitive stage in remarkably local ways. I saw a wonderful example of this when my daughter displayed both ToM and failure of ToM in the same breath. She had changed preschools and was visiting her old class. She told everyone about life in her new school: “Then, after lunch, we play on the swings. There are swings at my new school. And then, after that, we go inside and Carolee reads us a story. Then, after that . . .” ToM: “play on the swings”—wait, they don’t know that my school has swings; I need to tell them. Failure of ToM: “Carolee reads us a story.” Carolee, the teacher at her new school. The same logic should apply—tell them who Carolee is. But because Carolee was the most wonderful teacher alive, ToM failed. Afterward I asked her, “Hey, why didn’t you tell everyone that Carolee is your teacher?” “Oh, everyone knows Carolee.” How could everyone not?





Feeling Someone Else’s Pain


			ToM leads to a next step—people can have different feelings than me, including pained ones.12 This realization is not sufficient for empathy. After all, sociopaths, who pathologically lack empathy, use superb ToM to stay three manipulative, remorseless steps ahead of everyone. Nor is this realization strictly necessary for empathy, as kids too young for ToM show rudiments of feeling someone else’s pain—a toddler will try to comfort someone feigning crying, offering them her pacifier (and the empathy is rudimentary in that the toddler can’t imagine someone being comforted by different things than she is).

			Yes, very rudimentary. Maybe the toddler feels profound empathy. Or maybe she’s just distressed by the crying and is self-interestedly trying to quiet the adult. The childhood capacity for empathy progresses from feeling someone’s pain because you are them, to feeling for the other person, to feeling as them.

			The neurobiology of kid empathy makes sense. As introduced in chapter 2, in adults the anterior cingulate cortex activates when they see someone hurt. Ditto for the amygdala and insula, especially in instances of intentional harm—there is anger and disgust. PFC regions including the (emotional) vmPFC are on board. Observing physical pain (e.g., a finger being poked with a needle) produces a concrete, vicarious pattern: there is activation of the periaqueductal gray (PAG), a region central to your own pain perception, in parts of the sensory cortex receiving sensation from your own fingers, and in motor neurons that command your own fingers to move.* You clench your fingers.

			Work by Jean Decety of the University of Chicago shows that when seven-year-olds watch someone in pain, activation is greatest in the more concrete regions—the PAG and the sensory and motor cortices—with PAG activity coupled to the minimal vmPFC activation there is. In older kids the vmPFC is coupled to increasingly activated limbic structures.13 And by adolescence the stronger vmPFC activation is coupled to ToM regions. What’s happening? Empathy is shifting from the concrete world of “Her finger must hurt, I’m suddenly conscious of my own finger” to ToM-ish focusing on the pokee’s emotions and experience.

			Young kids’ empathy doesn’t distinguish between intentional and unintentional harm or between harm to a person and to an object. Those distinctions emerge with age, around the time when the PAG part of empathic responses lessens and there is more engagement of the vmPFC and ToM regions; moreover, intentional harm now activates the amygdala and insula—anger and disgust at the perpetrator.* This is also when kids first distinguish between self- and other-inflicted pain.

			More sophistication—by around age seven, kids are expressing their empathy. By ages ten through twelve, empathy is more generalized and abstracted—empathy for “poor people,” rather than one individual (downside: this is also when kids first negatively stereotype categories of people).

			There are also hints of a sense of justice. Preschoolers tend to be egalitarians (e.g., it’s better that the friend gets a cookie when she does). But before we get carried away with the generosity of youth, there is already in-group bias; if the other child is a stranger, there is less egalitarianism.14

			There is also a growing tendency of kids to respond to an injustice, when someone has been treated unfairly.15 But once again, before getting carried away with things, it comes with a bias. By ages four through six, kids in cultures from around the world respond negatively when they are the ones being shortchanged. It isn’t until ages eight through ten that kids respond negatively to someone else being treated unfairly. Moreover, there is considerable cross-cultural variability as to whether that later stage even emerges. The sense of justice in young kids is a very self-interested one.

			Soon after kids start responding negatively to someone else being treated unjustly, they begin attempting to rectify previous inequalities (“He should get more now because he got less before”).16 By preadolescence, egalitarianism gives way to acceptance of inequality because of merit or effort or for a greater good (“She should play more than him; she’s better/worked harder/is more important to the team”). Some kids even manage self-sacrifice for the greater good (“She should play more than me; she’s better”).* By adolescence, boys tend to accept inequality more than girls do, on utilitarian grounds. And both sexes are acquiescing to inequality as social convention—“Nothing can be done; that’s the way it is.”





Moral Development


			With ToM, perspective taking, nuanced empathy, and a sense of justice in place, a child can start wrestling with telling right from wrong.

			Piaget emphasized how much kids’ play is about working out rules of appropriate behavior (rules that can differ from those of adults)* and how this involves stages of increasing complexity. This inspired a younger psychologist to investigate the topic more rigorously, with enormously influential consequences.

			In the 1950s Lawrence Kohlberg, then a graduate student at the University of Chicago and later a professor at Harvard, began formulating his monumental stages of moral development.17

			Kids would be presented with moral conundrums. For example: The only dose of the only drug that will save a poor woman from dying is prohibitively expensive. Should she steal it? Why?

			Kohlberg concluded that moral judgment is a cognitive process, built around increasingly complex reasoning as kids mature. He proposed his famed three stages of moral development, each with two subparts.

			You’ve been told not to eat the tempting cookie in front of you. Should you eat it? Here are the painfully simplified stages of reasoning that go into the decision:





Level 1: Should I Eat the Cookie? Preconventional Reasoning


			Stage 1. It depends. How likely am I to get punished? Being punished is unpleasant. Aggression typically peaks around ages two through four, after which kids are reined in by adults’ punishment (“Go sit in the corner”) and peers (i.e., being ostracized).

			Stage 2. It depends. If I refrain, will I get rewarded? Being rewarded is nice.

			Both stages are ego-oriented—obedience and self-interest (what’s in it for me?). Kohlberg found that children are typically at this level up to around ages eight through ten.

			Concern arises when aggression, particularly if callous and remorseless, doesn’t wane around these ages—this predicts an increased risk of adult sociopathy (aka antisocial personality).* Crucially, the behavior of future sociopaths seems impervious to negative feedback. As noted, high pain thresholds in sociopaths help explain their lack of empathy—it’s hard to feel someone else’s pain when you can’t feel your own. It also helps explain the imperviousness to negative feedback—why change your behavior if punishment doesn’t register?

			It is also around this stage that kids first reconcile after conflicts and derive comfort from reconciliation (e.g., decreasing glucocorticoid secretion and anxiety). Those benefits certainly suggest self-interest motivating reconciliation. This is shown in another, realpolitik way—kids reconcile more readily when the relationship matters to them.





Level 2: Should I Eat the Cookie? Conventional Reasoning


			Stage 3. It depends. Who will be deprived if I do? Do I like them? What would other people do? What will people think of me for eating the cookie? It’s nice to think of others; it’s good to be well regarded.

			Stage 4. It depends. What’s the law? Are laws sacrosanct? What if everyone broke this law? It’s nice to have order. This is the judge who, considering predatory but legal lending practices by a bank, thinks, “I feel sorry for these victims . . . but I’m here to decide whether the bank broke a law . . . and it didn’t.”

			Conventional moral reasoning is relational (about your interactions with others and their consequences); most adolescents and adults are at this level.





Level 3: Should I Eat the Cookie? Postconventional Reasoning


			Stage 5: It depends. What circumstances placed the cookie there? Who decided that I shouldn’t take it? Would I save a life by taking the cookie? It’s nice when clear rules are applied flexibly. Now the judge would think: “Yes, the bank’s actions were legal, but ultimately laws exist to protect the weak from the mighty, so signed contract or otherwise, that bank must be stopped.”

			Stage 6: It depends. Is my moral stance regarding this more vital than some law, a stance for which I’d pay the ultimate price if need be? It’s nice to know there are things for which I’d repeatedly sing, “We Will Not Be Moved.”

			This level is egoistic in that rules and their application come from within and reflect conscience, where a transgression exacts the ultimate cost—having to live with yourself afterward. It recognizes that being good and being law-abiding aren’t synonymous. As Woody Guthrie wrote in “Pretty Boy Floyd,” “I love a good man outside the law, just as much as I hate a bad man inside the law.”*

			Stage 6 is also egotistical, implicitly built on self-righteousness that trumps conventional petty bourgeois rule makers and bean counters, The Man, those sheep who just follow, etc. To quote Emerson, as is often done when considering the postconventional stage, “Every heroic act measures itself by its contempt of some external good.” Stage 6 reasoning can inspire. But it can also be insufferable, premised on “being good” and “being law abiding” as opposites. “To live outside the law, you must be honest,” wrote Bob Dylan.

			Kohlbergians found hardly anyone consistently at stage 5 or stage 6.

			—

			Kohlberg basically invented the scientific study of moral development in children. His stage model is so canonical that people in the business dis someone by suggesting they’re stuck in the primordial soup of a primitive Kohlberg stage. As we’ll see in chapter 12, there is even evidence that conservatives and liberals reason at different Kohlberg stages.

			Naturally, Kohlberg’s work has problems.

			The usual: Don’t take any stage model too seriously—there are exceptions, maturational transitions are not clean cut, and someone’s stage can be context dependent.

			The problem of tunnel vision and wrong emphases: Kohlberg initially studied the usual unrepresentative humans, namely Americans, and as we will see in later chapters, moral judgments differ cross-culturally. Moreover, subjects were male, something challenged in the 1980s by Carol Gilligan of NYU. The two agreed on the general sequence of stages. However, Gilligan and others showed that in making moral judgments, girls and women generally value care over justice, in contrast to boys and men. As a result, females tilt toward conventional thinking and its emphasis on relationships, while males tilt toward postconventional abstractions.18

			The cognitive emphasis: Are moral judgments more the outcome of reasoning or of intuition and emotion? Kohlbergians favor the former. But as will be seen in chapter 13, plenty of organisms with limited cognitive skills, including young kids and nonhuman primates, display rudimentary senses of fairness and justice. Such findings anchor “social intuitionist” views of moral decision making, associated with psychologists Martin Hoffman and Jonathan Haidt, both of NYU.19 Naturally, the question becomes how moral reasoning and moral intuitionism interact. As we’ll see, (a) rather than being solely about emotion, moral intuition is a different style of cognition from conscious reasoning; and (b) conversely, moral reasoning is often flagrantly illogical. Stay tuned.

			The lack of predictability: Does any of this actually predict who does the harder thing when it’s the right thing to do? Are gold medalists at Kohlbergian reasoning the ones willing to pay the price for whistle-blowing, subduing the shooter, sheltering refugees? Heck, forget the heroics; are they even more likely to be honest in dinky psych experiments? In other words, does moral reasoning predict moral action? Rarely; as we will see in chapter 13, moral heroism rarely arises from super-duper frontal cortical willpower. Instead, it happens when the right thing isn’t the harder thing.





Marshmallows


			The frontal cortex and its increasing connectivity with the rest of the brain anchors the neurobiology of kids’ growing sophistication, most importantly in their capacity to regulate emotions and behavior. The most iconic demonstration of this revolves around an unlikely object—the marshmallow.20

			In the 1960s Stanford psychologist Walter Mischel developed the “marshmallow test” to study gratification postponement. A child is presented with a marshmallow. The experimenter says, “I’m going out of the room for a while. You can eat the marshmallow after I leave. But if you wait and don’t eat it until I get back, I’ll give you another marshmallow,” and leaves. And the child, observed through a two-way mirror, begins the lonely challenge of holding out for fifteen minutes until the researcher returns.

			Studying hundreds of three- to six-year-olds, Mischel saw enormous variability—a few ate the marshmallow before the experimenter left the room. About a third lasted the fifteen minutes. The rest were scattered in between, averaging a delay of eleven minutes. Kids’ strategies for resisting the marshmallow’s siren call differed, as can be seen on contemporary versions of the test on YouTube. Some kids cover their eyes, hide the marshmallow, sing to distract themselves. Others grimace, sit on their hands. Others sniff the marshmallow, pinch off an infinitely tiny piece to eat, hold it reverentially, kiss it, pet it.

			Various factors modulated kids’ fortitude (shown in later studies described in Mischel’s book where, for some reason, it was pretzels instead of marshmallows). Trusting the system mattered—if experimenters had previously betrayed on promises, kids wouldn’t wait as long. Prompting kids to think about how crunchy and yummy pretzels are (what Mischel calls “hot ideation”) nuked self-discipline; prompts to think about a “cold ideation” (e.g., the shape of pretzels) or an alternative hot ideation (e.g., ice cream) bolstered resistance.

			As expected, older kids hold out longer, using more effective strategies. Younger kids describe strategies like “I kept thinking about how good that second marshmallow would taste.” The problem, of course, is that this strategy is about two synapses away from thinking about the marshmallow in front of you. In contrast, older kids use strategies of distraction—thinking about toys, pets, their birthday. This progresses to reappraisal strategies (“This isn’t about marshmallows. This is about the kind of person I am”). To Mischel, maturation of willpower is more about distraction and reappraisal strategies than about stoicism.

			So kids improve at delayed gratification. Mischel’s next step made his studies iconic—he tracked the kids afterward, seeing if marshmallow wait time predicted anything about their adulthoods.

			Did it ever. Five-year-old champs at marshmallow patience averaged higher SAT scores in high school (compared with those who couldn’t wait), with more social success and resilience and less aggressive* and oppositional behavior. Forty years postmarshmallow, they excelled at frontal function, had more PFC activation during a frontal task, and had lower BMIs.21 A gazillion-dollar brain scanner doesn’t hold more predictive power than one marshmallow. Every anxious middle-class parent obsesses over these findings, has made marshmallows fetish items.





CONSEQUENCES


			We’ve now gotten a sense of various domains of behavioral development. Time to frame things with this book’s central question. Our adult has carried out that wonderful or crummy or ambiguous behavior. What childhood events contributed to that occurring?

			A first challenge is to truly incorporate biology into our thinking. A child suffers malnutrition and, as an adult, has poor cognitive skills. That’s easy to frame biologically—malnutrition impairs brain development. Alternatively, a child is raised by cold, inexpressive parents and, as an adult, feels unlovable. It’s harder to link those two biologically, to resist thinking that somehow this is a less biological phenomenon than the malnutrition/cognition link. There may be less known about the biological changes explaining the link between the cold parents and the adult with poor self-esteem than about the malnutrition/cognition one. It may be less convenient to articulate the former biologically than the latter. It may be harder to apply a proximal biological therapy for the former than for the latter (e.g., an imaginary neural growth factor drug that improves self-esteem versus cognition). But biology mediates both links. A cloud may be less tangible than a brick, but it’s constructed with the same rules about how atoms interact.

			How does biology link childhood with the behaviors of adulthood? Chapter 5’s neural plasticity writ large and early. The developing brain epitomizes neural plasticity, and every hiccup of experience has an effect, albeit usually a miniscule one, on that brain.

			We now examine ways in which different types of childhoods produce different sorts of adults.





LET’S START AT THE VERY BEGINNING: THE IMPORTANCE OF MOTHERS


			Nothing like a section heading stating the obvious. Everybody needs a mother. Even rodents; separate rat pups from Mom a few hours daily and, as adults, they have elevated glucocorticoid levels and poor cognitive skills, are anxious, and, if male, are more aggressive.22 Mothers are crucial. Except that well into the twentieth century, most experts didn’t think so. The West developed child-rearing techniques where, when compared with traditional cultures, children had less physical contact with their mothers, slept alone at earlier ages, and had longer latencies to be picked up when crying. Around 1900 the leading expert Luther Holt of Columbia University warned against the “vicious practice” of picking up a crying child or handling her too often. This was the world of children of the wealthy, raised by nannies and presented to their parents before bedtime to be briefly seen but not heard.

			This period brought one of history’s strangest one-night stands, namely when the Freudians and the behaviorists hooked up to explain why infants become attached to their mothers. To behaviorists, obviously, it’s because mothers reinforce them, providing calories when they’re hungry. For Freudians, also obviously, infants lack the “ego development” to form a relationship with anything/anyone other than Mom’s breasts. When combined with children-should-be-seen-but-not-heard-ism, this suggested that once you’ve addressed a child’s need for nutrition, proper temperature, plus other odds and ends, they’re set to go. Affection, warmth, physical contact? Superfluous.

			Such thinking produced at least one disaster. When a child was hospitalized for a stretch, dogma was that the mother was unnecessary—she just added emotional tumult, and everything essential was supplied by the staff. Typically, mothers could visit their children once a week for a few minutes. And when kids were hospitalized for extended periods, they wasted away with “hospitalism,” dying in droves from nonspecific infections and gastrointestinal maladies unrelated to their original illness.23 This was an era when the germ theory had mutated into the belief that hospitalized children do best when untouched, in antiseptic isolation. Remarkably, hospitalism soared in hospitals with newfangled incubators (adapted from poultry farming); the safest hospitals were poor ones that relied on the primitive act of humans actually touching and interacting with infants.

			In the 1950s the British psychiatrist John Bowlby challenged the view of infants as simple organisms with few emotional needs; his “attachment theory” birthed our modern view of the mother-infant bond.*24 In his trilogy Attachment and Loss, Bowlby summarized the no-brainer answers we’d give today to the question “What do children need from their mothers?”: love, warmth, affection, responsiveness, stimulation, consistency, reliability. What is produced in their absence? Anxious, depressed, and/or poorly attached adults.*

			Bowlby inspired one of the most iconic experiments in psychology’s history, by Harry Harlow of the University of Wisconsin; it destroyed Freudian and behaviorist dogma about mother-infant bonding.25 Harlow would raise an infant rhesus monkey without a mother but with two “surrogates” instead. Both were made of a chicken-wire tube approximating a torso, with a monkey-ish plastic head on top. One surrogate had a bottle of milk coming from its “torso.” The other had terry cloth wrapped around the torso. In other words, one gave calories, the other a poignant approximation of a mother monkey’s fur. Freud and B. F. Skinner would have wrestled over access to chicken-wire mom. But infant monkeys chose the terry-cloth mom.* “Man cannot live by milk alone. Love is an emotion that does not need to be bottle- or spoon-fed,” wrote Harlow.



			Evidence for the most basic need provided by a mother comes from a controversial quarter. Starting in the 1990s, crime rates plummeted across the United States. Why? For liberals the answer was the thriving economy. For conservatives it was the larger budgets for policing, expanded prisons, and three-strikes sentencing laws. Meanwhile, a partial explanation was provided by legal scholar John Donohue of Stanford and economist Steven Levitt of the University of Chicago—it was the legalization of abortions. The authors’ state-by-state analysis of the liberalization of abortion laws and the demographics of the crime drop showed that when abortions become readily available in an area, rates of crime by young adults decline about twenty years later. Surprise—this was highly controversial, but it makes perfect, depressing sense to me. What majorly predicts a life of crime? Being born to a mother who, if she could, would have chosen that you not be. What’s the most basic thing provided by a mother? Knowing that she is happy that you exist.*26

			Harlow also helped demonstrate a cornerstone of this book, namely what mothers (and later peers) provide as children grow. To do so, he performed some of the most inflammatory research in psychology’s history. This involved raising infant monkeys in isolation, absent mother or peers; they spent the first months, even years, of their lives without contact with another living being, before being placed in a social group.*

			Predictably, they’d be wrecks. Some would sit alone, clutching themselves, rocking “autistically.” Others would be markedly inappropriate in their hierarchical or sexual behaviors.

			There was something interesting. It wasn’t that these ex-isolates did behaviors wrong—they didn’t aggressively display like an ostrich, make the sexually solicitive gestures of a gecko. Behaviors were normal but occurred at the wrong time and place—say, giving subordination gestures to pipsqueaks half their size, threatening alphas they should cower before. Mothers and peers don’t teach the motoric features of fixed action patterns; those are hardwired. They teach when, where, and to whom—the appropriate context for those behaviors. They give the first lessons about when touching someone’s arm or pulling a trigger can be among the best or worst of our behaviors.

			I saw a striking example of this among the baboons that I study in Kenya, when both a high-ranking and a low-ranking female gave birth to daughters the same week. The former’s kid hit every developmental landmark earlier than the other, the playing field already unlevel. When the infants were a few weeks old, they nearly had their first interaction. Daughter of subordinate mom spotted daughter of dominant one, toddled over to say hello. And as she got near, her low-ranking mother grabbed her by the tail and pulled her back.

			This was her first lesson about her place in that world. “You see her? She’s much higher ranking than you, so you don’t just go and hang with her. If she’s around, you sit still and avoid eye contact and hope she doesn’t take whatever you’re eating.” Amazingly, in twenty years those two infants would be old ladies, sitting in the savanna, still displaying the rank asymmetries they learned that morning.





ANY KIND OF MOTHER IN A STORM


			Harlow provided another important lesson, thanks to another study painful to contemplate. Infant monkeys were raised with chicken-wire surrogates with air jets in the middle of their torsos. When an infant clung, she’d receive an aversive blast of air. What would a behaviorist predict that the monkey would do when faced with such punishment? Flee. But, as in the world of abused children and battered partners, infants held harder.

			Why do we often become attached to a source of negative reinforcement, seek solace when distressed from the cause of that distress? Why do we ever love the wrong person, get abused, and return for more?

			Psychological insights abound. Because of poor self-esteem, believing you’ll never do better. Or a codependent conviction that it’s your calling to change the person. Maybe you identify with your oppressor, or have decided it’s your fault and the abuser is justified, so they seem less irrational and terrifying. These are valid and can have huge explanatory and therapeutic power. But work by Regina Sullivan of NYU demonstrates bits of this phenomenon miles from the human psyche.

			Sullivan would condition rat pups to associate a neutral odor with a shock.27 If a pup that had been conditioned at ten days of age or older (“older pups”) was exposed to that odor, logical things happened—amygdala activation, glucocorticoid secretion, and avoidance of the odor. But do the same to a younger pup and none of that would occur; remarkably, the pup would be attracted to the odor.

			Why? There is an interesting wrinkle related to stress in newborns. Rodent fetuses are perfectly capable of secreting glucocorticoids. But within hours of birth, the adrenal glands atrophy dramatically, becoming barely able to secrete glucocorticoids. This “stress hyporesponsive period” (SHRP) wanes over the coming weeks.28

			What is the SHRP about? Glucocorticoids have so many adverse effects on brain development (stay tuned) that the SHRP represents a gamble—“I won’t secrete glucocorticoids in response to stress, so that I develop optimally; if something stressful happens, Mom will handle it for me.” Accordingly, deprive infant rats of their mothers, and within hours their adrenals expand and regain the ability to secrete plenty of glucocorticoids.

			During the SHRP infants seem to use a further rule: “If Mom is around (and I thus don’t secrete glucocorticoids), I should get attached to any strong stimulus. It couldn’t be bad for me; Mom wouldn’t allow that.” As evidence, inject glucocorticoids into the amygdalae of young pups during the conditioning, and the amygdalae would activate and the pups would develop an aversion to the odor. Conversely, block glucocorticoid secretion in older pups during conditioning, and they’d become attracted to the odor. Or condition them with their mother present, and they wouldn’t secrete glucocorticoids and would develop an attraction. In other words, in young rats even aversive things are reinforcing in Mom’s presence, even if Mom is the source of the aversive stimuli. As Sullivan and colleagues wrote, “attachment [by such an infant] to the caretaker has evolved to ensure that the infant forms a bond to that caregiver regardless of the quality of care received.” Any kind of mother in a storm.

			If this applies to humans, it helps explain why individuals abused as kids are as adults prone toward relationships in which they are abused by their partner.29 But what about the flip side? Why is it that about 33 percent of adults who were abused as children become abusers themselves?

			Again, useful psychological insights abound, built around identification with the abuser and rationalizing away the terror: “I love my kids, but I smack them around when they need it. My father did that to me, so he could have loved me too.” But once again something biologically deeper also occurs—infant monkeys abused by their mothers are more likely to become abusive mothers.30





DIFFERENT ROUTES TO THE SAME PLACE


			I anticipated that, with mothers now covered, we’d next examine the adult consequences of, say, paternal deprivation, or childhood poverty, or exposure to violence or natural disasters. And there’d be the same question—what specific biological changes did each cause in children that increased the odds of specific adult behaviors?

			But this plan didn’t work—the similarities of effects of these varied traumas are greater than the differences. Sure, there are specific links (e.g., childhood exposure to domestic violence makes adult antisocial violence more likely than does childhood exposure to hurricanes). But they all converge sufficiently that I will group them together, as is done in the field, as examples of “childhood adversity.”

			Basically, childhood adversity increases the odds of an adult having (a) depression, anxiety, and/or substance abuse; (b) impaired cognitive capabilities, particularly related to frontocortical function; (c) impaired impulse control and emotion regulation; (d) antisocial behavior, including violence; and (e) relationships that replicate the adversities of childhood (e.g., staying with an abusive partner).31 And despite that, some individuals endure miserable childhoods just fine. More on this to come.

			We’ll now examine the biological links between childhood adversity and increased risk of these adult outcomes.





THE BIOLOGICAL PROFILE


			All these forms of adversity are obviously stressful and cause abnormalities in stress physiology. Across numerous species, major early-life stressors produce both kids and adults with elevated levels of glucocorticoids (along with CRH and ACTH, the hypothalamic and pituitary hormones that regulate glucocorticoid release) and hyperactivity of the sympathetic nervous system.32 Basal glucocorticoid levels are elevated—the stress response is always somewhat activated—and there is delayed recovery back to baseline after a stressor. Michael Meaney of McGill University has shown how early-life stress permanently blunts the ability of the brain to rein in glucocorticoid secretion.

			As covered in chapter 4, marinating the brain in excess glucocorticoids, particularly during development, adversely effects cognition, impulse control, empathy, and so on.33 There is impaired hippocampal-dependent learning in adulthood. For example, abused children who develop PTSD have decreased volume of the hippocampus in adulthood. Stanford psychiatrist Victor Carrion has shown decreased hippocampal growth within months of the abuse. As a likely cause, glucocorticoids decrease hippocampal production of the growth factor BDNF (brain-derived neurotrophic factor).

			So childhood adversity impairs learning and memory. Crucially, it also impairs maturation and function of the frontal cortex; again, glucocorticoids, via inhibiting BDNF, are likely culprits.

			The connection between childhood adversity and frontocortical maturation pertains to childhood poverty. Work by Martha Farah of the University of Pennsylvania, Tom Boyce of UCSF, and others demonstrates something outrageous: By age five, the lower a child’s socioeconomic status, on the average, the (a) higher the basal glucocorticoid levels and/or the more reactive the glucocorticoid stress response, (b) the thinner the frontal cortex and the lower its metabolism, and (c) the poorer the frontal function concerning working memory, emotion regulation, impulse control, and executive decision making; moreover, to achieve equivalent frontal regulation, lower-SES kids must activate more frontal cortex than do higher-SES kids. In addition, childhood poverty impairs maturation of the corpus callosum, a bundle of axonal fibers connecting the two hemispheres and integrating their function. This is so wrong—foolishly pick a poor family to be born into, and by kindergarten, the odds of your succeeding at life’s marshmallow tests are already stacked against you.34

			Considerable research focuses on how poverty “gets under the skin.” Some mechanisms are human specific—if you’re poor, you’re more likely to grow up near environmental toxins,*35 in a dangerous neighborhood with more liquor stores than markets selling produce; you’re less likely to attend a good school or have parents with time to read to you. Your community is likely to have poor social capital, and you, poor self-esteem. But part of the link reflects the corrosive effects of subordination in all hierarchical species. For example, having a low-ranking mother predicts elevated glucocorticoids in adulthood in baboons.36

			Thus, childhood adversity can atrophy and blunt the functioning of the hippocampus and frontal cortex. But it’s the opposite in the amygdala—lots of adversity and the amygdala becomes larger and hyperreactive. One consequence is increased risk of anxiety disorders; when coupled with the poor frontocortical development, it explains problems with emotion and behavior regulation, especially impulse control.37

			Childhood adversity accelerates amygdaloid maturation in a particular way. Normally, around adolescence the frontal cortex gains the ability to inhibit the amygdala, saying, “I wouldn’t do this if I were you.” But after childhood adversity, the amygdala develops the ability to inhibit the frontal cortex, saying, “I’m doing this and just try to stop me.”

			Childhood adversity also damages the dopamine system (with its role in reward, anticipation, and goal-directed behavior) in two ways.

			First, early adversity produces an adult organism more vulnerable to drug and alcohol addiction. The pathway to this vulnerability is probably threefold: (a) effects on the developing dopamine system; (b) the excessive adult exposure to glucocorticoids, which increases drug craving; (c) that poorly developed frontal cortex.38

			Childhood adversity also substantially increases the risk of adult depression. Depression’s defining symptom is anhedonia, the inability to feel, anticipate, or pursue pleasure. Chronic stress depletes the mesolimbic system of dopamine, generating anhedonia.* The link between childhood adversity and adult depression involves both organizational effects on the developing mesolimbic system and elevated adult glucocorticoid levels, which can deplete dopamine.39

			Childhood adversity increases depression risk via “second hit” scenarios—lowering thresholds so that adult stressors that people typically manage instead trigger depressive episodes. This vulnerability makes sense. Depression is fundamentally a pathological sense of loss of control (explaining the classic description of depression as “learned helplessness”). If a child experiences severe, uncontrollable adversity, the most fortunate conclusion in adulthood is “Those were terrible circumstances over which I had no control.” But when childhood traumas produce depression, there is cognitively distorted overgeneralization: “And life will always be uncontrollably awful.”





TWO SIDE TOPICS


			So varied types of childhood adversity converge in producing similar adult problems. Nonetheless, two types of adversity should be considered separately.





Observing Violence


			What happens when children observe domestic violence, warfare, a gang murder, a school massacre? For weeks afterward there is impaired concentration and impulse control. Witnessing gun violence doubles a child’s likelihood of serious violence within the succeeding two years. And adulthood brings the usual increased risks of depression, anxiety, and aggression. Consistent with that, violent criminals are more likely than nonviolent ones to have witnessed violence as kids.*40

			This fits our general picture of childhood adversity. A separate topic is the effects of media violence on kids.

			Endless studies have analyzed the effects of kids witnessing violence on TV, in movies, in the news, and in music videos, and both witnessing and participating in violent video games. A summary:

			Exposing children to a violent TV or film clip increases their odds of aggression soon after.41 Interestingly, the effect is stronger in girls (amid their having lower overall levels of aggression). Effects are stronger when kids are younger or when the violence is more realistic and/or is presented as heroic. Such exposure can make kids more accepting of aggression—in one study, watching violent music videos increased adolescent girls’ acceptance of dating violence. The violence is key—aggression isn’t boosted by material that’s merely exciting, arousing, or frustrating.

			Heavy childhood exposure to media violence predicts higher levels of aggression in young adults of both sexes (“aggression” ranging from behavior in an experimental setting to violent criminality). The effect typically remains after controlling for total media-watching time, maltreatment or neglect, socioeconomic status, levels of neighborhood violence, parental education, psychiatric illness, and IQ. This is a reliable finding of large magnitude. The link between exposure to childhood media violence and increased adult aggression is stronger than the link between lead exposure and IQ, calcium intake and bone mass, or asbestos and laryngeal cancer.

			Two caveats: (a) there is no evidence that catastrophically violent individuals (e.g., mass shooters) are that way because of childhood exposure to violent media; (b) exposure does not remotely guarantee increased aggression—instead, effects are strongest on kids already prone toward violence. For them, exposure desensitizes and normalizes their own aggression.*





Bullying


			Being bullied is mostly another garden-variety childhood adversity, with adult consequences on par with childhood maltreatment at home.42

			There is a complication, though. As most of us observed, exploited, or experienced as kids, bullying targets aren’t selected at random. Kids with the metaphorical “kick me” signs on their backs are more likely to have personal or family psychiatric issues and poor social and emotional intelligence. These are kids already at risk for bad adult outcomes, and adding bullying to the mix just makes the child’s future even bleaker.

			The picture of the bullies is no surprise either, starting with their disproportionately coming from families of single moms or younger parents with poor education and employment prospects. There are generally two profiles of the kids themselves—the more typical is an anxious, isolated kid with poor social skills, who bullies out of frustration and to achieve acceptance. Such kids typically mature out of bullying. The second profile is the confident, unempathic, socially intelligent kid with an imperturbable sympathetic nervous system; this is the future sociopath.

			There is an additional striking finding. You want to see a kid who’s really likely to be a mess as an adult? Find someone who both bullies and is bullied, who terrorizes the weaker at school and returns home to be terrorized by someone stronger.43 Of the three categories (bully, bullied, bully/bullied), they’re most likely to have prior psychiatric problems, poor school performance, and poor emotional adjustment. They’re more likely than pure bullies to use weapons and inflict serious damage. As adults, they’re most at risk for depression, anxiety, and suicidality.

			In one study kids from these three categories read scenarios of bullying.44 Bullied victims would condemn bullying and express sympathy. Bullies would condemn bullying but rationalize the scenario (e.g., this time it was the victim’s fault). And bully/bullied kids? They would say bullying is okay. No wonder they have the worst outcome. “The weak deserve to be bullied, so it’s fine when I bully. But that means I deserve to be bullied at home. But I don’t, and that relative bullying me is awful. Maybe then I’m awful when I bully someone. But I’m not, because the weak deserve to be bullied. . . .” A Möbius strip from hell.*





A KEY QUESTION


			We’ve now examined adult consequences of childhood adversity and their biological mediators. A key question persists. Yes, childhood abuse increases the odds of being an abusive adult; witnessing violence raises the risk for PTSD; loss of a parent to death means more chance of adult depression. Nevertheless, many, maybe even most victims of such adversity turn into reasonably functional adults. There is a shadow over childhood, demons lurk in corners of the mind, but overall things are okay. What explains such resilience?

			As we’ll see, genes and fetal environment are relevant. But most important, recall the logic of collapsing different types of trauma into a single category. What counts is the sheer number of times a child is bludgeoned by life and the number of protective factors. Be sexually abused as a child, or witness violence, and your adult prognosis is better than if you had experienced both. Experience childhood poverty, and your future prospects are better if your family is stable and loving than broken and acrimonious. Pretty straightforwardly, the more categories of adversities a child suffers, the dimmer his or her chances of a happy, functional adulthood.45





A SLEDGEHAMMER


			What happens when everything goes wrong—no mother or family, minimal peer interactions, sensory and cognitive neglect, plus some malnutrition?46



			These are the Romanian institution kids, poster children for just how nightmarish childhood can be. In the 1980s the Romanian dictator Nicolae Ceauşescu banned contraceptives and abortions and required women to bear at least five children. Soon institutions filled with thousands of infants and kids abandoned by impoverished families (many intent on reclaiming their child when finances improved).* Kids were warehoused in overwhelmed institutions, resulting in severe neglect and deprivation. The story broke after Ceauşescu’s 1989 overthrow. Many kids were adopted by Westerners, and international attention led to some improvements in the institutions. Since then, children adopted in the West, those eventually returned to their families, and those who remained institutionalized have been studied, primarily by Charles Nelson of Harvard.

			As adults, these kids are mostly what you’d expect. Low IQ and poor cognitive skills. Problems with forming attachments, often bordering on autistic. Anxiety and depression galore. The longer the institutionalization, the worse the prognosis.

			And their brains? Decreased total brain size, gray matter, white matter, frontal cortical metabolism, connectivity between regions, sizes of individual brain regions. Except for the amygdala. Which is enlarged. That pretty much says it all.





CULTURE, WITH BOTH A BIG AND A LITTLE C


			Chapter 9 considers the effects of culture on our best and worst behaviors. We now preview that chapter, focusing on two facts—childhood is when culture is inculcated, and parents mediate that process.

			There is huge cultural variability in how childhood is experienced—how long and often kids are nursed; how often they are in contact with parents and other adults; how often they’re spoken to; how long they cry before someone responds; at what age they sleep alone.

			Considering cross-cultural child rearing often brings out the most invidious and neurotic in parents—do other cultures do a better job at it? There must be the perfect combo out there, a mixture of the Kwakiutl baby diet, the Trobriand sleeping program, and the Ituri approach to watching Baby Mozart videos. But there is no anthropological ideal of child rearing. Cultures (starting with parents) raise children to become adults who behave in the ways valued by that culture, a point emphasized by the anthropologist Meredith Small of Cornell University.47

			We begin with parenting style, a child’s first encounter with cultural values. Interestingly, the most influential typology of parenting style, writ small, grew from thinking about cultural styles, writ large.

			Amid the post–World War II ruins, scholars tried to understand where Hitler, Franco, Mussolini, Tojo, and their minions came from. What are the roots of fascism? Two particularly influential scholars were refugees from Hitler, namely Hannah Arendt (with her 1951 book The Origins of Totalitarianism) and Theodor Adorno (with the 1950 book The Authoritarian Personality, coauthored with Else Frenkel-Brunswik, Daniel Levinson, and Nevitt Sanford). Adorno in particular explored the personality traits of fascists, including extreme conformity, submission to and belief in authority, aggressiveness, and hostility toward intellectualism and introspection—traits typically rooted in childhood.48

			This influenced the Berkeley psychologist Diana Baumrind, who in the 1960s identified three key parenting styles (in work since replicated and extended to various cultures).49 First is authoritative parenting. Rules and expectations are clear, consistent, and explicable—“Because I said so” is anathema—with room for flexibility; praise and forgiveness trump punishment; parents welcome children’s input; developing children’s potential and autonomy is paramount. By the standards of the educated neurotics who would read (let alone write . . .) this book, this produces a good adult outcome—happy, emotionally and socially mature and fulfilled, independent and self-reliant.

			Next is authoritarian parenting. Rules and demands are numerous, arbitrary, and rigid and need no justification; behavior is mostly shaped by punishment; children’s emotional needs are low priorities. Parental motivation is often that it’s a tough, unforgiving world and kids better be prepared. Authoritarian parenting tends to produce adults who may be narrowly successful, obedient, conformist (often with an undercurrent of resentment that can explode), and not particularly happy. Moreover, social skills are often poor because, instead of learning by experience, they grew up following orders.

			And then there is permissive parenting, the aberration that supposedly let Boomers invent the 1960s. There are few demands or expectations, rules are rarely enforced, and children set the agenda. Adult outcome: self-indulgent individuals with poor impulse control, low frustration tolerance, plus poor social skills thanks to living consequence-free childhoods.

			Baumrind’s trio was expanded by Stanford psychologists Eleanor Maccoby and John Martin to include neglectful parenting.50 This addition produces a two-by-two matrix: parenting is authoritative (high demand, high responsiveness), authoritarian (high demand, low responsiveness), permissive (low demand, high responsiveness), or neglectful (low demand, low responsiveness).

			Importantly, each style usually produces adults with that same approach, with different cultures valuing different styles.

			Then comes the next way cultural values are transmitted to kids, namely by peers. This was emphasized in Judith Rich Harris’s The Nurture Assumption. Harris, a psychologist without an academic affiliation or doctorate, took the field by storm, arguing that the importance of parenting in shaping a child’s adult personality is exaggerated.51 Instead, once kids pass a surprisingly young age, peers are most influential. Elements of her argument included: (a) Parental influence is often actually mediated via peers. For example, being raised by a single mother increases the risk of adult antisocial behavior, but not because of the parenting; instead, because of typically lower income, kids more likely live in a neighborhood with tough peers. (b) Peers have impact on linguistic development (e.g., children acquire the accent of their peers, not their parents). (c) Other young primates are mostly socialized by peers, not mothers.

			The book was controversial (partially because the theme begged to be distorted—“Psychologist proves that parents don’t matter”), drawing criticism and acclaim.* As the dust has settled, current opinion tends to be that peer influences are underappreciated, but parents still are plenty important, including by influencing what peer groups their kids experience.

			Why are peers so important? Peer interactions teach social competence—context-dependent behavior, when to be friend or foe, where you fit in hierarchies. Young organisms utilize the greatest teaching tool ever to acquire such information—play.52

			What is social play in the young? Writ large, it’s an array of behaviors that train individuals in social competence. Writ medium, it’s fragments of the real thing, bits and pieces of fixed action patterns, a chance to safely try out roles and improve motor skills. Writ small and endocrine, it’s a demonstration that moderate and transient stress—“stimulation”—is great. Writ small and neurobiological, it’s a tool for deciding which excess synapses to prune.

			The historian Johan Huizinga characterized humans as “Homo Ludens,” Man the Player, with our structured, rule-bound play—i.e., games. Nevertheless, play is universal among socially complex species, ubiquitous among the young and peaking at puberty, and all play involves similar behaviors, after some ethological translating (e.g., a dominant dog signals the benevolence needed to initiate play by crouching, diminishing herself; translated into baboon, a dominant kid presents her rear to someone lower ranking).

			Play is vital. In order to play, animals forgo foraging, expend calories, make themselves distracted and conspicuous to predators. Young organisms squander energy on play during famines. A child deprived of or disinterested in play rarely has a socially fulfilling adult life.

			Most of all, play is intrinsically pleasurable—why else perform a smidgen of a behavioral sequence in an irrelevant setting? Dopaminergic pathways activate during play; juvenile rats, when playing, emit the same vocalizations as when rewarded with food; dogs spend half their calories wagging their tails to pheromonally announce their presence and availability for play. As emphasized by the psychiatrist Stuart Brown, founder of the National Institute for Play, the opposite of play is not work—it’s depression. A challenge is to understand how the brain codes for the reinforcing properties of the variety of play. After all, play encompasses everything from mathematicians besting each other with hilarious calculus jokes to kids besting each other by making hilarious fart sounds with their armpits.

			One significant type of play involves fragments of aggression, what Harlow called “rough and tumble” play—kids wrestling, adolescent impalas butting heads, puppies play-biting each other.53 Males typically do it more than females, and as we’ll see soon, it’s boosted by prenatal testosterone. Is rough-and-tumble play practice for life’s looming status tournament, or are you already in the arena? A mixture of both.

			Expanding beyond peers, neighborhoods readily communicate culture to kids. Is there garbage everywhere? Are houses decrepit? What’s ubiquitous—bars, churches, libraries, or gun shops? Are there many parks, and are they safe to enter? Do billboards, ads, and bumper stickers sell religious or material paradises, celebrate acts of martyrdom or kindness and inclusiveness?

			—

			And then we get to culture at the level of tribes, nations, and states. Here, briefly, are some of the broadest cultural differences in child-rearing practices.





Collectivist Versus Individualist Cultures


			As will be seen in chapter 9, this is the most studied cultural contrast, typically comparing collectivist East Asian cultures with überindividualist America. Collectivist cultures emphasize interdependence, harmony, fitting in, the needs and responsibilities of the group; in contrast, individualist cultures value independence, competition, the needs and rights of the individual.

			On average, mothers in individualist cultures, when compared with those in collectivist ones, speak louder, play music louder, have more animated expressions.54 They view themselves as teachers rather than protectors, abhor a bored child, value high-energy affect. Their games emphasize individual competition, urge hobbies involving doing rather than observing. Kids are trained in verbal assertiveness, to be autonomous and influential. Show a cartoon of a school of fish with one out front, and she’ll describe it to her child as the leader.*

			Mothers in collectivist cultures, in contrast, spend more time than individualist mothers soothing their child, maintaining contact, and facilitating contact with other adults. They value low arousal affect and sleep with their child to a later age. Games are about cooperation and fitting in; if playing with her child with, say, a toy car, the point is not exploring what a car does (i.e., being automobile), but the process of sharing (“Thank you for giving me your car; now I’ll give it back to you”). Kids are trained to get along, think of others, accept and adapt, rather than change situations; morality and conformity are nearly synonymous. Show the cartoon of the school of fish, and the fish out front must have done something wrong, because no one will play with him.

			Logically, kids in individualist cultures acquire ToM later than collectivist-culture kids and activate pertinent circuits more to achieve the same degree of competence. For a collectivist child, social competence is all about taking someone else’s perspective.55

			Interestingly, kids in (collectivist) Japan play more violent video games than do American kids, yet are less aggressive. Moreover, exposing Japanese kids to media violence boosts aggression less than in American kids.56 Why the difference? Three possible contributing factors: (a) American kids play alone more often, a lone-wolf breeding ground; (b) Japanese kids rarely have a computer or TV in their bedroom, so they play near their parents; (c) Japanese video-game violence is more likely to have prosocial, collectivist themes.

			More in chapter 9 on collectivist versus individualist cultures.





Cultures of Honor


			These cultures emphasize rules of civility, courtesy, and hospitality. Taking retribution is expected for affronts to the honor of one’s self, family, or clan; failing to do so is shameful. These are cultures filled with vendettas, revenge, and honor killings; cheeks aren’t turned. A classic culture of honor is the American South, but as we’ll see in chapter 9, such cultures occur worldwide and with certain ecological correlates. A particularly lethal combo is when a culture of victimization—we were wronged last week, last decade, last millennium—is coupled with a culture of honor’s ethos of retribution.

			Parenting in cultures of honor tends to be authoritarian.57 Kids are aggressive, particularly following honor violations, and staunchly endorse aggressive responses to scenarios of honor violation.





Class Differences


			As noted, an infant baboon learns her place in the hierarchy from her mother. A human child’s lessons about status are more complex—there is implicit cuing, subtle language cues, the cognitive and emotional weight of remembering the past (“When your grandparents emigrated here they couldn’t even . . .”) and hoping about the future (“When you grow up, you’re going to . . .”). Baboon mothers teach their young appropriate behavioral context; human parents teach their young what to bother dreaming about.

			Class differences in parenting in Western countries resemble parenting differences between Western countries and those in the developing world. In the West a parent teaches and facilitates her child exploring the world. In the toughest corners of the developing world, little more is expected than the awesome task of keeping your child alive and buffered from the menacing world.*

			In Western cultures, class differences in parenting sort by Baumrind’s typologies. In higher-SES strata, parenting tends to be authoritative or permissive. In contrast, parenting in society’s lower-SES rungs is typically authoritarian, reflecting two themes. One concerns protecting. When are higher-SES parents authoritarian? When there is danger. “Sweetie, I love that you question things, but if you run into the street and I scream ‘Stop,’ you stop.” A lower-SES childhood is rife with threat. The other theme is preparing the child for the tough world out there—for the poor, adulthood consists of the socially dominant treating them in an authoritarian manner.

			Class differences in parenting were explored in a classic study by the anthropologist Adrie Kusserow of St. Michael’s College, who did fieldwork observing parents in three tribes—wealthy families on Manhattan’s Upper East Side; a stable, blue-collar community; and a poor, crime-ridden one (the last two both in Queens).58 The differences were fascinating.

			Parenting in the poor neighborhood involved “hard defensive individualism.” The neighborhood was rife with addiction, homelessness, incarceration, death—and parents’ aim was to shelter their child from the literal and metaphorical street. Their speech was full of metaphors about not losing what was achieved—standing your ground, keeping up your pride, not letting others get under your skin. Parenting was authoritarian, toughening the goal. For example, parents teased kids far more than in the other neighborhoods.

			In contrast, working-class parenting involved “hard offensive individualism.” Parents had some socioeconomic momentum, and kids were meant to maintain that precarious trajectory. Parents’ speech about their hopes for their kids contained images of movement, progress, and athletics—getting ahead, testing the waters, going for the gold. With hard work and the impetus of generations of expectations, your child might pioneer landfall in the middle class.

			Parenting in both neighborhoods emphasized respect for authority, particularly within the family. Moreover, kids were fungible members of a category, rather than individualized—“You kids get over here.”

			Then there was the “soft individualism” of upper-middle-class parenting.* Children’s eventual success, by conventional standards, was a given, as were expectations of physical health. Far more vulnerable was a child’s psychological health; when children could become anything, parents’ responsibility was to facilitate their epic journey toward an individuated “fulfillment.” Moreover, the image of fulfillment was often postconventional—“I hope my child will never work an unsatisfying job just for money.” This, after all, is a tribe giddied by tales of the shark in line to become CEO chucking it to learn carpentry or oboe. Parents’ speech brimmed with metaphors of potential being fulfilled—flowering, blooming, growing, blossoming. Parenting was authoritative or permissive, riddled with ambivalence about parent-child power differentials. Rather than “You kids, clean up this mess,” there’d be the individuated, justifying request—“Caitlin, Zach, Dakota, could you clean things up a bit please? Malala is coming for dinner.”*

			We’ve now seen how childhood events—from the first mother-infant interaction to the effects of culture—have persistent influences, and how biology mediates such influences. When combined with the preceding chapters, we have finished our tour of environmental effects on behavior, from the second before a behavior occurs to a second after birth. In effect, we’ve done “environment”; time for next chapter’s “genes.”

			But this ignores something crucial: environment doesn’t begin at birth.





NINE LONG MONTHS


			The Cat in the Hat in the Womb


			The existence of prenatal environmental influences caught the public’s imagination with some charming studies demonstrating that near-term fetuses hear (what’s going on outside the womb), taste (amniotic fluid), and remember and prefer those stimuli after birth.

			This was shown experimentally—inject lemon-flavored saline into a pregnant rat’s amniotic fluid, and her pups are born preferring that flavor. Moreover, some spices consumed by pregnant women get into amniotic fluid. Thus we may be born preferring foods our mothers ate during pregnancy—pretty unorthodox cultural transmission.59

			Prenatal effects can also be auditory, as shown by inspired research by Anthony DeCasper of the University of North Carolina.60 A pregnant woman’s voice is audible in the womb, and newborns recognize and prefer the sound of their mother’s voice.* DeCasper used ethology’s playbook to show this: A newborn can learn to suck a pacifier in two different patterns of long and short sucks. Generate one pattern, and you hear Mom’s voice; the other, another woman’s voice. Newborns want Mom’s voice. Elements of language are also learned in utero—the contours of a newborn’s cry are similar to the contours of speech in the mother’s language.

			The cognitive capacities of near-term fetuses are even more remarkable. For example, fetuses can distinguish between two pairs of nonsense syllables (“biba” versus “babi”). How do you know? Get this—Mom says “Biba, biba, biba” repeatedly while fetal heart rate is monitored. “Boring (or perhaps lulling),” thinks the fetus, and heart rate slows. Then Mom switches to “babi.” If the fetus doesn’t distinguish between the two, heart rate deceleration continues. But if the difference is noted—“Whoa, what happened?”—heart rate increases. Which is what DeCasper reported.61

			DeCasper and colleague Melanie Spence then showed (using the pacifier-sucking-pattern detection system) that newborns typically don’t distinguish between the sounds of their mother reading a passage from The Cat in the Hat and from the rhythmically similar The King, the Mice, and the Cheese.62 But newborns whose mothers had read The Cat in the Hat out loud for hours during the last trimester preferred Dr. Seuss. Wow.

			Despite the charm of these findings, this book’s concerns aren’t rooted in such prenatal learning—few infants are born with a preference for passages from, say, Mein Kampf. However, other prenatal environmental effects are quite consequential.





BOY AND GIRL BRAINS, WHATEVER THAT MIGHT MEAN


			We start with a simple version of what “environment” means for a fetal brain: the nutrients, immune messengers, and, most important, hormones carried to the brain in the fetal circulation.

			Once the pertinent glands have developed in a fetus, they are perfectly capable of secreting their characteristic hormones. This is particularly consequential. When hormones first made their entrance in chapter 4, our discussion concerned their “activational” effects that lasted on the order of hours to days. In contrast, hormones in the fetus have “organizational” effects on the brain, causing lifelong changes in structure and function.

			Around eight weeks postconception, human fetal gonads start secreting their steroid hormones (testosterone in males; estrogen and progesterone in females). Crucially, testosterone plus “anti-Müllerian hormone” (also from the testes) masculinize the brain.

			Three complications, of increasing messiness:


In many rodents the brain isn’t quite sexually differentiated at birth, and these hormonal effects continue postnatally.

				A messier complication: Surprisingly few testosterone effects in the brain result from the hormone binding to androgen receptors. Instead, testosterone enters targets cells and, bizarrely, is converted to estrogen, then binds to intracellular estrogen receptors (while testosterone has its effects outside the brain either as itself or, after intracellular conversion to a related androgen, dihydrotestosterone). Thus testosterone has much of its masculinizing effect in the brain by becoming estrogen. The conversion of testosterone to estrogen also occurs in the fetal brain. Wait. Regardless of fetal sex, fetal circulation is full of maternal estrogen, plus female fetuses secrete estrogen. Thus female fetal brains are bathed in estrogen. Why doesn’t that masculinize the female fetal brain? Most likely it’s because fetuses make something called alpha-fetoprotein, which binds circulating estrogen, taking it out of action. So neither Mom’s estrogen nor fetal-derived estrogen masculinizes the brain in female fetuses. And it turns out that unless there is testosterone and anti-Müllerian hormone around, fetal mammalian brains automatically feminize.63

				Now for the übermessy complication. What exactly is a “female” or “male” brain? This is where the arguments begin.



			To start, male brains merely consistently drool reproductive hormones out of the hypothalamus, whereas female brains must master the cyclic secretion of ovulatory cycles. Thus fetal life produces a hypothalamus that is more complexly wired in females.

			But how about sex differences in the behaviors that interest us? The question is, how much of male aggression is due to prenatal masculinizing of the brain?

			Virtually all of it, if we’re talking rodents. Work in the 1950s by Robert Goy of the University of Wisconsin showed that in guinea pigs an organizational effect of perinatal testosterone is to make the brain responsive to testosterone in adulthood.64 Near-term pregnant females would be treated with testosterone. This produced female offspring who, as adults, appeared normal but were behaviorally “masculinized”—they were more sensitive than control females to an injection of testosterone, with a greater increase in aggression and male-typical sexual behavior (i.e., mounting other females). Moreover, estrogen was less effective at eliciting female-typical sexual behavior (i.e., a back-arching reflex called lordosis). Thus prenatal testosterone exposure had masculinizing organizational effects, so that these females as adults responded to the activational effects of testosterone and estrogen as males would.

			This challenged dogma that sexual identity is due to social, not biological, influences. This was the view of sociologists who hated high school biology . . . and of the medical establishment as well. According to this view, if an infant was born with sexually ambiguous genitalia (roughly 1 to 2 percent of births), it didn’t matter which gender they were raised, as long as you decided within the first eighteen months—just do whichever reconstructive surgery was more convenient.*65

			So here’s Goy reporting that prenatal hormone environment, not social factors, determines adult sex-typical behaviors. “But these are guinea pigs” was the retort. Goy and crew then studied nonhuman primates.

			A quick tour of sexually dimorphic (i.e., differing by sex) primate behavior: South American species such as marmosets and tamarins, who form pair-bonds, show few sex differences in behavior. In contrast, most Old World primates are highly dimorphic; males are more aggressive, and females spend more time at affiliative behaviors (e.g., social grooming, interacting with infants). How’s this for a sex difference: in one study, adult male rhesus monkeys were far more interested in playing with “masculine” human toys (e.g., wheeled toys) than “feminine” ones (stuffed animals), while females had a slight preference for feminine.66

			What next, female monkeys prefer young-adult fantasy novels with female protagonists? Why should human toys be relevant to sex differences in monkeys? The authors speculate that this reflects the higher activity levels in males, and how masculine toys facilitate more active play.



					Male rhesus monkeys show a strong preference for playing with stereotypically “masculine” versus “feminine” human toys.

					Visit bit.ly/2o8ogEL for a larger version of this graph.



			Goy studied highly sexually dimorphic rhesus monkeys. There were already hints that testosterone has organizational effects on their behavior—within weeks of birth, males are more active than females and spend more time in rough-and-tumble play. This is long before puberty and its burst of testosterone secretion. Furthermore, even if you suppress their testosterone levels at birth (low, but nevertheless still higher than those of females), males still do more roughing and tumbling. This suggested that the sex difference arose from fetal hormone differences.

			Goy proved this by treating pregnant monkeys with testosterone and examining their female offspring. Testosterone exposure throughout pregnancy produced daughters who were “pseudohermaphrodites”—looked like males on the outside but had female gonads on the inside. When compared with control females, these androgenized females did more rough-and-tumble play, were more aggressive, and displayed male-typical mounting behavior and vocalizations (as much as males, by some measures). Importantly, most but not all behaviors were masculinized, and these androgenized females were as interested as control females in infants. Thus, testosterone has prenatal organizational effects on some but not all behaviors.

			In further studies, many carried out by Goy’s student Kim Wallen of Emory University, pregnant females received lower doses of testosterone, and only in the last trimester.67 This produced daughters with normal genitalia but masculinized behavior. The authors noted the relevance of this to transgender individuals—the external appearance of one sex but the brain, if you will, of the other.*





And Us


			Initially it seemed clear that prenatal testosterone exposure is also responsible for male aggression in humans. This was based on studies of a rare disorder, congenital adrenal hyperplasia (CAH). An enzyme in the adrenal glands has a mutation, and instead of making glucocorticoids, they make testosterone and other androgens, starting during fetal life.

			The lack of glucocorticoids causes serious metabolic problems requiring replacement hormones. And what about the excessive androgens in CAH girls (who are typically born with ambiguous genitals and are infertile as adults)?

			In the 1950s psychologist John Money of Johns Hopkins University reported that CAH girls had pathologically high levels of male-typical behaviors, a paucity of female-typical ones, and elevated IQ.

			That sure stopped everyone in their tracks. But the research had some problems. First, the IQ finding was spurious—parents willing to enroll their CAH child in these studies averaged higher levels of education than did controls. And the gender-typical behaviors? “Normal” was judged by 1950s Ozzie and Harriet standards—CAH girls were pathologically interested in having careers and disinterested in having babies.

			Oops, back to the drawing board. Careful contemporary CAH research has been conducted by Melissa Hines of the University of Cambridge.68 When compared with non-CAH girls, CAH girls do more rough-and-tumble play, fighting, and physical aggression. Moreover, they prefer “masculine” toys over dolls. As adults they score lower on measures of tenderness and higher in aggressiveness and self-report more aggression and less interest in infants. In addition, CAH women are more likely to be gay or bisexual or have a transgender sexual identity.*

			Importantly, drug treatments begun soon after birth normalize androgen levels in these girls, so that the excessive androgen exposure is solely prenatal. Thus prenatal testosterone exposure appears to cause organizational changes that increase the incidence of male-typical behaviors.

			A similar conclusion is reached by an inverse of CAH, namely androgen insensitivity syndrome (AIS, historically called “testicular feminization syndrome”).69 A fetus is male—XY chromosomes, testes that secrete testosterone. But a mutation in the androgen receptor makes it insensitive to testosterone. Thus the testes can secrete testosterone till the cows come home but there won’t be any masculinization. And often the individual is born with a female external phenotype and is raised as a girl. Along comes puberty, she’s not getting periods, and a trip to the doctor reveals that the “girl” is actually a “boy” (with testes typically near the stomach, plus a shortened vagina that dead-ends). The individual usually continues with a female identity but is infertile as an adult. In other words, when human males don’t experience the organizational prenatal effects of testosterone, you get female-typical behaviors and identification.

			Between CAH and AIS, the issue seems settled—prenatal testosterone plays a major role in explaining sex differences in aggression and various affiliative prosocial behaviors in humans.

			Careful readers may have spotted two whopping big problems with this conclusion:70


Remember that CAH girls are born with a “something’s very different” Post-it—the ambiguous genitalia, typically requiring multiple reconstructive surgeries. CAH females are not merely prenatally androgenized. They’re also raised by parents who know something is different, have slews of doctors mighty interested in their privates, and are treated with all sorts of hormones. It’s impossible to attribute the behavioral profile solely to the prenatal androgens.

				Testosterone has no effects in AIS individuals because of the androgen receptor mutation. But doesn’t testosterone have most of its fetal brain effects as estrogen, interacting with the estrogen receptor? That aspect of brain masculinization should have occurred despite the mutation. Complicating things, some of the masculinizing effects of prenatal testosterone in monkeys don’t require conversion to estrogen. So we have genetically and gonadally male individuals with at least some brain masculinization raised successfully as females.



			The picture is complicated further—AIS individuals raised female have higher-than-expected rates of being gay, and of having an other-than-female or neither-female-nor-male-sex/gender self-identification.

			Argh. All we can say is that there is (imperfect) evidence that testosterone has masculinizing prenatal effects in humans, as in other primates. The question becomes how big these effects are.

			Answering that question would be easy if you knew how much testosterone people were exposed to as fetuses. Which brings up a truly quirky finding, one likely to cause readers to start futzing awkwardly with a ruler.

			Weirdly, prenatal testosterone exposure influences digit length.71 Specifically, while the second finger is usually shorter than the fourth finger, the difference (the “2D:4D ratio”) is greater in men than in women, something first noted in the 1880s. The difference is demonstrable in third-trimester fetuses, and the more fetal testosterone exposure (as assessed by amniocentesis), the more pronounced the ratio. Moreover, CAH females have a more masculine ratio, as do females who shared their fetal environment (and thus some testosterone) with a male twin, while AIS males have a more feminine ratio. The sex difference in the ratio occurs in other primates and rodents. And no one knows why this difference exists. Moreover, this oddity is not alone. A barely discernible background noise generated by the inner ear (“otoacoustic emissions”) shows a sex difference that reflects prenatal testosterone exposure. Go explain that.

			The 2D:4D ratio is so variable, and the sex difference so small, that you can’t determine someone’s sex by knowing it. But it does tell you something about the extent of fetal testosterone exposure.

			So what does the extent of exposure (as assessed by the ratio) predict about adult behavior? Men with more “masculine” 2D:4D ratios tend toward higher levels of aggression and math scores; more assertive personalities; higher rates of ADHD and autism (diseases with strong male biases); and decreased risk of depression and anxiety (disorders with a female skew). The faces and handwriting of such men are judged to be more “masculine.” Furthermore, some reports show a decreased likelihood of being gay.

			Women having a more “feminine” ratio have less chance of autism and more of anorexia (a female-biased disease). They’re less likely to be left-handed (a male-skewed trait). Moreover, they exhibit less athletic ability and more attraction to highly masculine faces. And they’re more likely to be straight or, if lesbian, more likely to take stereotypical female sexual roles.72

			This constitutes some of the strongest evidence that (a) fetal androgen exposure has organizational effects on adult behavior in humans as in other species, and (b) that individual differences in the extent of such exposure predict individual differences in adult behavior.*73 Prenatal endocrine environment is destiny.

			Well, not exactly. These effects are small and variable, producing a meaningful relationship only when considering large numbers of individuals. Do testosterone’s organizational effects determine the quality and/or quantity of aggression? No. How about the organizational plus the activational effects? Not those either.





Expanding the Scope of “Environment”


			Thus the fetal brain can be influenced by hormones secreted by the fetus. But in addition, the outside world alters a pregnant woman’s physiology, which in turn affects the fetal brain.

			The most obvious version of this is how food ingested by a pregnant female influences what nutrients are delivered to the fetal circulation.* At an extreme, maternal malnutrition broadly impairs fetal brain development.*74 Moreover, pathogens acquired by the mother can be passed to the fetus—for example, the protozoan parasite Toxoplasma gondii can infect someone pregnant (typically after exposure to infected cat feces) and eventually reach the fetal nervous system, potentially wreaking serious havoc. And this is also the world of maternal substance abuse producing heroin and crack babies or fetal alcohol syndrome.

			Importantly, maternal stress impacts fetal development. There are indirect routes—for example, stressed people consume less healthy diets and consume more substances of abuse. More directly, stress alters maternal blood pressure and immune defenses, which impact a fetus. Most important, stressed mothers secrete glucocorticoids, which enter fetal circulation and basically have the same bad consequences as in stressed infants and children.

			Glucocorticoids accomplish this through organizational effects on fetal brain construction and decreasing levels of growth factors, numbers of neurons and synapses, and so on. Just as prenatal testosterone exposure generates an adult brain that is more sensitive to environmental triggers of aggression, excessive prenatal glucocorticoid exposure produces an adult brain more sensitive to environmental triggers of depression and anxiety.

			In addition, prenatal glucocorticoid exposure has effects that blend classical developmental biology with molecular biology. To appreciate this, here’s a highly simplified version of the next chapter’s focus on genes: (a) each gene specifies the production of a specific type of protein; (b) a gene has to be “activated” for the protein to be produced and “deactivated” to stop producing it—thus genes come with on/off switches; (c) every cell in our bodies contains the same library of genes; (d) during development, the pattern of which genes are activated determines which cells turn into nose, which into toes, and so on; (e) forever after, nose, toes, and other cells retain distinctive patterns of gene activation.

			Chapter 4 discussed how some hormones have activational effects by altering on/off switches on particular genes (e.g., testosterone-activating genes related to increased growth in muscle cells). The field of “epigenetics” concerns how some hormonal organizational effects arise from permanently turning particular genes on or off in particular cells.75 Plenty more on this in the next chapter.

			This helps explain why your toes and nose work differently. More important, epigenetic changes also occur in the brain.

			This domain of epigenetics was uncovered in a landmark 2004 study by Meaney and colleagues, one of the most cited papers published in the prestigious journal Nature Neuroscience. They had shown previously that offspring of more “attentive” rat mothers (those that frequently nurse, groom, and lick their pups) become adults with lower glucocorticoid levels, less anxiety, better learning, and delayed brain aging. The paper showed that these changes were epigenetic—that mothering style altered the on/off switch in a gene relevant to the brain’s stress response.* Whoa—mothering style alters gene regulation in pups’ brains. Remarkably, Meaney, along with Darlene Francis of the University of California, Berkeley, then showed that such rat pups, as adults, are more attentive mothers—passing this trait epigenetically to the next generation.* Thus, adult behavior produces persistent molecular brain changes in offspring, “programming” them to be likely to replicate that distinctive behavior in adulthood.76

			More findings flooded in, many provided by Meaney, his collaborator Moshe Szyf, also of McGill, and Frances Champagne of Columbia University.77 Hormonal responses to various fetal and childhood experiences have epigenetic effects on genes related to the growth factor BDNF, to the vasopressin and oxytocin system, and to estrogen sensitivity. These effects are pertinent to adult cognition, personality, emotionality, and psychiatric health. Childhood abuse, for example, causes epigenetic changes in hundreds of genes in the human hippocampus. Moreover, Stephen Suomi of the National Institutes of Health and Szyf found that mothering style in monkeys has epigenetic effects on more than a thousand frontocortical genes.*

			This is totally revolutionary. Sort of. Which segues to a chapter summary.





CONCLUSIONS


			Epigenetic environmental effects on the developing brain are hugely exciting. Nonetheless, curbing of enthusiasm is needed. Findings have been overinterpreted, and as more researchers flock to the subject, the quality of studies has declined. Moreover, there is the temptation to conclude that epigenetics explains “everything,” whatever that might be; most effects of childhood experience on adult outcomes probably don’t involve epigenetics and (stay tuned) most epigenetic changes are transient. Particularly strong criticisms come from molecular geneticists rather than behavioral scientists (who generally embrace the topic); some of the negativity from the former, I suspect, is fueled by the indignity of having to incorporate the likes of rat mothers licking their pups into their beautiful world of gene regulation.

			But the excitement should be restrained on a deeper level, one relevant to the entire chapter. Stimulating environments, harsh parents, good neighborhoods, uninspiring teachers, optimal diets—all alter genes in the brain. Wow. And not that long ago the revolution was about how environment and experience change the excitability of synapses, their number, neuronal circuits, even the number of neurons. Whoa. And earlier the revolution was about how environment and experience can change the sizes of different parts of the brain. Amazing.

			But none of this is truly amazing. Because things must work these ways. While little in childhood determines an adult behavior, virtually everything in childhood changes propensities toward some adult behavior. Freud, Bowlby, Harlow, Meaney, from their differing perspectives, all make the same fundamental and once-revolutionary point: childhood matters. All that the likes of growth factors, on/off switches, and rates of myelination do is provide insights into the innards of that fact.

			Such insight is plenty useful. It shows the steps linking childhood point A to adult point Z. It shows how parents can produce offspring whose behaviors resemble their own. It identifies Achilles’ heels that explain how childhood adversity can make for damaged and damaging adults. And it hints at how bad outcomes might be reversed and good outcomes reinforced.

			There is another use. In chapter 2 I recounted how it required the demonstration of hippocampal volume loss in combat vets with PTSD to finally convince many in power that the disorder is “real.” Similarly, it shouldn’t require molecular genetics or neuroendocrinology factoids to prove that childhood matters and thus that it profoundly matters to provide childhoods filled with good health and safety, love and nurturance and opportunity. But insofar as it seems to require precisely that sort of scientific validation at times, more power to those factoids.





