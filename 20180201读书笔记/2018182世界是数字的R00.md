## 记忆时间

2020-01-22；2020-04-14；2021-08-20

## 卡片

### 0101. 主题卡 —— 世界是数字的

计算机、通信系统，以及它们所支撑的数字产品已经无处不在。

计算机包括硬件和软件。计算机的范畴很大，大到超级计算机、个人 PC、智能手机，小到照相机、摄影机、GPS 导航仪、智能手环。硬件，应该明白计算机的构成，它如何表示和处理信息，某些术语和数字的含义，以及计算机随着时间推移都有了哪些变化；软件，关键是要知道怎么精确地描述任务，包括抽象的算法（同时考虑数据量的增加在多大程度上延长计算时间）和具体的程序。知道软件是由什么构成的，不同的编程语言怎么编写程序，程序怎么变成软件（通常是基于组件构建起来的），这样就能理解我们所用软件背后的秘密。

通信系统。重要的是理解其中的信息流动、谁能够查看这些信息，以及信息是如何得到控制的。协议，即系统之间交换信息的规则，也非常重要。协议的内容影响深远，由今天互联网中的身份认证问题可见一斑。

数据是通过硬件及软件收集、存储和处理的，以及通信系统传送到世界各地的全部信息。数据的体现形式如电话、相片、音乐、电子邮件、语音处理、地图。信息的通用表示形式，通用数字形式（0 和 1）已经取代其他模拟表示形式，如数字照片取代含感光材料的胶卷、数字音乐取代含磁化布局的录音带、数字邮件取代纸质邮件、电子地图取代纸质地图；信息的通用数字处理器，处理数字信息的数字计算机取代了处理专门模拟信息的实体机器。不同的计算机在「能计算什么」上的能力是一样的，差别只在于计算速度有多快，能存储的数据有多少。计算机是操作比特的数字设备。告诉处理器做什么的指令，被编码为比特，而且通常与数据保存在同样的存储器中。改变指令可以改变计算机行为，而这也正是计算机之所以成为通用机器的原因所在。比特的含义取决于上下文，一个人的指令可能是另一个人的数据。虽然有适合处理某种数据的特定技术存在，但复制、加密、压缩、错误检测等等操作全都可以在比特的层面上执行，与比特所表示的事物无关；信息的通用数字网络，互联网把处理数字信息的数字计算机连接在一起。计算机、手机被联到邮件、搜索、社交网络、购物、网上银行等各种服务上。网络中从一个处理器传输到另一个处理器的数据和指令，同样也都是比特。互联网和电话网有可能融合为一个更通用的网络，恰似我们亲眼见证的计算和通信功能融合于今天的手机。

1『互联网与电话网的融合，这说的不就是物联网么，Kernighan（作者）太 NB 了。（2021-08-19）』

每人都有必要了解计算机与通信知识，可以让你知道推销人员或服务热线向你隐瞒了的事实，并采取相应的措施。

### 0102. 主题卡 —— 逻辑结构与具体实现

我们经常会区分逻辑结构和具体实现，这个问题有无数种表现形式，典型如计算机。无论其制造方式变化得有多快，其逻辑架构一直以来并没有太大不同。甚至可以认为，所有计算机的逻辑特征都是一样的，即它们都可以完成相同的计算。从软件角度看，代码作为一个抽象层，隔离了具体的实现。实现可以改变，而使用它们的代码可以不变。虚拟机、虚拟操作系统，甚至真正的操作系统都是利用接口来分离逻辑结构与具体实现的。想一想，编程语言也具备这个功能，有了它我们才可以跟计算机对话，就好像所有计算机都能听懂我们的话一样。当然，编程语言也是我们人类可以理解的。

### 0201. 术语卡 —— 服务

操作系统为应用程序定义了一组操作（也叫服务），比如将数据存储至文件或者从文件中取出数据、建立网络连接、获取键盘输入、报告鼠标移动和按钮点击、绘制屏幕，等等。操作系统以标准化的或者说大家协商一致的方式提供这些服务，而应用程序通过执行一种特殊的指令来请求这些服务，并将控制权移交给操作系统中特定的地址。操作系统根据请求完成计算，然后再将控制权和结果返回给应用程序。操作系统的这些「入口」被称为系统调用（system call），而对这些系统调用的详细说明实际上恰恰解释了操作系统能做什么。系统调用可以直接拿操作系统内部的代码作为入口，也可以是对某个（为相应服务而准备的）库函数的调用。但多数情况下，即便是程序员也不用关心上述区别。正因为如此，谁也说不清楚到底有多少个系统调用，但通常一两百个总是有的。

### 0202. 术语卡 —— 变量

变量是 RAM 中的一个位置，可以让程序在运行期间存储数据。之所以称它为变量，是因为它的值会随着程序的执行而变化。在高级语言里，声明变量就相当于我们在玩具汇编语言中为一个内存位置起一个名字。打个比方，声明就好比一出戏里的演员表。在这里，我们把这个变量叫做 username。当然也可以给它起别的名字，但 username 让人一看就知道它在程序中扮演什么角色。

### 0203. 术语卡 —— 测试

通过这个例子还可以学到编程的另一个重要方面：测试。测试远不止是随机地向程序抛出几个数值那么简单。好的测试人员会绞尽脑汁地想象程序会在什么情况下出错，想象那些「边缘」或「边界」情形，比如根本没有数据或者被零除。好的测试人员会想到输入都是负值的可能性。但问题是，随着程序越写越大，想象出所有测试用例的难度也越来越大，因为用户可能会以任意次序、在任意时间输入任意值。没有完美的解决方案，这时候认真地设计和实现程序就显得很关键。比如从一开始就在程序里添加检测和比较代码，以便在出现问题时，程序自己就可以第一时间捕获。

### 0204. 术语卡 —— 操作系统

操作系统，是用来控制和分配计算机资源的。操作系统里的文件系统，是计算机逻辑组织和物理实现的集中体现。操作系统管理 CPU、管理内存、管理存储在磁盘上的信息、管理和协调外接设备的活动。

### 0205. 术语卡 —— 虚拟机

虚拟机是一个模拟计算机的程序。

我们甚至可以在一个操作系统的控制下运行另一个虚拟操作系统。使用 VMware、Parallels 和（开源的）Xen 等虚拟操作系统软件，可以在一台 Mac OS X 主机上运行另一个客户操作系统，比如 Windows 或 Linux。主机操作系统会拦截客户操作系统的请求，代替它执行那些需要具备操作系统级权限才能执行的操作，如访问文件系统或网络。主机在执行完操作后，将结果返回给客户机。在主机和客户机系统都是为相同硬件编译的情况下，客户系统大多数时候都得到硬件的全速支持，响应的及时性给人感觉就像在裸机上运行一样。

这里有必要说一说「虚拟」这个词的另一种用法。一个模拟计算机的程序，无论它模拟的是真实的计算机还是想象中的计算机（比如本书前面提到的玩具计算机），经常也被称为虚拟机。换句话说，计算机只以软件形式存在，而这种软件的行为就如同硬件一般。这种虚拟机很常见。浏览器都有一个虚拟机用于解释 JavaScript 程序，所有 Java 程序也都是通过虚拟机来解释的，而每台 Android 手机上同样有一个类似的 Java 虚拟机。

### 0301. 人名卡 —— 冯·诺依曼

[约翰·冯·诺伊曼 - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/%E7%BA%A6%E7%BF%B0%C2%B7%E5%86%AF%C2%B7%E8%AF%BA%E4%BC%8A%E6%9B%BC)

约翰·冯·诺伊曼（德语：John von Neumann，1903-1957），天才中的天才，电子计算机的鼻祖人物。

原名诺依曼·亚诺什·拉约什（匈牙利语：Neumann János Lajos），出生于匈牙利的美国籍犹太人数学家，现代电子计算机与博弈论的重要创始人，在泛函分析、遍历理论、几何学、拓扑学和数值分析等众多数学领域及计算机学、量子力学和经济学中都有重大贡献。从小就以过人的智力与记忆力而闻名，一生中发表了大约 150 篇论文，其中有 60 篇纯数学论文，20 篇物理学以及 60 篇应用数学论文。他最后的作品是一个在医院未完成的手稿，后来以书名《计算机与人脑》发布，表现了他生命最后时光的兴趣方向。他先后任职于美国普林斯顿大学、美国普林斯顿高等研究院等机构。[2][3]

6 岁时已能用古希腊语同父亲闲谈，还可以心算 8 位数除法，8 岁时自学微积分。年少时的他不但对数学很有兴趣，亦喜欢阅读历史和社会方面的书籍，读过的书籍和论文能很快一句不漏地将内容复述出来，而且多年以后仍是如此。1913 年，他的父亲马克斯·诺伊曼被授予世袭贵族头衔，这样在德国他的后代可以以「冯·诺伊曼」为姓 [4]，冯·诺伊曼晋身贵族。1926 年，冯·诺伊曼以 22 岁的年龄获得了布达佩斯大学数学博士学位，相继在柏林大学和汉堡大学担任数学讲师。

1930 年，冯·诺伊曼接受了普林斯顿大学客座教授的职位。初到美国时，他在纽约对当地居民表演过默记电话簿的惊人记忆力。1931 年，冯·诺伊曼成为普林斯顿大学终身教授。1933 年转入普林斯顿高等研究院，与爱因斯坦等人成为该院最初的四位教授之一，不须上课。这一年，他部分解决了希尔伯特第五问题，证明了局部欧几里得紧群是李群。1937 年成为美国公民，1938 年获博修奖。

1954 年，冯·诺伊曼任美国原子能委员会委员。1954 年夏天，右肩受伤，手术时发现患有骨癌，治疗期间，依然参加每周三次的原子能委员会会议，甚至美国国防部长，陆、海、空三军参谋长聚集在病房开会。晚年，有学生请教他做事的方法，他说：简单（simple）。1957 年 2 月 8 日，冯·诺伊曼在华盛顿瓦尔特·立德军医中心去世，享年 53 岁。他死后葬于新泽西州默瑟县的普林斯顿公墓（Princeton Cemetery）。

1945 年 6 月，冯·诺伊曼与戈德斯坦、勃克斯等人，联名发表了一篇长达 101 页纸的报告，即计算机史上著名的「101页报告」，是现代计算机科学发展里程碑式的文献。明确规定用二进制替代十进制运算，并将计算机分成 5 大组件，这一卓越的思想为电子计算机的逻辑结构设计奠定了基础，已成为计算机设计的基本原则。1951 年，EDVAC 计算机宣告完成。（[EDVAC报告书的第一份草案 - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/EDVAC%E5%A0%B1%E5%91%8A%E6%9B%B8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%BB%BD%E8%8D%89%E6%A1%88)）John von Neumann. The Computer and the Brain [计算机与人脑]. 1958 （英语）（去世后出版）

2『已下载书籍「2020006John-von-Neumann-Selected-Letters」。』

### 0302. 人名卡 —— Brian W. Kernighan

[布莱恩·柯林汉 - 维基百科，自由的百科全书](https://zh.wikipedia.org/zh-cn/%E5%B8%83%E8%90%8A%E6%81%A9%C2%B7%E6%9F%AF%E6%9E%97%E6%BC%A2)

布莱恩·柯林汉（Brian Kernighan，1942-）本书的作者，生于加拿大多伦多，加拿大计算机科学家，曾服务于贝尔实验室，为普林斯顿大学教授。《C程序设计语言》作者之一。

1960-1964 年间，就读于多伦多大学，取得工程物理学（Engineering physics）学士。在普林斯顿大学取得电子工程博士，毕业后留在普林斯顿大学任教，直到 2000 年退休为止。贝尔实验室的成员之一，曾参加 UNIX 系统的早期开发，之后又加入了贝尔实验室九号计划（Plan 9）的研发。在 1970 年代，布莱恩·柯林汉首先提出了 UNIX 这个名称，用来取代之前的 UNICS（Uniplexed Information and Computing System）。模仿所见即所得（What You See Is What You Get，WYSIWYG），他提出所见即所有（What You See Is All You Get，WYSIAYG）这个名词。

1972 年，在「A Tutorial Introduction to the Language B」中，首次提出 Hello world 程式，作为编程语言入门的示范。1978 年，与丹尼斯·里奇共同写作了《C程序设计语言》（The C Programming Language），在当中除了介绍 C 语言之外，也形成了所谓 K&R C 的程式风格（Coding Style）。

1-3『真没想到，原来 K&R 风格里的 K 指的就是 Kernighan，意外收获。（2021-08-19）』

《C程序设计语言》、《程序设计实践》、《The Go Programming Language》。

### 0401. 金句卡 —— 任何足够先进的技术都与魔术无异

 ——  阿瑟·C. 克拉克，「技术及未来前景」，《三号行星的报告》，1972 年

### 0402. 金句卡 —— 程序之于算法，犹如建筑之于图纸

算法是忽略具体实例而对过程进行的一种抽象或理想化的描述，是分毫不差且没有歧义的「菜谱」。算法通过一组确定的基本操作来表达，这些操作的含义是完全已知且明确的。算法描述了应用这些基本操作的一系列步骤，涵盖所有可能的情况，而且保证最终能够停止。另一方面，程序则不是抽象的，它陈述了一台真正的计算机要完成某个任务所必须执行的具体步骤。程序之于算法，犹如建筑之于图纸，一个是实际存在的，一个是理想化的。

### 0403. 金句卡 —— 编程语言完成人机交互，操作系统完成机机交互

我们需要或者希望计算机无所不能，如此一来就需要巨大的编程工作量，而世界上却没有那么多程序员。因此，让计算机代替人处理更多的编程细节就成为这个领域永恒的话题。这自然也就引出了编程语言，即让我们能够表达完成某个任务所需计算步骤的语言。同样，管理一台计算机的资源也十分困难，而现代计算机的复杂性更是让这种困难有增无减。因此，我们也需要让计算机来控制自己的操作，而由此就有了所谓的操作系统。操作系统，是用来控制和分配计算机资源的。操作系统里的文件系统，是计算机逻辑组织和物理实现的集中体现。

### 0501. 任意卡 —— iPhone 和 App Store 上架的时间

电话。在 1990 年，电话机还都是傻大个儿，用转盘或者 12 个数字按钮拨号，没有显示面板。它通过电话线连到家里或办公室墙上的插座，只能用来跟人讲话，或者通过特殊设备连接传真机和计算机。到了 2000 年，手机已经流行起来，但也只能打打电话。而现在手机已经非常普及，在几乎所有地方都超过了固定电话装机数量。比如埃及和突尼斯，有手机的人已经分别占总人口数的 80% 和 75% 之多。智能手机则可以通过从应用市场下载并安装程序来扩展出各种神奇的功能，例如苹果公司在 2007 年中发布了 iPhone 手机，2008 年 App Store 开张，为 iPhone 提供各种软件；安卓市场也类似。

2『 iPhone 和 App Store 上架的时间，做一张信息数据卡片。』—— 已完成

### 0502. 任意卡 —— 不同约束条件下做的取舍

计算机系统是设计上多方权衡、不断取舍的极佳范例，提醒我们设计中永远不可能处处如意，天下没有免费的午餐。桌面电脑、笔记本电脑、平板电脑、手机，同是计算设备，但它们在尺寸、重量、计算能力和成本等约束条件上，则分别作出了明显不一样的取舍。

## 前言

《世界是数字的》简明扼要但又深入全面地解释了计算机和通信系统背后的秘密，旨在让没有技术背景的读者更好地理解自己生活的这个数字世界。这本书解释了如今计算和通信的运作方式，包括硬件、软件、互联网，还有万维网，同时还探讨了新技术引发的社会、政治和法律问题，让你明白现实当中的一些难题和迫不得已的折中。

很多人对普林斯顿大学的认知来自获得奥斯卡金像奖的电影《美丽心灵》。这部电影以 1994 年获得诺贝尔经济学奖的数学家小约翰·福布斯·纳什（Jr.John Forbes Nash）为原型。1950 年代，20 出头的纳什在普林斯顿攻读博士期间，发表了一篇关于非合作博弈的博士论文，确立了他博弈论大师的地位。而同时代的普林斯顿可谓大师云集，爱因斯坦、冯·诺依曼、列夫谢茨（数学系主任）、阿尔伯特·塔克、阿伦佐·切奇、哈罗德·库恩…… 都在这里。普林斯顿大学直到今天的在校学生也不过 7000 多人，但却人才辈出：两位美国总统、44 位州长、33 位诺贝尔奖得主。

但这只是计算机时代的一个小小的缩影，不为人知的部分就像隐藏在海平面之下的冰山一样巨大。因为看不到，所以我们并不觉得家用电器、汽车、飞机，以及无所不在、司空见惯的电子设备 —— 照相机、摄像机、游戏机、DVD 播放机、GPS 导航仪中都隐藏着计算机。

不经意间，对这些设备品头论足的信息也会进入我们的视野。就像有一次某报道引用了惠普一位高层领导的话：「本质上，数码相机就是一台带镜头的计算机。」同一篇报道中也引用了一位用户的话，这位用户好像不怎么高兴：「这哪是相机啊，根本就是一台计算机！」这是在抱怨有时候使用计算机也不容易。而且，电话网络、电视和有线网络、空中交通管制、电网，还有银行和金融服务等公共设施对计算机的依赖程度也超出了我们的想象。

理查德·穆勒（Richard Muller）的那本《未来总统的物理课》解释了作为国家领导人应该知道的科学和技术造成的社会问题，比如核威胁、恐怖主义、能源、全球变暖，等等。不想当总统但生活在信息时代的公民也应该很好地了解这些问题。虽然科学原理与推理论证有很多不同，但穆勒却能够很好聚焦于每个主题，聚焦于每个人都应该了解的物理常识。他的写作手法启发我让这本书成为「未来总统的计算机与通信」。作为总统应该了解哪些计算机和通信知识？一位信息时代的公民应该知道哪些？虽然每个人心目中的想法不一样，但这本书是我给出的答案。

2『已下载书籍「2020004未来总统的物理课 | 2020004Physics-and-Technology-for-Future-Presidents」。』

这本书涵盖了三个核心技术领域：硬件、软件和通信。整本书都围绕这三个主题展开。

硬件是看得见摸得着的。不管是在家里，还是在办公室，计算机都是我们可以看到，可以触摸的。当然，还有我们每天随身携带的手机。计算机的内部都有什么，它是怎么运转的，是根据什么原理制造的？它怎么保存和处理信息？什么是比特，什么是字节？怎么用它们来表示音乐、电影，还有一切？

软件是告诉计算机做什么的指令，几乎看不见，摸不着。通过计算可以做什么，计算速度可以有多快？怎么告诉计算机做什么？为什么让软件不出错很难？为什么它们有时候很难用？

通信就是计算机、手机和其他设备之间为了我们的需要而进行的对话，同时也让我们人和人之间能够交流，涉及互联网、万维网、电子邮件、社交网络等多种途径。这些东西的工作原理是什么？它们的好处显而易见，但有什么风险吗？特别是隐私和安全方面，该怎么解决呢？

在这三个主题之外，人们通常都会想到数据。数据指的是通过硬件及软件收集、存储和处理的，以及通信系统传送到世界各地的全部信息。其中部分数据是自愿公开的，主要是用户上传的照片和视频，有率性而为、不顾及后果的，也有时时警惕、谨小慎微的。还有一些是我们个人的信息，通常是在我们并不知情的情况下被收集和共享，根本没得商量。

无论你是总统，还是平民百姓，都应该了解这个计算机世界，因为它对每个人都有切身影响。无论工作和生活与技术距离有多遥远，你总有机会接触技术和搞技术的人。了解一些计算机和通信的常识都将对你大有助益，最低限度也能让你知道推销人员或服务热线什么时候向你隐瞒了事实。没错，无知有害。假如你不知道病毒、网络钓鱼以及类似的风险是怎么回事，那你受害的机率一定会大大增加；假如你不知道社交网络怎么泄露甚至任意传播你认为是个人隐私的信息，那你无意间泄露的很可能比自己想象的还要多；假如你对商业利益集团不顾一切从你的个人信息中挖掘线索这件事毫不知情，那你就会为了蝇头小利而出卖自己的隐私；假如你不知道在咖啡店和飞机上使用个人银行服务是有风险的，那么你的钱和身份就会让网络窃贼有可乘之机。

1-2-3『

作者的书籍网站：[kernighan.com}](http://kernighan.com/)

已下载书籍：「2020001The-Go-Programming-Language」、「2020002Millions-Billions-Zillions」、「2020003UNIX-A-History-And-A-Memoir」。

』

电话。在 1990 年，电话机还都是傻大个儿，用转盘或者 12 个数字按钮拨号，没有显示面板。它通过电话线连到家里或办公室墙上的插座，只能用来跟人讲话，或者通过特殊设备连接传真机和计算机。到了 2000 年，手机已经流行起来，但也只能打打电话。而现在手机已经非常普及，在几乎所有地方都超过了固定电话装机数量。比如埃及和突尼斯，有手机的人已经分别占总人口数的 80% 和 75% 之多。智能手机则可以通过从应用市场下载并安装程序来扩展出各种神奇的功能，例如苹果公司在 2007 年中发布了 iPhone 手机，2008 年 App Store 开张，为 iPhone 提供各种软件；安卓市场也类似。

2『 iPhone 和 App Store 上架的时间，做一张信息数据卡片。』—— 已完成

相片。在 1990 年，大部分相机都使用胶卷来拍摄相片。胶卷就是一卷柔软的塑料片，上面覆盖了特殊成分的感光材料。拍摄的时候，光线照射到底片上，化学性质发生改变，留下影像；拍摄完后，需要利用一系列精致的化学反应进行冲洗显影。一卷胶卷最多可以拍摄 36 张照片，冲洗之前并不能看到照出来的是什么样子，并且冲洗过程在任何地方最短也要一个小时，多的长达一星期。这一切都完成之后，你得到的也只不过是一份印好的相纸。如果想和朋友分享，你只能把相片拿给他们看，或者装在信封里寄过去。你的相片可能放在纸袋子或者相册里，散落在家中的各处。若是想再多要一份相片，你就要奔波到照相馆去花额外的钱。那时候摄影开销相当贵，导致大多数人并不会去拍很多相片。但那时数码相机已经露出取代胶片相机的苗头，从慢慢入侵到迅速占领，直到后来，胶卷从普通人的视线中几近消失。如今，相片已经可以用电子邮件发送给别人或上传到 Facebook、Flickr 等网站，并且可以下载到数码相框来充分展示。如果你还需要纸质相片，那么可以用高品质的相片打印机，它也已经很便宜了。手机摄像头则正在取代廉价的傻瓜相机。

音乐。在 1990 年，音乐是通过 CD 或录音带专辑来发行的，密纹唱片（LP）虽然还很常见，但已经快要过时了。如果你想要复制一首歌和朋友分享，或者放在车里听，那么最常见的办法是使用卡带录音机，尽管也有办法复制 CD 盘片。那时候还没有下载音乐这一说。在 1999 年，Napster 横空出世改变了一切，让大家可以通过网络共享音乐。不过，它很快就关闭了，或者准确点说，被唱片界的传统势力所扼杀。但后来的在线销售单曲服务却把它的遗志发扬光大，比如 2003 年开张的 iTunes 商店就很红火。在便携式硬件方面，我们则经历了便携式收录机、卡带随身听、CD 播放机、以 iPod 为代表的 MP3 播放器，直到后来用手机来听歌。

电子邮件。20 年前用电子邮件的人屈指可数，大多数人甚至都对此一无所知。1998 年，梅格·瑞恩和汤姆·汉克斯主演的《电子情书》才让电子邮件走入公众视野，而之前人们对「邮件」一词的认识仅限于邮递员送来的信件。现如今，大多数人都用 Gmail 之类的在线邮件服务进行个人通信。他们的邮件保存在互联网上，这样用手机也可以访问。

语音处理。在 1990 年，几乎不可能让计算机理解口语，后来也仅仅是在实验室环境下才能识别有限的词汇。现在，只要给商家打个技术支持或投诉电话，就不可避免要和语音识别系统进行「交谈」，遇到真人接线员的机会则变得极少。这并不算什么了不起的进步。机器翻译也差不多是同样的情况，虽然尚未达到完美，但已经很实用：它可以把将近 60 种语言中的任意一种翻译成其他语种。语音合成，也就是根据文本发出语音，尽管还很容易听出与真人语音的差别，但也已经很常用了。

地图。在 1990 年，如果你不知道如何到一个地方去，那就要先在电话黄页本上找到地址，打开纸质地图找找看在哪里，然后一边开车一边对着地图人肉导航。迷路之后要自己弄明白身处何地，如何回到正确路线。还要事先猜测交通路况，或者寄希望于交通广播台。如果想知道从天上看道路是什么样子，那只能去包飞机了！10 年前出现的 GPS 导航设备为行车人解决了大部分导航问题，如今手机内部已经集成了具有类似功能的系统。用街景地图就可以在到达一个地方之前先观其大略。

本书的目的就是揭开魔法的神秘帷幕，让读者了解这些系统是如何运作的。相片、音乐和电影如何能一瞬间传遍全球？电子邮件是如何运转的？你的电子邮件私密性如何？为什么垃圾邮件容易发送却难以清除？手机真的知道你的位置吗？iPhone 和安卓手机有什么区别，为什么它们在根本上又是一回事？读过本书之后，你将会对计算机和通信系统的运转有相当靠谱的了解，并知道这些技术如何影响我们的生活。

首先是信息的通用数字表示。传统上用来存储相片、音乐等不同类型信息的机制是错综复杂的，而现在它们已经被一种统一的机制所取代。这种取代之所以可行，是因为信息被表示为数字形式而不是专门形式（比如底片上曝过光的感光材料，或者录音带上的磁化布局）。纸质邮件被数字邮件所取代，纸质地图被电子地图所取代。总之，信息的不同模拟表示形式被统一的数字表示形式所取代。

其次是通用数字处理器。所有的信息都用数字计算机这样一种统一的设备进行处理。处理信息通用数字表示的数字计算机代替了处理专门的模拟信息所用的实体机器。行文至此，我一直在竭力避免使用「计算机」这个词，但实际上手机就是相当复杂的计算机，其运算能力已经比得上五六年前的笔记本电脑。我们稍后会发现，不同的计算机在「能计算什么」上的能力是一样的，差别只在于计算速度有多快，能存储的数据有多少。以前只能在台式机和笔记本电脑上做的事，必然会越来越多地可以在手机上完成。如果有什么要特别指出的话，那就是，这个趋同化的过程只会越来越快。

1『从模拟信号到数字信号的转变。』

再次是通用数字网络。互联网把处理数字表示的数字计算机联接在一起。计算机、手机被联到邮件、搜索、社交网络、购物、网上银行等各种服务上。与别人互发电子邮件的时候，完全不需要考虑对方在哪里、选择用什么方式存取电子邮件。用笔记本电脑、手机、平板电脑都可以搜索商品、比较价格、下单购物。社交网络也让你通过手机和计算机与家人朋友保持联系。显然，要让所有这些计算服务正常工作，网络这个基础设施的影响是巨大的。

最后，海量的数字化数据也在持续不断地被收集和更新。全球的地图、航线和街区照片都可以免费获取。搜索引擎为了有效地应对查询而孜孜不倦地扫描着整个互联网。成千上万的书都做成了数字形式。社交网络和资源分享站点为我们保存了关于我们自己的巨量数据。当我们访问网上商店和服务时，它们一方面让我们读取其后台数据，另一方面又在搜索引擎和社交网络的协助和怂恿下默默记录着我们的一举一动。互联网服务供应商记录下我们在网上所有互动操作的联接信息，或许甚至更多。

对硬件部分来说，应该明白计算机的构成，它如何表示和处理信息，某些术语和数字的含义，以及计算机随着时间推移都有了哪些变化。

软件方面，关键是要知道怎么精确地描述任务，包括抽象的算法（同时考虑数据量的增加在多大程度上延长计算时间）和具体的程序。知道软件是由什么构成的，不同的编程语言怎么编写程序，程序怎么变成软件（通常是基于组件构建起来的），这样就能理解我们所用软件背后的秘密。但愿讲编程的那几章也会让你跃跃欲试，亲自动手写出一些代码来。

通信系统是无远弗届，无所不在的。重要的是理解其中的信息流动、谁能够查看这些信息，以及信息是如何得到控制的。协议，即系统之间交换信息的规则，也非常重要。协议的内容影响深远，由今天互联网中的身份认证问题可见一斑。

不少计算方面的概念对理解这个世界很有帮助。比如，我经常会区分逻辑结构与具体实现。这个根本问题有无数种表现方式。计算机就是一个典型的例子。无论其制造方式变化得有多快，其逻辑架构一直以来并没有太大不同。甚至可以认为，所有计算机的逻辑特征都是一样的，即它们都可以完成相同的计算。从软件角度看，代码作为一个抽象层，隔离了具体的实现。实现可以改变，而使用它们的代码可以不变。虚拟机、虚拟操作系统，甚至真正的操作系统都是利用接口来分离逻辑结构与具体实现的。想一想，编程语言也具备这个功能，有了它我们才可以跟计算机对话，就好像所有计算机都能听懂我们的话一样。当然，编程语言也是我们人类可以理解的。

计算机系统是设计上多方权衡、不断取舍的极佳范例，提醒我们设计中永远不可能处处如意，天下没有免费的午餐。桌面电脑、笔记本电脑、平板电脑、手机，同是计算设备，但它们在尺寸、重量、计算能力和成本等约束条件上，则分别作出了明显不一样的取舍。

计算机系统也是把大型、复杂系统切分成小型、易管理（可独立创建）组件的好例子。软件分层、API、协议和标准莫不如此。

我在导论中提到的「通用」对于理解数字技术同样重要。下面就来概括一下。

首先是通用的数字信息表示。化学有 100 多个元素，物理有十几个基本粒子。而数字计算机只有两个元素，0 和 1，其他一切都由此衍生出来。比特可用来表示任何信息，从最简单的真假、是否、对错之类的二元选择，到数字、字母，乃至一切事物。复杂的事物比如购物、浏览和手机历史中关于你生活的点点滴滴，则是由简单的数据项组成，后者又可以用更简单的形式来表示，如此往复，直到表示成一个一个的比特。

其次是通用的数字处理器。计算机是操作比特的数字设备。告诉处理器做什么的指令，被编码为比特，而且通常与数据保存在同样的存储器中。改变指令可以改变计算机行为，而这也正是计算机之所以成为通用机器的原因所在。比特的含义取决于上下文，一个人的指令可能是另一个人的数据。虽然有适合处理某种数据的特定技术存在，但复制、加密、压缩、错误检测等等操作全都可以在比特的层面上执行，与比特所表示的事物无关。运行通用操作系统的通用计算机取代各种专用设备的进程还将继续。未来很可能出现根据生物计算原理设计的其他处理器，或许还会出现量子计算机。但是，数字计算机还会伴随我们很长时间。

第三是通用的数字网络，网络中从一个处理器传输到另一个处理器的数据和指令，同样也都是比特。互联网和电话网有可能融合为一个更通用的网络，恰似我们亲眼见证的计算和通信功能融合于今天的手机。互联网肯定会向前发展，至于是沿续其随心所欲、自由发展的特点，还是会受到商业和政府更多的制约，并没有明确的答案。（我想多半是后者，很不幸啊。）

最后，通用的数字系统无所不在。在整合多领域技术进步的基础上，数字设备向着小型、廉价和高性能的方向发展。某一领域的技术进步，比如存储密度，经常会影响到所有数字设备。

1『通用的数字信息编码、通用的数字处理器、通用的数字网络，世界是数字的。』

最后，读者诸君务必牢记一点，无论今天的技术多么千变万化，人是不变的。无论从哪方面来看，现代的人类与几千年前的人类并没有太大区别。抚今追昔，历史上干好事的人有多大比例，今天也差不多；历史上干坏事的人有多大比例，今天同样也差不多。没错，社会、法律和政治都在适应技术变革，但这是个缓慢的过程，步调并不一致，而且世界不同角落的解决方案也各不相同。

## 0100硬件

然而，有些历史趋势却值得我们关注，尤其是这一点：以一定的成本，在给定大小的空间内能装进电路和设备的数量，随着时间而呈指数式增长。随着数字设备越来越强大和廉价，林林总总的机械系统已经被更为统一的电子系统所代替。

计算设备的历史悠久，不过早期的计算设备大多数是专用的，通常用于预测天文事件及其发生方位。例如，关于巨石阵，一个尚未证实的推测就认为它是一座天文观测站。公元前 100 年制造的安提基瑟拉机器就是一台天文计算机，其机械结构之精妙令人叹为观止。算盘之类的演算工具也已经使用了近千年时间，在亚洲尤为流行。计算尺发明于 17 世纪早期，也就是约翰·纳皮尔提出对数概念没多久之后。

虽然尚存在争议，但一般认为，当今意义上的计算机始于 19 世纪的英国，由查尔斯·巴贝奇提出。巴贝奇是一位科学家，对航海和天文学感兴趣，而这两项事业都需要通过写满了数值的表格来计算方位。巴贝奇花费了毕生精力来制造用于计算的设备，试图把冗长乏味、易出错的手工算术运算机械化。但由于各种原因，包括与资助人之间的关系疏离，他的雄心壮志始终没有得偿所愿。不过，他的设计是正确的，现代人利用他那个时代的工具和材料按其设计可以制造出他的机器。如今，在伦敦的科学博物馆、加州山景城的计算机历史博物馆等地，都能看到这样的机器。

一位年轻的女士受巴贝奇鼓舞而对数学和他那个计算设备产生了兴趣。这位女士就是诗人乔治·拜伦的女儿，奥古斯塔·爱达·拜伦，也就是后来的勒芙蕾丝伯爵夫人。她写过一份详细说明，讲述如何用分析引擎（巴贝奇所计划制造机器里最高级的一个）进行科学计算，并推测这种机器也可用于非数值计算，比如作曲。爱达·勒芙蕾丝通常被认为是世界上第一位程序员，编程语言 Ada 也是以她的名字而命名的。

在 19 世纪后期，赫尔曼·何乐礼为美国人口统计局设计并制造了制表机，用它制作人口统计数据表格要比手工快得多。何乐礼借用了雅卡尔织布机的思路，用卡片纸上的孔洞把人口统计数据编码成他的机器能处理的格式。令何乐礼名声大噪的是，1880 年的人口数据花了六年才制成表，而用了他的穿孔卡片和制表机之后，1890 年的数据仅一年就完工。他创立了一家公司，经过多次并购之后成为国际商业机器公司，也就是我们现在熟知的 IBM。

巴贝奇的机器是由齿轮、转轮、杠杆、拉杆组合起来的复杂机械，而 20 世纪电子学的发展使得人们有条件去设想没有运动部件的计算机会是什么样子。到了 20 世纪 40 年代，在费城的宾夕法尼亚大学，由布莱斯波·埃克特和约翰·莫奇利设计制造的 ENIAC（电子数值积分计算机的英文首字母缩写）横空出世，成为全电子计算机的最重要标志。ENIAC 占满了一间大屋，需要消耗很多电力，每秒钟能做大约五千次加法。它本来是为了计算弹道等军事用途而制造的，但直到 1946 年「二战」已结束多时它才完工。ENIAC 的一些部件，现存放在宾大的摩尔工程学院作展览。

巴贝奇清楚地意识到，计算设备可以把操作指令和数据保存为同样的形式，但 ENIAC 并没有把指令像数据那样保存在存储器中，而是通过扳动开关和重新连线来实现编程。第一台真正实现了存储程序的计算机于 1949 年在英国面世，称为 EDSAC（延迟存储电子自动计算机的英文首字母缩写）。

早期的电子计算机使用电子管作为基本计算元件。电子管是一种大小和形状类似于柱形电灯泡的电子设备，缺点是昂贵、易碎、笨重、费电。而随着 1947 年晶体管和 1958 年集成电路的相继发明，计算机的新时代才真正开始。用此技术制造的设备是当今的电子系统越来越小、越来越迅速以及越来越便宜的原因所在。

3『

[电子管_百度百科](https://baike.baidu.com/item/%E7%94%B5%E5%AD%90%E7%AE%A1)

电子管，是一种最早期的电信号放大器件。 被封闭在玻璃容器（一般为玻璃管）中的阴极电子发射部分、控制栅极、加速栅极、阳极（屏极）引线被焊在管基上。利用电场对真空中的控制栅极注入电子调制信号，并在阳极获得对信号放大或反馈振荡后的不同参数信号数据。

』

1、数字计算机包含处理器和存储器。处理器执行简单的指令，速度非常快。它可以根据早先计算的结果以及外界的输入，决定接下来做什么。存储器包含数据和处理数据的指令。

2、计算机是一种通用的机器。它从存储器中读取指令，而人把不同的指令放到存储器中，可以改变它要执行的计算。指令和数据要通过使用场景区分，一个人的指令可以是另一个人的数据。

3、图灵的结论：从能够执行完全相同的计算的意义上说，这种结构的所有计算机（包括你以后可能会看到的任何计算机）具有完全相同的计算能力。当然，它们的性能可能千差万别，但在不考虑速度和存储器容量的前提下，它们的能力则是等价的。最小最简单的计算机也能够完成大计算机所能完成的计算。的确，可以通过编程让任何计算机模拟其他计算机，而图灵正是这样证明了他的结论。

4、计算机的逻辑结构自冯·诺依曼之后并没有太大改变，但物理结构已经发生了巨大变化。摩尔定律已经应验了大约 50 年，成为迄今为止几乎完全兑现的预言。摩尔定律预言了在既定的空间和成本之下，个别器件的大小和价格会呈指数级下降，而它们的计算能力呈指数级增长。它们都是数字计算机：所有一切最终都要化简为比特，单独或成组地以数字形式表示信息，所谓信息可能是指令也可能是数据。这些比特的含义取决于它们的上下文。可以化简为比特的任何事物，都可以通过数字计算机来重现和处理。

从某种角度讲，互译结果相当不错，甚至还考虑到了一些语法问题。然而，翻译过程忽略了一个谈论计算机时至关重要的事实，即「比特」（bit）并不等同于「碎块」（piece），而「化简为比特」（reduced to bits）与「切分成碎块」（cut in pieces）的意思也完全不同。类似的结论同样适用于今天的语音识别、面部识别及其他图像处理：计算机可以做得很出色，但没人会说它们的能力可以与人类比肩。顺便说一下，如果你再试一次，翻译的结果又会不一样，其算法和底层的数据好像变化很频繁。

计算机可以下出特级大师水平的国际象棋，但在人脸识别上却不及幼儿。早些时候，计算机下国际象棋的水平其实很烂，但随着机器的速度不断加快，它下棋的水平明显提高，但这一切几乎完全是因为它比人类对手能够多看几步。类似地，语言翻译、语音识别等领域最大的进步，主要还体现在庞大的数据量（比如不同语言的平行文本数量），而有了这些数据计算机才能接近人的表现。

图灵测试想做的，就是在隐身、忽略响应时间等因素的前提下，通过对人和计算机提问，看看能否区分哪个是人，哪个是计算机。

尽管如此，我们还是有太多太多的事物不知道怎么用比特来表示，更不必说怎么用计算机来处理了。比如，日常生活中最重要的一些事物：艺术、创意力、真理、美、爱、荣誉和价值。我想在一定的时期内，这些事物仍将超出计算机的能力之外。

1『现实世界里知道如何用比特表示的事物只是一部分，而在这部分里能够用计算机处理的又只占一小部分。』

## 总结汇总

1、计算机里有什么可以从 2 个维度来看。功能组成上看，计算机里有什么组件、是做什么的、它们是如何连接起来的；从物理结构上看，这些东西是啥样子的，它们是如何被制作出来的。前一个维度的功能组成一直没变过，而后一个维度的物理结构一直在更新，典型如摩尔定律。

2、有关计算机表示信息的三个核心思想：计算机处理的信息是数字信息而非模拟信息；计算机处理的信息是二进制编码的；较大的信息用比特组来表示。

3、CPU 是一个小型指令系统，关键的一点是它能根据它所处理的数据决定下一步做什么。

## 0101. 计算机里有什么

先大略看看计算机里面都有些什么东西。这个问题可以从两方面来看：逻辑上或者说功能上的组成，即每一部分是什么、做什么、怎样做、之间如何连接；以及物理上的结构，即每一部分长什么样子、如何建造起来的。

这些计算机看起来很不一样，用起来也感觉不一样，但这仅仅是表象，其实根本没区别。为什么这么说呢？可以拿汽车来打个大致的比方。在功能构成上，这一百多年来的汽车都是一样的。每辆汽车都有个发动机，通过燃烧某种燃料来驱动发动机运转，这样车就能开了；都有个方向盘，这样司机（也许可以比作软件）就能操纵车的方向；还有储存燃料的地方，以及留给乘客和行李的位置。但是这一个多世纪以来汽车在物理构成上却变化巨大：造车的材料日新月异，行驶越来越快，越来越安全，可靠性和舒适性与过去不可同日而语。

计算机也是一样的道理。当今的计算机在逻辑结构上和 20 世纪 50 年代的非常相似，但是物理指标的进步却远甚于汽车。当今的计算机和 50 年前的比起来，体积更小，价格更廉，运行更快，也更可靠，有些字面上的指标甚至提高了百万倍。计算机如此普及，其根本原因就在于此。

一件东西的功能表现与物理特性之间的区别，也就是说它能做什么与它是怎样建造起来的（或者说内部的工作方式）之间的区分，是很重要的。就计算机而言，「它是如何建造出来的」这个问题的答案在以惊人的速度变化着，「它运行起来有多快」也是如此，但是「它能做什么」的答案却没什么变化。后面将会反复提到这两方面的区别。

如果我们画一张抽象图展示计算机内部有什么，也就是它逻辑上或者功能上的体系结构，那么 Mac 和 PC 的结构都是如下图所示：一个处理器（CPU）、一些主存储器（内存）、一些大容量存储器（磁盘）和各种各样的其他部件，一组叫做总线的线缆把所有这些连接起来，在各部件之间传输信息。

计算机的基本组成，包括处理器、存放指令和数据的存储器以及输入输出设备，在 60 多年前就已经是标准了。这种体系结构通常称为冯·诺依曼体系结构，以约翰·冯·诺依曼的名字命名。他在 1946 年与阿瑟·勃克斯、赫尔曼·戈德斯坦共同撰写的论文《电子计算仪器逻辑设计的初步讨论》中描述了这种体系结构。尽管目前人们对于以冯·诺依曼来命名这种体系结构是否掩盖了其他人的贡献尚有争议，但这篇论文条理清晰，见解深刻，即便在今天也值得一读。例如，论文的第一句就指出：「为了让这台完整的设备成为通用的计算机器，它必须包含某些主要元件用于运算、存储数据、控制以及连接操作人员。」翻译成现在的术语就是，CPU 提供运算和控制功能，内存和磁盘用于存储数据，键盘、鼠标和显示器用于连接操作人员。

2『已下载论文「2020001Logical_Design_of_an_Electronic_Computing_Instrument」；已下载书籍「2020005计算机与人脑」；已下载原文书籍「2020005The_Computer_and_the Brain」、「2020006John_von_Neumann_Selected_Letters」。』

如果我们说计算机有大脑的话，处理器，或者叫中央处理单元（缩写为 CPU）就是计算机的大脑。处理器进行运算，来回搬运数据，并控制着一切别的操作。CPU 有一张指令表，它可以执行的操作是有限的，但执行起来速度异常之快，高达每秒钟几十亿次。它可以根据先前的计算结果决定接下来执行什么指令，所以在很大程度上，它可以主宰自己的命运。

比如你可能看到对 CPU 的描述是「英特尔双核酷睿 2.1 GHz」。这是什么意思呢？这款 CPU 是英特尔制造的，一片封装的内部实际上有两个 CPU。在这句话里，「核」的意思就是处理器。2.1 GHz 看起来更有趣。CPU 的速度大体上是以每秒钟执行的操作数量、指令数量或更小的动作数量来度量的。CPU 使用一个跟心跳或者钟表嘀嗒类似的内部时钟来控制基本操作的节拍，度量 CPU 速度的指标之一就是看这个内部时钟每秒振动多少次。每秒钟心跳一次或者嘀嗒一次就是 1 赫兹，记为 1 Hz。这个单位名称是为了纪念德国工程师海因里希·赫兹，他在 1888 年发现了产生电磁辐射的方法，由此直接导致无线电广播和其他无线系统的诞生。广播电台发射的广播信号频率为兆赫（百万赫兹），比如 102.3 MHz。现在的计算机通常运行在十亿赫兹的数量级上，也就是吉赫，记为 GHz。

主存储器，也就是随机访问存储器（缩写为 RAM，即内存 ），里面保存了处理器和计算机的其他部件正在活跃使用的信息；CPU 可以改变内存里的内容。内存里不仅保存了 CPU 正在处理的数据，还保存了让 CPU 如何处理数据所需运行的指令。这一点至关重要：通过把不同的指令加载进内存，就可以让计算机做不同的计算。这样，存储程序型计算机就成为通用的设备：同一台计算机，只要在内存里放上适当的指令，就可以运行文字处理程序、制作数据表格、上网浏览、收发电子邮件、计算纳税款，还可以播放电影。存储程序这个理念的重要性怎么强调都不为过。

内存是计算机运行的时候存储信息的地方。运行中的程序，比如 Word、iTunes 或浏览器，它们的指令就放在内存里；这些程序操作的数据，比如屏幕上显示的照片、正在编辑的文档、正在播放的音乐等，也是放在内存里的；而 Windows、Mac OS X 或其他操作系统，也就是能让你在同一时间运行多个应用程序的幕后功臣，它们运行时的指令还是放在内存里。

内存之所以被称为「随机访问」，是因为 CPU 能以同样的速度快速访问其中任何地方的信息。以任何顺序随机访问不同位置时，速度不会受到任何影响。与之相比，老式录像带的访问方式则称为「顺序访问」，比如想观看电影尾声的时候只能从头开始慢悠悠地「快进」，跳过前面的内容。

内存是易失性的，也就是掉电之后里面的内容会消失，当前活跃的信息就都丢掉了。所以要养成小心谨慎的好习惯，经常保存正在做的工作，尤其是在台式机上，不小心踢掉电源线可不是闹着玩的。

你计算机上的内存大小是有限的。表示容量的单位是字节。一字节大小的内存，可以放入单个字符（比如 W 或者 @），可以放入一个整数比如 42，还可以放入大数值的一部分。第 2 章会展示内存或者计算机其他部件中的信息是如何表示的，因为这是计算的一个基础问题，但在此之前，你可以把内存想象成一大堆完全一样的小盒子，上面从 1 开始编号到一二十亿，每个盒子里可以放进一小片信息。

内存的容量有多大？我正在用的这台笔记本电脑，内存有 20 亿个字节，或者说 2 吉字节，也就是 2 GB。有很多人还认为这么些内存太小了点。原因是，虽然所有程序同时开起来的时候，内存再多也不够用，但是内存越大，可供发挥的空间就越大，而这往往也可以说成是计算得越快。如果你想要计算机运行更快的话，多买内存看起来是最佳策略。

内存的容量很大，但还是有限的，并且掉电之后内容会消失。大容量存储器则能在掉电后仍保存着里面的信息。最常见的大容量级存储是磁盘，有时也称为硬盘或硬驱。磁盘能保存的信息比内存大得多，并且是非易失性的，也就是说，磁盘上的信息不论通电还是断电都一直在那里。于是数据、指令和其他信息都长期保存在磁盘上，仅在需要时临时读入内存。磁盘空间比内存便宜 100 倍，只是访问起来要慢得多。磁盘保存信息的方法是对旋转的金属盘片表面的磁性材料上的微小区间进行不同方向的磁化。计算机工作时的嗡嗡声和咔嗒声就是磁盘把磁头移向盘片表面正确位置时发出来的。

硬盘是用来展示逻辑结构和物理实现之区别的好例子。在 Windows 下运行资源管理器或者在 Mac OS X 下运行 Finder，可以看到硬盘里的内容组织为层次分明的文件夹和文件，但真正的数据是完全存放在旋转的机械装置、没有活动部件的集成电路或者其他存储设备里的。计算机里装的究竟是哪种「磁盘」其实无关紧要，事实上是硬盘里的硬件电路和操作系统里称为文件系统的重要软件一起创建了这种有组织的结构。

这种逻辑结构跟人的思维相当匹配，或者更合适的说法是，到如今我们已经完全习惯了这种组织方式，所以别的存储设备也提供了同样的组织方式，哪怕是它们使用了完全不同的物理方法来实现存储。比如说，CD-ROM 或者 DVD 使用了看起来跟硬盘上的文件系统一样的方式来储存信息；USB 设备、数码相机和可插存储卡等其他小玩意也都这样；就连现在已经完全淘汰的老古董软盘，在逻辑层次上看起来也完全一样。

在体系结构示意图里，这些设备看上去是通过一组线缆连接在一起的。借用电气工程的术语，这组线缆称为总线。实际上，计算机内部有好几组总线，每组总线都具有适合其功能的特性。比如 CPU 和内存之间的总线，线路短，传输快，但是价格贵；而连接到耳机插孔的总线，线路长，传输慢，但是价格便宜。有些总线在机箱外也露出了一部分，比如无所不在的通用串行总线，也就是把外设插入计算机所用的 USB 总线。

手机没有硬盘，但是有非易失性的闪存，这样它就能够在关机时仍保存电话本、应用程序和其他信息。手机能连接的外部设备没那么多，但还是可能有蓝牙、耳机和外部话筒插孔和 USB 接口等。

由于计算机里面的很多东西都抽象成了逻辑结构，所以能实际观察和触摸硬盘、集成电路芯片、制造芯片所用的晶圆等东西对于学习计算机是很有用的，而观察一些设备的进化史也很有趣。比如现在的笔记本电脑硬盘跟 10 年前的没什么区别，只是容量增大了 10 倍或者 100 倍，但从外表根本看不出来。另一方面，承载计算机部件的电路板却能看出明显的发展。部件的数量在减少，因为更多的电路被做到了内部，布线更加精细，电路的引脚比起 20 年前多了很多，也密集了很多。下图展示了一块 20 世纪 90 年代后期的台式机电路板，CPU、内存等部件安装或者插入到电路板上，通过反面的印刷线路连接起来。

计算机里的电子线路是由大量基本元件搭建起来的，但基本元件的类型却只有很少几种。其中最重要的一种是逻辑门电路，用来根据一个或两个输入值计算一个输出值，也就是用输入的电压或电流信号来控制输出的电压或电流信号。只要把足够多的门电路用正确的方式连接起来，就能执行任何计算。查尔斯·佩措尔德的《编码》是介绍这方面知识的好书，还有一些网站用图形动画的方式演示逻辑电路如何进行数学运算和其他计算。

2『已下载书籍「2020004编码」和原书「2020004Code」。』

当今最重要的电路元件是晶体管，是 1947 年由约翰·巴丁、瓦尔特·布拉顿和威廉·肖克利在贝尔实验室发明的，他们因此获得了 1956 年的诺贝尔物理学奖。在计算机里面，晶体管基本上就是个开关，也就是用电压控制电流通断的设备。任何复杂系统都可以构建在这么简单的基础之上。

门电路过去是用分立元件搭建的。制造 ENIAC 的时候，用的是跟灯泡差不多大小的电子管，而 20 世纪 60 年代的计算机则用的是铅笔上橡皮头那么大的单独的晶体管。下图展示了第一颗晶体管的仿制品（左边）、电子管和封装起来的 CPU 芯片。电子管大约 10 厘米长，CPU 实际的电路部分是在中间，大约 1 平方厘米。图上这么大的现代 CPU 芯片里则可以集成上亿颗晶体管。

如今的逻辑门电路是创建在集成电路上的。集成电路（integrated circuits）缩写为 IC，通常也称为芯片或微芯片。集成电路在一个平面里（很薄的硅片）包含了电路板的所有部件和布线，通过一系列复杂的光学和化学流程制造出来，这样就得到了没有分立元件也没有传统布线的电路。因此，集成电路比分立部件的电路小得多也可靠得多。芯片是在直径 12 英寸（30 厘米）的晶圆上批量制造的，然后把晶圆切割成独立的芯片，单独包装。芯片通常安装在比它大很多的封装壳上，用几十到几百条引脚连接到系统的其他部分。下图展示了一片封装起来的集成电路。实际的电路部分在中心，大约 1 平方厘米。

集成电路是 1958 年由罗伯特·诺伊斯和杰克·基尔比各自独立发明的。诺伊斯在 1990 年逝世，基尔比则在 2000 年因此获得了诺贝尔物理学奖。集成电路是数字电子设备的主角，但其他技术也发挥了作用，包括硬盘中的磁存储、CD 和 DVD 中的激光、光纤网络中的激光等。过去 50 年里，所有这一切都在尺寸、容量和价格方面发生了惊人的改进。

1965 年的时候，戈登·摩尔，也就是后来的 Intel 公司联合创始人及长期 CEO，发表了一篇文章《在集成电路里填入更多部件》。他注意到，随着技术的发展，在给定大小的集成电路内部可以制造的设备（主要是晶体管）数量大约每年翻一番。后来他又把这个速率修正为两年，另外有些人则认为是 18 个月。由于计算能力大体上可以用晶体管数量来代表，这就意味着计算能力只要两年或更短时间就能翻倍，也就是说，20 年下来可以翻十番，集成度提高 2 的 10 次方也就是大约 1000 倍，经过 40 年则可以提高 100 万倍或更多。

这种指数式增长，也就是通常说的摩尔定律，已经持续了 50 年，于是当今集成电路里的晶体管数量早已超过了 1965 年那时候的 100 万倍。巨大的量变引发了质变。摩尔定律的实际图表，尤其是处理器芯片的数据，显示出晶体管数量从 20 世纪 70 年代早期 Intel 8008 CPU 的几千个晶体管，已经发展到现在廉价家用笔记本电脑的处理器的上亿个。

用来描述电路规模的基本数值是集成电路里的特征尺寸（即其中的最小尺寸），比如导线的宽度。在过去的很多年里，这个数值在稳步缩减。我在 1980 年设计的第一片集成电路（也是我设计的唯一一片）用的是 3.5 微米的特征尺寸；如今的很多集成电路，特征尺寸是 32 纳米，也就是 32 米的 10 亿分之一，下一步将会是 22 纳米。对比一下，一张纸的厚度或者一根头发的粗细是 100 微米，即十分之一毫米。

1『之前在电脑报里看到的，CPU多少多少 nm 工艺，知道就是特征尺寸，即导线的宽度。』

集成电路的设计和制造是相当复杂的业务，竞争异常激烈。而且制造运行（生产线）也很昂贵，新建的工厂可以轻而易举花掉几十亿美元。如果一个公司的技术和资金跟不上，就会在竞争中严重处于劣势。如果一个国家没有这样的资源，就要为了这些技术依赖外国，存在严重的战略问题。

到某个阶段，摩尔定律会失效。以前曾多次有人断定摩尔定律的极限已到来，但后来又发现了突破极限的方法。然而现在，我们已经到了这样的阶段：有的电路里仅包含极少数原子，这么小的结构已经很难控制。CPU 速度已经不再每两年翻一番（部分原因是芯片越快散热越多），但是内存容量仍然在按这个规律翻倍。与此同时，现在的处理器在一片芯片里可以有多颗 CPU。

比较一下今天的个人电脑和 1981 年最初的 IBM PC，对比是惊人的：第一代 PC 的处理器主频是 4.77 MHz，现在一片 2.3 GHz CPU 的时钟频率快了大约 500 倍；第一代 PC 有 64 千字节内存（「千」缩写为 K），现在一台 4 GB 内存的计算机大约是它的 6 万倍；第一代 PC 至多有 750 KB 软盘，没有硬盘，现在机器的磁盘空间则增加了 100 万倍；第一代 PC 的显示器是 11 英寸的，只能在黑色背景上显示 24 行 80 列的绿色字符，而我写这本书的时候所用的 24 英寸显示器能显示 1600 万颜色；在 1981 年，买一台配备了 64 KB 内存、160 KB 单软驱的 PC 要花 3000 美元，相当于 30 年后的 5000-10000 美元，而现在买一台 2 GHz 处理器、4 GB 内存、400 GB 硬盘的笔记本电脑只要花几百美元。

1『老的显示器显示的是 80 列的绿色字符。原来这就是代码习惯设置 80 个字符一行的来源，至少算一个来源，还有个原因是方便多屏显示，细长的比较合适。』

20 世纪计算机科学的伟大发现之一是，现在的数字计算机、最初的 PC 以及再往前体积更大、计算能力更弱的老式计算机器，它们在逻辑或者功能上的特性是完全一样的。如果我们不考虑速度、存储容量这些因素，这些计算机可以做完全一样的计算。

## 0102. 比特、字节与信息表示

计算机表示信息的三个基本思想。首先，计算机是数字处理器。它们存储和处理离散的信息，这些信息表现为不连续的块，具有不连续的值，基本上就是一个个数值。而与之相对的模拟信息，则是平滑变化的值。其次，计算机用比特表示信息。比特就是二进制数字，即一个非 0 即 1 的值。计算机中的一切都用比特来表示。计算机内部使用二进制，而不是人们所熟悉的十进制。再次，较大的信息以比特组表示。数值、字母、单词、姓名、声音、照片、电影，以及处理这些信息的程序所包含的指令，都是用比特组来表示的。

为什么用二进制而不用十进制？因为制造只有两种状态（如开和关）的物理设备，比制造有十种状态的设备更容易。这种简单的性质在数不清的技术中都得到了利用，比如：电流（流动或不流动）、电压（高或低）、电荷（存在或不存在）、磁性（南或北）、光（亮或暗）、反射率（反光或不反光）。约翰·冯·诺依曼很早就清楚地认识到了这一点，他在 1946 年说过：「我们储存器中最基本的单位自然是采用二进制系统，因为我们不打算度量电荷的不同级别。」

为什么我们要知道或者要关心二进制数呢？这个问题问得好。至少在我的课上，理解另一种不熟悉的数制，相当于做了一次量化推理的练习，而有了这个训练之后，对我们习以为常的十进制的理解也将更深一层。除此之外，另一个意义在于，比特的数量在一定程度上揭示了涉及的空间、时间或者复杂性。再从根本上说，计算机值得我们花时间去理解，而二进制正是其运作的核心所在。

现实生活中也能找到一些与计算机无关的应用二进制的场景，或许是因为人们都认为大小、长短的加倍、减半是一种自然而然的运算。比如，高德纳在《计算机程序设计艺术》中描述了 14 世纪英国的酒器单位，分为 13 个二进制量级：2 吉耳是 1 超品（chopin），2 超品是 1 品脱，2 品脱是 1 夸脱，依此类推，直到 2 百瑞尔（barrel）是 1 豪格海（hogshead），2 豪格海是 1 派普（pipe），2 派普是 1 坦恩（tun）。这些单位中差不多还有一半仍然在英制液体度量体系中使用。当然，其中一些很令人陶醉的词，比如费尔金（firkin）和基尔德坎（kilderki）（2 费尔金是 1 百瑞尔），今天已经很难得见了。

2『已下载书籍「2020007计算机程序设计艺术」和原文书籍「2020007The_Art_of_Computer_Programming」。』

首先，我们谈一谈模拟与数字的区别。「模拟」（analog）与「类似的」（analogous）词根相同，表达的意思是：值随着其他因素变化而平滑变化。现实生活中的很多事物都具有模拟性质，比如水龙头或汽车方向盘。如果你想让车转个小弯，轻轻打一打方向盘即可，打多打少由你自己来定。拿它跟转向灯作个比较，后者要么开要么关，没有中间状态。在模拟装置中，某些事物（汽车转弯幅度）会随另一些事物（方向盘转动幅度）的变化平滑而连续地变化。变化过程没有间断，一个事物的微小变化就意味着另一个事物的微小变化。

数字系统处理的是离散值：可能的取值是有限的（转向灯只可能是关闭的或在左右方向打开）。某个事物小小的变化，要么不引发其他事物变化，要么就引发其他事物的突变，使其从一个离散的值跳到另一个离散的值。

比如手表。「模拟」手表有时针、分针和秒针，秒针每分钟转一圈。虽然现代的手表都由内部的数字电路控制，但时针和分针仍然随着时间流逝而平滑移动，而且三根表针都能走遍所有可能的位置。数字手表或手机时钟显示的时间只有数值。显示屏每秒变化一次，每分钟更新一次分钟的值，但不会显示分钟的小数位。

有人要问，为什么用数字而不用模拟呢？我们这个世界可是模拟的呀，而且手表、速度表等等模拟设备也更容易让人一目了然。但不管怎样，很多现代的技术都是数字的，而且我们这本书也是在讲述数字的故事。外部世界的数据 —— 声音、图片、运动、温度，等等一切，在输入端都会尽可能早地转换为数字形式，而在输出端则会尽可能晚地转换回模拟形式。原因就在于数字化的数据容易处理，无论最初来源是什么，数字化数据都可以用多种方式来存储、传输和处理，但模拟信息则不行。第 9 章将会介绍，通过删除冗余和不重要的信息，还可以压缩数字化信息。为了安全和隐私可以对它进行加密，可以将它与其他数据合并，可以复制它而不出错，可以通过互联网把它发送到任何地方，可以将它保存到几乎无限种设备中。而对于模拟信息，上述很多做法是根本行不通的。

与模拟系统相比，数字系统还有另一个优势，就是它更容易扩展。比如说，给模拟天文馆增加一颗新发现的星星，专业人员必须辛苦地做出光照效果来；而在数字天文馆，只要在数据文件里添加一行信息即可。我的数字手表可以连续不断地以百分之一秒显示时间流逝，而要让模拟手表做到这一点可就太难了。不过，模拟系统有时候也有它的优势，像泥版、石雕、羊皮纸、图书和照片等古老的媒体，都经历了数字格式未曾经历过的时间考验。

把照片转换为数字形式，应该是最容易想象的了。假设我们给自家的小猫拍张照片。胶卷相机的成像，是通过把胶片感光区曝露给被拍物体反射的光线实现的，胶片上不同区域接收到的不同颜色的光量不同，从而影响胶片上的染料。在胶片显影、印相时，彩色染料数量决定了显示出来的颜色变化。

对数码相机来说，镜头把影像聚焦到一块位于红、绿、蓝滤镜后面的矩形感光器阵列上，感光器由微小的光敏探测器组成。每个探测器存储一定数量的电荷，与落在它上面的光量成正比。这些电荷被转换为数字值，照片的数字表示就是这些表现光强度的数值序列。探测器越小，数量越多，电荷测量的结果就越精细，数字化图像就能越精确地反映原始的影像。

传感器阵列的每个单元都由一组能够捕获红、绿、蓝光的探测器构成，每个单元对应一个像素，即像元。3000×2000 像素的图像，包含 600 万个像元，或 600 万像素，对今天的数码相机而言并不算大。像素的颜色通常由三个值表示，分别代表红、绿、蓝光的强度，因此 600 万像素的图像总共要存储 1800 万个颜色值。屏幕在显示图像时，使用的是红、绿、蓝光三元组的阵列，其亮度与像素亮度一致。如果你用放大镜仔细观察手机或电脑屏幕，很容易看到每个独立的彩色块。

第二个模数转换的例子是声音，尤其是音乐。之所以说音乐是个不错的例子，原因在于以它为代表的数字信息的所有权，第一次引起了社会、经济和法律上的广泛关注。数字音乐与唱片或磁带不同，你可以在自己家的计算机里无限次地复制它，完全免费，而且还可以通过互联网把它复制发送到世界的任何角落，不会有任何音质损失，同样完全免费。唱片业把这当成了严重的威胁，试图通过法律或政治手段阻止数字音乐的拷贝。

什么是声音？音源通过振动或快速运动引起空气压力的波动，人的耳朵把这种压力变化转换为神经活动，经大脑解释之后就形成了「声音」。1870 年代，托马斯·爱迪生制造了一个叫做「留声机」的机器，这台机器能把声波转换为蜡筒上类似的螺旋沟槽，而通过这些沟槽又能再次创造出同样的气压波动来。把声音转换为沟槽就是「录音」，而从沟槽换回到气压波动就是「回放」。爱迪生的发明迅速地得到改进，1940 年代就出现了密纹唱片（long-playing record）或简称 LP，而且至今还在使用（尽管数量已经不多了）。麦克风随着时间推移把变化的声压转换为变化的值并记录下来，然后根据这些值在乙烯基的盘片上压制出与声压一致的螺旋沟槽。播放 LP 时，唱针随着沟槽起伏，其运动轨迹被转换为波动的电流，电流经过放大后驱动扬声器或耳机，通过它们的振动薄膜产生声音。

把空气压力随时间的变化形象地绘制出来并不难。其中压力可以用任何物理方法来表示，在此我们假设用电路中的电压。当然，电流、光的亮度，以及爱迪生发明的留声机中的纯机制装置都没有问题。

图中声波的高度表示声音强度或大小，水平方向的坐标轴表示时间：每秒钟声波的数量就是声调或频率。假设我们以固定时间间隔连续测量这条曲线的高度（在这里就是电压值），就会得到下图所示的这些垂直线条。

测量得到的数值连接起来与曲线近似，测量越频繁，越准确，结果也就越吻合。测量得到的数值序列是波形的数字化表示，可以存储、复制、操作它们，也可以把它们发送到任何地方。如果有设备把这些数值转换成对应的电压或电流，然后再通过电压或电流驱动音箱或耳机，就能够实现回放。从声波到数值是模数转换，相应的设备叫 A/D 转换器；反过来当然是数模转换，或者叫 D/A。转换过程并不是完美无缺的，两个方向的转换都会损失一点信息。但大多数情况下，这种损失是人所觉察不到的。

与 LP 唱片上的模拟沟槽不同，CD 用长长的螺旋状轨道在盘面的一侧记录数值。轨道上任意一个区块的表面要么平滑，要么是一个微小的凹坑。这些下凹或平滑的区块就是用来编码声波的数字值的，每个区块是一位，连续的多位表示二进制编码中的一个数值。光盘旋转时，一束激光照射到轨道上，而光电传感器则检测每个区块上反射回来的光量多少。如果光量不多，说明是凹坑；如果反射光很强，说明不是凹坑。标准 CD 编码采样率为每秒 44 100 次，而每次采样获得两个振幅值（立体声的左、右声道），精确度为 65 536（即 216，这并非巧合）分之一。轨道上的每个区块非常非常小，小到只有用显微镜才能看见，一张 CD 的表面上有 60 亿个小区块。（DVD 中的区块更小，由于区块更小，激光束频率更高，DVD 的存储容量近 5GB，而 CD 大约为 700MB。）

音频 CD 的出现几乎让 LP 没有了立足之地，相比之下，CD 的优点实在太多了：落上点灰尘也不用太担心了，更没有磨损一说，而且绝对小巧。但到了我写这本书的时候，LP 开始在某种程度上复苏，流行音乐 CD 的人气则日渐衰退。有朝一日，CD 很可能也会像 LP 一样变成古董，这倒让我很高兴，因为我收藏的音乐全部都是 CD 格式的。我现在完全拥有它们，而它们的存在将比我的生命更久远。CD 还有第二个用途，那就是作为存储、分发软件及数据的介质，不过这个功能已经被 DVD 取代，而 DVD 很可能又会被下载所取代。

声音和图片经常会被压缩，因为这两种媒体包含很多人类根本感知不到的细节。对于音乐，典型的压缩技术是 MP3，大约能把音频文件的体积压缩到原来的十分之一，同时几乎让人感觉不到音质下降。对于图片，最常用的压缩技术是 JPEG（是制定该标准的联合图像专家组  —— Joint Photographic Experts Group 的英文字头），它的压缩率也能达到 10 倍甚至更高。上文提到很多处理对数字信息能做，但对模拟信息却很难（或不可能），压缩就是一个例子。

1870 年代，摄影师埃德沃德·迈布里奇向世人证明，快速连续地显示一系列静态图片能够创造出运动的错觉。今天，电影显示影像的速度是每秒 24 帧，而电视大约是 25 到 30 帧，这个速度足以让人的眼睛把顺序播放的影像感知为动画。而通过组合（并同步）声音和影像，就可以创造出数字电影。而利用压缩技术减少空间占用，则催生了包括 MPEG（代表 Moving Picture Experts Group）在内的标准电影格式。实际上，视频的表示要比单纯的音频表示更复杂，一方面是它本身就复杂，另一方面很大程度上还因为它受到了电视的拖累，而电视在其存在的大部分时间内都是模拟的。模拟电视在世界范围内正逐渐被淘汰，而美国 2009 年已经将广播电视切换成了数字信号。

还有一些信息很方便以数字形式来表示，因为除了想好如何表示它之外，根本不需要做什么转换。比如这本书中的文字、字母、数字和标点符号，我们称为其普通文本。可以为其中每个字母指定一个唯一的数值，如 A 是 1，B 是 2 等等，这不就是一种数字化表示方法嘛。而事实也正是如此，只不过在表示标准中，A 到 Z 用的是 65 到 90，a 到 z 用的是 97 到 122，数字 0 到 9 用的是 48 到 57，而标点符号等其他字符用的是其他数值。这个表示标准叫做 ASCII，即 American Standard Code for Information Interchange（美国信息交换标准代码）。

不同地区有不同的字符集标准，但也有一个世界通用的标准叫 Unicode，它为所有语言的所有字符都规定了一个唯一的数值。这是一个非常庞大的字符集，人类的创造力是无穷无尽的，但在建立自身书写系统方面却很少有规则。目前，Unicode 涵盖的字符远远超过 100 000 个，而且这个数字还在稳步增长。可想而知，Unicode 中的大部分都是包括中文在内的亚洲字符集，但决不限于此。要了解 Unicode 都包含哪些字符集，可以访问 unicode.org，这个站点内容丰富。

3『[Unicode – The World Standard for Text and Emoji](https://home.unicode.org/) 和 [Overview – Unicode](https://home.unicode.org/basic-info/overview/)』

 表示数字信息的最基本单位是比特（bit）。英文 bit 是合并 binary digit（二进制数字）之后造出来的，造这个词的人是统计学家约翰·图基，时间是 1940 年代中期。（图基还在 1958 年发明了单词 software ——  软件。）
 
 一个比特表示开 / 关、真 / 假之类的二选一的情形没有问题，但我们经常还要面对更多选项，表示更复杂的事物。为此，可以使用一组比特，然后为不同的 0 和 1 的组合赋予不同的含义。比如，可以用两个比特来表示大学四年：新生（00）、大二（01）、大三（10）和毕业班（11）。如果再多考虑一种情况，比如研究生，那两个比特就不够用了，因为两个比特只有 4 种组合，没有第五种可能。但是三个比特没问题，实际上三个比特能表示 8 种不同的情况，这样我们就可以把教师、教工和博士后都包含进来。三个比特的全部组合为：000、001、010、011、100、101、110 和 111。比特数与它们所能表示的情况数之间有一个关系，很简单：N 个比特能表示 2^N 种组合，即 2×2×2…×2（乘 N 次）。
 
 由于计算机中的一切都是以二进制形式来处理，因此像大小、容量等概念一般都是用 2 的几次幂来表达的。如果有 N 比特，那么就有 2^N 种可能的值，所以知道 2 的幂是多少（比如到 2^10）是很有用的。但随着数值越来越大，完全记住它们也没有什么必要。好在有一种简便的方法，可以得到它们的近似值：2 的某次幂与 10 的某次幂接近，它们的对应关系严格有序，容易记忆.
 
 1『2 的 10 次方、20 次方、30 次方、40 次方、50 次方（每个幕隔 10 次方），分别对应于 10 的 3 次方、6 次方、9 次方、12 次方、15 次方（每个幕隔 3 次方）。这个真是相当的有趣啊，哈哈。』
 
 （这个对照表最后包含的表示大小的单位叫「拍」或 10^15，其英文单词发音不是「皮」而是「拍」。另外，书后附有一个更全的词汇表，列出了更多单位。）随着数值增长，这个近似值的误差也会增大，不过到了 10^15 这么大的时候误差也就 12.6%，所以还是可以在很大范围内使用的。经常会有人混淆上述 2 的幂与 10 的幂之间的关系（有时候是想用来支持他们的观点），于是 kilo 或 1K 可能是指 1000，但也可能指 2^10 即 1024。一般来说，这种混淆导致的误差并不大，因此在涉及很大的比特数时，用 2 和 10 的幂来做心算没什么问题。
 
 在所有现代计算机中，数据处理及内存组织的基本单位都是 8 个比特。8 比特被称为 1 字节，而字节（byte）这个词是由 IBM 的计算机设计师维尔纳·巴克霍尔兹（Werner Buchholz）在 1956 年发明的。一个字节可以编码 256 个不同的值（28，即 8 个 0 和 1 的所有不同组合），这个值可以是一个 0 到 255 间的整数，也可以是 ASCII 字符集中的一个字符，或者其他什么。通常，为了表示更大或更复杂的数据，需要用到多个字节的字节组。两个字节有 16 比特，也就是 16 位，可以表示 0 到 2^16-1（65 535）之间的数值。两个字节也可以表示 Unicode 字符集中的一个字符。
 
 1『Unicode 字符是 16 位编码的。』
 
 这是两个字符，即「东京」，每个字符占两个字节。四个字节是 32 位，既可以表示「东京」，也可以表示最大直至 2^32-1 的值，这个最大值大约是 43 亿。用一组字节表示什么都可以，但 CPU 自己特别定义了一些适中的字节组（比如表示不同大小的整数），以及处理这些字节组的指令。

二进制写起来太长了，比十进制格式长三倍还多，因此我们常用另一种替代数制，即十六进制。十六进制的基数是 16，因此也就有 16 个数字（就像十进制有 10 个数字，二进制有 2 个数字一样），分别是 0、1、…、9、A、B、C、D、E、F。每个十六进制数字表示 4 个比特，对于一般的数值，十六进制 0 相当于二进制 0000，依此类推，十六进制 9 相当于二进制 1001。接下去，十六进制 A 相当于二进制 1010（十进制 10），十六进制 B 相当于二进制 1011（十进制 11），依此类推，十六进制 F 相当于二进制 1111（十进制 15）。

除非你是程序员，否则能看到十六进制数的机会并不多。一个例子就是网页中的颜色值。前面说过，计算机中一个像素的颜色值大都使用三个字节来表示，一个表示红色分量，一个表示绿色分量，最后一个表示蓝色分量，这就是所谓的 RGB 编码。红绿蓝三个组分分别用一个字节表示，因此红色分量就有 256 种可能的值，三个组分中的绿色分量也有 256 种可能的值，同样，三个组分中的蓝色分量也有 256 种可能的值。于是一个像素可能的颜色值就是 256×256×256 种，听起来好多啊。我们可以用 2 和 10 的幂来简单估计一下这个数有多大。这个数是 2^8×2^8×2^8，即 2^24 或 2^4×2^20，大约是 16×10^6，即 1600 万。在描述计算机显示器的情况下，你可能听说过这个数（超过 1600 万种颜色！）。

一个深红色的像素可以表示为 FF0000，换句话说，就是红色分量最多，没有绿色和蓝色；而一个鲜蓝色（并非深蓝色），即类似很多网页中链接的颜色，可以表示为 0000CC。黄色是红加绿，因此 FFFF00 就是最深的黄色。阴影的灰色具有等量的红、绿、蓝组分，因此一个中等灰度的像素应该是 808080，也就是红、绿、蓝组分的数量都相等。黑色和白色分别是 000000 和 FFFFFF。

有时候，在某计算机的广告中，我们会看到「64 位」这个说法（Windows 7 家庭高级版 64 位）。什么意思呢？计算机在内部操作数据时，是以不同大小的块为单位的，这些块包含数值（32 位和 64 位表示数值比较方便）和地址，而地址也就是信息在 RAM 中的位置。前面所说的 64 位，指就是地址。大约 25 年前，16 位地址升级到了 32 位地址（足够访问 4GB 的 RAM），而现在 32 位又升级到 64 位。我不想预测什么时候会从 64 变成 128，总得过上好一阵子吧，先不必想那么多。

1『解答了很早之前的一个疑问。电脑多少位指的是地址的编码位数。』

关于比特和字节，我们讨论到现在最重要的是必须知道，一组比特的含义取决于它们的上下文，光看这些比特看不出来。一个字节可以只用 1 个比特来表示男或女，另外 7 个空闲不用，也可以用来保存一个不大的整数，或者一个 # 之类的 ASCII 字符，它还可能是另一种书写系统中一个字符的一部分，或者用 2、4 或 8 个字节表示的一个大数的一部分，一张照片或一段音乐的一部分，甚至是供 CPU 执行的一条指令的一部分。

事实上，一个程序的指令就是另一个程序的数据。从网上下载一个新程序，或者从 CD-ROM 或 DVD 中安装该程序时，它就是数据，所有比特将无一例外地被复制一遍。但在运行这个程序时，它的比特会被当成指令，CPU 在处理这些比特时，又会把它们当成数据。

## 0103. 深入了解 CPU

中央处理器如何工作？ 它处理什么，怎么处理？直观来讲，CPU 有一个小型指令系统，包含着它能够执行的基本操作。它可以做算术题，加、减、乘、除，跟计算器一样。它可以从 RAM 中取得要操作的数据，然后再把结果保存到 RAM，与很多计算器中的存储操作一样。CPU 还要控制计算机的其他组件，确保鼠标、键盘等外围设备输入的数据得到响应，让信息在屏幕上得以显示，同时还要控制和协调连接到计算机的其他所有器件。

最重要的是，它可以作出决定 —— 尽管是简单的决定：它可以比较数值（这个数比那个数大吗？） 或者比较其他数据（这段信息与那段信息一样吗？），还能根据结果决定接下来做什么。这一条最重要，因为这意味着 CPU 能做的虽然比计算器多不了多少，但它可以在无人看管的情况下完成自己的工作。正如冯·诺依曼所说的：「要让这种机器完全自动化，即让它在计算开始后不再依赖人工操作。」

由于 CPU 能根据它所处理的数据决定下一步做什么，因此它就能自己运行整个系统。虽然其指令系统并不大，或者说并不复杂，但 CPU 每秒可以执行数十亿次运算，所以它能完成极为复杂的处理。

我想了一下，就管这个编造的机器叫「玩具」计算机吧，因为它不是真的，但又具有真正计算机的很多特性。实际上，它跟 1960 年代末的小型机差不多是一个水平，某种程度上与冯·诺依曼论文中的例子相近。这个玩具有用来存储指令和数据的 RAM，还有一块额外的存储区叫累加器，其容量足以存储一个数值。累加器类似于计算器的显示屏，保存用户最近输入的数值，或者最近计算的结果。玩具还有一个指令表，只包含 10 个指令，都是前面提到过的基本操作。

CPU 反复执行简单的循环：从存储器中取得下一条指令，该指令正常情况下保存在存储器的下一个位置，但也可以是使用 GOTO 或 IFZERO 指定的位置；对指令进行译码，也就是搞清楚这条指令要干什么，然后为执行该指令做好准备；执行指令，从存储器中取得信息，完成算术或逻辑运算，保存结果，总之是执行与指令匹配的组合操作；然后再从头取得指令，开始下一次循环。真正的处理器也执行同样的「取指令－译码－执行」循环，只不过为了加快处理速度，还会配备精心设计的各种机制。但核心只有循环，与前面重复把数值加起来的例子一样。

真正计算机的指令比我们玩具计算机的多，但性质相同。比如，有更多移动数据的指令，更多完成算术运算及操作不同大小和类型数值的指令，更多比较和分支指令，以及控制计算机其他组件的指令。典型的 CPU 有几十到数百个不同的指令；指令和数据通常要占用多个内存位置，通常为 2 至 8 个字节。真正的处理器有多个累加器，通常是 16 或 32 个，所以可以保存多个中间结果，而且都是速度极快的存储器。真正的程序与我们的玩具示例相比可谓庞大，有的甚至多达数百万条指令。

计算机体系结构是研究 CPU 与其他计算机组件连接的一门学科。在大学里，它通常是计算机科学和电子工程的交叉领域。

计算机体系结构研究的一个问题是指令集，也就是处理器配备的指令表。是设计较多的指令去处理各式各样的计算，还是设计较少的指令以简化制造并提升速度？体系结构涉及复杂的权衡，要综合考虑功能、速度、复杂性、可编程能力（如果太复杂，程序员将无法利用其功能）、电源消耗及其他问题。用冯·诺依曼的话说：「一般来讲，运算器内在的经济性取决于期望的机器运行速度…… 与期望的简易性或低价位之间的折中。」

CPU 与 RAM 和计算机的其他组件是如何连接的？处理器非常快，通常执行一条指令只需要零点几纳秒。相对而言，RAM 则慢得让人难以忍受 —— 从存储器中取得数据或指令大概要花 25 到 50 纳秒。当然，这里的快指的是绝对速度，而慢则是相对于 CPU 而言。假如 CPU 不必等待数据，那它可能早就执行完上百条指令了。现代计算机会在 CPU 和 RAM 之间使用少量的高速存储器来保存最近使用过的指令和数据，这种高速存储器叫作缓存。如果可以从缓存中找到信息，那么就会比等待 RAM 返回数据快得多。

设计师在设计体系结构的时候也有一套方法，能够让处理器跑得更快。比如，可以把 CPU 设计为交替地取得和执行指令，而同一时刻会有几个指令处于执行过程的不同阶段，这种设计叫做流水线。（与汽车装配线很相似。）结果呢，虽然某个特定的指令仍旧要花同样的时间完成，但其他指令都有机会得到处理，从整体上看完成这些指令则会快很多。另一种方法是并行执行多条互不干扰、互不依赖的指令，就相当于多条平行的汽车装配线。有时候，只要指令的操作不会相互影响，甚至可以不按顺序执行。

另外一种可能是同时运行多个 CPU。今天的笔记本电脑，甚至连手机都已经有多个 CPU 了。英特尔酷睿双核处理器在一块集成电路芯片上集成了两个 CPU（核心）。在一块芯片上集成越来越多的处理器已经成为明显的趋势。由于集成电路特征尺寸越来越小，因而可以集成在一块芯片上的晶体管数量必将越来越多，这些晶体管可以构成更多 CPU，也可以构成更多缓存。

处理器应用的领域决定了设计者要权衡哪些要素。很长时间以来，处理器主要的应用领域是桌面计算机，而桌面环境下的电源和物理空间都比较充足。因此，设计者只要专注于让处理器尽可能地快就好了，电源是用之不竭的，而散热只要多加风扇就行。笔记本电脑要求的权衡要素有了明显不同，一方面空间有限，另一方面在不插电的情况下，笔记本要靠沉重又昂贵的电池供电。其他方面条件不变，笔记本处理器必然要相对慢一些，耗电少一些。

手机和其他超轻便设备进一步提高了设计要求，因为尺寸、重量和电源各方面都有了更多限制。此时，单靠小范围调整设计是行不通的。虽然英特尔是台式机和笔记本处理器的主要供应商，但几乎所有的手机都使用「ARM」处理器，因为它耗电更少。ARM 处理器是指获得英国 ARM Holdings 公司许可制造的处理器。

比较不同 CPU 的速度并不是特别有意义。即便是最基本的算术运算，其处理方式也可以完全不同，很难直接比较。比如，同样是计算两个数的和并保存结果，有的处理器需要用三个指令（比如我们的玩具计算机），有的则需要两个，而有的可能只需要一个。有的 CPU 具有并行处理能力，或者说能够同时执行多条指令，从而让这些指令在不同阶段上执行。为了降低处理器的耗电量，牺牲执行速度，甚至根据是不是电池供电动态调整速度都是很常见的。对于某个处理器比另一个处理器「更快」的说法，不必太当真，因为很多情况下都要具体问题具体分析。

说到这里，有必要花点时间简单介绍一下缓存，这是一个在计算领域中广泛适用的思想。在 CPU 中，缓存是容量小但速度快的存储器，用于存储最近使用的信息，以避免访问 RAM。通常，CPU 会在短时间内连续多次访问某些数据和指令。例如，加法计算程序循环体中的多条指令，对每个输入值都要执行一遍。如果这些指令存储在缓存中，就不用在每次循环时都从 RAM 读取它们，这会让程序的速度快 50 倍。类似地，把 Sum 存储在数据缓存中也能提高访问速度。

典型的 CPU 有两到三个缓存，容量依次增大，但速度递减，一般称为一级缓存、二级缓存和三级缓存。最大的缓存能存储以兆字节计的数据（我的 Macbook 有 3MB 的二级缓存），大多数 CPU 的指令和数据缓存都是独立的。缓存之所以有用，关键在于最近用过的信息很可能再次被用到，而把它们存储在缓存里就意味着减少对 RAM 的等待。缓存通常会一次性加载一组信息块，比如只请求一个字节，但会加载 RAM 中一段连续的地址。因为相邻的信息也可能被用到，要用的时候它们同样已经在缓存里了，换句话说，对邻近信息的引用也不需要等待。

除了发现性能提升之外，用户是感受不到这种缓存的。但缓存的思想却无处不在，只要你现在用到的东西不久还会用到，或者可能会用到与之邻近的东西，那运用缓存思维就没错。CPU 中的多个累加器本质上也是一种缓存，只不过是高速缓存而已。RAM 也可以作为磁盘的缓存，而 RAM 和磁盘又都可以作为网络数据的缓存。计算机网络经常会利用缓存加速访问远程服务器，而服务器本身也有缓存。

在使用浏览器上网的时候，你可能见过「清空缓存」的字眼。对网页中的图片和其他体积较大的资源，浏览器会在本地保存一份副本，因为再次访问同一网页时，使用本地副本比重新下载速度快。缓存不能无限地增长，因此浏览器会悄悄地删除旧项目，以腾出空间给新的，它还给你提供了删除所有缓存内容的命令。

你自己随时可以检验缓存的效果。比如可以做下面两个实验，一是打开 Word 或 Firefox 等大程序，看看从启动到加载完成并可以使用要花多长时间。然后退出程序，立即重新启动它。正常情况下，第二次启动的速度会明显加快，因为程序的指令还在 RAM 里，而 RAM 正在充当磁盘的缓存。使用其他程序一段时间后，RAM 里会填满该程序的指令和数据，原先的程序就会从缓存中被删除。

二是在谷歌里搜索几个不太常见的单词或短语，注意谷歌查询结果要花多长时间。接着再搜索同样的关键词。返回搜索结果的时间会明显缩短，因为谷歌已经在其服务器上缓存了搜索结果。这个缓存对其他搜索相同关键词的人也有好处，因为缓存在谷歌的服务器上，不在你的机器里。要验证缓存在谷歌服务器上，可以在你搜索完之后，让别人在他们自己的计算机上搜索同样的关键词。虽然不能完全保证，但一般来说第二次搜索速度会快很多。

人们很容易认为计算机不是 PC 就是 Mac，因为那是我们最常见到的。实际上，还有很多其他类型的计算机。这些计算机无论大小，都具有相同的核心特性，即都能完成逻辑运算，并且都具有类似的体系结构，只不过在设计的时候会不同程度地考虑成本、供电、大小、速度等因素。手机和平板电脑也是计算机，它们运行操作系统并支持更加丰富多样的运算环境。比这还小的系统是嵌入式系统，日常生活里能见到的几乎所有数字设备里都有嵌入式系统，比如数码相机、摄像机、GPS 导航系统、家电、游戏机，等等 。

更大的计算机在很多年前就已经实现多个 CPU 共享内存了。如果能把大任务分解成小任务，而分解后的小任务又可以通过不同 CPU 协作完成，CPU 相互之间不会出现太长的等待，也不会有太多的相互干扰，那么就能以这种方式加快完成大任务。除了在大型系统中广泛应用，这种集成多个处理器的多核芯片在个人计算机中也已经司空见惯，而且未来很可能会普及。

超级计算机往往有大量的处理器和大量的内存，这些处理器本身可能带有一些特殊指令，在处理某种数据时，它们比通用的处理器速度更快。今天的超级计算机通常是高速计算机集群，CPU 仍然是普通的 CPU，并没有什么特殊的硬件。网站 [Home | TOP500 Supercomputer Sites](https://www.top500.org/) 每六个月就重新公布一次全世界最快的 500 台计算机。最快速度的纪录不断被打破，几年前还能跻身排行榜前几名的计算机，今天可能已经在榜单上找不到了。2011 年 6 月最快的计算机有 50 多万个 CPU，每秒可以执行 8×10^15 次数学运算。

分布式计算指的是很多更加独立的计算机（比如不共享内存），而且地理上更加分散，甚至位于世界的不同地方。这样一来，通信更加成为瓶颈，但却能够实现计算机之间的远距离协作。大规模的 Web 服务，比如搜索引擎、在线商店和社交网络，都是分布式计算系统。在这种系统中，数以千计的计算机协作，可以为海量用户迅速地提供结果。

所有这些计算系统都有相同的基本原理。它们都使用通用处理器，可以通过编程完成无穷无尽种任务。每个处理器都有一个有限的简单指令表，能够完成算术运算、比较数据、基于前置计算结果选择下一条指令。不管物理结构的变化让人多么眼花缭乱，它们的一般体系结构从 1940 年代至今并没有太大的变化。

或许很难想象，这些计算机都具有相同的逻辑功能，可以完成一模一样的计算（暂且不论对速度和内存的要求）。1930 年代，这个结果就已经被几个人分别独立地证明过，其中包括英国数学家艾伦·图灵。对于非专业人员，图灵的手段最容易理解。他描述了一个非常简单的计算机（比我们的玩具计算机还简单），展示了它能够计算任何可以计算的任务。他描述的这种计算机，我们今天叫做图灵机。然后，他展示了如何创建一种图灵机，模拟其他图灵机，这种图灵机现在被称为通用图灵机。写一个模拟通用图灵机的程序很容易，而写一个程序让通用图灵机模拟真实的计算机也是可能的（尽管不容易）。实际上，从能够计算什么的角度讲，所有计算机都是等价的，尽管运行速度明显不可能等价。

第二次世界大战期间，图灵从理论转到实践：他领导开发了用于破译德军情报的计算机。1950 年，他发表了一篇名为「计算机器与智能」（Computing machinery and intelligence）的论文，其中提出一个测试（即今天所谓的图灵测试），人们可以通过该测试来评估计算机是否能表现出人类的智能。想象一下，一台计算机和一个人，通过键盘和显示器与另一个提问者交流。通过问答，提问者能确定哪个是人，哪个是计算机吗？图灵的想法是，如果不能明显地将二者区分开，那么计算机就表现出了智能的行为。

2『已下载原论文「2020002Computing-machinery-and-intelligence」。』

## 0200软件

1、算法。算法就是一系列精确、无歧义的步骤，可以执行某种任务，然后停止。算法描述了不依赖于任何实现的计算过程。这些步骤由定义明确的基本操作或原始操作构成。算法有很多，我们只介绍了基本的搜索和排序算法。

2、复杂性。算法的复杂性是对算法要执行的工作量的抽象描述。度量的依据是基本操作（如检测数据项、比较数据项），而表述的是计算次数与数据项数的关系。算法的复杂性可以分为几个层次，就我们介绍的几种算法而言，既有对数级算法（数据量加倍，计算次数只加一）也有线性算法（计算次数与数据量成正比，最常见也最容易表达） ，还有指数级算法（数据量加一，计算次数加倍）。复杂性度量的是最坏情况（实际的问题很可能要简单得多），而且描述的是一种渐近性质（只有数据量很大的时候才适用）。

3、编程。算法是抽象的，而程序是具体的。程序是让计算机完成一个任务的所有步骤的具体描述。程序必须考虑内存和时间的限制、数值的大小和精度，以及偏激和恶意用户。

4、编程语言。编程语言是表达所有计算步骤的记号库，人们可以籍此轻松写出代码来，而且代码可以被翻译成计算机最终可以执行的二进制形式。翻译方式有很多种，但最常见的是使用编译器，有时候还要用汇编器，把用 C 等语言编写的程序转换成二进制形式，以便在计算机上运行。不同的处理器有不同的指令集和指令形式，因此编译器也会有相应的差异。解释器和虚拟机是模拟真正或假想计算机的程序，可以面向它们编译并运行代码。JavaScript 程序就是面向解释器编译运行的。

5、库。编写一个在真正计算机上运行的程序要牵扯很多细节，涉及很多常用操作。库以及类似的机制可以提供预制的组件，供程序员在编程时使用。有了库，程序员就可以在既有工作成果基础上开展新工作。今天的编程工作通常都是组织既有组件与编写原创代码并重。组件可能是库函数（比如 JavaScript 程序中用到的那些函数），也可能是像 Google Maps 一样的大型系统，或者是其他 Web 服务。然而，从底层来看，它们都是由程序员使用我们介绍过的语言或没介绍过的类似语言指令编写的。

6、接口。接口或者 API（应用程序编程接口）是提供服务的软件与使用该服务的软件之间的一种约定。库和组件通过 API 提供服务。操作系统通过自身的系统调用接口让硬件看起来更有章可循，而且可以编程控制。

7、抽象和虚拟化。使用软件可以隐藏实现的细节或者把实现伪装成其他东西，比如虚拟内存、虚拟机和解释器。

8、Bug。计算机不懂宽容，因此容易犯错的程序员必须写出某种程度上没有错误的程序来。所有大型程序都有 bug，也就是说有时候会不听使唤。某些 bug 仅仅只是惹人讨厌，比如设计得不好，并不像真正的错误那么严重。（「这不是 bug，而是一个功能」是程序员中流行的说法。）而有些 bug 只有在极端情况下或者罕见的情境中才会出现，往往很难再现，更不用说修复了。但有些 bug 确实严重，甚至会威胁到人身安全。随着软件在关键系统中的应用越来越广，对计算设备中软件责任的认定也变得越来越重要。过去那种「买不买由你，一旦售出概不负责」的说辞应该改一改了。对待软件也应该像对待硬件一样，厂商必须尽到保护用户的合理责任。根据经验，因为程序是基于既有组件构建的，而原有 bug 都会消灭掉，至少从原理上讲，新程序中的错误应该越来越少。然而，与这些进步因素相对的是随着计算机和语言的发展，系统承载的需求将越来越多样，市场和消费者呼唤新功能带来的压力也会越来越大，于是无法避免的隐患也会层出不穷。总之，bug 将成为我们心中永远的痛。

好消息是，计算机是一种通用机器，能够执行任何计算。虽然它只有很少的指令，但执行这些指令的速度却极快，而且它能够很大程度上控制自己的运行。坏消息是，如果没有人告诉它该做什么，它就什么都不会做，而且得事无巨细一五一十地告诉它。计算机是「魔法师的学徒」，能够不知疲倦、分毫不差地执行指令，但下达给它的任务书也必须高度精确。

能够让计算机完成某种任务的指令序列通称软件。软件的「软」与硬件的「硬」相对，寓意看不见，摸不着。硬件是有形的：如果失手把计算机掉在脚上，你会喊疼。软件则没有这个问题。

在接下来的几章中，我们要讨论软件，即如何告诉计算机做什么。第 4 章会概括地谈谈软件，并着重讨论一下算法，它们实际上是诸多焦点任务的理想化解决方案。第 5 章讨论编程和编程语言，我们用它来表达一系列计算步骤。第 6 章介绍主要的软件系统，无论你知道与否，反正每天都在用。本部分最后一章是第 7 章，讲讲 JavaScript 编程。

在此期间，要把这些牢记于心：现代系统越来越多地采用通用硬件（如处理器、内存，以及与外界相连接的接口），同时靠软件来实现特定的行为。人们普遍认为，软件更便宜、更灵活，比硬件更好修改（特别是跟已经出厂的设备比）。例如，如果用一台计算机来控制汽车的动力和刹车，那么防抱死和电子稳定控制显然应该是软件的功能。

计算机是许多关键系统的核心，并且软件控制着这些系统。MRI（核磁共振）和 CT（电脑断层）扫描等医学成像系统，就是用计算机来控制信号，并生成供医生解读的图像（胶片已经被数字图像取代）。现代汽车都有数十个小型计算机，分别负责管理制动和稳定性控制系统，无论哪个出问题，后果都不堪设想。火车、轮船、飞机也概莫能外。

事实表明，只要软件不可靠不耐用，我们就一定会遇到麻烦。而随着人们对软件越来越依赖，潜在的麻烦也只会越来越大。后面我们还会介绍到，很难写出一点问题都没有的软件。逻辑或实现上的任何一点错误或疏忽，都可能导致程序出问题。即使正常使用中不会发生这些问题，也会给敌人留下可乘之机。

## 总结汇总

1、算法是一系列精准、无歧义的计算步骤，可以执行某项任务，然后停止。算法不依赖于任何实现的计算过程。复杂性是用来度量算法效率的（计算时间与要处理数据量之间的关系）。

2、编程语言是能让我们表达完成某个任务所需计算步骤的语言。

3、操作系统，是用来控制和分配计算机资源的。操作系统里的文件系统，是计算机逻辑组织和物理实现的集中体现。应用程序，表示所有在操作系统平台上完成某种任务的软件或程序。

4、以 JavaScript 为例阐述了编程语言的基本概念，语法、语义等等。

## 0201算法

重塑人们对「我们能计算多快」的认识，多年来一直是计算机科学研究的主题。而用数据量来表示运行时间/次数（如 N 、logN 、N^2 或 NlogN ），则是这一领域研究成果的集中体现。它不去纠结于这台计算机是不是比那一台更快，或者你是不是一个比我更优秀的程序员之类的问题，而是抓住了程序或算法背后的复杂性。正因为如此，才非常合适比较或推断出某些计算是否可行。（一个问题固有的复杂性和解决这个问题的算法的复杂性并不是一个概念。比如，排序是一个 NlogN 问题，但快速排序是一个 NlogN 算法，而选择排序则是一个 N^2 算法。）

算法和复杂性的研究是计算机科学的一个重要组成部分，既有理论也有实践。我们感兴趣的是哪些问题可以计算，哪些不可以，以及如何在无需更多内存的情况下计算得更快（或者是同样速度下使用更少的内存）。我们期待全新的、更好的计算方法。快速排序就是一个典型的例子，尽管它已经出现很多年了。

现实生活中，有许多重要的算法比我们这里介绍的简单的搜索和排序更专业更复杂。例如，压缩算法旨在让声音（MP3）、图片（JPEG）和电影（MPEG）占用更少的存储空间。错误检测和校正算法也很重要。数据在存储和传输（例如通过嘈杂的无线信道）过程中可能会受到损害；控制数据冗余的算法可以检测甚至纠正某些错误。密码学高度依赖于算法，它需要发送只让好人看懂而不能让坏人破解的加密消息。

对于必应和谷歌等搜索引擎而言，算法同样至关重要。从原理上讲，搜索引擎所做的大量工作都很简单，无非是收集网页、组织信息，使其便于搜索，所不同之处在于数据规模极大。如果每天有数十亿次查询，要搜索数十亿个网页，那么即使 NlogN 的复杂性也是无法接受的。为了跟上日益增长的 Web 数据量，满足我们通过它进行搜索的需求，人们在改进算法和编程方面投入了大量聪明才智，以确保搜索引擎能够足够快。

什么是软件？一个通俗的比喻是做菜用的菜谱。菜谱会列出做某个菜所需的原材料、烹饪步骤以及预期结果。类似地，程序也要描述待操作的数据，讲清楚要对数据做什么，以及产出什么结果。不过，菜谱与任何程序都不能比，因为它含糊，容易产生歧义。所以这个比喻并不是非常恰当。比如，我家那本《烹饪的快乐》（Joy of Cooking）在说到打鸡蛋时，说要「放在一个小碗里：1 个鸡蛋」，但没说必须先把蛋磕开，把壳去掉。

用纳税申报表来作比喻更准确一些：这些表格极其详尽地说明了你应该做什么（从第 29 行减去第 30 行。如果结果是 0 或更小，则输入 0。给第 31 行乘上 25%，……）。虽然这个比喻也不完美，但与菜谱相比，纳税申报表在说明计算过程方面更胜一筹：数学计算必不可少，数据从一个位置被复制到另一个位置，后续的计算取决于之前计算的结果。

对于纳税来说，这个过程应该是完整的，无论什么情况下都应该得出一个结果，即应纳税额。应该是毫无疑义的，只要开始的数据相同，任何人都应该得到相同的最终结果。而且应该在有限的时间内完成。从我个人经验来看，这几条都是理想化的，因为术语并不总是很明了，计算说明也比税务机关的说法含糊很多，而且要使用什么数据经常也不好确定。

算法，就是保证特定计算过程正确执行的一系列步骤，它是计算机科学中的菜谱或纳税申报表，只不过编制得更仔细、更准确、更清楚。算法的每一步都表达为一种基本操作，其含义都是完全确定的，如「两个数相加」。任何事物都没有歧义，输入数据的性质也是既定的。所有可能的情况都会涵盖，而算法绝不会遇到一种它不知道接下来该做什么的情况。（计算机科学家有时候也不免书生气，因此通常会给算法多加一个限定条件：任何算法最终必须停止。根据这个标准，经典的洗发水使用说明「起泡、冲洗、重复」就不能说是算法了。）

设计、分析和实现高效的算法是学院派计算机科学的工作核心，而在现实世界中也有很多算法意义重大。我没有打算滴水不漏地解释或说明各种算法，但我想让大家了解相关的思想，即详尽地描述一系列操作步骤，不管执行这些步骤的实体有没有智能或创造力，都能对这些步骤是什么意思以及如何执行做到毫无疑义。另外，我还想谈一谈算法的效率，也就是计算时间与要处理的数据量之间存在什么关系。为此，我会分析几个常见且容易理解的基本算法。

假设我们想找出谁是房间里个子最高的人。我们可以四下里看看，然后猜一猜会是谁。然而，算法则必须精确地列出每一个步骤，从而让不会说话的计算机都能遵照执行。最基本的做法就是依次询问每个人的身高，并记住到目前为止谁最高。于是，我们可能会问「约翰，你多高？玛丽，你呢？」等等。如果我们第一个问的是约翰，那么当时他是最高的。如果玛丽更高，则现在她是最高的人，否则，约翰仍然最高。无论如何，我们都会接着问第三个人。在问完每个人之后，我们就会知道究竟谁最高以及到底有多高。类似的方法还可以找出最有钱的人，或者名字在字母表中最靠前的人，或者谁的生日最接近年底，谁在 5 月出生，以及谁叫克里斯。

会遇到一些复杂的情况。比如如何处理重复的数据，或者说要是有两三个人的身高一样怎么办？我们可以决定只记录第一个人、只记录最后一个人，或者随机记录其中某一个人，再或者记录他们所有人。请注意，找出同样身高的所有人是比较困难的。因为必须记住所有这些人的名字，不问完最后一个人，我们是无法知道这些信息的。这个例子涉及数据结构，即如何表示计算过程中所需的信息。数据结构对很多算法而言都是非常重要的，但在这里我们不会谈太多。

但是，如果让计算机来做这件事，就必须多加小心。例如，要考虑到假如纸上没有身高值怎么办？这对人来说不是问题，因为我们知道这意味着什么也不用做。但对计算机来说，我们必须告诉它如何测试这种情况，出现这种情况该怎么办。假如不事先测试，那它就会尝试用零去除 sum，而这个操作是未定义的。算法和计算机必须处理所有可能的情况。如果你看到过「0 美元 00 美分」的支票，或者收到过尚欠余额为 0 元的账单，那就是因为计算机系统没有全面测试所有可能的情况。

如果我们事先不知道有多少个数据项怎么办（这种情况很常见）？那就得重写算法，让它在累计和的同时计算有多少项。

算法的一个关键属性是其效率有多高 —— 对于给定的数据量，它们的处理速度是快还是慢，要花多长时间？对于上面给出的例子，计算机要执行多少步，或者需要花的时间有多长，与它必须处理的数据量成正比：如果房间里的人多出一倍，就要多花一倍时间才能找到最高的人，或者才能计算出平均身高；如果人数是现在的十倍，就要花十倍的时间。如果计算时间与数据量成正比或叫线性比例，那该算法就叫做线性时间算法或线性算法。以数据量为横坐标，以时间为纵坐标画一条线，得到的将是一条向右上方延伸的直线。我们平时遇到的大多数算法都是线性的，因为它们对某些数据所执行的基本操作是相同的，数据越多工作量也会同比例增加。

线性算法的基本形式都一样。可能需要进行一些初始化，如把累计和的初值设置为 0，或者把最大的身高值设置为一个较小的值。然后依次检查每一项，对它完成一次简单的计算，如计数、与上一个值比较，或进行简单的变换。最后，可能需要再做一些计算，如计算平均值、打印累计和或最大的身高值。如果对每一项执行操作所花的时间相同，那么总时间与数据项数就是呈正比的关系。

那我们还可以做得更好一些吗？假设我们面前有一大堆打印出来的人名和电话号码，或者一沓名片。如果名字并没有特定的顺序，而我们想找到迈克·史密斯（Mike Smith）的号码，那就必须一个名字一个名字地找，直至找到他的名字为止（或者没找到，因为根本就没有这个人）。如果名字是以字母顺序排列的，我们就可以做得更好。

想想我们是怎么从老式的电话簿中查人名的。首先，我们会从接近中间的地方开始查。如果要找的名字比中间页上的名字在字母表中靠前，那后半本就不用看了，直接翻到前半本的中间（整本电话簿的四分之一处）；否则，前半本就不用看了，直接翻到后半本的中间（整本电话簿的四分之三处）。由于名字按字母顺序排列，每一步我们都知道接下来到哪一半里去找。最终，我们一定会找到那个名字，或者可以断定电话簿里根本就没有这个人。

这个搜索算法被称为二分搜索，因为每次检查或比较都会把数据项一分为二，而其中一半今后就不会再理会了。这其实也是常见的分而治之策略的一个应用。它的速度有多快？每一步都会舍弃一半数据项，因此所需要的步数就等于在处理最后一项之前，最初的项数被 2 除开的次数。

假设最初有 1024 个名字（这个数容易计算）。一次比较，就可以舍弃 512 个。再比较一次，还剩 256 个，然后是 128 个、64 个、32 个，接着是 16 个、8 个、4 个、2 个，最后剩下 1 个。总共比较了 10 次。显然，2^10 等于 1024 并非巧合。比较次数作为 2 的指数就能得到最初的数，而从 1 到 2 到 4…… 到 1024，每次都是乘以 2。如果你还记得学校里讲过的对数，那你应该知道一个数的对数就是底数（这里是 2）要得到该数需要自乘的次数。1024（以 2 为底）的对数等于 10，就是因为 2^10 等于 1024。

二分搜索的关键是数据量的增长只会带来工作量的微小增长。如果有 1000 个名字按字母顺序排列，那为了找到其中一个必须检查 10 个名字。如果有 2000 个名字，也只要检查 11 个名字，因为看完第一个名字立即就能舍弃 2000 个中的 1000 个，而这又回到了从 1000 个中查找的情形（检查 10 次）。如果有 1 000 000 个名字，也就是 1000 的 1000 倍，那么前 10 次测试就能减少到 1000，另外 10 次测试即可减少到 1，总共 20 次测试。1 000 000 是 106，约等于 2^20，因此 1 000 000（以 2 为底）的对数约等于 20。

不过，得先把这些名字按照字母顺序排列起来呀，怎么做到呢？如果没有这个先行步骤，就不能使用二分搜索。这就引出了另一种基本算法 —— 排序，把数据按顺序排好，后续搜索才能更快。

假设我们要把一些名字按照字母顺序排好，以便后面更有效地使用二分搜索。那么可以使用一个叫选择排序的算法，因为它会不断从未经排序的名字中选择下一个名字。这个算法的技巧，就是前面讨论的找出房间里最高的那个人所用的技巧。

选择排序的工作量有多大？它每次都会遍历剩余的数据项，每次都会找到字母顺序中的下一个名字。对于 16 个名字的排序，查找第一个名字要检查 16 个名字，查找第二个名字需要 15 步，查找第三个名字需要 14 步，依此类推，加起来总共要检查 16+15+14+...+3+2+1 个名字。当然，我们也可能很幸运，发现这些名字已经都按字母排好序了。但研究算法的计算机科学家可都是悲观主义者，他们假设的是最坏的情况（即这些名字都是按照字母顺序的反序排列的）。

检查名字的遍数与最初的数据项数成正比（我们例子中的数据项有 16 个，可以用一般化的 N 表示）。而每一遍要处理的项数都比前一遍少一项，所以选择排序算法一般化的工作量是：

这个序列加起来等于 N×(N+1)/2（最简单的办法是把两头的项成对地加起来），也就是 N^2/2+N/2。忽略除数 2，可见选择排序的工作量与 N^2+N 成正比。随着 N 不断增大，N^2 最终会大得让 N 也可以忽略不计（例如，如果 N 是 1000，则 N^2 就是 1 000 000）。因此，结果就是工作量近似地与 N^2 即 N 的平方成正比，而这个增长率叫做二次增长。二次增长不如线性增长，事实上，差得很远。要排序的数据项增加到原来的 2 倍，时间会增加到原来的 4 倍；数据项增加到 10 倍，时间会增加到 100 倍；数据项增加到 1000 倍，时间会增加到 1 000 000 倍！这可不太好。

幸运的是，有办法让排序更快一些。我们简单地介绍一种巧妙的方法 —— 快速排序（Quicksort），这个算法是英国计算机科学家托尼·霍尔在 1962 年前后发明的（霍尔获得了 1980 年的图灵奖，获奖理由是包括快速排序在内的多项贡献）。快速排序也是分而治之的一个绝佳示例。

要使用快速排序算法给这些名字排序，首先要遍历一次所有名字，把介于 A 到 M 之间的名字放到一组里，把介于 N 到 Z 之间的名字放到另一组里。这样就把所有名字分成了两个组，每个组里包含一半名字（假设这些名字的分布不会很不均匀）。

现在，遍历 A-M 组，把 A 到 F 分成一组，G 到 M 分成另一组；遍历 N-Z 组，把 N-S 分成一组，T-Z 分成一组。到现在为止，遍历了所有名字两次，分成了四个组，每个组包含四分之一的名字：

接下来再遍历每个组，把 A-F 分为 ABC 和 DEF，把 G-M 分成 GHIJ 和 KLM；同样，对 N-S 和 T-Z 也如法炮制。这样，就有了 8 个组，每组差不多有 2 个名字：

当然，到最后我们不仅仅要看名字的第一个字母，比如要把 IBM 排到 Intel 前面，把 Skype 排到 Sony 前面，就得继续比较第二个字母。但就这样多排一两遍，即可以得到 16 个组，每组 1 个名字，而且所有名字都按字母顺序排好了。

整个过程的工作量有多大？每一遍排序都要检查 16 个名字。假设每次分割都很完美，则每一遍分成的组分别会包含 8、4、2、1 个名字。而遍数就是 16 反复除以 2 直到等于 1 为止除过的次数。结果就是以 2 为底 16 的对数，也就是 4。因此，排序 16 个名字的工作量就是 16log216。在遍历 4 遍数据的情况下，快速排序总共需要 64 次操作，而选择排序则需要 136 次。

该算法可以对任何数据进行排序，但只有在每次都能把数据项分割成大小相等的组时，它才是最有效的。对于真实的数据，快速排序必须猜测数据的中位值，以便每次都能分割出相同大小的组。好在，只要对少量数据项进行采样，就可以估计出这个值。一般来说（忽略某些细节上的差别），快速排序在对 N 个数据项排序时，要执行 NlogN 次操作，即工作量与 NlogN 成正比。这与线性增长比要差一些，但还不算太坏，在 N 特别大的情况下，它比二次增长即 N^2 增长可以好太多了。

刚才，我们对算法的「复杂性」或运行时间进行了简单的剖析。一个极端是 logN，即二分搜索的复杂性，它表示随着数据量的增加，工作量的增长非常缓慢。最常见的情况是线性增长，或者说简单的 N，此时工作量与数据量是成正比的。然后是快速排序的 NlogN，比 N 差（增长快），但在 N 非常大的情况下仍然特别实用。还有就是 N^2，或者二次增长，增长速度太快了，既让人无法忍受又不切实际。

除了这些之外，还有其他很多种复杂性，有的容易理解（例如三次增长，即 N^3，比二次增长还差，但道理相同），有的则很难懂，只有少数专业人士才会研究。但有一个还是非常值得了解一下，因为它在现实当中很常见，而且从复杂性上说特别糟糕，这就是所谓的指数级增长，用数学方法表示是 2^N（与 N^2 可不一样）。指数级算法的工作量增长极快：增加一个数据项，工作量就会翻一番。从某种意义上讲，指数级算法与 logN 算法是两个极端，后者数据项翻一番，工作量才增加一步。

什么情况下会用到指数级算法呢？那就是除了一个一个地尝试所有可能性，没有更好的办法的情况。谢天谢地，指数级算法总算是有点用武之地的。有些算法，特别是密码学中的算法，都是让特定计算任务具有指数级难度的。对于这样的算法，只要选择了足够大的 N，其他人在不知道某个秘密捷径的情况下，是不可能通过计算直接解决问题的。

现在你只要知道有些问题容易解决，而有些问题则要难得多就可以了。实际上，关于解决问题的难易程度，也可以表达得更加精确一些。所谓「容易」的问题，都具有「多项式」级复杂性。换句话说，解决这些问题的时间可以用 N^2 这样的多项式来表示，其中指数可以大于 2，但都是可能被解决的。计算机科学家称这类问题为「P」（即「Polynomial」，多项式），因为它们可在多项式时间内解决。

现实中大量的问题或者说很多实际的问题似乎都需要指数级算法来解决，也就是说，我们还不知道对这类问题有没有多项式算法。这类问题被称为「NP」问题。NP 问题的特点是，它可以快速验证某个解决方案是否正确，但要想迅速找到一个解决方案却很难。NP 的意思是「非确定性多项式」（nondeterministic polynomial），这个术语大概的意思是：这些问题可以用一个算法在多项式时间内靠猜测来解决，而且该算法必须每次都能猜中。在现实生活中，没有什么能幸运到始终都做出正确的选择，所以这只是理论上的一种设想而已。

1『欣喜，原来 NP 问题是相对于 P 问题来说的，P 问题即多项式问题。』

很多 NP 问题都会牵扯大量技术细节，三言两语也解释不清楚。不过倒是有一个问题很好解释，乍一看还挺有意思的，而且其实际应用也比较广。这就是「旅行推销员问题」（Traveling Salesman Problem）。一个推销员必须从他居住的城市出发，到其他几个城市去推销，然后再回家。目标是每个城市只到一次（不能重复），而且走过的总距离最短。这个问题跟最短校车或者垃圾车路线有异曲同工之妙。很早以前我在研究这个问题的时候，其原理经常被应用于设计电路板上孔洞的位置，或者部署船只到墨西哥湾的特定地点采集水样。

旅行推销员问题已经被仔细推敲了 50 多年，尽管能用它来解决的问题更加多样化了，但解决方案的核心依然是从所有路径中更巧妙地找出最短路径。同样，有许多的其他问题，尽管类型不同，形式各异，也都面临同样的命运：我们没有什么好办法有效地解决它们。

对于研究算法的人来说，这个现实令人沮丧。我们不知道到底是这些问题本质上就很难解决呢，还是因为人类不够聪明，所以至今都没有找到更好的解决办法。当然，不管怎么说，人们更愿意相信它们「本质上就很难解决」。

1970 年，斯蒂芬·库克（Stephen Cook）证明了一个非同小可的数学结论，就是说所有这些问题其实都是等价的，只要我们找到一个多项式时间算法（复杂性类似 N^2）解决其中一个问题，那我们据此就能找到所有问题的多项式时间算法。库克因此获得了 1982 年的图灵奖。

美国克雷数学研究所（Clay Mathematics Institute）公布了 7 个悬而未决的问题，解决其中一个就可以获得 100 万美元奖金。而问题之一是：P 是否等于 NP？换句话说，这些难题到底跟那些简单的问题是不是一类？（7 个问题中的另一个，可以追溯到 20 世纪初的「庞加莱猜想」，已经被俄罗斯数学家格里戈里·佩雷尔曼解决，奖金已经在 2010 年发放。所以，现在还剩下 6 个待解决的问题。）

对于这种复杂性，有几个地方需要特别注意。虽然 P=NP 问题很重要，但它更多的是一个理论问题，而不是一个实际问题。正如计算机科学家所说的，复杂性结果就是「最坏的」结果，有些问题的实例可能需要投入全部时间和精力，但并不是所有实例都那么难解决。这些问题也具有「渐近」的特点，也就是说，只有 N 值特别大的情况下才值得考虑。在现实生活中，或许大多数问题都能找到简单的解决办法，或许从实用角度看，近似的结果也是完全可以接受的，或许 N 很小，考虑不考虑渐近问题根本无关紧要。

举例来说，如果你只需要对几十或者几百个数据项进行排序，那选择排序可能就足够快了，尽管其复杂性是二次方的，而且与快速排序的 NlogN 相比是每况愈下。如果你只需造访五六个城市，要尝试所有可能的路线不是什么难事儿，但如果是 60 个城市，就有点不可行了，而 600 个城市也这么做根本就是不可能的。最后，在大多数情况下，一个近似的解决方案可能就足够好了，完全没有必要追求所谓的绝对最佳方案。而能够给出合理近似答案的算法可能要多少有多少，其中很多可能还更切合实际。

1『很赞同作者的这个观点，够用即可，不必追求完美。』

## 0202编程与编程语言

上一章我们讨论了算法。算法是忽略具体实例而对过程进行的一种抽象或理想化的描述，是分毫不差且没有歧义的「菜谱」。算法通过一组确定的基本操作来表达，这些操作的含义是完全已知且明确的。算法描述了应用这些基本操作的一系列步骤，涵盖所有可能的情况，而且保证最终能够停止。另一方面，程序则不是抽象的，它陈述了一台真正的计算机要完成某个任务所必须执行的具体步骤。程序之于算法，犹如建筑之于图纸，一个是实际存在的，一个是理想化的。

1『隐喻：程序之于算法如同建筑之余图纸一般。』

换一个角度看，程序又是以计算机能够直接处理的某种形式表达出的一个或多个算法。程序必须考虑实际的问题，比如内存不足、处理器速度不快、无效或恶意的输入、网络连接中断，以及（看不见摸不着，但却经常会导致其他问题恶化的）人性弱点。因此，如果说算法是理想化的菜谱，那程序就是让烹饪机器人冒着敌人的炮火为军队准备一个月的给养所需的操作说明书。

我们需要或者希望计算机无所不能，如此一来就需要巨大的编程工作量，而世界上却没有那么多程序员。因此，让计算机代替人处理更多的编程细节就成为这个领域永恒的话题。这自然也就引出了编程语言，即让我们能够表达完成某个任务所需计算步骤的语言。同样，管理一台计算机的资源也十分困难，而现代计算机的复杂性更是让这种困难有增无减。因此，我们也需要让计算机来控制自己的操作，而由此就有了所谓的操作系统。

1『这个角度来看，编程语言用来完成人机交互，操作系统用来完成机机交互。』

1949 年，EDSAC 的诞生标志着第一批真正可编程的电子计算机登上了历史舞台。那时候，给这些计算机编程要把表示指令和数据的数值打在穿孔卡片或纸上，然后把这些数值加载到存储器中执行。用这种编程方式哪怕写一个非常小的程序也十分艰难。首先是不可能一次就做对，其次是发现错误以后修改、增删指令或数据也不容易。

20 世纪 50 年代，出现了能代替人处理某些琐事的程序，因而程序员可以使用有意义的单词来表示指令（如用 ADD 代替数字 5），使用名字来指代特定的内存位置（如用 Sum 代替数字 14）。这其中蕴含的用程序操作程序的思想，一直都是软件领域各种重大进步的核心驱动力。这种代替人执行具体操作的程序被称为汇编器（assembler），因为它最初也用来组装（assemble）程序中由其他程序员事先写好的部分。相应的语言叫做汇编语言，而这个层次上的编程叫做汇编语言编程。有了汇编器，给程序添加或删除指令就方便多了，因为汇编器会负责跟踪数据和指令在存储器中的位置，程序员就不必管这些琐碎的事儿了。

不同处理器的汇编语言只能用于为该处理器编写程序。汇编语言通常都与 CPU 的指令一一对应，能够以特定方式将指令编码为二进制格式，也知道信息在存储器中如何存放。这也就意味着，用某种 CPU（如 Mac 或 PC 中的 Intel 处理器）的汇编语言编写的程序，与在不同 CPU（如手机中的 ARM 处理器）上完成相同任务的其他汇编程序差别会很大。把为一种处理器编写的汇编程序移植到其他处理器，实际上接近于重写一遍执行相同任务的程序。

20 世纪 50 年代末到 60 年代初，计算机在代替程序员做更多事方面又前进了一大步，而这无疑也是人类编程史上最重要的一步。那就是独立于任何 CPU 体系结构的「高级」编程语言问世了。高级语言让人类得以用接近自然语言的方式来表达计算过程。

用高级语言编写的代码经过一个翻译程序，可被翻译为目标处理器的汇编指令。这些汇编指令则会进一步被转换为比特，从而能够加载到存储器中并执行。这个翻译程序通常被称作编译器 —— 同样是一个信息量有限的老术语。

一个编译器可能把这个玩具程序转换成三条指令，另一个编译器则可能把它转换为一条指令。而相应的汇编器将负责把各自的汇编语言指令转换为实际的位模式，同时为 X、Y、Z 这几个量在存储器中留出位置。当然啦，针对这两台计算机的位模式也不一样。

在实际当中，编译器在内部可能会被分成一个「前端」和多个「后端」。「前端」负责把高级语言的程序转换为中间形式，而「后端」则负责把中间表现形式转换成不同体系结构的汇编指令。这种做法要比使用多个完全不同的编译器更简单。

相比汇编语言，高级语言拥有很多优势。首先，它让更多的人得以学会编程，而且编程效率也大大提高。用高级语言编程接近人类的思维方式，因此学习和使用的难度都降低了。人们不需要熟悉 CPU 指令表，就可以使用高级语言高效地编程。其次，高级语言程序独立于各种体系结构，通常无需任何修改即可在不同的体系结构上运行，只要像上图所示换个编译器编译一下就行。于是，程序可以只写一次，随处运行了。这也大幅降低了为多种计算机开发程序的成本。而编译环节也为发现各种拼写错误、语法错误（如少写括号或操作未定义的量）等疏漏提供了机会，在生成可执行程序之前必须纠正这些错误。这些错误在汇编语言程序中很难发现，因为必须假设汇编指令的任何序列都合法。（当然，语法正确的程序仍有可能充斥着各种编译器检测不出来的语义错误。）高级语言的重要意义无论怎么强调都不过分。

第一批高级语言专注于特定的领域。其中一门最早的语言叫做 FORTRAN，这个名字源自「Formula Translation」（公式转换）。FORTRAN 由约翰·巴库斯（John Backus）在 IBM 领导的一个小组开发，在表达科学和工程计算方面非常成功。许多科学家和工程师（包括我）学习的第一门编程语言就是 FORTRAN。FORTRAN 到今天仍然有很多用户。自 1958 年以来，FORTRAN 经历了几次大的变革，但其核心没有变过。巴库斯 1977 年获得图灵奖，其中部分原因就是他领导开发了 FORTRAN。

20 世纪 50 年代末的第二个主要的高级语言是 COBOL（Common Business Oriented Language，面向商业的通用语言），格蕾斯·霍普（Grace Hopper）对汇编语言高级替代品的研究对它产生了重大影响。霍普与霍华德·艾肯（Howard Aiken）当时使用的是哈佛 Mark I 和 II（当时的机械计算机），后来又使用过 Univac（Universal Automatic Computer，通用自动计算机）。她是认识到高级语言和编译器具有巨大潜力的先驱之一。COBOL 是专门针对商业数据处理的语言，其功能非常适合表达库存管理、开发票、做工资等方面的计算。COBOL 现在也有人在用，虽然变化较大但仍然有它自己的特点。

BASIC（Beginner's All-purpose Symbolic Instruction Code，初学者通用符号指令代码）也是当时问世的一门语言，是约翰·凯梅尼（John Kemeny）和汤姆·库尔茨（Tom Kurtz）于 1964 年在达特茅斯开发出来的。BASIC 当初的设计目标是要成为学习编程的入门语言。它特别简单，只需要非常有限的计算资源，因此也成为了第一批个人计算机中的第一个高级语言。事实上，微软公司的创始人比尔·盖茨和保罗·艾伦发迹，也是始于为 1975 年的 Altair 微型计算机编写 BASIC 编译器，这个编译器是微软公司的第一个产品。今天，Microsoft Visual Basic 作为 BASIC 的一个主要分支，仍然被微软公司积极地维护着。

在计算机价格昂贵、速度又慢而且资源有限的时期，人们都担心用高级语言写出来的程序效率太低，因为编译器生成的汇编代码远不如一个熟练的汇编程序员写得好。编译器作者付出了很大努力，希望生成的代码能够达到手写代码一样的高质量，而这为高级语言的流行奠定了基础。今天，计算机速度提升了上百万倍，而且有了充足的内存，程序员很少需要担心指令级的效率问题，尽管编译器和编译器作者仍然很关心。

FORTRAN、COBOL 和 BASIC 获得成功的部分原因，是它们都专注于某个特定的领域，而且有意避免大而全的定位。20 世纪 70 年代，出现了专门为「系统编程」开发的语言。所谓系统编程，就是编写汇编器、编译器、编程工具乃至操作系统等程序员使用的工具。迄今为止，这些语言中最成功的是 C，由丹尼斯·里奇（Dennis Ritchie）于 1973 年在贝尔实验室开发，至今仍然有着非常广泛的应用。从那时到现在，C 的变化很小，今天的一段 C 程序与 30 年前的相比，几乎没有多大差别。

20 世纪 80 年代又出现了 C++，是比雅尼·斯特劳斯特鲁普（Bjarne Stroustrup）同样在贝尔实验室开发的，定位是应对大型程序开发过程中的复杂性。C++ 由 C 发展而来，而且看起来也跟 C 相似。多数情况下，C 程序也是有效的 C++ 程序（上面的程序就是），但反之却绝对不行。

今天，我们在计算机中使用的大部分软件都是用 C 或 C++ 编写的。我写这本书所用的 Mac，其中安装的大多数软件都是用 C 和 Objective-C（C 的一种方言）写的。一开始我用 Word（C 和 C++ 程序），备份则放在 Unix 和 Linux（都是 C 程序）操作系统上，而我上网使用的是 Firefox 和 Chrome（都是用 C++ 写的）。

20 世纪 90 年代，随着因特网和万维网的发展，更多语言被开发出来。计算机处理器的速度继续加快，内存容量继续增大，而编程是否高效、是否便捷变得比机器效率更重要，此时诞生的 Java 和 JavaScript 在这方面考虑得比较多。

20 世纪 90 年代初，詹姆斯·高斯林（James Gosling）在 Sun Microsystems 公司开发了 Java。Java 最初的目标是开发小型嵌入式系统，例如家用电器和电子设备中的系统，因此对速度要求不高，但对灵活性的要求很高。Java 的目标后来变成了在网页中运行，虽然没有成功，但它在 Web 服务中的应用却非常广泛：打开 eBay 之类的网站，虽然你的计算机在运行 C++ 和 JavaScript 程序，但 eBay 可能正在用 Java 来生成网页，然后发给你的浏览器。Java 比 C++ 简单（但复杂度有越来越接近的趋势），但比 C 复杂。另外，由于去掉了一些危险的特性，并且内建内存管理等避免出错的机制，因此 Java 也比 C 安全。出于这个原因，Java 普遍成为编程课上要学习的第一门语言。

这也引出了程序和编程方面一个非常重要的共识：针对某个特定的任务，总会有多种写程序的方式。从这个意义上说，编程就像是文学创作。没错，风格以及恰如其分地运用语言对写作至关重要，对写程序同样至关重要，而且还是区分真正伟大的程序员与普通程序员的标志。程序员对特定的计算任务可以有如此丰富的表达方式，也意味着不难识别从他人程序中复制的非原创代码。我每次上编程课的时候都会着重强调这个观点，但还是有学生认为改改名字或者挪挪代码，就可以掩盖剽窃的事实。很抱歉，这是行不通的。

1『万法想通，编程语言最最底层的逻辑都是一样的，精通多个语言并且能够根据具体的场景来选择，是我这辈子的一个目标。』

JavaScript 同样是 C 衍生语言大家族的一员，但它与 C 的差别也非常大。它是布兰登·艾奇（Brendan Eich）1995 年在网景公司开发的。最初，设计 JavaScript 的意图是在浏览器中实现网页的动态效果，而今天，几乎每个网页里或多或少都会包含一些 JavaScript 代码。

从某些方面看，JavaScript 是所有语言中最容易实验的。这门语言本身也简单。你不需要为 JavaScript 程序找编译器，每个浏览器都内置了一个。于是乎，计算结果可以迅速出现在你眼前。后面我们还会介绍，给这个程序添上几行代码，然后把它放到网页中，全世界的任何人就都可以使用它了。

以后的语言将何去何从？我猜想，人们将继续使用更多的计算机资源让编程变得更容易。而且我们还会继续发展那些对程序员来说更安全的语言。比如，C 语言就像一柄双刃剑，用它写出的程序很容易遗留错误，而等到发现的时候却已经为时已晚（或许已经被用到了不法用途上）。使用较新的语言更容易防止或至少能检测到某些错误，但有时代价是运行速度慢或占用内存多。大多数情况下，这种取舍的方向是没错的，然而肯定还是有很多应用（比如汽车、飞机、航天器和武器的控制系统）对代码的效率和速度要求相当高，因此像 C 这样的高效语言仍然会有人用。

虽然所有语言在形式上都是等价的（都可以模拟图灵机），但这绝不是说它们都适用于所有的编程任务。写一个控制复杂网页的 JavaScript 程序，与写一个实现 JavaScript 编译器的 C++ 程序仍有天壤之别。同时擅长这两种编程任务的程序员并不多见，经验丰富的专业程序员也可能熟悉或粗通十几种语言，但他们不会对多种语言都同样熟练。

现在的编程语言多达几千种甚至上万种，但真正广泛使用的恐怕连 100 种都到不了。为什么会这么多？前面也提到过，每种语言都代表了对效率、表达力、安全性和复杂性的取舍。许多语言显然是为了弥补之前语言的不足才被发明的，它们不仅吸取了之前语言的教训，还能利用更多的计算资源，通常也会受到设计者个人偏好的强烈影响。新的应用领域也会催生专门面向该领域的新语言。

不管怎么样，编程语言都是计算机科学的一个重要而迷人的部分。正如美国语言学家本杰明·沃尔夫（Benjamin Whorf）所说：「语言塑造我们的思维方式，决定我们可以思考什么。」这个论断是否适用于自然语言还有争议，但对于我们发明的告诉计算机去做什么的人造语言来说，好像还是挺靠谱的。

现实中的编程往往是大规模的。大规模编程的方法与任何人想写一本书或承担任何大项目时一样：先搞清楚要做什么，然后从大概的规程着手，将其一级一级分解为较小的任务，再分别完成这些小任务，同时保证它们能够组合在一起。在编程中，每个小任务意味着一个人用某种编程语言可以写出来的精确的计算步骤。确保不同的程序员编写的代码能够在一起工作很有挑战性，而做不到这一点则是错误的主要来源。例如，1999 年美国航空航天局发射的火星气象卫星坠毁，就是因为飞行系统软件在计算推动力时使用的是公制单位，但输入的路线校正数据使用的则是英制单位，使得该卫星太过接近火星表面所致。

今天，稍有价值的程序可能都会包含几千甚至几万行代码。参加我的编程实践课的学生，分为几个小组，写两三千行代码通常需要 8 到 10 周时间，包括系统设计和学习一两门新语言的时间。他们的作品通常都是一些 Web 服务，涉及学生间的二手书交易或者为访问某些大学数据库提供便利。

编译器或 Web 浏览器可能有几十万到一百万行代码。大型系统则可能有几百万甚至上千万行代码，由数百或数千人共同开发，开发时间也长达几年乃至几十年。这种规模的软件需要程序员、测试人员、文档编写人员协同工作，还要有开发计划、最终期限，层层管理，历经无穷无尽的会议一步步走下去。（据我的一位同事说，他曾参与过一个重要系统的开发，针对该系统的每一行代码都要开一次会。那个系统有几百万行代码，所以他说得可能太夸张了，但老资格的程序员可能还是会说：「这话说得不过分。」）

今天如果要盖一间房子，你不必自己伐木取材，烧土做砖，你可以去买各种预制件，比如门、窗、卫浴器具、火炉和热水器。盖房起屋仍然是一个艰巨的任务，但却是你完全能够做到的，因为你可以使用其他人的工作成果，还有各种基础设施做保障。实际上是有一个完整的产业链条，从各个方面为你提供帮助。

编程未尝不是如此。所有重要的程序几乎没有从零开始写的，有许许多多别人已经写出来的东西可以拿来就用。举个例子，如果你在为 Windows 或 Mac 写程序，那么有很多库都能提供预制的菜单、按钮、文本编辑器、图形、网络连接、数据库访问功能。实际上，你的大部分时间要用来理解这些组件，然后再以自己的方式把它们「粘」在一块儿。当然，这些组件有很多也依赖其他更简单、更基础的库，经常要分好几层。而在最下层，就是支持所有程序运行的操作系统，它是负责管理硬件并确保一切井然有序的程序。

在最基本的层次上，编程语言提供了一种机制，叫做函数。这样，程序员就可以写出一段执行某个任务的代码，然后以某种形式把它包装起来，提供给别的程序员在其他程序里使用，而这些程序员不必知道那些代码具体如何完成该任务。

这里的代码「调用」（也就是使用）了两个 C 语言内置的函数：scanf 和 printf。其中，scanf 用于从输入源读取数据（类似我们玩具程序中的 GET），而 printf 打印输出（类似 PUT）。函数有函数名，接收完成任务所需的输入数据值，完成计算后把结果返回给调用它的程序。这里的语法及其他细节都是 C 语言特有的，可能会与其他语言不一样，但内在思想是一致的。函数使我们可以基于组件搭建程序，而这些组件则是独立创建、可以由任何程序员按需使用的。把一组相关的函数集合起来，就叫做库。例如，C 有一个标准函数库，用于读写磁盘和其他地方的数据，scanf 和 printf 就是这个库里的函数。

函数库提供的服务是通过 API（Application Programming Interface，应用编程接口）的形式描述给程序员的。API 会罗列出所有函数，说明每个函数的用途、用法、需要的输入数据，以及生成什么值。API 也会描述数据结构，也就是传进来传出去的数据的组织形式，以及为请求服务必须遵守哪些条条框框和计算将返回什么结果。这种说明书必须面面俱到、严谨准确，因为基于它编写的程序最终会由一台不会说话的计算机而不是一个随和友善的人去解读。

API 文档中不仅包含对语法的要求，也包括大量辅助说明，用以帮程序员更有效地使用函数库。今天的大型系统开发通常都会用到 SDK（Software Development Kit，软件开发工具包），以便程序员在极其复杂的软件库里找到有用的函数。

这些缺陷被称为 bug，这个词是因我们前面提到的格蕾斯·霍普而流行起来的。那还是 1947 年，霍普的同事在哈佛 Mark II（他们当时使用的一种机械计算机）中发现了一只虫子（死了的蛾子），结果她就说她们是在给这台机器「除虫」（debug）。那只死虫子后来被保存下来，还做成了标本供后人瞻仰。如果你去华盛顿，可以在史密森尼美国历史博物馆里看到它。

导致 bug 的原因多种多样，甚至可以写出一本书来专门论述（确实有这类书）。其中比较常见的原因包括忘记处理可能发生的情况、在测试某个条件时写错了逻辑或算术表达式、用错了公式、访问了没有分配给程序的存储器、错误地操作了某种数据、没有验证用户输入等等。

软件中的错误如果暴露到网络上，就会成为可能遭受攻击的漏洞。攻击者通常会利用这些漏洞以自己的恶意代码重写内存。正是由于 bug 广泛存在，所以各种重要的软件才会频繁升级，比如浏览器，它已经成为很多网络黑客关注的焦点。就拿我使用的 Firefox 来说吧，短短 18 个月就经历了 19 次修订，共修复近 100 个安全漏洞。这可不是什么特例，但也并不意味着 Firefox 程序员都不够格。这说明编写一个耐用的程序非常难，而坏人始终都在寻找你的弱点。

现实中软件面临的另一个复杂性在于外界环境瞬息万变，因此程序必须不断适应新情况。新的硬件问世后，它所需要的软件可能得进行系统级的改动。新的法律法规出台，程序的逻辑可能就必须调整 —— 众所周知，税法每次有什么变化，相关软件都要升一次级。计算机、工具和语言过了时，就需要有新的替代品问世。数据格式过时的情况更加常见 —— 今天的 Word 软件打不开 20 世纪 90 年代初写就的 Word 文档。自然，存储和处理这些数据的物理设备也一样在代代更替。而随着人的退休、死亡或被公司解雇，专业知识也会消逝。学生在校期间开发的系统随着他们的毕业，也面临相同的遭遇。

必须持续不断地小幅更新，这是软件开发和维护的一大问题，但是不做还不行。如果不那么做，程序就会遭遇「比特腐烂」，一段时间之后，也许就不能用了，或者想更新都更新不了了，因为重新编译无法通过，或者它所依赖的库已经变得太多了。与此同时，无论更新还是修复问题，通常都会产生新的 bug，或者改变用户熟悉的行为。

软件所有权引发了很多棘手的法律问题，我认为比硬件的问题还要多，但作为一个程序员，这也可能是我的偏见。与硬件相比，软件是一个比较新的领域，1950 年之前，还没有软件呢。软件独立成为经济发展的一支力量，还只是近二三十年的事。因此，相关的法律、商业惯例和社会规范等机制还来不及完善。软件是有价值的，但又是无形的。开发和维护相当规模的代码需要投入持续的艰苦劳动。与此同时，又可以没有限制地复制软件或者在全世界范围内分发它，却不发生任何成本。你可以很容易地把它改来改去，而不管怎么改，它都是看不见的。凡此种种，导致软件的所有权问题特别棘手。

标准是对某些产品如何制造或者应该具有什么用途的准确、详细的说明。软件标准的例子涉及编程语言（即语法和语义的定义）、数据格式（如何表示信息）、算法处理（完成某个计算的特定步骤），等等。

某些标准是事实标准，比如 Word 软件的 .doc 格式。事实标准指的是没有正式的名义，但每个人都在用。「标准」这个词最好只用于正式的说明书，通常由政府或协会等中立的团体制定和维护，规定某物如何制造和运作。标准的定义是足够完整和准确的，独立的实体可以反馈意见或提供中立的实现。我们每时每刻都受益于硬件标准，尽管我们根本想象不到有多少个硬件标准。如果我买了一台新电视，我之所以可以把它的电源插头插到我家的插座上，就是因为有标准规定插头的大小和形状以及电视和插座的电压。电视可以接收信号并显示画面，是因为广播和有线电视也有标准。而使用标准的 HDMI、USB、S-Video 数据线和连接器，我还可以把其他设备连接到电视上。然而，每台电视都有它自己的遥控器，每一部手机也都有各自的充电器，则是因为它们都还未实现标准化。

计算机领域也有各种各样的标准，字符集有 ASCII 和 Unicode，编程语言有 C 和 C++，算法有加密的和压缩的，还有通过网络交换信息的各种协议。有时候甚至还有相互竞争的标准，让人觉得有点浪费。（正如计算机科学家安迪·特南鲍姆（Andy Tanenbaum）所说：「多个标准的好处在于让人有多个选择。」）过去的例子有录像带的 Betamax 和 VHS 标准，而最近的例子是高清视频盘的 HD-DVD 和 Blu-ray Disc 标准。对于这两种情况，前者以一种标准最终胜出告终，后者则很可能两种标准共存，就像美国存在两种不兼容的手机技术一样。

标准很重要。有了标准，大家各自制造的东西才能集成到一起，多个供应商才能同台竞技，而专有系统则会把每个人限制死。（专有系统的所有者自然愿意把人们都限制在它的平台上。）标准也有缺点 —— 如果标准本身质量不高或者已经过时，但所有人又都被迫使用它，那它就会阻碍进步。不过，与它的优点相比，这些缺点还是能够接受的。

程序员编写的代码，无论使用的是汇编语言还是（更可能的）某种高级语言，都被称为源代码。而编译源代码得到的适合某种处理器执行的编码，叫做目标码。正如已经介绍的它们之间的其他区别一样，区别源代码和目标码看起来有点迂腐，但却非常重要。源代码是程序员可以读懂的，尽管可能得费点时间和精力。因此源代码是可以仔细研究并加以改编的，它所包含的任何创新和思想也是可见的。相对而言，目标码则经过了很大程度的转换，一般不太可能再恢复为类似源代码的形式，也无法从中提取出什么结构再加以改造，甚至连理解它都是不可能的。正因为如此，大多数商业软件只以目标码的形式分发，而源代码是重要的机密，因此说比喻也好，事实也罢，反正它会被锁得严严实实的。

开放源代码则是指另一种做法，即源代码可以被任何人自由阅读、研究和改进。

早期，大多数软件由公司开发，源代码是一般人看不到的，那是属于开发者所有的商业秘密。在麻省理工学院工作的理查德·斯托曼（Richard Stallman），曾经希望能够修改和加强自己使用的某些程序，但这些程序的源代码是别人私有的，自己根本看不到。为此，斯托曼感到很懊恼。1983 年，他发起了一个叫 GNU（即「GNU's Not Unix」，gnu.org）的项目，致力于开发一些重要软件（比如操作系统和编程语言的编译器）的自由和开放版本。他还创办了一个非营利组织，叫自由软件基金会（Free Software Foundation）。这个组织的目标是开发那些永远「自由」的软件，也就是说这些软件不是私有的、不会受到所有权的限制。为此，自由软件的实现在分发时都必须遵守一个独创的版权许可，叫做 GNU 通用公共许可（GNU General Public License）或简称 GPL。

GPL 的序言如是说：「大多数软件及其他实用作品的许可，目的都是剥夺你分享和修改作品的自由。相比而言，GNU 通用公共许可则意在保证你分享和修改程序各个版本的自由，也就是确保该程序对所有用户来说仍然是自由软件。」GPL 规定，基于该许可的软件可以被自由使用，而如果再把它分发给其他人，则必须公开源代码，并同样遵守「所有用户都可以自由使用」的许可。GPL 是一种强有力的许可，一些违反其条款的公司已经被禁止使用其代码，或者公开了以许可约束的代码为基础的源代码。

GNU 项目由很多公司、组织和个人支持，发布了大量软件开发工具和应用程序，这些软件全部采用 GPL 许可。其他类似的开源软件也采用类似的许可方式。很多时候，开源软件都为专有商业软件设立了标杆。比如，Firefox 和 Chrome 浏览器是开源的，大多数 Web 服务器上运行的 Apache Web 服务器软件也是开源的，Android 手机操作系统也是开源的。所有主要的编程语言都有开源的编译器，大多数程序员工具（包括我用来生成本书的工具）也都有开源版本。

Linux 操作系统或许是最广为人知的开源系统了（虽然它并不属于 GNU 项目），它被个人和大型商业企业广泛使用，比如谷歌的全部基础设施都运行在 Linux 之上。如果你想得到 Linux 内核源代码，访问网站 kernel.org 即可免费下载。下载后既可以自己使用，也可以对它进行任意修改。不过，要是你想以任何形式再次发布（比如，把它作为操作系统放到一个小工具里发布），那必须遵守相同的 GPL 协议开放源代码。

开放源代码是很值得研究的。把源代码送人还怎么赚钱呢？为什么程序员愿意为开源项目做贡献呢？志愿者编写的开源软件比大型专业团队协作开发的专有软件更好吗？源代码可以随便下载会不会威胁到国家安全？

这些问题持续吸引着经济学家和社会学家，也有一些答案慢慢浮出了水面。例如，红帽子（Red Hat）是一家在纽约证券交易所公开上市的公司，该公司 2011 年的年收入近 10 亿美元，市值超过 80 亿美元。他们发布的 Linux 源代码可以在网上免费下载，但公司通过支持、集成和其他收费服务可以获得收入。一些开源程序员本身就在那些使用并支持开源软件的企业工作，IBM 是一个明显的例子，但绝非特例。这些公司通过影响开源软件的发展，通过让其他人修复 bug 和改进功能而获得收益。并不是所有开源软件都能独领风骚，开源版本不如它所模仿的商业版本的情况也比比皆是。但是，对于一些核心的程序员工具和系统来说，开源软件生生不息，的确很难被比下去。

## 0203软件系统

两种主要的软件：操作系统和应用程序。操作系统是软件中的基础层，它负责管理计算机硬件，并为其他被称作应用程序的程序运行提供支持。

20 世纪 50 年代初，还没有应用程序与操作系统之分。计算机的能力非常有限，每次只能运行一个程序，这个程序会接管整台机器。而程序员要使用计算机，运行自己的程序，必须事先预约时间段（身份低微的学生只能预约在半夜）。随着计算机变得越来越复杂，再靠非专业人员使用它们效率就会很低。于是，操作计算机的工作就交给了专业操作员。计算机操作员的任务就是把程序输入计算机，然后把计算结果送交相应的程序员。操作系统最初就是为了代替人工操作员完成上述工作才诞生的。

硬件不断发展，控制它们的操作系统也日益完善。而随着硬件越来越强大、越来越复杂，就有必要集中更多的资源来控制它们。第一批广泛使用的操作系统诞生于 20 世纪 50 年代末、60 年代初。这些操作系统通常是由硬件厂商提供的，IBM 和 Univac 都推出过自己的操作系统。后来，就连小一点的公司像 Digital Equipment 和 Data General 也都开发过自有操作系统。

操作系统也是很多大学和业界实验室的研究目标。MIT（麻省理工学院）作为这方面的先驱，在 1961 年开发了一个名为 CTSS（Compatible Time-Sharing System，兼容分时系统）的系统，该系统比同时代与之竞争的其他产品都先进得多，用起来的感觉也非常好。1969 年，贝尔实验室的肯·汤普森（Ken Thompson）和丹尼斯·里奇（Dennis Ritchie），结合他们对 CTSS 以及更完善但却不那么成功的 Multics 系统的第一手经验，开始着手开发 Unix。今天，除了微软开发的那些操作系统之外，大多数操作系统要么源自当初贝尔实验室的 Unix 系统，要么是与 Unix 兼容但独立分发的 Linux 版本。里奇和汤普森因为开发了 Unix 而一起荣获 1983 年图灵奖。

现代的计算机确实是一个复杂的「怪物」。它由很多部件组成，包括处理器、内存、磁盘、显示器、网卡，等等。为了有效地使用这些部件，需要同时运行多个程序，其中一些程序等着某些事件发生（如网页下载），另一些程序则必须实时作出响应（跟踪鼠标移动或在你玩游戏的时候刷新显示器），还有一些会干扰其他程序（启动新程序时需要在已经很拥挤的 RAM 中再腾出空地儿来）。简直就是一片混乱。

要管理如此复杂的局面，唯一的办法就是用程序来管理程序，这也是让计算机自己帮自己的又一个例子。这么一个程序就叫作操作系统。家用和商用计算机中最常见的操作系统是微软开发的各种版本的 Windows。我们日常见到的计算机 90% 都是由 Windows 管理的。苹果电脑运行的是 Mac OS X，它是一种 Unix 变体，也是在消费领域中仅次于 Windows 的第二大操作系统。而很多做幕后工作的计算机（当然也有一些直接面向用户的计算机）运行的是 Unix 或 Linux。手机中也有操作系统，它们是精简版 Windows、Unix、Linux 或其他特殊系统。例如，iPhone 和 iPad 运行的 iOS 就源自 Mac OS X，而我的 Android 手机、电视机、TiVo、亚马逊 Kindle 和巴诺 Nook 运行的都是 Linux 操作系统。我甚至可以登录自己的 Android 手机，在上面运行标准的 Unix 命令。

操作系统控制和分配计算机资源。

首先，它负责管理 CPU，调度和协调当前运行的程序。它控制 CPU 在任意时刻执行的程序，包括应用程序和后台进程（如杀毒软件和检查更新的程序）。它会将一个暂时等待的程序（比如等待用户在上面单击的对话框）挂起。它会阻止个别程序多占资源。如果一个程序占用 CPU 时间太多，操作系统会强行将其中断以便其他任务得以正常执行。操作系统通常都需要管理数十个同时运行的进程或任务。其中有些是由用户启动的程序，但大多数还是一般用户看不到的系统任务。在 Mac OS X 上通过 Activity Monitor，或在 Windows 上通过任务管理器，可以看到系统当前都运行有哪些程序。

其次，操作系统管理 RAM。它把程序加载到内存中以便执行指令。如果 RAM 空间不足，装不下所有程序，它就会将某些程序暂时挪到磁盘上，等有了空间之后再挪回来。它确保不同的程序相互分离、互不干扰，即一个程序不能访问分配给另一个程序或操作系统自身的内存。这样做既是为了保持清晰，同时也是一种安全措施，谁也不想让一个流氓程序或错误百出的程序到处乱窜。（Windows 中常见的「蓝屏死机」现象就是因为这种保护做得不到位造成的。）

为了有效利用 RAM，必须事先进行周密设计。一种思路是在必要时把程序的一部分加载到 RAM，而在程序处于非活动状态时再把它转存回磁盘，这个过程称为交换（swapping）。程序编写得就好像整台计算机都归它自己使用一样，而且也不必考虑 RAM 的限制。这样就大大简化了编程工作。另一方面，操作系统必须支持这种「假象」，方法就是在内存地址转换硬件的帮助下，不断地换入换出程序块，让程序认为它一直都在访问真实内存中的真实地址。这种机制被称为虚拟内存（virtual memory）。所谓「虚拟」，意思就是营造一种假象，而实际上这些内存并不存在。

1『活动监视器里，内存页面里，确实有一个「已使用的交换」的数据。』

第三，操作系统管理存储在磁盘上的信息。文件系统是操作系统中的一个主要组成部分，负责提供我们在计算机中都见过的那种文件夹和文件般的分层机制。

最后，操作系统管理和协调外接设备的活动。它维护屏幕上的多个窗口，确保每个窗口都能显示正确的信息，而且在这些窗口被移动、缩放或隐藏后再次显示时，都能准确地恢复原貌。它把键盘和鼠标的输入送往需要这些输入的程序。它处理通过有线或无线网络连接进进出出的流量。它将数据发送给打印机和 DVD 刻录机，从扫描仪取得数据。

1『这么了解后，操作系统实在是太 NB 了，mac os 给我带来的感触更直接。』

请注意，我说过操作系统也是程序。它跟我们在上一章讲到的其他程序一样，都是用同一类编程语言编写的，最常用的是 C 和 C++。早期的操作系统很小，因为工作比较简单。最早的操作系统每次只运行一个程序，所以程序交换量很小。没有太多内存可供分配（最多也就几百 KB）也决定了内存管理很简单。而且也没有太多外部设备需要管理，跟今天的外设比起来显然要少得多。今天的操作系统已经非常庞大（动辄包含数百万行代码）非常复杂了，因为它们的任务本身就非常复杂。

就以 Unix 操作系统第 6 版为例，它是今天很多操作系统（不包括 Windows）的鼻祖。它在 1975 年的时候是一个包含 9000 行 C 代码的汇编程序，两个人（肯·汤普森和丹尼斯·里奇）就可以把它写出来。如今的 Windows 7 拥有大约 1 亿行代码，Linux 的代码也超过了 1000 万行，它们都是几千人历经几十年工作的成果。当然，就这么直接拿来比也不太合适，毕竟现在的计算机要复杂得多，而且今天的环境和设备也要复杂得多。操作系统包含的组件同样也有差别。

2『各操作系统的代码数量做一张信息卡片。』

既然操作系统也是程序，那么从理论上说你也可以写出自己的操作系统来。事实上，Linux 最早就是由芬兰大学生李纳斯·托沃兹（Linus Torvalds）在 1991 年写出来的，他当时的想法就是从头编写一个自己的 Unix 版本。他在互联网上发布了自己写的一个简陋程序，邀请别人试用和帮忙。自那时起，Linux 就逐渐成为软件业中一支重要的力量，很多大大小小的公司都在使用。上一章提到过，Linux 是开源软件。今天，Linux 除了核心的全职开发人员之外，还有数千名志愿者参与开发，而托沃兹负责总体把控和最终裁决。

2『Linux 创始的时间做一张信息卡片。』

我们甚至可以在一个操作系统的控制下运行另一个虚拟操作系统。使用 VMware、Parallels 和（开源的）Xen 等虚拟操作系统软件，可以在一台 Mac OS X 主机上运行另一个客户操作系统，比如 Windows 或 Linux。主机操作系统会拦截客户操作系统的请求，代替它执行那些需要具备操作系统级权限才能执行的操作，如访问文件系统或网络。主机在执行完操作后，将结果返回给客户机。在主机和客户机系统都是为相同硬件编译的情况下，客户系统大多数时候都得到硬件的全速支持，响应的及时性给人感觉就像在裸机上运行一样。

这里有必要说一说「虚拟」这个词的另一种用法。一个模拟计算机的程序，无论它模拟的是真实的计算机还是想象中的计算机（比如本书前面提到的玩具计算机），经常也被称为虚拟机。换句话说，计算机只以软件形式存在，而这种软件的行为就如同硬件一般。这种虚拟机很常见。浏览器都有一个虚拟机用于解释 JavaScript 程序，所有 Java 程序也都是通过虚拟机来解释的，而每台 Android 手机上同样有一个类似的 Java 虚拟机。

2『虚拟机是一个模拟计算机的程序。这个定义让我对虚拟机的概念清楚一些了。虚拟机的定义做一张术语卡片。』

CPU 的结构是经过特殊设计的。计算机加电后，CPU 会开始执行存放在非易失性存储器中的一些指令。这些指令继而从一小块闪存中读出足以运行某些设备的代码。这些代码在运行过程中再从磁盘、CD、USB 存储器或网络连接的既定位置读出更多指令。这些指令再继续读取更多指令，直到加载了足够完成有效工作的代码为止。这个准备开始的过程叫做启动（booting），源自拉着靴带（bootstrap）给自己穿上靴子的典故。具体细节可能不同，但基本思想是一样的，即少量指令足以找到更多指令，后者依次再找到更多的指令。

计算机启动过程中通常还要检查硬件，以便知道有哪些设备接入了计算机，比如有无打印机或者无线设备。还会检查内存和其他组件，以确保它们都可以正常工作。启动过程还会为接入的设备加载软件（驱动程序），以便操作系统能够使用这些设备。上述过程都需要时间，而我们从开机到计算机能用的这段时间内通常都会等得不耐烦。

操作系统运行起来之后，它就会转而执行一个简单循环，依次把控制权交给准备运行或需要关注的每个应用程序。如果我在字处理程序中输入眼下这些字的时候，顺便收了一下邮件，又到网上逛了逛，同时还在后台播放音乐，那么操作系统会让 CPU 依次处理这些进程，并根据需要在它们之间切换。每个程序会得到一段极短的时间，在程序请求系统服务后或者分配给它的时间用完时结束。

操作系统会响应各种事件，比如音乐结束、邮件或网页到达，或者用户按下了键盘上的按键。对这些事件，操作系统都会作出必要的处理，通常是把相应的事件转发给相关的应用程序。如果我重新排列屏幕上的窗口，操作系统会告诉显示器把窗口放在什么地方，并告诉每个应用程序它们各自窗口的哪一部分可见，以便重新绘制窗口。如果我选择「文件> 退出」或单击窗口右上角的「×」按钮退出应用程序，系统会通知应用程序它马上要「死」了，以便它赶紧「安排后事」（比如，弹出对话框询问用户「您想保存这个文件吗？」）。然后，操作系统会回收该程序占用的所有资源，并告诉那些窗口得见天日的其他程序，必须重绘各自的窗口了。

操作系统提供了硬件和其他软件之间的接口。有了这个接口，硬件就好像能听懂人的话了，而程序员编程因此就会变得简单。用这个圈子里的行话说，操作系统提供了一个平台，在这平台上可以构建应用程序。

操作系统为应用程序定义了一组操作（也叫服务），比如将数据存储至文件或者从文件中取出数据、建立网络连接、获取键盘输入、报告鼠标移动和按钮点击、绘制屏幕，等等。

1『对「服务」的定义：操作系统为应用程序定义的一组操作。』

操作系统以标准化的或者说大家协商一致的方式提供这些服务，而应用程序通过执行一种特殊的指令来请求这些服务，并将控制权移交给操作系统中特定的地址。操作系统根据请求完成计算，然后再将控制权和结果返回给应用程序。操作系统的这些「入口」被称为系统调用（system call），而对这些系统调用的详细说明实际上恰恰解释了操作系统能做什么。系统调用可以直接拿操作系统内部的代码作为入口，也可以是对某个（为相应服务而准备的）库函数的调用。但多数情况下，即便是程序员也不用关心上述区别。正因为如此，谁也说不清楚到底有多少个系统调用，但通常一两百个总是有的。

2『对「系统调用」的概念再去了解一下。原书这个章节里有一张图表达的相当清楚了，可以时常看看。（操作系统、系统调用、驱动程序和应用程序之间的关系图）』

操作系统中的另一个部件 —— 设备驱动程序。设备驱动程序是一种沟通操作系统与特定硬件设备（如打印机和鼠标）的程序。驱动程序的代码知道怎么让特殊的设备履行自己的职责，比如从特定的鼠标得到移动和按钮信息、让磁盘通过旋转的磁表面读取和写入信息、让打印机在纸上留下记号、让特定的无线网卡发送和接收无线电信号。

就说打印机吧。操作系统只会发出标准的请求，比如「在这个位置上打印这段文本」、「绘制这幅图像」、「移到下一页」、「描述你的能力」、「报告你的状态」，等等。而且，还是以适合所有打印机的标准方式发出这些请求。然而，打印机的能力是有差别的，比如支不支持彩色打印、双面打印，或者不同纸张大小。打印机专属的驱动程序，要负责把操作系统请求转换为特定设备完成相应任务必需的指令。一句话，就是操作系统发送通用的请求，而具体的设备驱动程序负责在各自硬件上落实、执行请求。

驱动程序把操作系统与特定设备独有的性质隔离开来（任何设备，比如各种键盘，都有一些操作系统要用到的基本性质和操作），操作系统通过驱动程序的接口以统一的方式访问相应设备，从而方便在设备之间切换。通用的操作系统都包含很多驱动程序。例如，Windows 为满足各种潜在用户的需要，在发行时就已经带有各种各样的设备驱动程序。每个设备的制造商都有自己的网站，提供新版本或更新的驱动程序下载。

启动过程中有一个环节就是把当前可用设备的驱动程序加载到运行的系统中。可用的设备越多，加载要花的时间就越长。新设备随时有可能出现。在把外部磁盘插入 USB 插槽后，Windows 会检测到这个新设备，（根据设备驱动程序接口的某个部分）确认它是一个磁盘，然后加载 USB 磁盘驱动程序与这个磁盘通信。Mac OS X 操作系统也一样。一般来说，没有必要不断升级新的驱动程序，因为所有设备的接口都是标准化的，操作系统本身已经包含了必要的代码，而驱动设备的特殊程序也已经包含在设备自身的处理器中。

总之，这些设备俨然与主流通用计算机一般无二。它们都有强劲的处理器、大容量的内存，以及一些外围设备（比如数码相机上的镜头和显示屏）。它们的用户界面极其精美。它们可以通过网络连接与其他系统通信 （手机使用电话网络和 Wi-Fi，游戏机手柄使用红外线和蓝牙），不少还提供 USB 接口，以支持移动硬盘的临时接入。

随着这种趋势的不断演进，选择市面上现成的操作系统要比自己从头写一个来得更实际。除非用途特殊，否则在 Linux 基础上改一改是成本最低的，关键是 Linux 非常稳定、容易修改、方便移植，而且免费。相对而言，自己开发一个专有系统，或者取得某个商业系统的许可，都会引入巨大开销。当然，改造 Linux 的缺点在于必须把改造后的操作系统部分代码按照 GPL 许可发布，由此可能引发如何保护设备中知识产权的问题。不过，从 Kindle 和 TiVo 的案例来看，似乎也没有什么不能解决的。

文件系统是操作系统的一个组成部分，它能够让硬盘、CD 和 DVD、移动存储设备，以及其他各种存储器等物理存储媒体，变成看起来像是由文件和文件夹组成的层次结构。我们常说计算机有逻辑组织和物理实现两大概念，文件系统就是这两大概念的集中体现。文件系统能够在各种不同的设备上组织和存储信息，但操作系统则为所有这些设备都提供相同的接口。文件系统存储信息的方式以及存储多久，最终甚至会衍生出一些法律问题来。所以说，研究文件系统的另一个目的，就是要理解为什么「删除文件」并不代表其内容会永远消失。

1『文件系统是计算机逻辑组织和物理实现的集中体现。』

几乎所有人都用过 Windows 的资源管理器或者 Mac OS X 的 Finder，这两个工具都能列出自最顶层（比如 Windows 中的 C 盘）开始的文件系统的层次结构。在这个层次结构里，一个文件夹（folder）可以包含其他文件夹和文件。换句话说，点开一个文件夹，就可以看到更多文件夹和文件。（Unix 系统则一直使用目录（directory）而不是文件夹的概念。）这里的文件夹是一个组织结构的概念，而实际的文档内容、图片、音乐、电子表格、网页等，则保存在文件中。在计算机中，所有这些信息都存储在文件系统内，只要用鼠标点击几下就可以找到。文件系统中不光存储数据，还存储着可以执行的程序（比如浏览器）、代码库、设备驱动程序，以及构成操作系统自身的文件。这些文件的数量大得惊人，就说我这个普普通通的 MacBook 吧，其中存储的文件已经超过了九十万个。

文件系统管理所有这些信息，方便其他程序和操作系统的其他部件读写这些信息。它统筹安排所有的读写操作，确保这些操作有效进行，且不会相互干扰。它记录数据的物理位置，确保它们各就各位，不会让你的电子邮件意外地窜到你的电子表格或纳税申报单里面去。在支持多用户的系统中，还要保证信息的隐私权和安全性，不能让一个用户在未经允许的情况下访问另一个用户的文件。另外，可能还需要限制每个用户有权使用的硬盘空间，也就是所谓的配额管理。

在最低级的层次上，文件系统服务是通过系统调用来提供的。程序员通常要借助代码库来使用这些系统调用，以简化编程过程中常见的文件处理操作。

无论什么样的存储设备，在文件系统中一律只表现为文件夹和文件。从这一点来说，文件系统是把物理实现抽象为逻辑组织的绝佳范例。但它又是怎么做到的呢？

一块 100GB 的硬盘可以存储 1000 亿字节的数据，但这块硬盘上的软件可能会将其看成 1 亿个 1000 字节大的块。（现实中，块的大小应该是 2 的幂，而且每个块还会更大。这里使用十进制是为了便于说明其中的关系。）这样，一个 2500 字节大的文件（比如一封普通的邮件），就需要 3 个这样的块来存储。因为 2 个块存不下，而 3 个块有富余。文件系统不会在同一个块内存储不同文件的信息，因而就免不了有一些浪费。因为存储每个文件的最后一个块不会完全用完（在我们举的这个例子中，最后一个块就会闲置 500 字节）。考虑到简化记录工作所节省的工作量，这点代价还是值得的，更何况磁盘存储器已经那么便宜了。

这个文件所在的文件夹条目中会包含文件的名字、2500 字节的文件大小、创建或修改时间，以及其他细节信息（权限、类型等，取决于操作系统）。所有这些信息都可以通过资源管理器或者 Finder 看到。

这个文件夹条目中还会包含文件在磁盘上的位置信息，也就是 1 亿个块中的哪 3 个块存储着这个文件。管理这些位置信息的方法有很多种。比如，文件夹条目可以包含一组块编号，也可以引用一个自身包含一组块编号的块，或者只包含第一个块的编号，第一个块又包含第二个块的编号，依此类推。下面这幅示意图展示了文件夹引用块列表的大致情况：

存储同一个文件的块在磁盘上不一定连续。事实上，这些块一般都不挨着，至少存储大文件的块是这样的。兆字节级别的文件需要占用上千个块，这些块通常会分散在磁盘的各个地方。尽管从这幅图中看不出来，但文件夹及块列表本身也存储在块中。

文件夹也是一个文件，只不过这个文件中包含着文件夹和文件的位置信息。由于涉及文件内容和组织的信息必须精准、一致，所以文件系统保留了自己管理和维护文件夹内容的权限。用户和软件只能请求文件系统来间接地修改文件夹内容。

没错，文件夹也是文件。从存储方式上讲，它们跟文件没有任何区别。只不过文件系统会全权负责管理文件夹内容，任何应用软件都不能直接修改该内容。除此之外，它们都保存在硬盘上的块中，由相同的机制进行管理。

在应用程序要访问已有的某个文件时，文件系统必须从其顶级层次开始搜索该文件，在相应文件夹里查找文件路径中的每一部分。举个例子，假设要在 Mac 中查找 /Users/bwk/book/book.txt。文件系统首先要在其顶层搜索 Users，然后在该文件夹里搜索 bwk，接着在找到的文件夹里搜索 book，最后再在找到的文件夹里搜索 book.txt。这是一种化整为零的思路。也就是说，路径中的层次会逐步缩小要搜索的文件或文件夹的范围，同时把其他不相干的部分过滤掉。正因为如此，不同层次中的文件可以使用相同的名字，唯一的要求是完整的路径必须独一无二。实践中，应用程序和操作系统会记住当前的文件夹，因而文件系统不必每次都从顶层开始搜索。而为了加快处理速度，系统还可能会缓存频繁用到的文件夹。

应用程序在创建新文件时会向文件系统发送请求，文件系统会在相应的文件夹中增加一个新条目，包含文件名、日期等项，还有文件大小为零（因为还没有为这个新文件分配磁盘块）。接下来，应用程序要向文件中写入某些数据时（比如向一封邮件中写几句话），文件系统会找到足够多的当前没有使用的或者「空闲」的块来保存相应内容，并把数据复制过去。然后把这些块插入到文件夹的块列表中，最后返回给应用程序。

不难想象，文件系统还要维护一个磁盘上当前未被使用（也就是还没有成为某些文件一部分）的块的列表。每当应用程序请求新磁盘块，它就可以从这些空闲的块中拿出一些来满足请求。这个空闲块的列表同样也保存在文件系统的块中，但只能由操作系统访问，应用是访问不到的。

删除文件时，过程恰好相反：文件占用的块会回到空闲列表，而文件夹中该文件的条目会被清除，结果就好像文件被删除了一样。现实中的情况并不完全如此，而是加入了一些有意思的比喻。当你在 Windows 和 Mac OS X 中删除一个文件时，这个文件会跑到「回收站」或「垃圾桶」里去。「回收站」和「垃圾桶」不过是另外一个文件夹，但具备某些常规文件夹所不具备的属性。正因为如此，才称其为「回收站」嘛。删除文件时，相应的文件夹条目将从当前文件夹被复制到名叫「回收站」或「垃圾桶」的文件夹里，然后会清除掉原来的文件夹条目。但是，这个文件占用的块以及其中的内容没有丝毫变化！从「回收站」里还原文件的过程正好相反，就是把相应条目恢复到它原来所在的文件夹中。

「清空回收站」倒是跟我们本节一开始描述的过程很相似。此时「回收站」或「垃圾桶」里的文件夹条目会被清除，相应的块会真正再添加到空闲块列表中。不管是明确地执行这个操作，还是文件系统因为空闲空间过少而在后台静默地清空，这个过程都将实实在在地发生。

假设是你明确地执行清空操作。那么这个操作首先清除「回收站」文件夹中的条目，然后把其中文件占用的块回写到空闲块列表。但是，这些文件的内容并没有被删除。换句话说，原始文件占用的每个块中的所有字节都会原封不动地呆在原地。除非相应的块从空闲块列表中被「除名」并奉送给某个应用程序，否则这些字节不会被新内容覆盖。

这意味着什么呢？意味着你认为已经删除的信息实际上还保存在硬盘上。如果有人知道怎么读取它们，仍然可以把它们读出来。任何可以不通过文件系统而能够逐块读取硬盘的程序，都可以看到那些被「删除」的内容。

显然，这样有一个潜在的好处。就是在硬盘出问题的情况下，还有可能恢复其中的信息，尽管文件系统可能已经一团糟了。可是不能保证数据真正被删除也有问题。假如你想删除的文件里包含隐私，甚至一些见不得人的东西，你肯定希望它们被删除后永远销声匿迹。对精于此道的坏蛋或者执法机关的专家来说，恢复磁盘中的内容只是小菜一碟。因此，假如你在文件中记录了自己穷凶极恶的想法，或者在妄想症支配下写了很多胡话，那最好使用能够把这些信息从空闲块中彻底擦干净的程序。比如 Mac 中的「安全擦除」选项在释放磁盘块之前，会先用随机生成的比特重写其中的内容。

现实当中的你还应该知道更加保险的做法。因为即使用新信息重写了原有内容，一名训练有素的敌人仍旧可以凭借他掌握的大量资源发现蛛丝马迹。军事级的文件擦除会用随机的 1 和 0 对要释放的块进行多遍重写。更为保险的做法是把整块硬盘放到强磁场里进行消磁。而最保险的做法则是物理上销毁硬盘，这也是保证其中内容彻底销声匿迹的唯一可靠方法。如果你的磁盘一直都在执行自动备份（就像我在上班时使用的计算机一样），或者你的文件保存在网络文件系统中而不是本地硬盘上，那么这些招数恐怕也都不灵光了。

文件夹条目本身也存在类似的问题。删除一个文件时，文件系统会让相应文件夹条目不再指向有效的文件。为此，它可能只会把一个表示「本条目不再使用」的比特位设置为 1。这样在将来需要恢复该文件的原始信息，包括所有未被重新分配的块的内容时，只要把这个比特位重置为 0 就可以了。事实上，1980 年代微软 MS-DOS 中的文件恢复系统采用的就是这种办法。对于待释放的空闲条目，该系统会把相应文件名的第一个字符设置为一个特殊值。这样，如果用户很快又要恢复文件，那么实现起来会简单很多。

知道了文件在被创建人删除后还可能存在很长时间，有助于我们理解一些法律程序，比如当事人坦白和文档保全的意义。在法庭上，这种案例屡见不鲜。有时候，一封陈年邮件就可能影响对被告的量刑，至少会让犯罪嫌疑人的陈述露出马脚。如果这些记录只存在于纸面上，那么徒手将其撕碎就能轻易销毁证据。但数字化记录是会扩散的，还可能通过移动硬盘等媒体藏匿于很多地方。（维基解密 2010 年得到的大批机密外交文件就保存在很多张 CD 中。）明智的人都应该时刻注意自己在邮件里的措辞，甚至应该注意通过计算机发表的任何言论。

刚才我们讨论的是硬盘驱动器（包括移动硬盘）上的常规文件系统。我们大多数的信息都保存在这些硬盘上，而且我们对它们也非常熟悉。不过，这个文件系统也同样适用于其他媒体。例如，已经退出历史舞台的软盘，在逻辑上具有同样的层次结构，但细节上有所不同。CD-ROM 和 DVD 同样以文件系统的方式提供访问界面，同样由不同层次的文件夹和文件组成，只不过一般为只读，不能写。固态硬盘通过闪存来模拟常规硬盘，但重量更轻，耗电更省。

USB 闪存盘和 SD（Secure Digital，安全数字式）闪存卡已经无处不在。把它们插入到一台 Windows 计算机中，它们就会像一块新硬盘一样。通过资源管理器可以查看其中的内容，并像在普通硬盘上一样执行读写操作。唯一的区别就是它们容量小一些，有时候速度可能也慢一些。

如果把它们插到一台 Mac 上，它们同样表现为分层的文件系统，可以通过 Finder 浏览，文件也可以拷来拷去。把它们插到 Unix 或 Linux 计算机上也一样，它们还是表现为文件系统。让这些硬件在不同操作系统中看起来具有同样的文件系统和同样的文件夹/文件结构的是软件。但在内部，文件组织采用的可能是微软的 FAT 系统，其他文件系统也都模仿该系统。但我们不需要去理会这个，这种抽象是非常完美的。（顺便说一句，FAT 是 File Allocation Table 的简写，即「文件分配表」，不是「肥胖」的意思。所以大家可别误以为微软的实现不好。）

值得一提的是，同样的思想也体现在网络文件系统上。在学校和公司里，把文件保存到服务器上是非常常见的做法。借助相应的软件，我们访问其他计算机上的文件系统时，就如同访问本地的硬盘一样。同样只要使用资源管理器、Finder 或者其他软件就可以。远端的文件系统可能与本地相同（比如两台 Windows 计算机），也可能不同（比如其中一台是 Mac 或 Linux 计算机）。但与闪存设备一样，软件把它们的差异隐藏了起来，我们看到的永远是与自己本地计算机中常规文件系统一样的界面。

网络文件系统经常用于备份，当然也可以作为主文件存储系统。必要时，可以把旧文件复制到便于存档的媒体上，保存到其他地方，以免发生火灾等事故时毁坏重要资料。有些磁盘系统会依赖一种叫 RAID（Redundant Array Of Independent Disks，独立磁盘冗余阵列）的技术，把数据和错误校验码分别写到多个磁盘上，以便某个磁盘损坏时能够从其他磁盘恢复数据。当然，这种系统也会增加彻底销毁数据的难度。

「应用程序」是一种统称，表示所有在操作系统平台上完成某种任务的软件或程序。应用程序可大可小，可以只完成特定的任务，也可以囊括大量功能。可以是花钱买的，也可以是免费送的。它的代码可以高度保密，也可以开放源码，甚至没有任何限制。或许可以把应用程序分成两类。一类是小型独立的应用，通常只帮用户做一件事；另一类是大型软件，包含非常多的操作，比如 Word、iTunes 或 Photoshop。

Unix 系统有一个列出目录中文件和文件夹的命令，它是 Windows 资源管理器和 Mac OS X Finder 的纯文本版。对文件执行复制、移动等操作的程序，在 Finder 和资源管理器中也都有对应的图形用户界面版。同样，这些程序也使用系统调用来提供文件夹包含内容的基本信息，也依赖于库函数去读、写、格式化和显示信息。

Word 之类的应用程序比浏览文件系统的程序要大得多。但很明显，Word 一定包含某种类似的文件系统程序，以便用户能够打开文件、读取文件内容和保存文档。Word 也包含非常完善的算法，随着文本变化持续更新显示界面的算法就是一例。它还提供精心设计的用户界面，用于显示信息和让用户调整字号、字体、颜色、布局等的各种选项。对这种程序而言，用户界面是至关重要的一部分。Word 以及其他具有巨大商业价值的大型程序都经历了不断改进和功能完善。我还真不知道 Word 有多少行代码，但要说它有几百万行 C 和 C++ 代码应该一点都不奇怪。

另一个大型、免费，有时候甚至是开源的应用程序是浏览器。从某种角度说，浏览器的复杂度甚至更高。从外部来看，浏览器会向 Web 服务器发送请求，从那里取得信息后再把它们显示出来。那它复杂在哪里呢？

首先，浏览器必须处理异步事件。所谓异步事件，就是在非预定时间发生、没有特定次序的事件。举个例子，你点击一个链接，浏览器就会发送一个对相应页面的请求。但发送完请求后，它不能就那么一直等着。它还得准备响应你的其他操作，比如滚动当前页面，或者在你点击「后退」按钮或另外一个链接时中断之前的请求，不管请求的页面是否已经到达。在你调整窗口大小时，它必须不断更新窗口中的内容，或许就因为你在等待新页面期间没事儿干，于是就会随手来回缩放起窗口来。如果页面中包含音频和视频，那它还要负责控制它们。编写异步系统一直是非常困难的，而浏览器就涉及很多异步操作。

浏览器必须支持很多种内容，包括静态文本和具有互动性的程序（可以动态改变网页中包含的内容）。对某些内容的支持可以委托给辅助程序（这是处理 PDF 文档和电影的标准做法），但浏览器本身必须提供相应的机制，以便启动这些辅助程序，为它们发送和接收数据以及请求，还要控制它们。

浏览器必须管理多个标签页或窗口，每个标签页和窗口都可能需要执行前述操作。它要为每个标签页和窗口单独保留一份历史记录，还要保存书签、收藏夹等数据库。它要支持访问本地文件系统，以便上传和下载文件。

浏览器自身还是一个平台，要提供不同层次的扩展接口。比如，要支持 Flash 和 Silverlight 插件、JavaScript 和 Java 虚拟机，以及 Firefox、Safari 和 Chrome 所支持的那些扩展程序。

浏览器既然包含那么多实现复杂功能的代码，其自身以及它所支持的插件、扩展程序免不了会存在一些漏洞，面临被攻击的风险。另外，一些无知、愚昧，甚至白痴用户，由于不理解浏览器的原理，不知道可能存在的风险，也会导致浏览器遭受攻击。总之，做浏览器开发确实不容易。

如果现在再回头读一读本节内容，你会不会想到些什么？没错，现在的浏览器非常像操作系统。它要管理资源、控制同时发生的活动，它向多个地方请求和保存资源，并且为其他程序运行提供了一个平台。

多年来的实践表明，把浏览器当成操作系统是可行的。换句话说，浏览器本身就是一个独立的系统，与什么操作系统在控制底层硬件无关。大概十几年前，这种想法已经浮出水面，但当时还存在很多实际的困难。今天，这种可能性已经触手可及。大量服务都可以只通过浏览器界面来访问了（邮件是最明显的例子），而这个趋势还在继续。谷歌已经发布了一个浏览器操作系统，叫 Chrome OS。这个操作系统完全依赖于 Web 服务。为此，谷歌还推出了运行 Chrome OS 的计算机 Chromebook。

与计算领域的很多其他东西一样，软件也是分层组织的。类似于地质学中的分层，软件中的不同层次可以隔离不同的关注点。在程序员的世界里，分层是解决复杂问题的一个核心思想。

通俗地讲，计算机的最底层是硬件。硬件，除了总线支持在系统运行期间添加和删除设备之外，其他方面几乎可以看成不可变的。

再往上就是所谓的操作系统层了。为了突出其核心地位，通常把这一层称为内核（kernel）。操作系统介于硬件和应用程序之间。无论底层是什么硬件，操作系统都要负责隐藏其特殊性，向应用程序提供统一的接口或界面，这个接口或界面不因硬件的种种差别而变化。在接口设计得当的情况下，同一个操作系统的接口完全可以适用于众多制造商生产的不同类型的 CPU。

Unix 操作系统的接口就是这样的。Unix 可以在各种处理器之上运行，但在任何处理器上都能提供相同的核心服务。事实上，操作系统就是一种通用的商品，底层的硬件除了价格和性能之外，其他方面都影响不大。而且，上层的软件也不依赖于它。把为一种处理器编写的程序移植到另一种处理器上，无非就是小心谨慎地用合适的编译器再编译一遍而已。当然，程序与硬件结合得越紧密，这种转换工作就越难做。无论如何，这种转换对很多程序来说都是可行的。举个大规模转换的例子，苹果公司在 2005 年到 2006 年，用了不到一年时间，就把它们的软件从 IBM 的 PowerPC 处理器转换到了 Intel 处理器上。

对 Windows 来说，问题就没有那么简单了。从 1978 年的 Intel 8086 CPU 开始，Windows 的开发就与 Intel 架构紧密结合，包括后来 Intel 发布的每一款 CPU。（处理器系列一般称为「x86」，是因为 Intel 的处理器很多年都以「86」这个编号结尾，包括 80286、80386、80486 等。）Windows 与 Intel 的结合是如此紧密，以至于这样的系统一度被世人称作「Wintel」。

操作系统再往上的一层是函数库。函数库提供通用的服务，这样一来，程序员就不必各自重复实现这些功能。有些库比较靠近底层，能够完成一些基本功能（完成数学计算，比如开方和求对数，或者像前面 date 命令一样计算日期和时间）。另外一些库的功能更强大（涉及加密、图形处理、压缩等）。图形用户界面上的组件，包括菜单、按钮、复选框、滚动条、选项卡面板等等，都需要编写很多代码。为此，只要把这些代码封装成函数库，任何人就都可以使用它们，而且还能保证统一的行为和外观。这就是为什么大多数 Windows 应用（至少它们的基本图形组件）看起来那么相似的原因。同样的情况在 Mac 上更是如此。如果所有软件开发商都重新发明、重新实现这些功能，那不仅会浪费大量资源，而且五花八门的界面也会让用户感到无所适从。

如前所述，典型的应用程序会使用函数库和操作系统服务，把它们集成到一起实现某种功能。不过，库函数与系统调用之间的区别并不十分明显。某个特定的服务可以作为系统调用实现，也可以借助使用了系统调用的库函数来实现。

1『一层层的抽象：硬件 → 操作系统 → 函数库 → 应用程序。』

有时候，内核、函数库和应用程序之间并不像我说的那么泾渭分明。毕竟，编写及连接软件组件的方式多种多样。例如，内核可以提供少量服务，而依赖上层的库来完成大部分工作。或者，它也可以自己承担大部分任务，而较少地依赖于库。操作系统与应用程序之间并没有清晰的界限。

那该如何区分它们呢？一个简单（但可能不够完美）的方法，就是把任何确保 A 应用程序不会干扰 B 应用程序的代码看成是操作系统的职能。比如，内存管理、文件系统、设备管理和 CPU 管理，这些都是操作系统的职能。内存管理需要决定在程序运行的时候把它们放到内存的什么位置，文件系统需要决定把信息保存到磁盘上的什么位置，设备管理需要确保两个应用程序不会同时占用打印机，也不能在没有协商的情况下就向显示器输出内容。最核心的还是 CPU 管理，因为这是操作系统履行前述各项职能的前提条件。

浏览器不属于操作系统，因为你可以运行任意浏览器，甚至同时运行多个浏览器，都不会干扰共享资源或者控制流程。听起来好像是可以做到泾渭分明似的，但要是较起真来，比如打起官司来，那可就难说了。美国司法部从 1994 年开始（到 2011 年结束的）对微软的反垄断诉讼，就涉及微软的 Internet Explorer 浏览器到底是操作系统的一部分，还是一个独立应用程序的问题。如果浏览器是操作系统的一部分（按照微软的主张），那么要求微软删除 IE 就是不合理的，而微软要求用户使用 IE 的做法就是正当的。如果浏览器是一个独立的应用程序，那么微软就涉嫌以非法手段强迫用户在非必要情况下使用 IE。当然，这个官司本身要复杂得多，但如何界定浏览器的归属问题确实非常重要。最终诉讼的结果是，法院认定浏览器是一个独立的应用程序，不属于操作系统。用法官托马斯·杰克逊（Thomas Jackson）的话说：「Web 浏览器和操作系统是相互独立的产品。」

## 0204学习编程

很早以前，我用过微软的 Visual Basic。它既是一门语言，也是一个编程环境。通过它可以方便地写出看起来很专业的 Windows 应用程序。而且，微软的 Office 办公套件以及很多其他办公软件里也都内置了某种 VB 的简化版。因此，会用 VB 就可以在日常工作中增强或更好地控制 Word 和 Excel（当然，弄不好也会为病毒入侵打开便利之门）。可惜 Visual Basic 已经不再是最好的选择了。尽管微软提供的免费版几乎没有删减任何功能，但比起十几年前，这门语言及其体系实在复杂得太多了。更重要的是，VB 只能在 Windows 系统上运行。而我需要找一个随便在什么平台上都能跑的语言。

为此，我选择了 JavaScript，因为它具有如下优点。首先，它无处不在，所有浏览器都支持它。几乎每一个网页多多少少都有 JavaScript 程序，而且其代码也很容易向别人展示。要是你用它写了一个程序，把它放在自己的网页里就能博得朋友和家人的赞美。其次，这门语言本身比较简单，对学习者的要求很低。当然，这并不意味着它能力弱。事实上，JavaScript 非常强大，可以完成极为复杂的计算任务。很多网页特效的背后都是 JavaScript，包括谷歌的在线办公程序 Google Docs 和其他类似的应用。最后，Twitter、Facebook、Amazon 等等这些世界级的大网站都提供了 JavaScript 的 API。

JavaScript 当然也有缺点。不同浏览器中的 JavaScript 实现并不像我们想象的那样完全一致。换句话说，在一个浏览器中能运行的程序，换一个浏览器可能就运行得不正常。但就本章的学习目标而言，这根本不是问题。做专业开发的前端工程师都有办法解决这个问题。另外，这门语言的某些特性不太好理解，有时候会显得比较怪异。JavaScript 程序通常只能在网页中运行，很少能独立存在。不过也有一些软件支持 JavaScript 程序，比如 Adobe 的 PDF 阅读器。由于它委身于浏览器，因此要学它，一般都得同时学一点 HTML（HTML 是一种描述网页结构的标记语言）。尽管存在这些缺点，JavaScript 仍然非常值得学习。

编程语言的某些基本概念是相通的，因为这些概念都是为了表达一系列计算步骤而发明的。任何编程语言都会提供一些手段，用于取得赖以完成计算的输入数据、进行算术计算、在计算期间存储和获取中间值并显示结果、根据之前的计算结果决定下一个计算步骤，以及在计算完成时保存结果。

是语言就有语法，而语法就是一系列规则，根据它们可以判断什么符合语法，什么不符合语法。编程语言对语法规则是锱铢必较的，哪怕有一点点地方违反语法，它都会提出抗议。语言还要有语义，语义规定了语言中所有元素的含义。

2『上面有关编程语言的基本概念做一张信息卡片。 —— 已归档「2020普通卡片06」』

理论上讲，一段程序的语法是否正确，以及语法正确的情况下其含义是什么，这些都不应该有歧义。但实际上，就像用自然语言写文章一样，任何歧义都没有的理想状态有时候很难达到。语言的最基本单位通常是字和词，这些字和词本身可能就有歧义，而对它们的不同理解更是司空见惯。因此，实现这些语言规则的时候可能就会有偏差。另外，语言本身也会与时俱进。综上所述，不同浏览器对 JavaScript 的实现多多少少都会存在差异。实际上，即使是同一款浏览器的不同版本，对 JavaScript 的实现也有差异。

JavaScript 这门语言实际上包含三个方面。第一是语言本身，包括让计算机完成算术计算的语句、测试条件，以及重复计算的规则等。第二是 JavaScript 代码库，也就是由别人写好的程序段，你可以在自己的程序里直接使用，而不必再花时间重写。比如数学函数、计算日历的函数，以及搜索和操作文本的函数。第三是访问浏览器和网页的接口，JavaScript 程序通过这些接口可以在其所在的网页中获得用户输入、响应用户动作（如单击按钮或填写表单）、让浏览器显示不同的内容或者切换到其他网页。

1『 JavaScript 的三个核心要点：语言本身、代码库和浏览器接口。』

这个程序的 alert 函数来自辅助与浏览器交互的 JavaScript 库，调用它会弹出一个对话框，对话框将显示位于引号中的文本。顺便说一句，你在写 JavaScript 程序时，必须使用标准的双引号（"），不要使用所谓的「智能引号」（本书后面会介绍）。这也是讲究语法规则的一个例子。另外，也不要使用 Word 等文字处理程序来生成 HTML 文件，而应该使用记事本或 TextEdit 这样的文本编辑器。在保存程序文件时，要把它保存成扩展名为 .html 的纯文本文件（也就是没有任何格式信息的文件）。

这个程序涉及几个新元素和新概念。首先，单词 var 添加或者说声明了一个变量。变量是 RAM 中的一个位置，可以让程序在运行期间存储数据。之所以称它为变量，是因为它的值会随着程序的执行而变化。在高级语言里，声明变量就相当于我们在玩具汇编语言中为一个内存位置起一个名字。打个比方，声明就好比一出戏里的演员表。在这里，我们把这个变量叫做 username。当然也可以给它起别的名字，但 username 让人一看就知道它在程序中扮演什么角色。

其次，这个程序使用了一个 JavaScript 库函数 prompt。prompt 与 alert 类似，都会弹出一个对话框。但不同的是，prompt 能收集用户的输入。用户在对话框中输入的任何内容都会成为 prompt 函数中可以使用的值。这个值通过下面这行代码被赋给了变量 username：

这里等号 = 的意思是：「完成右边的计算，把计算结果保存在左边的变量里。」这个等号也是语义的一个例子。等号执行的操作叫赋值。大多数编程语言都使用等号来表示赋值，而没有顾及等号在数学中表示相等的含义。换句话说，这里的 = 不表示相等，而表示复制值。

再提醒一下，这个程序会不断读取用户输入，而在用户输入「0」的时候，会弹出对话框显示之前输入的所有数值之和。这个程序里的一些特性前面刚刚介绍过，比如声明、赋值，还有 prompt 函数。其中第一行代码声明了两个后面会用到的变量 num 和 sum。第二行代码是一个赋值语句，把变量 sum 的值设为 0。第三行代码把变量 num 的值设为用户在对话框中输入的值。

这里真正值得讲的是 while 循环。计算机最擅长一遍又一遍地反复执行一系列指令，而程序员需要想清楚的则是如何通过编程语言来表达这种反复。在玩具语言里，我们添加了 GOTO 指令，用于跳转到程序中的另一个位置，而不是顺序执行下一条指令。还添加了 IFZERO 指令，用于测试一个条件并根据累加器的值决定是否跳转。

这些概念在大多数高级语言里都有，而且被抽象为一个叫做 while 循环的语句。这个循环语句在反复执行一系列指令时更有规律性，也更加有条理。这个程序里的 while 测试了（写在括号中的）一个条件，如果条件为真，则执行花括号 {...} 中所有语句。然后返回，再次测试同一个条件。这个循环一直反复，直至条件为假。此时，接着执行紧跟在右花括号后面的语句。

我并没有明确指定这个示例程序中数据的类型。但在内部，计算机会帮我们明确区分 123 这样的数值和 Hello 这样的任意字符串。有些语言要求程序员自己谨慎地表达这种区别，另一些语言则试图猜测程序员的意图。JavaScript 差不多就属于后一类，因此有时候明确知道数据类型以及如何处理相应的值是十分必要的。比如 parseInt 函数吧，它可以把文本内容转换成数值，也就是说它的输入数据可以被当成（123 这样的）整数而非三个十进制数字来看待。如果不用 parseInt，那么 prompt 返回的数据就会被当成文本，而 + 运算符就会把它追加到之前的文本后面。结果将是把用户输入的所有数字逐个拼接起来。这听起来似乎还挺有意思，但却不是我们想要的。

接下来的这个例子要完成的任务有点不一样，它要从输入的所有数值中找出最大的一个。而这也正是添加另一个控制流语句 if-else 的原因。所有高级语言都有这个语句（可能形式上稍有差异），用于条件判断。实际上，if-else 就是 IFZERO 的通用版本。JavaScript 中的 if-else 语句跟 C 中的一样。

if-else 语句有两种表现形式。一种就是这里所展示的这样，没有 else 子句。此时，只要括号中的条件为真，那么就会执行后面括号 {...} 中的语句。但不管怎样，都会执行紧跟在右花括号后面的语句。另一种形式就是还有一个 else 子句，它也带有一组语句，会在条件为假时执行。无论条件真假，整个 if-else 语句块后面的语句都会执行。

可能你也注意到了，这个示例程序是通过缩进来表示结构的：while 和 if 语句都缩进了。这是一种标准（值得提倡）的做法，能让人一眼就看出 while 和 if 语句都控制着哪些语句。甚至还有一些编程语言要求遵循一致的缩进规则。

把这段程序放到一个网页里就可以测试了。但专业的程序员不必把它放到网页里也能模拟其行为。他们会像计算机一样，认真地推敲每一条语句。如果你也能做到这样（这是确保理解程序的好办法），就可以推断出输入任意值程序都能得出正确的结果。

真的吗？如果输入中包含正数那没问题，但要是输入的都是负数呢？你会发现程序始终会说最大的数是零。

想一想这是为什么。这个程序把到目前为止发现的最大值保存在变量 max 中（就像找出房间里个子最高的人一样）。为了跟后续的数值进行比较，这个变量必须有一个初始值，因此程序一开始（在用户提供任何值之前）就把它设为零。要是用户真的输入了一个大于零的值固然好，就像输入身高一样。可要是用户输入的都是负值（在录入信用卡账单时有这种可能），程序不会输出最大的负值，而是会输出那个终止输入的值。

通过这个例子还可以学到编程的另一个重要方面：测试。测试远不止是随机地向程序抛出几个数值那么简单。好的测试人员会绞尽脑汁地想象程序会在什么情况下出错，想象那些「边缘」或「边界」情形，比如根本没有数据或者被零除。好的测试人员会想到输入都是负值的可能性。但问题是，随着程序越写越大，想象出所有测试用例的难度也越来越大，因为用户可能会以任意次序、在任意时间输入任意值。没有完美的解决方案，这时候认真地设计和实现程序就显得很关键。比如从一开始就在程序里添加检测和比较代码，以便在出现问题时，程序自己就可以第一时间捕获。

1『又见测试的重要性。但到目前为止自己对「测试」还是没有一个系统的了解以及去重视的觉悟，这块短板一定要补上。（2020-01-05 记录）』

JavaScript 作为一种扩展机制在高级 Web 应用中扮演着十分重要的角色。Google Maps 就是一个典型的例子，它提供了一个库和一套 API，于是所有地图操作就可以通过 JavaScript 程序（而不仅仅是鼠标点击）来控制了。任何人都可以编写自己的 JavaScript 程序在谷歌提供的地图上显示信息。这套 API 使用起来很方便，比如下面这段代码（当然还得有几行 HTML）。

1『 Chrome 里打开内置编译器的快捷键是「option+command+I」。』

第 11 章还会向大家证明，互联网应用发展的趋势是 JavaScript 应用会越来越多，包括地图这种可以编程控制的接口。在被迫公开源代码的环境下，要保护知识产权很难。如果你在使用 JavaScript 编程，就必须自己想办法。任何人都可以通过在网页上单击右键并选择「查看源代码」看到你的源代码。有些 JavaScript 程序经过了混淆处理，可能是开发者有意为之，也可能是为了加快下载速度而被「瘦身」的结果。经过混淆的程序已经非常难破译了，除非碰上那些顽固的死磕分子。

大家回忆一下第 3 章关于编译器、汇编器和机器指令的内容。JavaScript 程序会以同样的方式被转换成可以执行的形式，但细节方面却有着明显差异。浏览器在遇到网页中的 JavaScript 代码时（比如解析到 <script> 标签时），就会把代码文本移交给 JavaScript 编译器 —— 通常是一个独立的程序或者是浏览器的一个库。编译器处理程序、检测错误，然后将其编译为与「玩具」类似的一个假想机器的汇编语言指令。当然，这套指令系统包含的指令要多得多，而实际上它正是我们上一章讲过的一种虚拟机。这个虚拟机接着会像玩具模拟器一样也运行一个模拟器，执行 JavaScript 程序设定的指令。模拟器与浏览器保持着密切交互，比如用户单击按钮，浏览器马上就会通知模拟器哪个按钮被单击了。在模拟器希望做点什么的时候，比如弹出一个对话框，它就会调用 alert 或 prompt 让浏览器照着去做。