# 2019065利用Python进行数据分析R02

## 记忆时间

## 卡片

### 0101. 主题卡——

这本书的主题核心，就是最大的反常识卡，并且注意时间脉络。

### 0201. 术语卡——

### 0202. 术语卡——

### 0203. 术语卡——

### 0301. 人名卡——

根据这些证据和案例，找出源头和提出术语的人是谁——产生一张人名卡，并且分析他为什么牛，有哪些作品，生平经历是什么。

维基百科链接：有的话。

#### 01. 出生日期

用一句话描述你对这个大牛的印象。

#### 02. 贡献及经历

#### 03. 论文及书籍

#### 04. 演讲汇总

找一个他的 TED 演讲，有的话。

### 0401. 金句卡——

最后根据他写的非常震撼的话语——产生一张金句卡。

## 07. 数据清洗和准备

### 1. 逻辑脉络

用 pandas 进行数据的清洗。处理缺失数据，删除或者填充的方法；数据转换，移除重复数据、利用函数或映射进行数据转换、替换值、重命名轴索引、离散化和分箱、检测和过滤异常值、排列和随机采样、计算指标/虚拟变量；字符串操作，大部分文本运算都直接做成了字符串对象的内置方法，对于更为复杂的模式匹配和文本操作，则可能需要用到正则表达式，pandas 对此进行了加强，它使你能够对整组数据应用字符串表达式和正则表达式，而且能处理烦人的缺失数据。

### 2. 摘录及评论

在数据分析和建模的过程中，相当多的时间要用在数据准备上：加载、清理、转换以及重塑。这些工作会占到分析师时间的 80% 或更多。有时，存储在文件和数据库中的数据的格式不适合某个特定的任务。许多研究者都选择使用通用编程语言（如 Python、Perl、R 或 Java）或 UNIX 文本处理工具（如 sed 或 awk）对数据格式进行专门处理。幸运的是，pandas 和内置的 Python 标准库提供了一组高级的、灵活的、快速的工具，可以让你轻松地将数据规变为想要的格式。

如果你发现了一种本书或 pandas 库中没有的数据操作方式，请尽管在邮件列表或 GitHub 网站上提出。实际上，pandas 的许多设计和实现都是由真实应用的需求所驱动的。在本章中，我会讨论处理缺失数据、重复数据、字符串操作和其它分析数据转换的工具。下一章，我会关注于用多种方法合并、重塑数据集。

在许多数据分析工作中，缺失数据是经常发生的。pandas 的目标之一就是尽量轻松地处理缺失数据。例如，pandas 对象的所有描述性统计默认都不包括缺失数据。缺失数据在 pandas 中呈现的方式有些不完美，但对于大多数用户可以保证功能正常。对于数值数据，pandas 使用浮点值 NaN（Not a Number）表示缺失数据。我们称其为哨兵值，可以方便的检测出来；在 pandas 中，我们采用了 R 语言中的惯用法，即将缺失值表示为 NA，它表示不可用 not available。在统计应用中，NA 数据可能是不存在的数据或者虽然存在，但是没有观察到（例如，数据采集中发生了问题）。当进行数据清洗以进行分析时，最好直接对缺失数据进行分析，以判断数据采集的问题或缺失数据可能导致的偏差。Python 内置的 None 值在对象数组中也可以作为 NA；pandas 项目中还在不断优化内部细节以更好处理缺失数据，像用户 API 功能，例如 pandas.isnull，去除了许多恼人的细节。表 7-1 列出了一些关于缺失数据处理的函数。

1『哨兵值 NaN 可以用 np.nan 来赋值，而检测哨兵值用 Series 对象里的 isnull() 函数。』

过滤掉缺失数据的办法有很多种。你可以通过 pandas.isnull 或布尔索引的手工方法，但 dropna 可能会更实用一些。对于一个 Series，dropna 返回一个仅含非空数据和索引值的 Series，data.dropna() ；这等价于 data[data.notnull()]；而对于 DataFrame 对象，事情就有点复杂了。你可能希望丢弃全 NA 或含有 NA 的行或列。dropna 默认丢弃任何含有缺失值的行。传入 how='all' 将只丢弃全为 NA 的那些行。用这种方式丢弃列，只需传入 axis=1 即可；另一个滤除 DataFrame 行的问题涉及时间序列数据。假设你只想留下一部分观测数据，可以用 thresh 参数实现此目的。

1『以上都是说的如何删除缺失值。』

填充缺失数据。你可能不想滤除缺失数据（有可能会丢弃跟它有关的其他数据），而是希望通过其他方式填补那些「空洞」。对于大多数情况而言，fillna 方法是最主要的函数。通过一个常数调用 fillna 就会将缺失值替换为那个常数值；若是通过一个字典调用 fillna，就可以实现对不同的列填充不同的值；fillna 返回的是一个新的对象，但你也可以修改已经存在的对象；对 reindexing 有效的那些插值方法也可用于 fillna；使用 fillna 你可以完成很多带有一点创造性的工作。例如，你可以将 Series 的平均值或中位数用于填充缺失值；表 7-2 列出了 fillna 的参考。

1『frame.fillna(0)，缺失值全部填充为 0；frame.fillna({1:0.5, 2:0})，将第 2 列的缺失值转换为 0.5，将第 3 列的缺失值转换为 0；_ = frame.fillna(0, inplace=True) 修改现有的对象；data.fillna(data.mean()) ，用 data 的平均值填充缺失值。』

数据转换。本章到目前为止介绍的都是数据的重排。另一类重要操作则是过滤、清理以及其他的转换工作。

移除重复数据。DataFrame 中出现重复行有多种原因。下面就是一个例子；DataFrame 的 duplicated 方法返回一个布尔型 Series，表示各行是否是重复行（前面出现过的行）；还有一个与此相关的 drop_duplicates 方法，它会返回一个 DataFrame，重复的数组会标为 False；这两个方法默认会判断全部列，你也可以指定部分列进行重复项判断。假设我们还有一列值，且只希望根据 k1 列过滤重复项。

1『data.drop_duplicates(['k1'])，对列名为 k1 的列进行过滤重复项操作；duplicated 和 drop_duplicates 默认保留的是第一个出现的值组合。传入 keep='last' 则保留最后一个。』

利用函数或映射进行数据转换。对于许多数据集，你可能希望根据数组、Series 或 DataFrame 列中的值来实现转换工作。我们来看看下面这组有关肉类的数据；假设你想要添加一列表示该肉类食物来源的动物类型。我们先编写一个不同肉类到动物的映射；Series 的 map 方法可以接受一个函数或含有映射关系的字典型对象，但是这里有一个小问题，即有些肉类的首字母大写了，而另一些则没有。因此，我们还需要使用 Series 的 str.lower 方法，将各个值转换为小写；我们也可以传入一个能够完成全部这些工作的函数；使用 map 是一种实现元素级转换以及其他数据清理工作的便捷方式。

1『添加一列表示该肉类食物来源的动物类型，即传递一个映射数据进去，感觉就是在不同的关系型数据表里，根据主键值进行数据的传递。』

```
In [55]: lowercased = data['food'].str.lower() 
In [56]: lowercased 
In [57]: data['animal'] = lowercased.map(meat_to_animal) 
In [58]: data 
```

替换值。利用 fillna 方法填充缺失数据可以看做值替换的一种特殊情况。前面已经看到，map 可用于修改对象的数据子集，而 replace 则提供了一种实现该功能的更简单、更灵活的方式。我们来看看下面这个 Series；-999 这个值可能是一个表示缺失数据的标记值。要将其替换为 pandas 能够理解的 NA 值，我们可以利用 replace 来产生一个新的 Series（除非传入 inplace=True）；如果你希望一次性替换多个值，可以传入一个由待替换值组成的列表以及一个替换值；要让每个值有不同的替换值，可以传递一个替换列表；传入的参数也可以是字典；笔记：data.replace 方法与 data.str.replace 不同，后者做的是字符串的元素级替换。我们会在后面学习 Series 的字符串方法。

1『data.replace(-999, np.nan)；』

```
In [62]: data.replace(-999, np.nan) 
In [63]: data.replace([-999, -1000], np.nan)
In [64]: data.replace([-999, -1000], [np.nan, 0])
In [65]: data.replace({-999: np.nan, -1000: 0})
```

重命名轴索引。跟 Series 中的值一样，轴标签也可以通过函数或映射进行转换，从而得到一个新的不同标签的对象。轴还可以被就地修改，而无需新建一个数据结构。接下来看看下面这个简单的例子；跟 Series 一样，轴索引也有一个 map 方法；你可以将其赋值给 index，这样就可以对 DataFrame 进行就地修改；如果想要创建数据集的转换版（而不是修改原始数据），比较实用的方法是 rename；特别说明一下，rename 可以结合字典型对象实现对部分轴标签的更新；rename 可以实现复制 DataFrame 并对其索引和列标签进行赋值。如果希望就地修改某个数据集，传入 inplace=True 即可。

```
In [67]: transform = lambda x: x[:4].upper() 
In [68]: data.index.map(transform) 
In [69]: data.index = data.index.map(transform) 
In [70]: data 
In [71]: data.rename(index=str.title, columns=str.upper) 
In [72]: data.rename(index={'OHIO': 'INDIANA'}, columns={'three': 'peekaboo'})
In [73]: data.rename(index={'OHIO': 'INDIANA'}, inplace=True) 
```

离散化和分箱。连续值经常需要离散化，或者分离成「箱子」进行分析。假设你有某项研究中一组人群的数据，你想将他们进行分组，放入离散的年龄框中；接下来将这些数据划分为「18 到 25」、「26 到 35」、「35 到 60」以及「60 以上」几个组。要实现该功能，你需要使用 pandas 的 cut 函数；pandas 返回的对象是一个特殊的 Categorical 对象。你看到的输出描述了由 pandas.cut 计算出的箱。你可以将它当作一个表示箱名的字符串数组，它在内部包含一个categories（类别）数组，它指定了不同的类别名称以及 codes 属性中的 ages（年龄）数据标签。

请注意，pd.value_counts(cats) 是对 pandas.cut 的结果中的箱数量的计数。与区间的数学符号一致，小括号表示边是开放的，中括号表示它是封闭的（包括边）。你可以通过传递 right=False 来改变哪一边是封闭的；你也可以通过向 labels 选项传递一个列表或数组来传入自定义的箱名；如果你传给 cut 整数个的箱来代替显式的箱边，pandas 将根据数据中的最小值和最大值计算出等长的箱。请考虑一些均匀分布的数据被切成四份的情况，precision=2 的选项将十进制精度限制在两位。qcut 是一个与分箱密切相关的函数，它基于样本分位数进行分箱。取决于数据的分布，使用 cut 通常不会使每个箱具有相同数据量的数据点。由于 qcut 使用样本的分位数，你可以通过 qcut 获得等长的箱；与 cut 类似，你可以传入自定义的分位数（0 和 1 之间的数据，包括边）；后续章节中，在讨论聚合和分组操作时，我们将会继续讨论 cut 和 qcut，因为这些离散化函数对于分位数和分组分析特别有用。

2『分箱操作应该可以应用于分割脚本里，实现如何把字符串列表分割为特定的一组组。实现代码用来替换掉原来的分割函数。』

```
In [75]: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
In [76]: bins = [18, 25, 35, 60, 100] 
In [77]: cats = pd.cut(ages, bins) 
In [78]: cats 

In [79]: cats.codes 
In [80]: cats.categories 
In [81]: pd.value_counts(cats) 

In [82]: pd.cut(ages, [18, 26, 36, 61, 100], right=False) 
In [83]: group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior'] 
In [84]: pd.cut(ages, bins, labels=group_names) 

In [85]: data = np.random.rand(20) 
In [86]: pd.cut(data, 4, precision=2) 
In [87]: data = np.random.randn(1000) # Normally distributed 
In [88]: cats = pd.qcut(data, 4) # Cut into quartiles 
In [91]: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.]) 
```

检测和过滤异常值。过滤或变换异常值（outlier）在很大程度上就是运用数组运算。来看一个含有正态分布数据的 DataFrame；假设你想要找出某列中绝对值大小超过 3 的值；要选出全部含有「超过 3 或 -3 的值」的行，你可以在布尔型 DataFrame 中使用 any 方法；根据这些条件，就可以对值进行设置。下面的代码可以将值限制在区间－3 到 3 以内；根据数据的值是正还是负，np.sign(data) 可以生成 1 和 -1。

```
In [92]: data = pd.DataFrame(np.random.randn(1000, 4)) 
In [93]: data.describe() 
In [94]: col = data[2] 
In [95]: col[np.abs(col) > 3] 

In [96]: data[(np.abs(data) > 3).any(1)] 
In [97]: data[np.abs(data) > 3] = np.sign(data) * 3 
In [98]: data.describe() 
In [99]: np.sign(data).head() 
```

排列和随机采样。利用 numpy.random.permutation 函数可以轻松实现对 Series 或 DataFrame 的列的排列工作（permuting，随机重排序）。通过需要排列的轴的长度调用 permutation，可产生一个表示新顺序的整数数组；整数数组可以用在基于 iloc 的索引或等价的 take 函数中；要选出一个不含有替代值的随机子集，你可以使用 Series 和 DataFrame 的 sample 方法；要生成一个带有替代值的样本（允许有重复选择），将 replace=True 传入 sample 方法；


```
In [100]: df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4))) 
In [101]: sampler = np.random.permutation(5) 
In [102]: sampler 
Out[102]: array([3, 1, 4, 2, 0])

In [104]: df.take(sampler) 
In [105]: df.sample(n=3) 
In [106]: choices = pd.Series([5, 7, -1, 6, 4]) 
In [107]: draws = choices.sample(n=10, replace=True) 
```

计算指标/虚拟变量。将分类变量转换为「虚拟」或「指标」矩阵是另一种用于统计建模或机器学习的转换操作。如果 DataFrame 中的一列有 k 个不同的值，则可以衍生一个 k 列的值为 1 和 0 的矩阵或 DataFrame。pandas 有一个 get_dummies 函数用于实现该功能，尽管你自行实现也不难。让我们回顾一下之前的一个示例 DataFrame；在某些情况下，你可能想在指标 DataFrame 的列上加入前缀，然后与其他数据合并。在 get_dummies 方法中有一个前缀参数用于实现该功能；如果 DataFrame 中的一行属于多个类别，则情况略为复杂。

让我们看看 MovieLens 的 1M 数据集，在第 14 章中有更为详细的介绍；为每个电影流派添加指标变量需要进行一些数据处理。首先，我们从数据集中提取出所有不同的流派的列表。使用全 0 的 DataFrame 是构建指标 DataFrame 的一种方式；现在，遍历每一部电影，将 dummies 每一行的条目设置为 1。为了实现该功能，我们使用 dummies.columns 来计算每一个流派的列指标；之后，使用 .loc 根据这些指标来设置值；之后，和前面一样，你可以将结果与 movies 进行联合。

笔记：对于更大的数据，上面这种使用多成员构建指标变量并不是特别快速。更好的方法是写一个直接将数据写为 NumPy 数组的底层函数，然后将结果封装进 DataFrame。

将 get_dummies 与 cut 等离散化函数结合使用是统计应用的一个有用方法；我们使用 numpy.random.seed 来设置随机种子以确保示例的确定性。我们将在本书后面的内容中再次讨论 pandas.get_dummies。

字符串操作。Python 能够成为流行的数据处理语言，部分原因是其简单易用的字符串和文本处理功能。大部分文本运算都直接做成了字符串对象的内置方法。对于更为复杂的模式匹配和文本操作，则可能需要用到正则表达式。pandas 对此进行了加强，它使你能够对整组数据应用字符串表达式和正则表达式，而且能处理烦人的缺失数据。

字符串对象方法。对于许多字符串处理和脚本应用，内置的字符串方法已经能够满足要求了。例如，以逗号分隔的字符串可以用 split 拆分成数段；split 常常与 strip 一起使用，以去除空白符（包括换行符）；利用加法，可以将这些子字符串以双冒号分隔符的形式连接起来；但这种方式并不是很实用。一种更快更符合 Python 风格的方式是，向字符串 "::" 的 join 方法传入一个列表或元组；其它方法关注的是子串定位。检测子串的最佳方式是利用 Python 的 in 关键字，还可以使用 index 和 find；注意 find 和 index 的区别：如果找不到字符串，index 将会引发一个异常（而不是返回－1）；与此相关，count 可以返回指定子串的出现次数；replace 用于将指定模式替换为另一个模式。通过传入空字符串，它也常常用于删除模式；表 7-3 列出了 Python 内置的字符串方法。这些运算大部分都能使用正则表达式实现。casefold 将字符转换为小写，并将任何特定区域的变量字符组合转换成一个通用的可比较形式。

```
In [134]: val = 'a,b, guido' 
In [135]: val.split(',') 
Out[135]: ['a', 'b', ' guido']

In [136]: pieces = [x.strip() for x in val.split(',')] 
In [137]: pieces 
Out[137]: ['a', 'b', 'guido']

In [140]: '::'.join(pieces) 
Out[140]: 'a::b::guido'

In [141]: 'guido' in val 
Out[141]: True 
In [142]: val.index(',') 
Out[142]: 1 
In [143]: val.find(':') 
Out[143]: -1
In [144]: val.index(':') 
--------------------------------------------------------------------------- 
ValueError: substring not found

In [145]: val.count(',') 
Out[145]: 2
In [146]: val.replace(',', '::') 
Out[146]: 'a::b:: guido' 
In [147]: val.replace(',', '') 
Out[147]: 'ab guido'
```

正则表达式。正则表达式提供了一种灵活的在文本中搜索或匹配（通常比前者复杂）字符串模式的方式。正则表达式，常称作 regex，是根据正则表达式语言编写的字符串。Python 内置的 re 模块负责对字符串应用正则表达式。re 模块的函数可以分为三个大类：模式匹配、替换以及拆分。当然，它们之间是相辅相成的。一个 regex 描述了需要在文本中定位的一个模式，它可以用于许多目的。我们先来看一个简单的例子：假设我想要拆分一个字符串，分隔符为数量不定的一组空白符（制表符、空格、换行符等）。描述一个或多个空白符的 regex 是 \s+。

1『re 模块的三大类：模式匹配、替换和拆分。』

调用 re.split ('\s+',text) 时，正则表达式会先被编译，然后再在 text 上调用其 split 方法。你可以用 re.compile 自己编译 regex 以得到一个可重用的 regex 对象；如果你想获得的是一个所有匹配正则表达式的模式的列表，你可以使用 findall 方法；如果打算对许多字符串应用同一条正则表达式，强烈建议通过 re.compile 创建 regex 对象。这样将可以节省大量的 CPU 时间；如果想避免正则表达式中不需要的转义（\），则可以使用原始字符串字面量如 r'C:\x'（也可以编写其等价式 'C:\\x'）。

match 和 search 跟 findall 功能类似。findall 返回的是字符串中所有的匹配项，而 search 则只返回第一个匹配项。match 更加严格，它只匹配字符串的首部。来看一个小例子，假设我们有一段文本以及一条能够识别大部分电子邮件地址的正则表达式；对 text 使用 findall 将得到一组电子邮件地址；search 返回的是文本中第一个电子邮件地址（以特殊的匹配项对象形式返回）。对于上面那个 regex，匹配项对象只能告诉我们模式在原字符串中的起始和结束位置；regex.match 则将返回 None，因为它只匹配出现在字符串开头的模式。

相关的，sub 方法可以将匹配到的模式替换为指定字符串，并返回所得到的新字符串；假设你不仅想要找出电子邮件地址，还想将各个地址分成 3 个部分：用户名、域名以及域后缀。要实现此功能，只需将待分段的模式的各部分用圆括号包起来即可；由这种修改过的正则表达式所产生的匹配项对象，可以通过其 groups 方法返回一个由模式各段组成的元组；对于带有分组功能的模式，findall 会返回一个元组列表；sub 还能通过诸如 \1、\2 之类的特殊符号访问各匹配项中的分组。符号 \1 对应第一个匹配的组，\2 对应第二个匹配的组，以此类推；Python 中还有许多的正则表达式，但大部分都超出了本书的范围。表 7-4 是一个简要概括。

```
In [148]: import re 
In [149]: text = "foo bar\t baz \tqux" 
In [150]: re.split('\s+', text) 
Out[150]: ['foo', 'bar', 'baz', 'qux']
In [151]: regex = re.compile('\s+') 
In [152]: regex.split(text) 
Out[152]: ['foo', 'bar', 'baz', 'qux']

In [153]: regex.findall(text) 
Out[153]: [' ', '\t ', ' \t']

text = """Dave dave@google.com Steve steve@gmail.com Rob rob@gmail.com Ryan ryan@yahoo.com """ 
pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}' 
#  re.IGNORECASE 使用正则表达式不区分大小写
regex = re.compile(pattern, flags=re.IGNORECASE)

In [155]: regex.findall(text) 
Out[155]: ['dave@google.com', 'steve@gmail.com', 'rob@gmail.com', 'ryan@yahoo.com']

In [156]: m = regex.search(text) 
In [157]: m 
Out[157]: <_sre.SRE_Match object; span=(5, 20), match='dave@google.com'> 
In [158]: text[m.start():m.end()] 
Out[158]: 'dave@google.com'

In [159]: print(regex.match(text)) 
None

In [161]: pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]{2,4})' 
In [162]: regex = re.compile(pattern, flags=re.IGNORECASE)

In [163]: m = regex.match('wesm@bright.net') 
In [164]: m.groups() 
Out[164]: ('wesm', 'bright', 'net')

In [165]: regex.findall(text) 
Out[165]: [('dave', 'google', 'com'), ('steve', 'gmail', 'com'), ('rob', 'gmail', 'com'), ('ryan', 'yahoo', 'com')]

In [166]: print(regex.sub(r'Username: \1, Domain: \2, Suffix: \3', text)) 
```

pandas 的矢量化字符串函数。清理待分析的散乱数据时，常常需要做一些字符串规整化工作。更为复杂的情况是，含有字符串的列有时还含有缺失数据；通过 data.map，所有字符串和正则表达式方法都能被应用于（传入 lambda 表达式或其他函数）各个值，但是如果存在 NA（null）就会报错。为了解决这个问题，Series 有一些能够跳过 NA 值的面向数组方法，进行字符串操作。通过 Series 的 str 属性即可访问这些方法。例如，我们可以通过 str.contains 检查各个电子邮件地址是否含有 "gmail"；也可以使用正则表达式，还可以加上任意 re 选项（如 IGNORECASE）；有两个办法可以实现矢量化的元素获取操作：要么使用 str.get，要么在 str 属性上使用索引；要访问嵌入列表中的元素，我们可以传递索引到这两个函数中；你可以利用这种方法对字符串进行截取；表 7-5 介绍了更多的 pandas 字符串方法。

```
In [171]: data.str.contains('gmail') 
In [172]: pattern 
Out[172]: '([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})' 
In [173]: data.str.findall(pattern, flags=re.IGNORECASE) 

In [174]: matches = data.str.match(pattern, flags=re.IGNORECASE) 
In [175]: matches 
In [176]: matches.str.get(1) 
In [177]: matches.str[0] 
In [178]: data.str[:5] 
```

## 08. 数据规整：聚合、合并和重塑

### 1. 逻辑脉络

数据的规整，包括聚合、合并和重塑。层次化索引，在一个轴上拥有多个（两个以上）索引级别，方便用低维度形式处理高维度数据；联合与合并数据集，pandas.merge 根据一个或多个键将行进行连接，pandas.concat 使对象在轴向上进行黏合或「堆叠」，combine_first 实例方法允许将重叠的数据拼接在一起，以使用一个对象中的值填充另一个对象中的缺失值；数据的重塑或透视，使用多层索引进行重塑，2 个基础操作，stack 将数据的列「旋转」为行，unstack 将数据的行「旋转」为列。

### 2. 摘录及评论

在许多应用中，数据可能分散在许多文件或数据库中，存储的形式也不利于分析。本章关注可以聚合、合并、重塑数据的方法。首先，我会介绍 pandas 的层次化索引，它广泛用于以上操作。然后，我深入介绍了一些特殊的数据操作。在第 14 章，你可以看到这些工具的多种应用。

层次化索引（hierarchical indexing）是 pandas 的一项重要功能，它使你能在一个轴上拥有多个（两个以上）索引级别。抽象点说，它使你能以低维度形式处理高维度数据。我们先来看一个简单的例子：创建一个 Series，并用一个由列表或数组组成的列表作为索引；看到的结果是经过美化的带有 MultiIndex 索引的 Series 的格式。索引之间的「间隔」表示「直接使用上面的标签」；对于一个层次化索引的对象，可以使用所谓的部分索引，使用它选取数据子集的操作更简单；有时甚至还可以在「内层」中进行选取。

层次化索引在数据重塑和基于分组的操作（如透视表生成）中扮演着重要的角色。例如，可以通过 unstack 方法将这段数据重新安排到一个 DataFrame 中；unstack 的逆运算是 stack；对于一个 DataFrame，每条轴都可以有分层索引；各层都可以有名字（可以是字符串，也可以是别的 Python 对象）。如果指定了名称，它们就会显示在控制台输出中；注意：小心区分索引名 state、color 与行标签；有了部分列索引，因此可以轻松选取列分组；可以单独创建 MultiIndex 然后复用。上面那个 DataFrame 中的（带有分级名称）列可以这样创建。

重排与分级排序。有时，你需要重新排列轴上的层级顺序，或者按照特定层级的值对数据进行排序。swaplevel 接收两个层级序号或层级名称，返回一个进行了层级变更的新对象（但是数据是不变的）；另一方面，sort_index 只能在单一层级上对数据进行排序。在进行层级变换时，使用 sort_index 以使得结果按照层级进行字典排序也很常见；如果索引按照字典顺序从最外层开始排序，那么数据选择性能会更好 —— 调用 sort_index(level=0) 或 sort_index 可以得到这样的结果。

按层级进行汇总统计。DataFrame 和 Series 中很多描述性和汇总性统计有一个 level 选项，通过 level 选项你可以指定你想要在某个特定的轴上进行聚合。考虑上述示例中的 DataFrame，我们可以按照层级在行或列上像下面这样进行聚合；上面的例子使用了 pandas 的 groupby 机制，这个机制的细节将会在本书后续章节讨论。

使用 DataFrame 的列进行索引。通常我们不会使用 DataFrame 中一个或多个列作为行索引；反而你可能想要将行索引移动到 DataFrame 的列中。下面是一个示例 DataFrame；DataFrame 的 set_index 函数会生成一个新的 DataFrame，新的 DataFrame 使用一个或多个列作为索引；默认情况下这些列会从 DataFrame 中移除，你也可以将它们留在 DataFrame 中；另一方面，reset_index 是 set_index 的反操作，分层索引的索引层级会被移动到列中。

联合与合并数据集。包含在 pandas 对象的数据可以通过多种方式联合在一起：pandas.merge 根据一个或多个键将行进行连接。对于 SQL 或其他关系型数据库的用户来说，这种方式比较熟悉，它实现的是数据库的连接操作；pandas.concat 使对象在轴向上进行黏合或「堆叠」；combine_first 实例方法允许将重叠的数据拼接在一起，以使用一个对象中的值填充另一个对象中的缺失值。

数据集的合并（merge）或连接（join）运算是通过一个或多个键将行链接起来的。这些运算是关系型数据库（基于 SQL）的核心。pandas 的 merge 函数是对数据应用这些算法的主要切入点。以一个简单的例子开始；这是一种多对一的合并。df1 中的数据有多个被标记为 a 和 b 的行，而 df2 中 key 列的每个值则仅对应一行。对这些对象调用 merge 即可得到；请注意，我并没有指定在哪一列上进行连接。如果连接的键信息没有指定，merge 会自动将重叠列名作为连接的键。但是，显式地指定连接键才是好的实现。可能你已经注意到了，结果里面 c 和 d 以及与之相关的数据消失了。默认情况下，merge 做的是「内连接」；结果中的键是交集。其他方式还有 "left"、"right" 以及 "outer"。外连接求取的是键的并集，组合了左连接和右连接的效果。表 8-1 对这些选项进行了总结。

```
In [39]: pd.merge(df1, df2) 
In [40]: pd.merge(df1, df2, on='key') 
In [44]: pd.merge(df1, df2, how='outer') 
```

多对多的合并有些不直观。看下面的例子；多对多连接产生的是行的笛卡尔积。由于左边的 DataFrame 有 3 个 "b" 行，右边的有 2 个，所以最终结果中就有 6 个 "b" 行。连接方式只影响出现在结果中的不同的键的值；要根据多个键进行合并，传入一个由列名组成的列表即可；结果中会出现哪些键组合取决于所选的合并方式，你可以这样来理解：多个键形成一系列元组，并将其当做单个连接键（当然，实际上并不是这么回事）。注意：在进行列－列连接时，DataFrame 对象中的索引会被丢弃。

对于合并运算需要考虑的最后一个问题是对重复列名的处理。虽然你可以手工处理列名重叠的问题（查看前面介绍的重命名轴标签），但 merge 有一个更实用的 suffixes 选项，用于指定附加到左右两个 DataFrame 对象的重叠列名上的字符串；indicator 添加特殊的列 _merge，它可以指明每个行的来源，它的值有 left_only、right_only 或 both，根据每行的合并数据的来源。merge 的参数请参见表 8-2。

根据索引合并。在某些情况下，DataFrame 中用于合并的键是它的索引。在这种情况下，你可以传递 left_index=True 或 right_index=True（或者都传）来表示索引需要用来作为合并的键；由于默认的 merge 方法是求取连接键的交集，因此你可以通过外连接的方式得到它们的并集；在多层索引数据的情况下，事情会更复杂，在索引上连接是一个隐式的多键合并；这种情况下，你必须以列表的方式指明合并所需多个列（请注意使用 how='outer’ 处理重复的索引值）；使用两边的索引进行合并也是可以的。

DataFrame 有一个方便的 join 实例方法，用于按照索引合并。该方法也可以用于合并多个索引相同或相似但没有重叠列的 DataFrame 对象。在之前的例子中，我们可以这样写；由于一些历史原因（例如，一些非常早期版本的 pandas）, DataFrame 的 join 方法进行连接键上的左连接，完全保留左边 DataFrame 的行索引。它还支持在调用 DataFrame 的某一列上连接传递的 DataFrame 的索引；最后，对于一些简单索引-索引合并，你可以向 join 方法传入一个 DataFrame 列表，这个方法可以替代下一节中将要介绍的使用更为通用的 concat 函数的方法。

另一种数据组合操作可互换地称为拼接、绑定或堆叠。NumPy 的 concatenate 函数可以在 NumPy 数组上实现该功能；在 Series 和 DataFrame 等 pandas 对象的上下文中，使用标记的轴可以进一步泛化数组连接。尤其是你还有许多需要考虑的事情：如果对象在其他轴上的索引不同，我们是否应该将不同的元素组合在这些轴上，还是只使用共享的值（交集）？连接的数据块是否需要在结果对象中被识别？「连接轴」是否包含需要保存的数据？在许多情况下，DataFrame 中的默认整数标签在连接期间最好丢弃。pandas 的 concat 函数提供了一种一致性的方式来解决以上问题。我将给出一些例子来表明它的工作机制。假设我们有三个索引不存在重叠的 Series。

对这些对象调用 concat 可以将值和索引粘合在一起；默认情况下，concat 是在 axis=0 上工作的，最终产生一个新的 Series。如果传入 axis=1，则结果就会变成一个 DataFrame（axis=1 是列）；这种情况下，另外的轴上没有重叠，从索引的有序并集（外连接）上就可以看出来。传入 join='inner' 即可得到它们的交集；在这个例子中，f 和 g 标签消失了，是因为使用的是 join='inner' 选项。你可以通过 join_axes 指定要在其它轴上使用的索引。

不过有个问题，参与连接的片段在结果中区分不开。假设你想要在连接轴上创建一个层次化索引。使用 keys 参数即可达到这个目的；如果沿着 axis=1 对 Series 进行合并，则 keys 就会成为 DataFrame 的列头；同样的逻辑也适用于 DataFrame 对象。如果传入的不是列表而是一个字典，则字典的键就会被当做 keys 选项的值。

```
In [82]: s1 = pd.Series([0, 1], index=['a', 'b']) 
In [83]: s2 = pd.Series([2, 3, 4], index=['c', 'd', 'e']) 
In [84]: s3 = pd.Series([5, 6], index=['f', 'g'])
In [85]: pd.concat([s1, s2, s3]) 

In [86]: pd.concat([s1, s2, s3], axis=1) 
In [87]: s4 = pd.concat([s1, s3]) 
```

此外还有两个用于管理层次化索引创建方式的参数（参见表 8-3）。举个例子，我们可以用 names 参数命名创建的轴级别；最后一个关于 DataFrame 的问题是，DataFrame 的行索引不包含任何相关数据；在这种情况下，传入 ignore_index=True 即可。

合并重叠数据。还有一种数据组合问题不能用简单的合并（merge）或连接（concatenation）运算来处理。比如说，你可能有索引全部或部分重叠的两个数据集。举个有启发性的例子，我们使用 NumPy 的 where 函数，它表示一种等价于面向数组的 if-else；Series 有一个 combine_first 方法，实现的也是一样的功能，还带有 pandas 的数据对齐；对于 DataFrame，combine_first 自然也会在列上做同样的事情，因此你可以将其看做：用传递对象中的数据为调用对象的缺失数据「打补丁」。

重塑和透视。重排列表格型数据有多种基础操作。这些操作被称为重塑或透视。

使用多层索引进行重塑。多层索引在 DataFrame 中提供了一种一致性方式用于重排列数据。以下是两个基础操作：stack，将数据的列「旋转」为行；unstack，将数据的行「旋转」为列。

对该数据使用 stack 方法即可将列转换为行，得到一个 Series；对于一个层次化索引的 Series，你可以用 unstack 将其重排为一个 DataFrame；默认情况下，unstack 操作的是最内层（stack 也是如此）。传入分层级别的编号或名称即可对其它级别进行 unstack 操作；如果不是所有的级别值都能在各分组中找到的话，则 unstack 操作可能会引入缺失数据；stack 默认会滤除缺失数据，因此该运算是可逆的；在对 DataFrame 进行 unstack 操作时，作为旋转轴的级别将会成为结果中的最低级别；当调用 stack，我们可以指明轴的名字。

将「长格式」旋转为「宽格式」。多个时间序列数据通常是以所谓的「长格式」（long）或「堆叠格式」（stacked）存储在数据库和 CSV 中的。我们先加载一些示例数据，做一些时间序列规整和数据清洗；我们将在第 11 章中更深入地讲解 PeriodIndex。简单地说，PeriodIndex 将 year 和 quarter 等列进行联合并生成了一种时间间隔类型：现在，ldata 看起来如下。

这种数据即所谓的多时间序列的长格式，或称为具有两个或更多个键的其他观测数据（这里，我们的键是 date 和 item）。表中的每一行表示一个时间点上的单个观测值。数据通常以这种方式存储在关系型数据库中，比如 MySQL，因为固定模式（列名称和数据类型）允许 item 列中不同值的数量随着数据被添加到表中而改变。在之前的例子中，date 和 item 通常是主键（使用关系型数据库的说法），提供了关系完整性和更简单的连接。在某些情况下，处理这种格式的数据更为困难，你可能更倾向于获取一个按 date 列时间戳索引的且每个不同的 item 独立一列的 DataFrame。DataFrame 的 pivot 方法就是进行这种转换的。

传递的前两个值是分别用作行和列索引的列，然后是可选的数值列以填充 DataFrame。假设你有两个数值列，你想同时进行重塑；如果遗漏最后一个参数，你会得到一个含有多层列的 DataFrame；请注意，pivot 方法等价于使用 set_index 创建分层索引，然后调用 unstack。

将「宽格式」旋转为「长格式」。旋转 DataFrame 的逆运算是 pandas.melt。它不是将一列转换到多个新的 DataFrame，而是合并多个列成为一个，产生一个比输入长的 DataFrame。看一个例子；key 列可能是分组指标，其它的列是数据值。当使用 pandas.melt，我们必须指明哪些列是分组指标。下面使用 key 作为唯一的分组指标；使用 pivot，可以重塑回原来的样子；因为 pivot 的结果从列创建了一个索引，用作行标签，我们可以使用 reset_index 将数据移回列；你还可以指定列的子集，作为值的列；pandas.melt 也可以不用分组指标。

## 09. 绘图和可视化

### 1. 逻辑脉络

一些基本的数据可视化操作，使用 pandas，matplotlib，和 seaborn。只是入门级的知识，深入的话去看有关可视化的书籍。

图片和子图；颜色、标记和线型；刻度、标签和图例；注解以及在 Subplot 上绘图；将图表保存到文件。

2『已下载书籍』

### 2. 摘录及评论

与其它开源库类似，Python 创建图形的方式非常多（根本罗列不完）。自从 2010 年，许多开发工作都集中在创建交互式图形以便在 Web 上发布。利用工具如 [Boken](https://docs.bokeh.org/en/latest/)和 [Plotly](https://github.com/plotly/plotly.py)，现在可以创建动态交互图形，用于网页浏览器。对于创建用于打印或网页的静态图形，我建议默认使用 matplotlib 和附加的库，比如 pandas 和 seaborn。对于其它数据可视化要求，学习其它的可用工具可能是有用的。我鼓励你探索绘图的生态系统，因为它将持续发展。

信息可视化（也叫绘图）是数据分析中最重要的工作之一。它可能是探索过程的一部分，例如，帮助我们找出异常值、必要的数据转换、得出有关模型的 idea 等。另外，做一个可交互的数据可视化也许是工作的最终目标。Python 有许多库进行静态或动态的数据可视化，但我这里重要关注于 matplotlib和基于它的库。

matplotlib 是一个用于创建出版质量图表的桌面绘图包（主要是 2D 方面）。该项目是由 John Hunter 于 2002 年启动的，其目的是为 Python 构建一个 MATLAB 式的绘图接口。matplotlib 和 IPython 社区进行合作，简化了从 IPython shell（包括现在的 Jupyter notebook）进行交互式绘图。matplotlib 支持各种操作系统上许多不同的 GUI 后端，而且还能将图片导出为各种常见的矢量（vector）和光栅（raster）图：PDF、SVG、JPG、PNG、BMP、GIF 等。除了几张，本书中的大部分图都是用它生成的。随着时间的发展，matplotlib 衍生出了多个数据可视化的工具集，它们使用 matplotlib 作为底层。其中之一是 [seaborn: statistical data visualization — seaborn 0.10.0 documentation](http://seaborn.pydata.org/)，本章后面会学习它。学习本章代码案例的最简单方法是在 Jupyter notebook 进行交互式绘图。在 Jupyter notebook 中执行下面的语句：

    %matplotlib notebook

1『上面在 Jupyter notebook 里的操作还没头绪。』

虽然 seaborn 这样的库和 pandas 的内置绘图函数能够处理许多普通的绘图任务，但如果需要自定义一些高级功能的话就必须学习 matplotlib API。matplotlib 的示例库和文档是学习高级特性的最好资源。

图片与子图。matplotlib 所绘制的图位于图片（Figure）对象中。你可以使用 plt.figure 生成一个新的图片；如果用的是 IPython，这时会弹出一个空窗口，但在 Jupyter 中，必须再输入更多命令才能看到。plt.figure 有一些选项，特别是 figsize，它用于确保当图片保存到磁盘时具有一定的大小和纵横比。不能通过空 Figure 绘图。必须用 add_subplot 创建一个或多个子图（subplot）才行；这条代码的意思是：图像应该是 2×2 的（即最多 4 张图），且当前选中的是 4 个 subplot 中的第一个（编号从 1 开始）。如果再把后面两个 subplot 也创建出来，最终得到的图像如图 9-2 所示；

```
In [16]: fig = plt.figure()
In [17]: ax1 = fig.add_subplot(2, 2, 1)
In [18]: ax2 = fig.add_subplot(2, 2, 2) 
In [19]: ax3 = fig.add_subplot(2, 2, 3)
```

注意：使用 Jupyter notebook 时有个细节需要注意，在每个单元格运行后，图表被重置，因此对于更复杂的图表，你必须将所有的绘图命令放在单个的 notebook 单元格中。我们将上面的代码在同一个单元格中运行；如果这时执行一条绘图命令（如 plt.plot ([1.5, 3.5, -2, 1.6])），matplotlib 就会在最后一个用过的 subplot（如果没有则创建一个）上进行绘制，隐藏创建 figure 和 subplot 的过程。因此，如果我们执行下列命令，你就会得到如图 9-3 所示的结果；'k--' 是一个线型选项，用于告诉 matplotlib 绘制黑色虚线图。上面那些由 fig.add_subplot 所返回的对象是 AxesSubplot 对象，直接调用它们的实例方法就可以在其它空着的格子里面画图了，如图 9-4 所示；你可以在 matplotlib 的文档中找到各种图表类型。

```
In [20]: plt.plot(np.random.randn(50).cumsum(), 'k--')
In [21]: _ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3) 
In [22]: ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
```

使用子图网格创建图片是非常常见的任务，所以 matplotlib 包含了一个便捷方法 plt.subplots，它创建一个新的图片，然后返回包含了已生成子图对象的 NumPy 数组；这是非常有用的，因为数组 axes 可以像二维数组那样方便地进行索引，例如， axes[0, 1]。你也可以通过使用 sharex 和 sharey 来表明子图分别拥有相同的 x 轴或 y 轴。当你在相同的比例下进行数据对比时，sharex 和 sharey 会非常有用；此外， matplotlib 还可以独立地缩放图像的界限。表 9-1 对此方法有更多介绍。

```
In [24]: fig, axes = plt.subplots(2, 3) 
In [25]: axes 
```

调整子图周围的间距。默认情况下，matplotlib 会在子图的外部和子图之间留出一定的间距。这个间距都是相对于图的高度和宽度来指定的，所以如果你通过编程或手动使用 GUI 窗口来调整图的大小，那么图就会自动调整。你可以使用图对象上的 subplots_adjust 方法更改间距，也可以用作顶层函数；wspace 和 hspace 用于控制宽度和高度的百分比，可以用作 subplot 之间的间距。下面是一个简单的例子，其中我将间距收缩到了 0（如图 9-5 所示）；你可能会注意到轴标签是存在重叠的。matplotlib并不检查标签是否重叠，因此在类似情况下你需要通过显式指定刻度位置和刻度标签的方法来修复轴标签。

```
subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)

fig, axes = plt.subplots(2, 2, sharex=True, sharey=True) 
for i in range(2): 
    for j in range(2): 
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5) 
plt.subplots_adjust(wspace=0, hspace=0)
```

颜色、标记和线类型。matplotlib 的主函数 plot 接收带有 x 和 y 轴的数组以及一些可选的字符串缩写参数来指明颜色和线类型。例如，要用绿色破折号绘制 x 对 y 的线，你需要执行；这种在一个字符串中指定颜色和线型的方式非常方便。在实际中，如果你是用代码绘图，你可能不想通过处理字符串来获得想要的格式。通过下面这种更为明确的方式也能得到同样的效果；常用的颜色可以使用颜色缩写，你也可以指定颜色码（例如，'#CECECE'）。你可以通过查看 plot 的文档字符串查看所有线型的合集（在 IPython 和 Jupyter 中使用 plot?）。

```
ax.plot(x, y, 'g--')
ax.plot(x, y, linestyle='--', color='g')

In [30]: from numpy.random import randn 
In [31]: plt.plot(randn(30).cumsum(), 'ko--')
```

线图可以使用标记强调数据点。因为 matplotlib 可以创建连续线图，在点之间进行插值，因此有时可能不太容易看出真实数据点的位置。标记也可以放到格式字符串中，但标记类型和线型必须放在颜色后面（见图 9-6）；还可以将其写成更为明确的形式。

```
In [30]: from numpy.random import randn 
In [31]: plt.plot(randn(30).cumsum(), 'ko--')

plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')
```

对于折线图，你会注意到后续的点默认是线性内插的。可以通过 drawstyle 选项修改（见图 9-7）；你可能注意到运行上面代码时有输出 <matplotlib.lines.Line2D at ...>。matplotlib 会返回引用了新添加的子组件的对象。大多数时候，你可以放心地忽略这些输出。这里，因为我们传递了 label 参数到 plot，我们可以创建一个 plot 图例，指明每条使用 plt.legend 的线。笔记：你必须调用 plt.legend（或使用 ax.legend，如果引用了轴的话）来创建图例，无论你绘图时是否传递 label 标签选项。

```
In [33]: data = np.random.randn(30).cumsum() 
In [34]: plt.plot(data, 'k--', label='Default') 
Out[34]: [<matplotlib.lines.Line2D at 0x7fb624d86160>] 
In [35]: plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post') 
Out[35]: [<matplotlib.lines.Line2D at 0x7fb624d869e8>] 
In [36]: plt.legend(loc='best')
```

刻度、标签和图例。对于大多数的图表装饰项，其主要实现方式有二：使用过程型的 pyplot 接口（例如，matplotlib.pyplot）以及更为面向对象的原生 matplotlib API。pyplot 接口的设计目的就是交互式使用，含有诸如 xlim、xticks 和 xticklabels 之类的方法。它们分别控制图表的范围、刻度位置、刻度标签等。其使用方式有以下两种：调用时不带参数，则返回当前的参数值（例如，plt.xlim () 返回当前的 X 轴绘图范围）；调用时带参数，则设置参数值（例如，plt.xlim ([0,10]) 会将 X 轴的范围设置为 0 到 10）。所有这些方法都是对当前或最近创建的 AxesSubplot 起作用的。它们各自对应 subplot 对象上的两个方法，以 xlim 为例，就是 ax.get_xlim 和 ax.set_xlim。我更喜欢使用 subplot 的实例方法（因为我喜欢明确的事情，而且在处理多个 subplot 时这样也更清楚一些）。当然你完全可以选择自己觉得方便的那个。

设置标题、轴标签、刻度以及刻度标签。为了说明自定义轴，我将创建一个简单的图像并绘制一段随机漫步（如图 9-8 所示）；


```
In [37]: fig = plt.figure() 
In [38]: ax = fig.add_subplot(1, 1, 1) 
In [39]: ax.plot(np.random.randn(1000).cumsum())
```






























