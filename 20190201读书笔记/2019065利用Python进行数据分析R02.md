# 2019065利用Python进行数据分析R02

## 记忆时间

## 卡片

### 0101. 主题卡——

这本书的主题核心，就是最大的反常识卡，并且注意时间脉络。

### 0201. 术语卡——

### 0202. 术语卡——

### 0203. 术语卡——

### 0301. 人名卡——

根据这些证据和案例，找出源头和提出术语的人是谁——产生一张人名卡，并且分析他为什么牛，有哪些作品，生平经历是什么。

维基百科链接：有的话。

#### 01. 出生日期

用一句话描述你对这个大牛的印象。

#### 02. 贡献及经历

#### 03. 论文及书籍

#### 04. 演讲汇总

找一个他的 TED 演讲，有的话。

### 0401. 金句卡——

最后根据他写的非常震撼的话语——产生一张金句卡。

## 07. 数据清洗和准备

### 1. 逻辑脉络

用 pandas 进行数据的清洗。处理缺失数据，删除或者填充的方法；数据转换，移除重复数据、利用函数或映射进行数据转换、替换值、重命名轴索引、离散化和分箱、检测和过滤异常值、排列和随机采样、计算指标/虚拟变量；字符串操作，大部分文本运算都直接做成了字符串对象的内置方法，对于更为复杂的模式匹配和文本操作，则可能需要用到正则表达式，pandas 对此进行了加强，它使你能够对整组数据应用字符串表达式和正则表达式，而且能处理烦人的缺失数据。

### 2. 摘录及评论

在数据分析和建模的过程中，相当多的时间要用在数据准备上：加载、清理、转换以及重塑。这些工作会占到分析师时间的 80% 或更多。有时，存储在文件和数据库中的数据的格式不适合某个特定的任务。许多研究者都选择使用通用编程语言（如 Python、Perl、R 或 Java）或 UNIX 文本处理工具（如 sed 或 awk）对数据格式进行专门处理。幸运的是，pandas 和内置的 Python 标准库提供了一组高级的、灵活的、快速的工具，可以让你轻松地将数据规变为想要的格式。

如果你发现了一种本书或 pandas 库中没有的数据操作方式，请尽管在邮件列表或 GitHub 网站上提出。实际上，pandas 的许多设计和实现都是由真实应用的需求所驱动的。在本章中，我会讨论处理缺失数据、重复数据、字符串操作和其它分析数据转换的工具。下一章，我会关注于用多种方法合并、重塑数据集。

在许多数据分析工作中，缺失数据是经常发生的。pandas 的目标之一就是尽量轻松地处理缺失数据。例如，pandas 对象的所有描述性统计默认都不包括缺失数据。缺失数据在 pandas 中呈现的方式有些不完美，但对于大多数用户可以保证功能正常。对于数值数据，pandas 使用浮点值 NaN（Not a Number）表示缺失数据。我们称其为哨兵值，可以方便的检测出来；在 pandas 中，我们采用了 R 语言中的惯用法，即将缺失值表示为 NA，它表示不可用 not available。在统计应用中，NA 数据可能是不存在的数据或者虽然存在，但是没有观察到（例如，数据采集中发生了问题）。当进行数据清洗以进行分析时，最好直接对缺失数据进行分析，以判断数据采集的问题或缺失数据可能导致的偏差。Python 内置的 None 值在对象数组中也可以作为 NA；pandas 项目中还在不断优化内部细节以更好处理缺失数据，像用户 API 功能，例如 pandas.isnull，去除了许多恼人的细节。表 7-1 列出了一些关于缺失数据处理的函数。

1『哨兵值 NaN 可以用 np.nan 来赋值，而检测哨兵值用 Series 对象里的 isnull() 函数。』

过滤掉缺失数据的办法有很多种。你可以通过 pandas.isnull 或布尔索引的手工方法，但 dropna 可能会更实用一些。对于一个 Series，dropna 返回一个仅含非空数据和索引值的 Series，data.dropna() ；这等价于 data[data.notnull()]；而对于 DataFrame 对象，事情就有点复杂了。你可能希望丢弃全 NA 或含有 NA 的行或列。dropna 默认丢弃任何含有缺失值的行。传入 how='all' 将只丢弃全为 NA 的那些行。用这种方式丢弃列，只需传入 axis=1 即可；另一个滤除 DataFrame 行的问题涉及时间序列数据。假设你只想留下一部分观测数据，可以用 thresh 参数实现此目的。

1『以上都是说的如何删除缺失值。』

填充缺失数据。你可能不想滤除缺失数据（有可能会丢弃跟它有关的其他数据），而是希望通过其他方式填补那些「空洞」。对于大多数情况而言，fillna 方法是最主要的函数。通过一个常数调用 fillna 就会将缺失值替换为那个常数值；若是通过一个字典调用 fillna，就可以实现对不同的列填充不同的值；fillna 返回的是一个新的对象，但你也可以修改已经存在的对象；对 reindexing 有效的那些插值方法也可用于 fillna；使用 fillna 你可以完成很多带有一点创造性的工作。例如，你可以将 Series 的平均值或中位数用于填充缺失值；表 7-2 列出了 fillna 的参考。

1『frame.fillna(0)，缺失值全部填充为 0；frame.fillna({1:0.5, 2:0})，将第 2 列的缺失值转换为 0.5，将第 3 列的缺失值转换为 0；_ = frame.fillna(0, inplace=True) 修改现有的对象；data.fillna(data.mean()) ，用 data 的平均值填充缺失值。』

数据转换。本章到目前为止介绍的都是数据的重排。另一类重要操作则是过滤、清理以及其他的转换工作。

移除重复数据。DataFrame 中出现重复行有多种原因。下面就是一个例子；DataFrame 的 duplicated 方法返回一个布尔型 Series，表示各行是否是重复行（前面出现过的行）；还有一个与此相关的 drop_duplicates 方法，它会返回一个 DataFrame，重复的数组会标为 False；这两个方法默认会判断全部列，你也可以指定部分列进行重复项判断。假设我们还有一列值，且只希望根据 k1 列过滤重复项。

1『data.drop_duplicates(['k1'])，对列名为 k1 的列进行过滤重复项操作；duplicated 和 drop_duplicates 默认保留的是第一个出现的值组合。传入 keep='last' 则保留最后一个。』

利用函数或映射进行数据转换。对于许多数据集，你可能希望根据数组、Series 或 DataFrame 列中的值来实现转换工作。我们来看看下面这组有关肉类的数据；假设你想要添加一列表示该肉类食物来源的动物类型。我们先编写一个不同肉类到动物的映射；Series 的 map 方法可以接受一个函数或含有映射关系的字典型对象，但是这里有一个小问题，即有些肉类的首字母大写了，而另一些则没有。因此，我们还需要使用 Series 的 str.lower 方法，将各个值转换为小写；我们也可以传入一个能够完成全部这些工作的函数；使用 map 是一种实现元素级转换以及其他数据清理工作的便捷方式。

1『添加一列表示该肉类食物来源的动物类型，即传递一个映射数据进去，感觉就是在不同的关系型数据表里，根据主键值进行数据的传递。』

```
In [55]: lowercased = data['food'].str.lower() 
In [56]: lowercased 
In [57]: data['animal'] = lowercased.map(meat_to_animal) 
In [58]: data 
```

替换值。利用 fillna 方法填充缺失数据可以看做值替换的一种特殊情况。前面已经看到，map 可用于修改对象的数据子集，而 replace 则提供了一种实现该功能的更简单、更灵活的方式。我们来看看下面这个 Series；-999 这个值可能是一个表示缺失数据的标记值。要将其替换为 pandas 能够理解的 NA 值，我们可以利用 replace 来产生一个新的 Series（除非传入 inplace=True）；如果你希望一次性替换多个值，可以传入一个由待替换值组成的列表以及一个替换值；要让每个值有不同的替换值，可以传递一个替换列表；传入的参数也可以是字典；笔记：data.replace 方法与 data.str.replace 不同，后者做的是字符串的元素级替换。我们会在后面学习 Series 的字符串方法。

1『data.replace(-999, np.nan)；』

```
In [62]: data.replace(-999, np.nan) 
In [63]: data.replace([-999, -1000], np.nan)
In [64]: data.replace([-999, -1000], [np.nan, 0])
In [65]: data.replace({-999: np.nan, -1000: 0})
```

重命名轴索引。跟 Series 中的值一样，轴标签也可以通过函数或映射进行转换，从而得到一个新的不同标签的对象。轴还可以被就地修改，而无需新建一个数据结构。接下来看看下面这个简单的例子；跟 Series 一样，轴索引也有一个 map 方法；你可以将其赋值给 index，这样就可以对 DataFrame 进行就地修改；如果想要创建数据集的转换版（而不是修改原始数据），比较实用的方法是 rename；特别说明一下，rename 可以结合字典型对象实现对部分轴标签的更新；rename 可以实现复制 DataFrame 并对其索引和列标签进行赋值。如果希望就地修改某个数据集，传入 inplace=True 即可。

```
In [67]: transform = lambda x: x[:4].upper() 
In [68]: data.index.map(transform) 
In [69]: data.index = data.index.map(transform) 
In [70]: data 
In [71]: data.rename(index=str.title, columns=str.upper) 
In [72]: data.rename(index={'OHIO': 'INDIANA'}, columns={'three': 'peekaboo'})
In [73]: data.rename(index={'OHIO': 'INDIANA'}, inplace=True) 
```

离散化和分箱。连续值经常需要离散化，或者分离成「箱子」进行分析。假设你有某项研究中一组人群的数据，你想将他们进行分组，放入离散的年龄框中；接下来将这些数据划分为「18 到 25」、「26 到 35」、「35 到 60」以及「60 以上」几个组。要实现该功能，你需要使用 pandas 的 cut 函数；pandas 返回的对象是一个特殊的 Categorical 对象。你看到的输出描述了由 pandas.cut 计算出的箱。你可以将它当作一个表示箱名的字符串数组，它在内部包含一个categories（类别）数组，它指定了不同的类别名称以及 codes 属性中的 ages（年龄）数据标签。

请注意，pd.value_counts(cats) 是对 pandas.cut 的结果中的箱数量的计数。与区间的数学符号一致，小括号表示边是开放的，中括号表示它是封闭的（包括边）。你可以通过传递 right=False 来改变哪一边是封闭的；你也可以通过向 labels 选项传递一个列表或数组来传入自定义的箱名；如果你传给 cut 整数个的箱来代替显式的箱边，pandas 将根据数据中的最小值和最大值计算出等长的箱。请考虑一些均匀分布的数据被切成四份的情况，precision=2 的选项将十进制精度限制在两位。qcut 是一个与分箱密切相关的函数，它基于样本分位数进行分箱。取决于数据的分布，使用 cut 通常不会使每个箱具有相同数据量的数据点。由于 qcut 使用样本的分位数，你可以通过 qcut 获得等长的箱；与 cut 类似，你可以传入自定义的分位数（0 和 1 之间的数据，包括边）；后续章节中，在讨论聚合和分组操作时，我们将会继续讨论 cut 和 qcut，因为这些离散化函数对于分位数和分组分析特别有用。

2『分箱操作应该可以应用于分割脚本里，实现如何把字符串列表分割为特定的一组组。实现代码用来替换掉原来的分割函数。』

```
In [75]: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
In [76]: bins = [18, 25, 35, 60, 100] 
In [77]: cats = pd.cut(ages, bins) 
In [78]: cats 

In [79]: cats.codes 
In [80]: cats.categories 
In [81]: pd.value_counts(cats) 

In [82]: pd.cut(ages, [18, 26, 36, 61, 100], right=False) 
In [83]: group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior'] 
In [84]: pd.cut(ages, bins, labels=group_names) 

In [85]: data = np.random.rand(20) 
In [86]: pd.cut(data, 4, precision=2) 
In [87]: data = np.random.randn(1000) # Normally distributed 
In [88]: cats = pd.qcut(data, 4) # Cut into quartiles 
In [91]: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.]) 
```

检测和过滤异常值。过滤或变换异常值（outlier）在很大程度上就是运用数组运算。来看一个含有正态分布数据的 DataFrame；假设你想要找出某列中绝对值大小超过 3 的值；要选出全部含有「超过 3 或 -3 的值」的行，你可以在布尔型 DataFrame 中使用 any 方法；根据这些条件，就可以对值进行设置。下面的代码可以将值限制在区间－3 到 3 以内；根据数据的值是正还是负，np.sign(data) 可以生成 1 和 -1。

```
In [92]: data = pd.DataFrame(np.random.randn(1000, 4)) 
In [93]: data.describe() 
In [94]: col = data[2] 
In [95]: col[np.abs(col) > 3] 

In [96]: data[(np.abs(data) > 3).any(1)] 
In [97]: data[np.abs(data) > 3] = np.sign(data) * 3 
In [98]: data.describe() 
In [99]: np.sign(data).head() 
```

排列和随机采样。利用 numpy.random.permutation 函数可以轻松实现对 Series 或 DataFrame 的列的排列工作（permuting，随机重排序）。通过需要排列的轴的长度调用 permutation，可产生一个表示新顺序的整数数组；整数数组可以用在基于 iloc 的索引或等价的 take 函数中；要选出一个不含有替代值的随机子集，你可以使用 Series 和 DataFrame 的 sample 方法；要生成一个带有替代值的样本（允许有重复选择），将 replace=True 传入 sample 方法；


```
In [100]: df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4))) 
In [101]: sampler = np.random.permutation(5) 
In [102]: sampler 
Out[102]: array([3, 1, 4, 2, 0])

In [104]: df.take(sampler) 
In [105]: df.sample(n=3) 
In [106]: choices = pd.Series([5, 7, -1, 6, 4]) 
In [107]: draws = choices.sample(n=10, replace=True) 
```

计算指标/虚拟变量。将分类变量转换为「虚拟」或「指标」矩阵是另一种用于统计建模或机器学习的转换操作。如果 DataFrame 中的一列有 k 个不同的值，则可以衍生一个 k 列的值为 1 和 0 的矩阵或 DataFrame。pandas 有一个 get_dummies 函数用于实现该功能，尽管你自行实现也不难。让我们回顾一下之前的一个示例 DataFrame；在某些情况下，你可能想在指标 DataFrame 的列上加入前缀，然后与其他数据合并。在 get_dummies 方法中有一个前缀参数用于实现该功能；如果 DataFrame 中的一行属于多个类别，则情况略为复杂。

让我们看看 MovieLens 的 1M 数据集，在第 14 章中有更为详细的介绍；为每个电影流派添加指标变量需要进行一些数据处理。首先，我们从数据集中提取出所有不同的流派的列表。使用全 0 的 DataFrame 是构建指标 DataFrame 的一种方式；现在，遍历每一部电影，将 dummies 每一行的条目设置为 1。为了实现该功能，我们使用 dummies.columns 来计算每一个流派的列指标；之后，使用 .loc 根据这些指标来设置值；之后，和前面一样，你可以将结果与 movies 进行联合。

笔记：对于更大的数据，上面这种使用多成员构建指标变量并不是特别快速。更好的方法是写一个直接将数据写为 NumPy 数组的底层函数，然后将结果封装进 DataFrame。

将 get_dummies 与 cut 等离散化函数结合使用是统计应用的一个有用方法；我们使用 numpy.random.seed 来设置随机种子以确保示例的确定性。我们将在本书后面的内容中再次讨论 pandas.get_dummies。

字符串操作。Python 能够成为流行的数据处理语言，部分原因是其简单易用的字符串和文本处理功能。大部分文本运算都直接做成了字符串对象的内置方法。对于更为复杂的模式匹配和文本操作，则可能需要用到正则表达式。pandas 对此进行了加强，它使你能够对整组数据应用字符串表达式和正则表达式，而且能处理烦人的缺失数据。

字符串对象方法。对于许多字符串处理和脚本应用，内置的字符串方法已经能够满足要求了。例如，以逗号分隔的字符串可以用 split 拆分成数段；split 常常与 strip 一起使用，以去除空白符（包括换行符）；利用加法，可以将这些子字符串以双冒号分隔符的形式连接起来；但这种方式并不是很实用。一种更快更符合 Python 风格的方式是，向字符串 "::" 的 join 方法传入一个列表或元组；其它方法关注的是子串定位。检测子串的最佳方式是利用 Python 的 in 关键字，还可以使用 index 和 find；注意 find 和 index 的区别：如果找不到字符串，index 将会引发一个异常（而不是返回－1）；与此相关，count 可以返回指定子串的出现次数；replace 用于将指定模式替换为另一个模式。通过传入空字符串，它也常常用于删除模式；表 7-3 列出了 Python 内置的字符串方法。这些运算大部分都能使用正则表达式实现。casefold 将字符转换为小写，并将任何特定区域的变量字符组合转换成一个通用的可比较形式。

```
In [134]: val = 'a,b, guido' 
In [135]: val.split(',') 
Out[135]: ['a', 'b', ' guido']

In [136]: pieces = [x.strip() for x in val.split(',')] 
In [137]: pieces 
Out[137]: ['a', 'b', 'guido']

In [140]: '::'.join(pieces) 
Out[140]: 'a::b::guido'

In [141]: 'guido' in val 
Out[141]: True 
In [142]: val.index(',') 
Out[142]: 1 
In [143]: val.find(':') 
Out[143]: -1
In [144]: val.index(':') 
--------------------------------------------------------------------------- 
ValueError: substring not found

In [145]: val.count(',') 
Out[145]: 2
In [146]: val.replace(',', '::') 
Out[146]: 'a::b:: guido' 
In [147]: val.replace(',', '') 
Out[147]: 'ab guido'
```

正则表达式。正则表达式提供了一种灵活的在文本中搜索或匹配（通常比前者复杂）字符串模式的方式。正则表达式，常称作 regex，是根据正则表达式语言编写的字符串。Python 内置的 re 模块负责对字符串应用正则表达式。re 模块的函数可以分为三个大类：模式匹配、替换以及拆分。当然，它们之间是相辅相成的。一个 regex 描述了需要在文本中定位的一个模式，它可以用于许多目的。我们先来看一个简单的例子：假设我想要拆分一个字符串，分隔符为数量不定的一组空白符（制表符、空格、换行符等）。描述一个或多个空白符的 regex 是 \s+。

1『re 模块的三大类：模式匹配、替换和拆分。』

调用 re.split ('\s+',text) 时，正则表达式会先被编译，然后再在 text 上调用其 split 方法。你可以用 re.compile 自己编译 regex 以得到一个可重用的 regex 对象；如果你想获得的是一个所有匹配正则表达式的模式的列表，你可以使用 findall 方法；如果打算对许多字符串应用同一条正则表达式，强烈建议通过 re.compile 创建 regex 对象。这样将可以节省大量的 CPU 时间；如果想避免正则表达式中不需要的转义（\），则可以使用原始字符串字面量如 r'C:\x'（也可以编写其等价式 'C:\\x'）。

match 和 search 跟 findall 功能类似。findall 返回的是字符串中所有的匹配项，而 search 则只返回第一个匹配项。match 更加严格，它只匹配字符串的首部。来看一个小例子，假设我们有一段文本以及一条能够识别大部分电子邮件地址的正则表达式；对 text 使用 findall 将得到一组电子邮件地址；search 返回的是文本中第一个电子邮件地址（以特殊的匹配项对象形式返回）。对于上面那个 regex，匹配项对象只能告诉我们模式在原字符串中的起始和结束位置；regex.match 则将返回 None，因为它只匹配出现在字符串开头的模式。

相关的，sub 方法可以将匹配到的模式替换为指定字符串，并返回所得到的新字符串；假设你不仅想要找出电子邮件地址，还想将各个地址分成 3 个部分：用户名、域名以及域后缀。要实现此功能，只需将待分段的模式的各部分用圆括号包起来即可；由这种修改过的正则表达式所产生的匹配项对象，可以通过其 groups 方法返回一个由模式各段组成的元组；对于带有分组功能的模式，findall 会返回一个元组列表；sub 还能通过诸如 \1、\2 之类的特殊符号访问各匹配项中的分组。符号 \1 对应第一个匹配的组，\2 对应第二个匹配的组，以此类推；Python 中还有许多的正则表达式，但大部分都超出了本书的范围。表 7-4 是一个简要概括。

```
In [148]: import re 
In [149]: text = "foo bar\t baz \tqux" 
In [150]: re.split('\s+', text) 
Out[150]: ['foo', 'bar', 'baz', 'qux']
In [151]: regex = re.compile('\s+') 
In [152]: regex.split(text) 
Out[152]: ['foo', 'bar', 'baz', 'qux']

In [153]: regex.findall(text) 
Out[153]: [' ', '\t ', ' \t']

text = """Dave dave@google.com Steve steve@gmail.com Rob rob@gmail.com Ryan ryan@yahoo.com """ 
pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}' 
#  re.IGNORECASE 使用正则表达式不区分大小写
regex = re.compile(pattern, flags=re.IGNORECASE)

In [155]: regex.findall(text) 
Out[155]: ['dave@google.com', 'steve@gmail.com', 'rob@gmail.com', 'ryan@yahoo.com']

In [156]: m = regex.search(text) 
In [157]: m 
Out[157]: <_sre.SRE_Match object; span=(5, 20), match='dave@google.com'> 
In [158]: text[m.start():m.end()] 
Out[158]: 'dave@google.com'

In [159]: print(regex.match(text)) 
None

In [161]: pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]{2,4})' 
In [162]: regex = re.compile(pattern, flags=re.IGNORECASE)

In [163]: m = regex.match('wesm@bright.net') 
In [164]: m.groups() 
Out[164]: ('wesm', 'bright', 'net')

In [165]: regex.findall(text) 
Out[165]: [('dave', 'google', 'com'), ('steve', 'gmail', 'com'), ('rob', 'gmail', 'com'), ('ryan', 'yahoo', 'com')]

In [166]: print(regex.sub(r'Username: \1, Domain: \2, Suffix: \3', text)) 
```

pandas 的矢量化字符串函数。清理待分析的散乱数据时，常常需要做一些字符串规整化工作。更为复杂的情况是，含有字符串的列有时还含有缺失数据；通过 data.map，所有字符串和正则表达式方法都能被应用于（传入 lambda 表达式或其他函数）各个值，但是如果存在 NA（null）就会报错。为了解决这个问题，Series 有一些能够跳过 NA 值的面向数组方法，进行字符串操作。通过 Series 的 str 属性即可访问这些方法。例如，我们可以通过 str.contains 检查各个电子邮件地址是否含有 "gmail"；也可以使用正则表达式，还可以加上任意 re 选项（如 IGNORECASE）；有两个办法可以实现矢量化的元素获取操作：要么使用 str.get，要么在 str 属性上使用索引；要访问嵌入列表中的元素，我们可以传递索引到这两个函数中；你可以利用这种方法对字符串进行截取；表 7-5 介绍了更多的 pandas 字符串方法。

```
In [171]: data.str.contains('gmail') 
In [172]: pattern 
Out[172]: '([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})' 
In [173]: data.str.findall(pattern, flags=re.IGNORECASE) 

In [174]: matches = data.str.match(pattern, flags=re.IGNORECASE) 
In [175]: matches 
In [176]: matches.str.get(1) 
In [177]: matches.str[0] 
In [178]: data.str[:5] 
```

## 08. 数据规整：聚合、合并和重塑

### 1. 逻辑脉络

数据的规整，包括聚合、合并和重塑。

### 2. 摘录及评论

在许多应用中，数据可能分散在许多文件或数据库中，存储的形式也不利于分析。本章关注可以聚合、合并、重塑数据的方法。首先，我会介绍 pandas 的层次化索引，它广泛用于以上操作。然后，我深入介绍了一些特殊的数据操作。在第 14 章，你可以看到这些工具的多种应用。






















