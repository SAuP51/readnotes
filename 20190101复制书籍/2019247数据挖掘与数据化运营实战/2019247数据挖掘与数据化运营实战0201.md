# 02 数据挖掘概述

数据挖掘是指从数据集合中自动抽取隐藏在数据中的那些有用信息的非平凡过程，这些信息的表现形式为规则、概念、规律及模式等。

2.1　数据挖掘的发展历史

2.2　统计分析与数据挖掘的主要区别

2.3　数据挖掘的主要成熟技术以及在数据化运营中的主要应用

2.4　互联网行业数据挖掘应用的特点

在第 1 章中介绍了什么是数据化运营，为什么要实现数据化运营，以及数据化运营的主要内容和必要条件。我们知道数据分析和数据挖掘技术是支撑企业数据化运营的基础和技术保障，没有有效的数据挖掘支持，企业的数据化运营就是无源之水，无本之木。

本章将为读者简单回顾一下数据挖掘作为一门学科的发展历史，并具体探讨统计分析与数据挖掘的主要区别，同时，将力求用简单、通俗、明了的文字把目前主流的、成熟的、在数据化运营中常用的统计分析和数据挖掘的算法、原理以及主要的应用场景做出总结和分类。

最后，针对互联网数据化运营中数据挖掘应用的特点进行梳理和总结。

## 2.1 数据挖掘的发展历史

数据挖掘起始于 20 世纪下半叶，是在当时多个学科发展的基础上发展起来的。随着数据库技术的发展应用，数据的积累不断膨胀，导致简单的查询和统计已经无法满足企业的商业需求，急需一些革命性的技术去挖掘数据背后的信息。同时，这期间计算机领域的人工智能（Artificial Intelligence）也取得了巨大进展，进入了机器学习的阶段。因此，人们将两者结合起来，用数据库管理系统存储数据，用计算机分析数据，并且尝试挖掘数据背后的信息。这两者的结合促生了一门新的学科，即数据库中的知识发现（Knowledge Discovery in Databases，KDD）。1989 年 8 月召开的第 11 届国际人工智能联合会议的专题讨论会上首次出现了知识发现（KDD）这个术语，到目前为止，KDD 的重点已经从发现方法转向了实践应用。

而数据挖掘（Data Mining）则是知识发现（KDD）的核心部分，它指的是从数据集合中自动抽取隐藏在数据中的那些有用信息的非平凡过程，这些信息的表现形式为：规则、概念、规律及模式等。进入 21 世纪，数据挖掘已经成为一门比较成熟的交叉学科，并且数据挖掘技术也伴随着信息技术的发展日益成熟起来。

总体来说，数据挖掘融合了数据库、人工智能、机器学习、统计学、高性能计算、模式识别、神经网络、数据可视化、信息检索和空间数据分析等多个领域的理论和技术，是 21 世纪初期对人类产生重大影响的十大新兴技术之一。

## 2.2 统计分析与数据挖掘的主要区别

统计分析与数据挖掘有什么区别呢？从实践应用和商业实战的角度来看，这个问题并没有很大的意义，正如「不管白猫还是黑猫，抓住老鼠才是好猫」一样，在企业的商业实战中，数据分析师分析问题、解决问题时，首先考虑的是思路，其次才会对与思路匹配的分析挖掘技术进行筛选，而不是先考虑到底是用统计技术还是用数据挖掘技术来解决这个问题。从两者的理论来源来看，它们在很多情况下都是同根同源的。比如，在属于典型的数据挖掘技术的决策树里，CART、CHAID 等理论和方法都是基于统计理论所发展和延伸的；并且数据挖掘中的技术有相当比例是用统计学中的多变量分析来支撑的。

相对于传统的统计分析技术，数据挖掘有如下一些特点：

❑数据挖掘特别擅长于处理大数据，尤其是几十万行、几百万行，甚至更多更大的数据。

❑数据挖掘在实践应用中一般都会借助数据挖掘工具，而这些挖掘工具的使用，很多时候并不需要特别专业的统计背景作为必要条件。不过，需要强调的是基本的统计知识和技能是必需的。

❑在信息化时代，数据分析应用的趋势是从大型数据库中抓取数据，并通过专业软件进行分析，所以数据挖掘工具的应用更加符合企业实践和实战的需要。

❑从操作者来看，数据挖掘技术更多是企业的数据分析师、业务分析师在使用，而不是统计学家用于检测。

更主流的观点普遍认为，数据挖掘是统计分析技术的延伸和发展，如果一定要加以区分，它们又有哪些区别呢？数据挖掘在如下几个方面与统计分析形成了比较明显的差异：

❑统计分析的基础之一就是概率论，在对数据进行统计分析时，分析人员常常需要对数据分布和变量间的关系做假设，确定用什么概率函数来描述变量间的关系，以及如何检验参数的统计显著性；但是，在数据挖掘的应用中，分析人员不需要对数据分布做任何假设，数据挖掘中的算法会自动寻找变量间的关系。因此，相对于海量、杂乱的数据，数据挖掘技术有明显的应用优势。

❑统计分析在预测中的应用常表现为一个或一组函数关系式，而数据挖掘在预测应用中的重点在于预测的结果，很多时候并不会从结果中产生明确的函数关系式，有时候甚至不知道到底是哪些变量在起作用，又是如何起作用的。最典型的例子就是「神经网络」挖掘技术，它里面的隐蔽层就是一个「黑箱」，没有人能在所有的情况下读懂里面的非线性函数是如何对自变量进行组合的。在实践应用中，这种情况常会让习惯统计分析公式的分析师或者业务人员感到困惑，这也确实影响了模型在实践应用中的可理解性和可接受度。不过，如果能换种思维方式，从实战的角度考虑，只要模型能正确预测客户行为，能为精细化运营提供准确的细分人群和目标客户，业务部门、运营部门不了解模型的技术细节，又有何不可呢？

❑在实践应用中，统计分析常需要分析人员先做假设或判断，然后利用数据分析技术来验证该假设是否成立。但是，在数据挖掘中，分析人员并不需要对数据的内在关系做任何假设或判断，而是会让挖掘工具中的算法自动去寻找数据中隐藏的关系或规律。两者的思维方式并不相同，这给数据挖掘带来了更灵活、更宽广的思路和舞台。

虽然上面详细阐述了统计分析与数据挖掘的区别，但是在企业的实践应用中，我们不应该硬性地把两者割裂开来，也无法割裂，在实践应用中，没有哪个分析师会说，「我只用数据挖掘技术来分析」，或者「我只用统计分析技术来分析」。正确的思路和方法应该是：针对具体的业务分析需求，先确定分析思路，然后根据这个分析思路去挑选和匹配合适的分析算法、分析技术，而且一个具体的分析需求一般都会有两种以上不同的思路和算法可以去探索，最后可根据验证的效果和资源匹配等一系列因素进行综合权衡，从而决定最终的思路、算法和解决方案。

鉴于实践应用中，统计分析与数据挖掘技术并不能完全被割裂开来，并且本书侧重于数据化运营的实践分享。所以在后续各章节的讨论中，将不再人为地给一个算法、技术贴上「统计分析」或「数据挖掘」的标签，后续各章节的技术分享和实战应用举例，都会本着针对不同的分析目的、项目类型来介绍主流的、有效的分析挖掘技术以及相应的特点和技巧。统计分析也罢，数据挖掘也好，只要有价值，只要在实战中有效，都会是我们所关注的，都会是我们所要分析分享的。

## 2.3 数据挖掘的主要成熟技术以及在数据化运营中的主要应用

2.3.1　决策树

决策树（Decision Tree）是一种非常成熟的、普遍采用的数据挖掘技术。之所以称为树，是因为其建模过程类似一棵树的成长过程，即从根部开始，到树干，到分枝，再到细枝末节的分叉，最终生长出一片片的树叶。在决策树里，所分析的数据样本先是集成为一个树根，然后经过层层分枝，最终形成若干个结点，每个结点代表一个结论。

决策树算法之所以在数据分析挖掘应用中如此流行，主要原因在于决策树的构造不需要任何领域的知识，很适合探索式的知识发掘，并且可以处理高维度的数据。在众多的数据挖掘、统计分析算法中，决策树最大的优点在于它所产生的一系列从树根到树枝（或树叶）的规则，可以很容易地被分析师和业务人员理解，而且这些典型的规则甚至不用整理（或稍加整理），就是现成的可以应用的业务优化策略和业务优化路径。另外，决策树技术对数据的分布甚至缺失非常宽容，不容易受到极值的影响。

目前，最常用的 3 种决策树算法分别是 CHAID、CART 和 ID3（包括后来的 C4.5，乃至 C5.0）。

CHAID (Chi-square Automatic Interaction Detector) 算法的历史较长，中文简称为卡方自动相互关系检测。CHAID 依据局部最优原则，利用卡方检验来选择对因变量最有影响的自变量，CHAID 应用的前提是因变量为类别型变量（Category）。

CART (Classification and Regression Tree) 算法产生于 20 世纪 80 年代中期，中文简称为分类与回归树，CART 的分割逻辑与 CHAID 相同，每一层的划分都是基于对所有自变量的检验和选择上的。但是，CART 采用的检验标准不是卡方检验，而是基尼系数（Gini）等不纯度的指标。两者最大的区别在于 CHAID 采用的是局部最优原则，即结点之间互不相干，一个结点确定了之后，下面的生长过程完全在结点内进行。而 CART 则着眼于总体优化，即先让树尽可能地生长，然后再回过头来对树进行修剪（Prune），这一点非常类似统计分析中回归算法里的反向选择（Backward Selection）。CART 所生产的决策树是二分的，每个结点只能分出两枝，并且在树的生长过程中，同一个自变量可以反复使用多次（分割），这些都是不同于 CHAID 的特点。另外，如果是自变量存在数据缺失（Missing）的情况，CART 的处理方式将会是寻找一个替代数据来代替（填充）缺失值，而 CHAID 则是把缺失数值作为单独的一类数值。

ID3（Iterative Dichotomiser）算法与 CART 是同一时期产生的，中文简称为迭代的二分器，其最大的特点在于自变量的挑选标准是：基于信息增益的度量选择具有最高信息增益的属性作为结点的分裂（分割）属性，其结果就是对分割后的结点进行分类所需的信息量最小，这也是一种划分纯度的思想。至于之后发展起来的 C4.5 可以理解为 ID3 的发展版（后继版），两者的主要区别在于 C4.5 采用信息增益率（Gain Ratio）代替了 ID3 中的信息增益度量，如此替换的主要原因是信息增益度量有个缺点，就是倾向于选择具有大量值的属性。这里给个极端的例子，对于 Member_Id 的划分，每个 Id 都是一个最纯的组，但是这样的划分没有任何实际意义。而 C4.5 所采用的信息增益率就可以较好地克服这个缺点，它在信息增益的基础上，增加了一个分裂信息（SplitInformation）对其进行规范化约束。

决策树技术在数据化运营中的主要用途体现在：作为分类、预测问题的典型支持技术，它在用户划分、行为预测、规则梳理等方面具有广泛的应用前景，决策树甚至可以作为其他建模技术前期进行变量筛选的一种方法，即通过决策树的分割来筛选有效地输入自变量。

关于决策树的详细介绍和实践中的注意事项，可参考本书 10.2 节。

2.3.2　神经网络

神经网络（Neural Network）是通过数学算法来模仿人脑思维的，它是数据挖掘中机器学习的典型代表。神经网络是人脑的抽象计算模型，我们知道人脑中有数以百亿个神经元（人脑处理信息的微单元），这些神经元之间相互连接，使得人的大脑产生精密的逻辑思维。而数据挖掘中的「神经网络」也是由大量并行分布的人工神经元（微处理单元）组成的，它有通过调整连接强度从经验知识中进行学习的能力，并可以将这些知识进行应用。

简单来讲，「神经网络」就是通过输入多个非线性模型以及不同模型之间的加权互联（加权的过程在隐蔽层完成），最终得到一个输出模型。其中，隐蔽层所包含的就是非线性函数。

目前最主流的「神经网络」算法是反馈传播（Backpropagation），该算法在多层前向型（Multilayer Feed-Forward）神经网络上进行学习，而多层前向型神经网络又是由一个输入层、一个或多个隐蔽层以及一个输出层组成的，「神经网络」的典型结构如图 2-1 所示。

图　2-1　「神经网络」的典型结构图

由于「神经网络」拥有特有的大规模并行结构和信息的并行处理等特点，因此它具有良好的自适应性、自组织性和高容错性，并且具有较强的学习、记忆和识别功能。目前神经网络已经在信号处理、模式识别、专家系统、预测系统等众多领域中得到广泛的应用。

「神经网络」的主要缺点就是其知识和结果的不可解释性，没有人知道隐蔽层里的非线性函数到底是如何处理自变量的，「神经网络」应用中的产出物在很多时候让人看不清其中的逻辑关系。但是，它的这个缺点并没有影响该技术在数据化运营中的广泛应用，甚至可以这样认为，正是因为其结果具有不可解释性，反而更有可能促使我们发现新的没有认识到的规律和关系。

在利用「神经网络」技术建模的过程中，有以下 5 个因素对模型结果有着重大影响：

❑层数。

❑每层中输入变量的数量。

❑联系的种类。

❑联系的程度。

❑转换函数，又称激活函数或挤压函数。

关于这 5 个因素的详细说明，请参考本书 10.1.1 节。

「神经网络」技术在数据化运营中的主要用途体现在：作为分类、预测问题的重要技术支持，在用户划分、行为预测、营销响应等诸多方面具有广泛的应用前景。

关于神经网络的详细介绍和实践中的注意事项，可参考本书 10.1 节。

2.3.3　回归

回归（Regression）分析包括线性回归（Linear Regression），这里主要是指多元线性回归和逻辑斯蒂回归（Logistic Regression）。其中，在数据化运营中更多使用的是逻辑斯蒂回归，它又包括响应预测、分类划分等内容。

多元线性回归主要描述一个因变量如何随着一批自变量的变化而变化，其回归公式（回归方程）就是因变量与自变量关系的数据反映。因变量的变化包括两部分：系统性变化与随机变化，其中，系统性变化是由自变量引起的（自变量可以解释的），随机变化是不能由自变量解释的，通常也称作残值。

在用来估算多元线性回归方程中自变量系数的方法中，最常用的是最小二乘法，即找出一组对应自变量的相应参数，以使因变量的实际观测值与回归方程的预测值之间的总方差减到最小。

对多元线性回归方程的参数估计，是基于下列假设的：

❑输入变量是确定的变量，不是随机变量，而且输入的变量间无线性相关，即无共线性。

❑随机误差的期望值总和为零，即随机误差与自变量不相关。

❑随机误差呈现正态分布 [1]。

如果不满足上述假设，就不能用最小二乘法进行回归系数的估算了。

逻辑斯蒂回归（Logistic Regression）相比于线性回归来说，在数据化运营中有更主流更频繁的应用，主要是因为该分析技术可以很好地回答诸如预测、分类等数据化运营常见的分析项目主题。简单来讲，凡是预测「两选一」事件的可能性（比如，「响应」还是「不响应」；「买」还是「不买」；「流失」还是「不流失」），都可以采用逻辑斯蒂回归方程。

逻辑斯蒂回归预测的因变量是介于 0 和 1 之间的概率，如果对这个概率进行换算，就可以用线性公式描述因变量与自变量的关系了，具体公式如下：

与多元线性回归所采用的最小二乘法的参数估计方法相对应，最大似然法是逻辑斯蒂回归所采用的参数估计方法，其原理是找到这样一个参数，可以让样本数据所包含的观察值被观察到的可能性最大。这种寻找最大可能性的方法需要反复计算，对计算能力有很高的要求。最大似然法的优点是在大样本数据中参数的估值稳定、偏差小，估值方差小。

关于线性回归和逻辑回归的详细介绍和在实践应用中的注意事项，可参考本书 10.3 节和 10.4 节。

[1] 正态分布也称常态分布，是具有两个参数 m 和 s2 的连续型随机变量分布，第一个参数 m 是服从正态分布的随机变量的均值，第二个参数 s2 是此随机变量的方差，服从正态分布的随机变量的概率规律为取与 m 邻近的值的概率大，而取离 m 越远的值的概率越小；s 越小，分布越集中在 m 附近，s 越大，分布越分散。

2.3.4　关联规则

关联规则（Association Rule）是在数据库和数据挖掘领域中被发明并被广泛研究的一种重要模型，关联规则数据挖掘的主要目的是找出数据集中的频繁模式（Frequent Pattern），即多次重复出现的模式和并发关系（Cooccurrence Relationships），即同时出现的关系，频繁和并发关系也称作关联（Association）。

应用关联规则最经典的案例就是购物篮分析（Basket Analysis），通过分析顾客购物篮中商品之间的关联，可以挖掘顾客的购物习惯，从而帮助零售商更好地制定有针对性的营销策略。

以下列举一个简单的关联规则的例子：

婴儿尿不湿→啤酒 [支持度 = 10%，置信度 = 70%]

这个规则表明，在所有顾客中，有 10% 的顾客同时购买了婴儿尿不湿和啤酒，而在所有购买了婴儿尿不湿的顾客中，占 70% 的人同时还购买了啤酒。发现这个关联规则后，超市零售商决定把婴儿尿不湿和啤酒摆放在一起进行促销，结果明显提升了销售额，这就是发生在沃尔玛超市中「啤酒和尿不湿」的经典营销案例。

上面的案例是否让你对支持度和置信度有了一定的了解？事实上，支持度（Support）和置信度（Confidence）是衡量关联规则强度的两个重要指标，它们分别反映着所发现规则的有用性和确定性。其中支持度：规则 X→Y 的支持度是指事物全集中包含 X∪Y 的事物百分比。支持度主要衡量规则的有用性，如果支持度太小，则说明相应规则只是偶发事件。在商业实战中，偶发事件很可能没有商业价值；置信度：规则 X→Y 的置信度是指既包含了 X 又包含了 Y 的事物数量占所有包含了 X 的事物数量的百分比。置信度主要衡量规则的确定性（可预测性），如果置信度太低，那么从 X 就很难可靠地推断出 Y 来，置信度太低的规则在实践应用中也没有太大用处。

在众多的关联规则数据挖掘算法中，最著名的就是 Apriori 算法，该算法具体分为以下两步进行：

（1）生成所有的频繁项目集。一个频繁项目集（Frequent Itemset）是一个支持度高于最小支持度阀值（min-sup）的项目集。

（2）从频繁项目集中生成所有的可信关联规则。这里可信关联规则是指置信度大于最小置信度阀值（min-conf）的规则。

关联规则算法不但在数值型数据集的分析中有很大用途，而且在纯文本文档和网页文件中，也有着重要用途。比如发现单词间的并发关系以及 Web 的使用模式等，这些都是 Web 数据挖掘、搜索及推荐的基础。

2.3.5　聚类

聚类（Clustering）分析有一个通俗的解释和比喻，那就是「物以类聚，人以群分」。针对几个特定的业务指标，可以将观察对象的群体按照相似性和相异性进行不同群组的划分。经过划分后，每个群组内部各对象间的相似度会很高，而在不同群组之间的对象彼此间将具有很高的相异度。

聚类分析的算法可以分为划分的方法（Partitioning Method）、层次的方法（Hierarchical Method）、基于密度的方法（Density-based Method）、基于网格的方法（Grid-based Method）、基于模型的方法（Model-based Method）等，其中，前面两种方法最为常用。

对于划分的方法（Partitioning Method），当给定 m 个对象的数据集，以及希望生成的细分群体数量 K 后，即可采用这种方法将这些对象分成 K 组（K≤m），使得每个组内对象是相似的，而组间的对象是相异的。最常用的划分方法是 K-Means 方法，其具体原理是：首先，随机选择 K 个对象，并且所选择的每个对象都代表一个组的初始均值或初始的组中心值；对剩余的每个对象，根据其与各个组初始均值的距离，将它们分配给最近的（最相似）小组；然后，重新计算每个小组新的均值；这个过程不断重复，直到所有的对象在 K 组分布中都找到离自己最近的组。

层次的方法（Hierarchical Method）则是指依次让最相似的数据对象两两合并，这样不断地合并，最后就形成了一棵聚类树。

聚类技术在数据分析和数据化运营中的主要用途表现在：既可以直接作为模型对观察对象进行群体划分，为业务方的精细化运营提供具体的细分依据和相应的运营方案建议，又可在数据处理阶段用作数据探索的工具，包括发现离群点、孤立点，数据降维的手段和方法，通过聚类发现数据间的深层次的关系等。

关于聚类技术的详细介绍和应用实践中的注意事项，可参考本书第 9 章。

2.3.6　贝叶斯分类方法

贝叶斯分类方法（Bayesian Classifier）是非常成熟的统计学分类方法，它主要用来预测类成员间关系的可能性。比如通过一个给定观察值的相关属性来判断其属于一个特定类别的概率。贝叶斯分类方法是基于贝叶斯定理的，已经有研究表明，朴素贝叶斯分类方法作为一种简单贝叶斯分类算法甚至可以跟决策树和神经网络算法相媲美。

贝叶斯定理的公式如下：

其中，X 表示 n 个属性的测量描述；H 为某种假设，比如假设某观察值 X 属于某个特定的类别 C；对于分类问题，希望确定 P (H|X)，即能通过给定的 X 的测量描述，来得到 H 成立的概率，也就是给出 X 的属性值，计算出该观察值属于类别 C 的概率。因为 P (H|X) 是后验概率（Posterior Probability），所以又称其为在条件 X 下，H 的后验概率。

举例来说，假设数据属性仅限于用教育背景和收入来描述顾客，而 X 是一位硕士学历，收入 10 万元的顾客。假定 H 表示假设我们的顾客将购买苹果手机，则 P (H|X) 表示当我们知道顾客的教育背景和收入情况后，该顾客将购买苹果手机的概率；相反，P (X|H) 则表示如果已知顾客购买苹果手机，则该顾客是硕士学历并且收入 10 万元的概率；而 P (X) 则是 X 的先验概率，表示顾客中的某个人属于硕士学历且收入 10 万元的概率；P (H) 也是先验概率，只不过是任意给定顾客将购买苹果手机的概率，而不会去管他们的教育背景和收入情况。

从上面的介绍可见，相比于先验概率 P (H)，后验概率 P (H|X) 基于了更多的信息（比如顾客的信息属性），而 P (H) 是独立于 X 的。

贝叶斯定理是朴素贝叶斯分类法（Naive Bayesian Classifier）的基础，如果给定数据集里有 M 个分类类别，通过朴素贝叶斯分类法，可以预测给定观察值是否属于具有最高后验概率的特定类别，也就是说，朴素贝叶斯分类方法预测 X 属于类别 Ci 时，表示当且仅当

P(Ci|X)＞P(Cj|X)1≤j≤m，j≠i

此时如果最大化 P (Ci|X)，其 P (Ci|X) 最大的类 Ci 被称为最大后验假设，根据贝叶斯定理

可知，由于 P (X) 对于所有的类别是均等的，因此只需要 P (X|Ci) P (Ci) 取最大即可。

为了预测一个未知样本 X 的类别，可对每个类别 Ci 估算相应的 P (X|Ci) P (Ci)。样本 X 归属于类别 Ci，当且仅当

P(Ci|X)＞P(Cj|X)1≤j≤m，j≠i

贝叶斯分类方法在数据化运营实践中主要用于分类问题的归类等应用场景。

2.3.7　支持向量机

支持向量机（Support Vector Machine）是 Vapnik 等人于 1995 年率先提出的，是近年来机器学习研究的一个重大成果。与传统的神经网络技术相比，支持向量机不仅结构简单，而且各项技术的性能也明显提升，因此它成为当今机器学习领域的热点之一。

作为一种新的分类方法，支持向量机以结构风险最小为原则。在线性的情况下，就在原空间寻找两类样本的最优分类超平面。在非线性的情况下，它使用一种非线性的映射，将原训练集数据映射到较高的维上。在新的维上，它搜索线性最佳分离超平面。使用一个适当的对足够高维的非线性映射，两类数据总可以被超平面分开。

支持向量机的基本概念如下：

设给定的训练样本集为 {(x1,y1),(x2,y2),…,(xn,yn)}，其中 xi∈Rn,y∈{-1,1}。

再假设该训练集可被一个超平面线性划分，设该超平面记为 (w,x)+b=0。

支持向量机的基本思想可用图 2-2 的两维情况举例说明。

图　2-2　线性可分情况下的最优分类线

图中圆形和方形代表两类样本，H 为分类线，H1、H2，分别为过各类样本中离分类线最近的样本并且平行于分类线的直线，它们之间的距离叫做分类间隔（Margin）。所谓的最优分类线就是要求分类线不但能将两类正确分开（训练错误为 0），而且能使分类间隔最大。推广到高维空间，最优分类线就成了最优分类面。

其中，距离超平面最近的一类向量被称为支持向量（Support Vector），一组支持向量可以唯一地确定一个超平面。通过学习算法，SVM 可以自动寻找出那些对分类有较好区分能力的支持向量，由此构造出的分类器则可以最大化类与类的间隔，因而有较好的适应能力和较高的分类准确率。

支持向量机的缺点是训练数据较大，但是，它的优点也是很明显的 —— 对于复杂的非线性的决策边界的建模能力高度准确，并且也不太容易过拟合 [1]。

支持向量机主要用在预测、分类这样的实际分析需求场景中。

[1] 过拟合，是指模型在训练的时候对样本「模拟」过好，不能反映真实的输入输出函数关系，所以一旦模型面对新的应用数据的时候，就表现为不准确的程度较大。

2.3.8　主成分分析

严格意义上讲，主成分分析（Principal Components Analysis）属于传统的统计分析技术范畴，但是正如本章前面所阐述的，统计分析与数据挖掘并没有严格的分割，因此在数据挖掘实战应用中也常常会用到这种方式，从这个角度讲，主成分分析也是数据挖掘商业实战中常用的一种分析技术和数据处理技术。

主成分分析会通过线性组合将多个原始变量合并成若干个主成分，这样每个主成分都变成了原始变量的线性组合。这种转变的目的，一方面是可以大幅降低原始数据的维度，同时也在此过程中发现原始数据属性之间的关系。

主成分分析的主要步骤如下：

1）通常要先进行各变量的标准化工作，标准化的目的是将数据按照比例进行缩放，使之落入一个小的区间范围之内，从而让不同的变量经过标准化处理后可以有平等的分析和比较基础。关于数据标准化的详细介绍，可参考本书 8.5.4 节和 9.3.2 节。

2）选择协方差阵或者相关阵计算特征根及对应的特征向量。

3）计算方差贡献率，并根据方差贡献率的阀值选取合适的主成分个数。

4）根据主成分载荷的大小对选择的主成分进行命名。

5）根据主成分载荷计算各个主成分的得分。

将主成分进行推广和延伸即成为因子分析（Factor Analysis），因子分析在综合原始变量信息的基础上将会力图构筑若干个意义较为明确的公因子；也就是说，采用少数几个因子描述多个指标之间的联系，将比较密切的变量归为同一类中，每类变量即是一个因子。之所以称其为因子，是因为它们实际上是不可测量的，只能解释。

主成分分析是因子分析的一个特例，两者的区别和联系主要表现在以下方面：

❑主成分分析会把主成分表示成各个原始变量的线性组合，而因子分析则把原始变量表示成各个因子的线性组合。这个区别最直观也最容易记住。

❑主成分分析的重点在于解释原始变量的总方差，而因子分析的重点在于解释原始变量的协方差。

❑在主成分分析中，有几个原始变量就有几个主成分，而在因子分析中，因子个数可以根据业务场景的需要人为指定，并且指定的因子数量不同，则分析结果也会有差异。

❑在主成分分析中，给定的协方差矩阵或者相关矩阵的特征值是唯一时，主成分也是唯一的，但是在因子分析中，因子不是唯一的，并且通过旋转可以得到不同的因子。

主成分分析和因子分析在数据化运营实践中主要用于数据处理、降维、变量间关系的探索等方面，同时作为统计学里的基本而重要的分析工具和分析方法，它们在一些专题分析中也有着广泛的应用。

2.3.9　假设检验

假设检验（Hypothesis Test）是现代统计学的基础和核心之一，其主要研究在一定的条件下，总体是否具备某些特定特征。

假设检验的基本原理就是小概率事件原理，即观测小概率事件在假设成立的情况下是否发生。如果在一次试验中，小概率事件发生了，那么说明假设在一定的显著性水平下不可靠或者不成立；如果在一次试验中，小概率事件没有发生，那么也只能说明没有足够理由相信假设是错误的，但是也并不能说明假设是正确的，因为无法收集到所有的证据来证明假设是正确的。

假设检验的结论是在一定的显著性水平下得出的。因此，当采用此方法观测事件并下结论时，有可能会犯错，这些错误主要有两大类：

❑第 Ⅰ 类错误：当原假设为真时，却否定它而犯的错误，即拒绝正确假设的错误，也叫弃真错误。犯第 Ⅰ 类错误的概率记为 α，通常也叫 α 错误，α=1 - 置信度。

❑第 Ⅱ 类错误：当原假设为假时，却肯定它而犯的错误，即接受错误假设的错误，也叫纳伪错误。犯第 Ⅱ 类错误的概率记为 β，通常也叫 β 错误。

上述这两类错误在其他条件不变的情况下是相反的，即 α 增大时，β 就减小；α 减小时，β 就增大。α 错误容易受数据分析人员的控制，因此在假设检验中，通常会先控制第 Ⅰ 类错误发生的概率 α，具体表现为：在做假设检验之前先指定一个 α 的具体数值，通常取 0.05，也可以取 0.1 或 0.001。

在数据化运营的商业实践中，假设检验最常用的场景就是用于「运营效果的评估」上，本书第 12 章将针对最常见、最基本的假设检验形式和技术做出比较详细的梳理和举例。

2.4　互联网行业数据挖掘应用的特点

相对于传统行业而言，互联网行业的数据挖掘和数据化运营有如下的一些主要特点：

❑数据的海量性。互联网行业相比传统行业第一个区别就是收集、存储的数据是海量的，这一方面是因为互联网的使用已经成为普通人日常生活和工作中不可或缺的一部分，另一方面更是因为用户网络行为的每一步都会被作为网络日志记录下来。海量的数据、海量的字段、海量的信息，尤其是海量的字段，使得分析之前对于分析字段的挑选和排查工作显得无比重要，无以复加。如何大浪淘沙挑选变量则为重中之重，对此很难一言以蔽之的进行总结，还是用三分技术，七分业务来理解吧。本书从第 7～12 章，几乎每章都用大量的篇幅讨论如何在具体的分析课题和项目中选择变量、评估变量、转换变量，乃至如何通过清洗后的核心变量完成最终的分析结论（挖掘模型）。

❑数据分析（挖掘）的周期短。鉴于互联网行业白热化的市场竞争格局，以及该行业相对成熟的高级数据化运营实践，该行业的数据分析（挖掘）通常允许的分析周期（项目周期）要明显短于传统行业。行业技术应用飞速发展，产品和竞争一日千里，都使该行业的数据挖掘项目的时间进度比传统行业的项目模式快得多。一方面要保证挖掘结果的起码质量，另一方面要满足这个行业超快的行业节奏，这也使得传统的挖掘分析思路和步调必须改革和升华，从而具有鲜明的 Internet 色彩。

❑数据分析（挖掘）成果的时效性明显变短。由于互联网行业的用户行为相对于传统行业而言变化非常快，导致相应的数据分析挖掘成果的时效性也比传统行业明显缩短。举例来说，互联网行业的产品更新换代很多是以月为单位的，新产品层出不穷，老产品要及时下线，因此，针对具体产品的数据分析（挖掘）成果的时效性也明显变短；或者说，用户行为变化快，网络环境变化快，导致模型的维护和优化的时间周期也明显变短，传统行业里的「用户流失预测模型」可能只需要每年更新优化一次，但是在互联网行业里类似的模型可能 3 个月左右就有必要更新优化了。

❑互联网行业新技术、新应用、新模式的更新换代相比于传统行业而言更加迅速、周期更短、更加具有颠覆性，相应地对数据分析挖掘的应用需求也更为苛刻，且要多样化。以中国互联网行业的发展为例，作为第一代互联网企业的代表，新浪、搜狐、雅虎等门户网站的 Web 1.0 模式（传统媒体的电子化）从产生到被以 Google、百度等搜索引擎企业的 Web 2.0 模式（制造者与使用者的合一）所超越，前后不过 10 年左右的时间，而目前这个 Web 2.0 模式已经逐渐有被以微博为代表 Web 3.0 模式（SNS 模式）超越的趋势。具体到数据分析所服务的互联网业务和应用来说，从最初的常规、主流的分析挖掘支持，到以微博应用为代表的新的分析需求，再到目前风头正健的移动互联网的数据分析和应用，互联网行业的数据分析大显身手的天地在不断扩大，新的应用源源不断，新的挑战让人们应接不暇，这一切都要求数据分析师自觉、主动去学习、去充实、去提升自己、去跟上互联网发展的脚步。



