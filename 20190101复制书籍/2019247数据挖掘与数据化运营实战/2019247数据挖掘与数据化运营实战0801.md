# 08 常见的数据处理技巧

工欲善其事，必先利其器。

——《论语·卫灵公》

8.1　数据的抽取要正确反映业务需求

8.2　数据抽样

8.3　分析数据的规模有哪些具体的要求

8.4　如何处理缺失值和异常值

8.5　数据转换

8.6　筛选有效的输入变量

8.7　共线性问题

在前面的章节里，重点谈到了数据挖掘实践中值得我们警惕和预防的错误观念（第 5 章），以及模型优化中主要用到的优化原则和方法（第 7 章）。虽然在「道」层面上的内容对于数据挖掘应用的影响是决定性的和根本性的，但是在「术」层面上的内容对于数据挖掘应用来说也是不可或缺的，只要应用得当定能「锦上添花」。另外，常见挖掘技术上的使用技巧，即所谓的「术」，属于数据分析师分析的基本功，有了基本功不代表你就可以进行完美的数据挖掘应用，但是如果没有基本功，你的数据挖掘应用肯定不会成功，从这个角度来看，这些「术」应该成为每个数据分析师必备的技能和知识，它们很重要也很基础。

提到数据挖掘中的技巧，首当其冲就是数据处理中的技巧，另外还包括各种挖掘算法的应用技巧，以及数据化运营整个闭环中的各环节所涉及的一些相应技巧。鉴于数据挖掘项目实践中有将近 60% 左右的时间和精力是用来熟悉、清理和转换数据的，因此本章专门针对数据处理中一些普遍性的，同时也是非常重要的一些技巧进行分析、总结和提炼。至于各种挖掘算法应用中的技巧和数据化运营中的其他技巧，将在随后相关的章节中分别进行讲解。

本章将对数据挖掘中最常见的一些判断和处理数据的方法进行展开阐述，对于本章的各节都可以看成是一个独立的环节，其中介绍了常见的容易犯错误的地方，同时每一节又会独立地从技术角度来思考挖掘过程中的风险点和需要注意的地方。

8.1　数据的抽取要正确反映业务需求

一个数据挖掘（分析）需求一旦被分析师接受和认可，数据分析师接下来要做的事情就是抽取分析用的数据，并熟悉数据。在数据挖掘实践中，因为抽取的数据不能正确反映业务需求而导致挖掘项目失败的例子并不少见，原因很简单，从错误的数据里，肯定是不能找到正确的分析挖掘结论的。举例来说，某业务分析需求是找出因为使用店铺装修工具而带来显著销售收入提升的用户群体特征，如果不对此需求详加思考，仅仅凭借字面意思，就去抽取使用了该装修工具并且有明显销售收入提升的人群，然后对该人群加以特征分析，其结果就很有可能是「垃圾进，垃圾出（Garbage In,Garbage Out）」，错误的结论将严重误导业务方接下来的业务应用。本案例里为什么上面的抽取数据思路有误呢？是其没有正确反映业务需求吗？难道不是严格按照需求描述来抽取数据的吗？

之所以说上述的抽取思路是错误的，是因为对于本案例所在的平台来说，用户可以有很多不同的付费工具、付费服务去提升他们的销售收入，比如用户在平台上的竞价排名就可以很有效地提升其销售额。很有可能在购买和使用了店铺装修工具的用户中，有相当数量和相当比例的人也同时使用了竞价排名等多种方式去提升销售额，换句话说，如果仅仅抽取使用了店铺装修工具并且带来显著销售收入提升的用户，而没有排除同时也使用了其他诸如竞价排名等方式的用户，那得到的特征人群的描述肯定是不符合当初的业务需求定义的。

在本案例中，要如何避免出现上述的错误呢？如何保证数据的抽取能尽可能反映和满足业务的需求呢？一个常用的方法就是使用控制变量，确保抽取的用户群里，不包含使用了竞价排名等主要的提升流量和销售收入手段的用户，尽可能使得这个用户群的确是因为仅仅使用了店铺装修工具而带来的销售收入提升。

在数据挖掘分析的实践中，如何尽量确保数据的抽取能正确反映业务需求呢？以下一些方法、原则及技巧可供参考和借鉴。

❑真正熟悉业务背景，这是确保数据抽取能正确反映业务需求的王道。如果分析师对于业务背景非常熟悉，那么在上述的案例中，面对相应的分析需求，他在脑海里的第一反应就应该是排除掉诸如竞价排名之类的影响，真正过滤出仅仅使用了店铺装修工具并且提升了销售收入的特定用户群体。熟悉业务背景，这句话看似老生常谈，却是历久弥坚。在很多时候最朴素的总是最珍贵的，最平凡的总是最核心的，生活的哲理也是数据挖掘的哲理，即所谓的万法归宗。

❑确保抽取的数据所对应的当时业务背景，与现在的业务需求即将应用的业务背景没有明显的重大改变。数据挖掘分析所针对的分析数据是有时效性的，如果应用场景的基础条件发生了根本变化，根据历史数据做出的挖掘结论对于变化了的业务环境来说是没有意义的。举例来说，如果最初的产品销售是基于猛烈的折扣和赠品活动来推动的，后期的销售并没有类似的折扣和赠品，那么基于前面折扣和赠品所带来的销售数据所做的付费用户特征分析，或者付费用户预测模型，是不能用到后期（没有折扣和赠品）对付费用户的预测上的。类似的业务环境改变的场景在瞬息万变的企业经营中是司空见惯的，数据分析师在分析挖掘实践中，一定要有意识地提醒自己，建模数据所对应的当时的业务环境，与现在业务需求所对应的业务环境是否已发生了根本性的变化，这样才能确保数据的抽取可正确反映业务需求。

8.2　数据抽样

「抽样」对于数据分析和挖掘来说是一种常见的前期数据处理技术和阶段，之所以要采取抽样措施，主要原因在于如果数据全集的规模太大，针对数据全集进行分析运算不但会消耗更多的运算资源，还会显著增加运算分析的时间，甚至太大的数据量有时候会导致分析挖掘软件运行时崩溃。而采用了抽样措施，就可以显著降低这些负面的影响；另外一个常见的需要通过抽样来解决的场景就是：在很多小概率事件、稀有事件的预测建模过程中，比如信用卡欺诈事件，在整个信用卡用户中，属于恶意欺诈的用户只占 0.2% 甚至更少，如果按照原始的数据全集、原始的稀有占比来进行分析挖掘，0.2% 的稀有事件是很难通过分析挖掘得到有意义的预测和结论的，所以对此类稀有事件的分析建模，通常会采取抽样的措施，即人为增加样本中的「稀有事件」的浓度和在样本中的占比。对抽样后得到的分析样本进行分析挖掘，可以比较容易地发现稀有事件与分析变量之间有价值、有意义的一些关联性和逻辑性。

在抽样的操作中，有下列一些思考点需要引起数据分析师的注意：

❑样本中输入变量（或自变量）的值域要与数据全集中输入变量（或自变量）的值域一致。如果自变量是连续型变量（Interval），样本中自变量的值域要与数据全集中的一致；如果自变量是类别型变量（Category），样本中自变量的种类要与数据全集中的保持一致。

❑样本中输入变量（或自变量）的分布要与数据全集中输入变量（或自变量）的分布保持一致，或者说至少要高度相似。无论自变量是连续型变量还是类别型变量，其在样本中的分布要能代表其在数据全集里的分布。

❑样本中因变量（或目标变量）的值域或者种类的分布，也要与数据全集中目标变量值域或者种类的分布保持一致，或者说要高度相似。

❑缺失值的分布。样本中缺失值的分布（频率）要与数据全集中缺失值的分布（频率）保持一致，或者说至少要高度相似。

❑针对稀有事件建模时要采用抽样措施。由于抽样所造成的目标事件在样本中的浓度被人为放大了，样本中的事件与非事件的比例与数据全集中两者的比例将不一致，所以在建模过程中，数据分析师要记得使用加权的方法恢复新样本对全体数据集的代表性。在目前主流的数据挖掘软件里，对这种加权恢复已经做了自动处理，这给数据分析师带来了很大的便利。

正因为数据分析师要对比样本与全集的一致性，所以在数据分析挖掘实践中，会发生多次抽样的情况，以决定合适的样本。

8.3　分析数据的规模有哪些具体的要求

「分析数据的规模」与 8.2 节介绍的「抽样」有很大的关联性，但是，抽样的目的主要是降低数据集的规模，而本节要探讨的「分析数据的规模」，主要是指用于数据分析挖掘建模时最起码的数据规模大小。在数据挖掘实践中，如果分析数据太少，是不适合进行有价值的分析挖掘的。那么，对于分析数据的规模有没有一个大致的经验判断呢？

分析数据的规模，重点是考量目标变量所对应的目标事件的数量。比如在银行信用卡欺诈预警模型里，目标事件就是实际发生了信用欺诈的案例或者涉嫌欺诈的信用卡用户，而目标事件的数量就是分析样本中实际的欺诈案例的数量或者涉嫌欺诈的信用卡的用户数量。一般情况下，数据挖掘建模过程会将样本划分为 3 个子样本集，分别为训练集（Training Set）、验证集（Validation Set）、测试集（Testing Set）。不过，在具体的挖掘实践中，根据样本数量的大小，有时候也可以只将样本划分为两个子集，即训练集和验证集，对于模型的实践验证，通常是通过另外的时间窗口的新数据来进行测试的。相对来说，训练集的样本数量要比验证集的数量更多。训练集的数据量大概应该占到样本总数据量的 40%～70%。在理想的状况下，训练集里的目标事件的数量应该有 1000 个以上，因为在太少的目标事件样本基础上开发的模型很可能缺乏稳定性。但是，这些经验上的参考数据并不是绝对的，在数据挖掘的项目实践中数据分析师需要在这些经验值与实际的业务背景之间做出权衡或进行折中。比如，如果训练集里的目标事件数量少于 1000 个，只要分析师根据业务判断觉得可行也是可以进行分析挖掘的，只是需要更加关注模型的稳定性的检验。

另外，预测模型的自变量一般应控制在 8～20 个之间，因为太少的自变量会给模型的稳定性造成威胁，而任何一个自变量的缺失或者误差都可能引起模型结果的显著变动。但是，太多的自变量也会让模型因为复杂而变得不稳定。

前面说过，训练集里目标事件最好要在 1000 个以上，在此基础上，训练集样本的规模一般应该在自变量数量的 10 倍以上，并且被预测的目标事件至少是自变量数目的 6～8 倍。

正如之前所强调的，上述的参考数据源于经验，仅供参考。在数据挖掘实践中，数据分析师还应该综合考虑实际的业务背景和实际的数据质量、规模来进行综合的判断。

8.4　如何处理缺失值和异常值

如果说前面的 3 节内容谈到的数据处理问题并不会在每个分析场景都能明显引起分析师的关注，那么本节讨论的「数据缺失和异常值」却是几乎在每个数据分析、挖掘实践中，分析师都会碰到的、最常见的数据问题。

8.4.1　缺失值的常见处理方法

在数据分析挖掘实践中，数据样本里的数据缺失是常见的现象，而这其中有的是数据存储错误造成的，有的是原始数据本身就是缺省的，比如用户登记的信息不全。在大多数情况下，数据分析师需要对缺失数据进行处理；在个别情况下，比如应用决策树算法的时候，该算法本身允许数据缺失值直接进入分析挖掘，因为在这种情况下缺失值本身已经被看做是一个特定的属性类别了。下列一些方法是数据分析师常用的处理数据缺失值的方法：

❑数据分析师首先应该知道数据缺失的原因，只有知根知底，才可以从容、正确地处理缺失值。不同数据的缺失有不同的原因，因此也应该有不同的解读和解决方法，而不应该一概而论，眉毛胡子一把抓。举例来说，如果用户问卷里的缺失值是因为被调查者漏掉了一个问题选项，那么这个缺失值代表了用户没有回答该问题；而一个信用卡激活日期的缺失，不能表明是「丢失」了信用卡的激活日期，按照系统的计算逻辑来看，凡是还没有激活的信用卡，其激活日期都是记为缺失的，即 NULL；还有的缺失是因为系统本身的计算错误造成的，比如某个字段除以零，某个负数取对数等错误的数学运算。上述 3 种缺失场景有着完全不同的缺失原因，所代表的意义也不同，分析师只有真正找到了缺失的原因，才可以有的放矢，并采取相应的对策进行有效处理。

❑分析师基于数据缺失的原因进行正确查找后，还要对于数据的缺失进行判断。这种数据缺失是本身已经具有特定的商业意义呢？还是的确需要进行特别的处理。在上面所列举的 3 种完全不同的缺失原因里，很明显，信用卡激活日期的缺失其本身是具有特定商业意义的，这种缺失代表了该用户还没有激活信用卡，这个商业的含义非常明确，已经不用对此类缺失进行任何处理了；而诸如系统本身的计算错误所造成的缺失，比如，某个字段除以零，或某个负数取对数，就应该采取相应的措施修正计算错误，比如，重新定义计算逻辑，或者采用后面提到的一系列的处理方法。

❑直接删除带有缺失值的数据元组（或观察对象）。这种操作手法最大的好处在于删除带缺失值的观察对象后，留下来的数据全部是有完整记录的，数据很干净，删除的操作步骤也很简单方便。但是，此种操作手法最大的不足在于，如果数据缺失的比例很大，直接删除带有缺失值的观察值后剩下的用于分析挖掘用的数据集可能会太少，不足以进行有效的分析挖掘；其次，直接删除含有缺失值的观察对象很可能会丢失一些重要的信息，因为这些被删除的观察对象还可能包含了很多没有缺失的别的字段或者变量的属性，这些属性或者数据也是很有意义的；另外，在建模完成后进行业务应用时，如果用来打分的新数据也带有缺失值，那么先前完全基于不带缺失值的分析样本所搭建起来的预测模型，面对这些数据进行打分预测时，很有可能无法对此进行打分赋值。所以，直接删除带有缺失值的观察对象的方法只适用于建模样本里缺失值比例很少，并且后期打分应用中的数据的缺失值比例也很少的情况。

❑直接删除有大量缺失值的变量。这种方法是针对那些缺失值占比超过相当比例变量，比如缺失值超过 20% 或者更多的情况。但是采用这种方法之前需要仔细考虑，这种大规模的缺失是否有另外的商业背景和含义，比如前面提到的信用卡激活日期的缺失实际上表明这些用户还没有激活信用卡，那么这群用户是属于另外一个类别，即还未激活的用户群体，在这种情况下，轻率地删除就会丢失这群用户的重要信息，得不偿失。

❑对缺失值进行替换（Substitute）。这种方法包括利用全集中的代表性属性，诸如众数或者均值等，或者人为定义的一个数据去代替缺失值的情况。具体来说，包括：对于类别型变量（Category）而言，用众数或者一个崭新的类别属性来代替缺失值；对于次序型变量（Ordinal）和区间型变量（Interval）而言，用中间值、众数、最大值、最小值、用户定义的任意其他值、平均值或仅针对区间型变量来代替缺失值。上述对缺失值进行替换的做法最大的好处在于简单、直观，并且有相当的依据，比如说，众数本身就说明了该值出现的几率最大。但是，不管怎么说，这种替换毕竟是人为的替换，不能完全代表缺少数据本身真实的含义，所以也属于「不得已而为之」的策略。

❑对缺失值进行赋值（Impute）。这种方法将通过诸如回归模型、决策树模型、贝叶斯定理等去预测缺失值的最近替代值，也就是把缺失数据所对应的变量当做目标变量，把其他的输入变量作为自变量，为每个需要进行缺失值赋值的字段分别建立预测模型。从理论上看，该种方法最严谨，但是成本较高，其包括时间成本和分析资源的投入成本。是否采用该方法，取决于具体数据挖掘的业务背景、数据资源质量以及需要投入的力度。

8.4.2　异常值的判断和处理

数据样本中的异常值（Outlier）通常是指一个类别型变量（Category）里某个类别值出现的次数太少、太稀有，比如出现的频率只占 0.1% 或更少，或者指一个区间型变量（Interval）里某些取值太大，比如，互联网买家用户最近 30 天在线购买的交易次数，个别用户可以达到 3000 次，平均每天购买 100 次，相比数据全集里该字段均值为 2 次而言，这里的 3000 交易次数就属于异常值。

通常来讲，如果不把异常值清理掉，对于数据分析结论或者挖掘模型效果的负面影响是非常大的，很可能会干扰模型系数的计算和评估，从而严重降低模型的稳定性。

对于异常值的判断内容如下：

❑对于类别型变量（Category）来说，如果某个类别值出现的频率太小，太稀有，就可能是异常值。具体拿经验值来参考，一般某个类别值的分布占比不到 1% 或者更少就很可能是异常值了。当然，这还需要数据分析师根据具体项目的业务背景和数据实际分布作出判断和进行权衡。有些情况下，纵然某个类别值的占比很少，但是如果跟目标变量里的目标事件有显著的正相关关系，这种稀有类别值的价值就不是简单的异常值所可以代表的。

❑对于区间型变量（Interval）来说，最简单有效的方法就是把所有的观察对象按照变量的取值按从小到大的顺序进行排列，然后从最大的数值开始倒推 0.1% 甚至更多的观察值，这些最大的数值就很可能属于异常值，可再结合业务逻辑加以判断。另外一个常用的判断异常值的方法就是以「标准差」作为衡量的尺度，根据不同的业务背景和变量的业务含义，把超过均值 n 个标准差以上的取值定义为异常值，这里 n 的取值范围取决于具体的业务场景和不同变量的合理分布，比如超过均值在正负 4 个标准差以上的数值就要认真评估，确定其是否是异常值。

对于异常值的处理相对来说就比较简单，主要的措施就是直接删除。

需要提醒读者的是，在数据挖掘实践中，对于「异常值」的处理是辩证的，在多数情况下，异常值的删除可以有效降低数据的波动，使得处理后的建模数据更加稳定，从而提高模型的稳定性。但是，在某些业务场景下，异常值的应用却是另一个专门的业务方向。比如在前面章节里提到的信用体系中的恶意欺诈事件，从数据分析的角度来看那也是对异常值的分析挖掘应用。对这些有价值的异常值的分析应用包括利用聚类分析技术识别异常值，利用稀有事件的预测模型搭建去监控、预测异常值出现的可能性等。这些应用，将在第 9 章和第 10 章专门进行介绍。

8.5　数据转换

对于数据挖掘分析建模来说，数据转换（Transformation）是最常用、最重要，也是最有效的一种数据处理技术。经过适当的数据转换后，模型的效果常常可以有明显的提升，也正因为这个原因，数据转换成了很多数据分析师在建模过程中最喜欢使用的一种数据处理手段。另一方面，在绝大多数数据挖掘实践中，由于原始数据，在此主要是指区间型变量（Interval）的分布不光滑（或有噪声）、不对称分布（Skewed Distributions），也使得数据转化成为一种必需的技术手段。

按照采用的转换逻辑和转换目的的不同，数据转换主要可以分为以下四大类：

❑产生衍生变量。

❑改善变量分布特征的转换，这里主要指对不对称分布（Skewed Distributions）所进行的转换。

❑区间型变量的分箱转换。

❑针对区间型变量进行的标准化操作。

8.5.1　生成衍生变量

这类转换的目的很直观，即通过对原始数据进行简单、适当的数学公式推导，产生更加有商业意义的新变量。举个简单的例子，在对原始数据中的用户出生年月日进行处理时，把当前的年月日减去用户出生年月日，得到一个新的字段「用户年龄」，这个新的字段作为一个区间型变量（Interval）明显比原始变量用户出生年月日要更有商业含义，也更加适合进行随后的数据分析建模应用。一般常见的衍生变量如下。

❑用户月均、年均消费金额和消费次数。

❑用户在特定商品类目的消费金额占其全部消费金额的比例。

❑家庭人均年收入。

❑用户在线交易终止的次数占用户在线交易成功次数的比例。

❑用户下单付费的次数占用户下单次数的比例。

从中不难发现，得到这些衍生变量所应用到的数学公式都很简单，但是其商业意义都是很明确的，而且跟具体的分析背景和分析思路密切相关。

衍生变量的产生主要依赖于数据分析师的业务熟悉程度和对项目思路的掌控程度，是数据分析师用思想创造出来的「艺术品」。如果没有明确的项目分析思路和对数据的透彻理解，是无法找到有针对性的衍生变量的。

8.5.2　改善变量分布的转换

在数据挖掘实践中，大多数区间型变量（Interval）原始分布状态偏差都较大，而且是严重不对称的。这种大偏度，严重不对称的分布出现在自变量中常常会干扰模型的拟合，最终会影响模型的效果和效率，如图 8-1 所示。如果通过各种数学转换，使得自变量的分布呈现（或者近似）正态分布，并形成倒钟形曲线，如图 8-2 所示，那么模型的拟合常常会有明显的提升，转换后自变量的预测性能也可能得到改善，最终将会显著提高模型的效果和效率。

图　8-1　某区间型变量的原始分布图（明显的偏差大，严重不对称）

图　8-2　变量经过取对数的转换，呈现倒钟形的正态分布图

常见的改善分布的转换措施如下：

❑取对数（Log）。

❑开平方根（Square Root）。

❑取倒数（Inverse）。

❑开平方（Square）。

❑取指数（Exponential）。

8.5.3　分箱转换

对于区间型变量（Interval），除了进行上面提到的改善分布的转换措施之外，还可以进行另外的转换尝试，即分箱转换。

分箱转换（Binning）就是把区间型变量（Interval）转换成次序型变量（Ordinal），其转换的主要目的如下：

❑降低变量（主要是指自变量）的复杂性，简化数据。比如，有一组用户的年龄，原始数据是区间型的，从 20～80 岁，每 1 岁都是 1 个年龄段；如果通过分箱转换，每 10 岁构成 1 个年龄组，就可以有效简化数据。

❑提升自变量的预测能力。如果分箱恰当，是可以有效提升自变量和因变量的相关性的，这样就可以显著提升模型的预测效率和效果；尤其是当自变量与因变量之间有比较明显的非线性关系时，分箱操作更是不错的手段，可用于探索和发现这些相关性；另外，当自变量的偏度很大时，分箱操作也是值得积极尝试的方法。

从上面的分析可以看出，分箱操作的价值与改善分布转换的价值类似，都是努力提升自变量的预测能力，强化自变量与因变量的线性（或非线性）关系，从而可以明显提升预测模型的拟合效果。两者有异曲同工之处，在数据挖掘实践中，经常会对这两种方式分别进行尝试，择其优者而用之。

8.5.4　数据的标准化

数据的标准化（Normalization）转换也是数据挖掘中常见的数据转换措施之一，数据标准化转换的主要目的是将数据按照比例进行缩放，使之落入一个小的区间范围之内，使得不同的变量经过标准化处理后可以有平等分析和比较的基础。

最简单的数据标准化转换是 Min-Max 标准化，也叫离差标准化，是对原始数据进行线性变换，使得结果在 [0,1] 区间，其转换公式如下：

其中，max 为样本数据的最大值，min 为样本数据的最小值。

关于数据的标准化转换，将在 9.3.2 节详细介绍。

总地来说，数据转换的方式多种多样，操作起来简单、灵活、方便，在实践应用中的价值也是比较明显的。但是，它也有缺点，其中主要的缺点在于，在具体的数据挖掘实践中有些非线性转换如 Log 转换、平方根转换、多次方转换等的含义无法用清晰的商业逻辑和商业含义向用户（业务应用方）解释。比如，你无法解释「把消费者在线消费金额取对数」在商业上是什么意思，这在一定程度上影响了业务应用方对模型的接受程度和理解能力。

当然，瑕不掩瑜，毕竟预测模型的最终目的是预测的准确度和精确度，数据转换在商业解释中的这点小小的遗憾当然无损其在强大的数据处理中的重要价值。

8.6　筛选有效的输入变量

虽然「筛选有效的输入变量」属于模型搭建的技术问题，可以放在后面有关模型搭建的章节里做专门的介绍，但是这个问题在很大程度上也会涉及数据的清洗、整理、探索等数据处理的技巧，所以这里将「筛选有效的输入变量」作为数据处理技巧来进行深入讲解。

不同类型的模型对于输入变量的要求各不相同，在本书涉及的各种模型和各种项目中，鉴于预测（响应）和分类模型所涉及的变量的筛选最为复杂，最为常见，所以本节将聚焦预测（响应）和分类模型中的输入变量筛选进行深入讲解，至于聚类中的变量筛选将在 9.3.3 节做深入讲解，其他类型的模型和应用中的输入变量筛选相对来说非常直观和简单，将在相应章节中进行讲解。

8.6.1　为什么要筛选有效的输入变量

为什么要筛选有效的输入变量？有以下 3 个方面的理由：

❑筛选有效的输入变量是提高模型稳定性的需要。过多的输入变量很可能会带来干扰和过拟合等问题，这会导致模型的稳定性下降，模型的效果变差。所以，优质的模型一定是遵循输入变量少而精原则的。

❑筛选有效的输入变量是提高模型预测能力的需要。过多地输入变量会产生共线性问题，所谓共线性是指自变量之间存在较强的，甚至是完全的线性相关性。当自变量之间高度相关时，数据的小小变化，比如误差的发生都会引起模型参数严重震荡，明显降低模型的预测能力，关于共线性问题，将在 8.6.3 节做详细介绍。并且，共线性的发生也增加了对模型结果的解释困难，因为要更深入地分析和判断每个自变量对目标变量的影响程度。

❑当然，筛选有效的输入变量也是提高运算速度和运算效率的需要。

在采取各种评价指标筛选有价值的输入变量之前，可以先直接删除明显的无价值的变量，这些明显的无价值变量包括的内容如下：

❑常数变量或者只有一个值的变量。

❑缺失值比例很高的变量，比如缺失值高达 95%，或者视具体业务背景而定。

❑取值太泛的类别型变量，最常见的例子就是邮政编码，除非采取进一步措施将各个地区的编码整合，减少类别的数量，否则原始的邮政编码数据无法作为输入变量来提供起码的预测功能。

8.6.2　结合业务经验进行先行筛选

这是所有筛选自变量的方法中最核心、最关键、最重要的方法。在本书之前讲解的内容中也反复强调了业务经验和业务判断对数据挖掘的重要影响。正如数据挖掘商业实战的其他各个环节一样，筛选自变量的环节也应该引进业务专家的意见和建议，很多时候业务专家一针见血的商业敏感性可以有效缩小自变量的考察范围，准确圈定部分最有价值的预测变量，从而提高判断和筛选的效率。

另一方面，业务经验和业务专家的建议难免碎片化，也可能难以面面俱到，更关键的是业务经验和业务专家的建议也需要数据进行科学的验证。所以，在本章的后面的内容中，将详细介绍在数据挖掘实战领域里比较成熟、有效的方法和指标，用于筛选目标变量。在这里要强调的是，下面的具体介绍主要是从原理和算法上进行剖析的，读者只需要从思想上知道并了解这些方法背后的原理就可以了。在实战操作中，不需要大家运用这些最基础的公式进行繁琐的计算。目前有很多成熟的数据挖掘分析软件能够把这些繁琐的计算工作完成得很出色。作为数据分析人员只需要知道其中的原理、思路、分析方法就可以了。当然只有真正从思想上理解并掌握了这些具体的原理和思路，才可以在数据挖掘商业实战中游刃有余，得心应手；如果仅仅知其然，不知其所以然，在具体的数据挖掘商业实战中将会举步维艰，束手无策。

8.6.3　用线性相关性指标进行初步筛选

最简单、最常用的方法就是通过自变量之间的线性相关性指标进行初步筛选。其中，尤以皮尔逊相关系数（Pearson Correlation）最为常用。Pearson 相关系数主要用于比例型变量与比例型变量、区间型变量与区间型变量，以及二元变量与区间型变量之间的线性关系描述。其计算公式如下：

线性相关性的相关系数 r 的取值范围为 [-1，+1]，根据经验来看，不同大小的 r，表示不同程度的线性相关关系。

❑|r|＜0.3, 表示低度线性相关。

❑0.3≤|r|＜0.5，表示中低度线性相关。

❑0.5≤|r|＜0.8，表示中度线性相关。

❑0.8≤|r|＜1.0，表示高度线性相关。

在建模前的变量筛选过程中，如果自变量属于中度以上线性相关的（＞0.6 以上）多个变量，只需要保留一个就可以了。

上述相关系数的计算公式只是从状态上计算了变量之间的相关关系，但是相关系数是通过样本数据得到的计算结果，来自样本的统计结果需要通过显著性检验才能知道其是否适用于针对总体数据的相关性。关于类似的统计显著性问题，作为统计分析中的基本知识，不在本书的讨论范围之内，并且在目前所有的分析软件里都可以自动计算，有心的读者可以自己在实践中进行体会和学习。

需要强调的是，有时候尽管上述公式计算出来的相关系数 r 等于 0，也只能说明线性关系不存在，不能排除变量之间存在其他形式的相关关系，比如曲线关系等。

尽管线性相关性检验是模型的变量筛选中最常用也最直观的有效方法之一，但是在很多时候，某个自变量和因变量的线性相关性却很小，这时可以通过跟其他自变量结合在一起而让其成为预测力很强的自变量。正因为如此，在挑选输入变量的时候，应该多尝试不同的评价指标和不同的挑选方法，减少因采用单一方法而导致的误删除，避免在一棵树上吊死的情况发生。

8.6.4　R 平方

R 平方（R-Square），也叫做 R2 或 Coefficient of Multiple Determination，该方法将借鉴多元线性回归的分析算法来判断和选择对目标变量有重要预测意义及价值的自变量。

最通俗的解释，R2 表示模型输入的各自变量在多大程度上可以解释目标变量的可变性，R2 的取值范围在 [0,1] 之间，R2 越大，说明模型的拟合越好。R2 的计算公式如下：

在上述 R2 公式中，R2 表示回归方程拟合的好坏，R2∈(0,1)，R2 越大表示回归方程同样本观测值的拟合程度越好。R 又被称为因变量 Y 与自变量 X1,X2,…,Xp 的样本复相关系数，它表示整体的 X1,X2,…,Xp 和 Y 的线性关系。

在 R2 计算公式中：

yi 表示目标变量的真实值；

fi 表示模型的预测值；

表示目标变量真实值的均值；

SSE 称为残差平方和，自由度为 P，P 代表自变量的个数；

SST 称为总平方和，自由度为 N-1，N 代表样本数量；

SSR 称为回归平方和，自由度为 N-P-1。

总平方和 SST 反映了因变量（目标变量）Y 的波动程度，SST 是由回归平方和 SSR 和残差平方和 SSE 两部分组成的。其中，回归平方和 SSR 是由解释变量，即自变量，输入变量 X 所引起的，残差平方和 SSE 是由其他随机因素所引起的。

在回归方程中，回归平方和越大，回归效果越好，因此可构造如下的统计量：

在零假设 H0:β1=β2=…βp=0 成立时（β 为各自变量在回归方程中的回归系数），统计量服从自由度为（p,N-p-1）的 f 分布。如果给定显著水平 α，则否定域为 F＞F1-α（p,N-p-1）。

当 F 值没有落在否定域之中时，零假设 H0:β1=β2=…=βp=0 成立，表明解释变量（自变量）X1,X2,…,Xp 对因变量（目标变量）Y 的多元线性回归不成立，X1,X2,…,Xp 与 Y 之间没有显著的线性关系。

对于每个自变量 Xi 做偏回归显著性检验，其公式为：，其中，SSR-i 为剔除变量 Xi 之后的回归平方和，SSR-SSR-i 反映了在引入 Xi 之后，Xi 对于回归平方和的贡献。

分别检查各自变量的 Fi 是否都大于相应的 F0.05。

如果全部 Fi 都大于 F0.05，则结束。

如果经检查发现有几个自变量的 Fi 小于 F0.05，则每次只能删除其中的一个 Xi，这个 Xi 是所有自变量中其 Fi 最无显著性的，然后再重新用剩下的自变量进行回归的构建，如此反复，直到所有的有显著性意义的自变量都进入回归方程，而没有显著性意义的变量都被剔除为止。

8.6.5　卡方检验

卡方检验（Chi-Square Statistics）在统计学里属于非参数检验，主要用来度量类别型变量，包括次序型变量等定性变量之间的关联性以及比较两个或两个以上的样本率。其基本思想就是比较理论频数和实际频数的吻合程度或拟合度。作为数据挖掘中筛选自变量的重要方法，卡方检验主要是通过类别型目标变量，最常见的就是二元目标变量，0,1 与类别型自变量之间的关联程度来进行检验的，关联性大的类别型自变量就有可能是重要的自变量，可以通过初步的筛选进入下一轮的考察。卡方检验的公式如下：

其中，表示各交叉分类频数的观测值，表示各交叉分类频数的期望值，各交叉分类频数观测值与期望值的偏差为。

当样本量较大时，X2 统计量近似服从自由度为 (R-1)(C-1) 的 X2（卡方）分布。从上述公式可以看出，X2 的值与期望值、观测值和期望值之差有关，X2 值越大表明观测值与期望值的差异越大，相对应的 P-Value 就越小，而 P-Value 代表的是上述差异发生的偶然性。所以，通常讲，如果 P-Value 值的小于 0.01，同时 X2，即是卡方（Chi-Square）比较大，则说明可以拒绝该自变量与因变量之间相互独立的原假设，也就是说该类别型自变量与目标变量之间有比较强的关联性，因此可以认为该自变量可能值得输入模型。

8.6.6　IV 和 WOE

当目标变量是二元变量（Binary），自变量是区间型变量（Interval）时，可以通过 IV（Information Value）和 WOE（Weight of Evidence）进行自变量的判断和取舍。在应用 IV 和 WOE 的时候，需要把区间型自变量转换成类别型（次序型）自变量，同时要强调的是目标变量必须是二元变量（Binary），这两点是应用 IV 和 WOE 的前提条件。

举例来说，在一个「预测用户是否在信用卡使用上有信用欺诈嫌疑」的项目里，目标变量是「是否存在信用欺诈行为」，是个二元变量（0,1），0 代表没有欺诈，1 代表有欺诈；同时，自变量里有一个字段「用户的年收入」，在数据仓库的原始记录里，该字段「用户的年收入」是属于区间型变量（Interval）的，如果采用 WOE 和 IV 的指标方法判断其是否具有预测价值，即是否适合作为自变量放进模型里去预测，就需要先把这个区间型的变量「用户的年收入」进行转换，使其变成类别型变量（次序型变量），比如「分箱」成为具有 4 个区间的类别型变量，且这些变量分别为小于 20 000 元、[20 000,60 000)、[60 000,100 000），以及 100 000 元以上，共 4 类。

上述举例中的 4 类区间，又称为变量「用户的年收入」的 4 个属性（Attribute），针对每个属性（Attribute），可以计算样本数据里的 WOE，公式如下：

其中

在上述公式中，和分别代表在该属性值里，样本数据所包含的预测事件和非事件的数量；nevent 和 Nnonevent 分别代表在全体样本数据里所包含的预测事件和非事件的总量。

而一个变量的总的预测能力是通过 IV（Information Value）来表现的，它是该变量的各个属性的 WOE 的加权总和，IV 代表了该变量区分目标变量中的事件与非事件的能力，具体计算公式如下。

与 IV 有相似作用的一个变量是 Gini 分数（Gini Score），Gini 分数的计算步骤如下：

1）根据该字段里每个属性所包含的预测事件（Event）与非事件（Nonevent）的比率，按照各属性的比率的降序进行排列。比如，该字段共有 m 个属性，排序后共有 m 个组，每个组对应一个具体的属性，第一组就是包含预测事件比率最高的那个组。

2）针对排序后的每个组，分别计算该组内的事件数量和非事件数量。

3）计算 Gini 指数，其公式如下：

上述公式中，Nevent 和 Nnonevent 分别代表样本数据里总的事件数量和非事件数量。

总体来说，应用 IV、WOE、Gini Score3 个指标时，可以在数据挖掘实践中实现以下目标：

❑通过 WOE 的变化来调整出最佳的分箱阀值。通常的做法是先把一个区间型变量分成 10～20 个临时的区间，分别计算各自的 WOE 的值，然后根据 WOE 在各区间的变化趋势，做相应的合并，最终实现比较合理的区间划分。

❑通过 IV 值或者 Gini 分数，筛选出有较高预测价值的自变量，投入模型的训练中。

8.6.7　部分建模算法自身的筛选功能

除了上述这些具体的、直接的指标计算和参考的方法之外，在数据挖掘商业实战中，还有一种「借力」的巧妙方法，那就是借助于一些成熟的算法进行初步的运算，利用模型的初步结果筛选出有价值的自变量，再把这些经过初期过滤的自变量放进模型和算法中进行真正意义上的建模和验证工作。

可供「借力」的算法或者模型包括决策树模型、回归（含线性回归和逻辑回归）模型等，在建模前期的变量筛选阶段，借力可以帮助初选出有价值的自变量。需要强调的是，在这些场景中，这些算法工具和模型可能无法实现最终的预测（分类）功能，而仅仅是用作自变量的初步筛选。

比如线性回归和逻辑回归，算法本身通过不断地增加或者剔除变量，来检验各输入变量对于预测的价值，这就是所谓的 Stepwise 算法，但是，即便如此，最好在使用之前先进行人为的初步筛选，从而把精简后的变量交给算法去选择。在大数据量建模的时候尤其要如此。

8.6.8　降维的方法

在数据挖掘的实战中，面对数量庞大的原始变量，除了上述种种指标及思路外，还有一种方法也会经常被应用，那就是数据降维，具体来说，包括主成分分析和变量聚类等。其中，对于主成分分析，已在 2.3.8 节中进行了详细介绍；对于变量聚类，将在 8.7 节的共线性问题中做专门介绍。

通过采取降维的措施和方法，可以有效精简输入变量的数目，在一定程度上实现有效筛选模型输入变量的目标。

8.6.9　最后的准则

本节到目前为止，谈到了数据挖掘实战中常见的筛选输入变量的各种方法和原理，这些分析技术层面的技巧和工具的熟练应用可以有效提高我们筛选输入变量的效率和质量。但是，业务环境千差万别，应用场景纷繁复杂，很多时候我们既要考虑技术层面的指标及判断方法，同时又要受实战环境中诸多因素的影响和制约，包括时间、资源、成本和目标等。

有些时候，尽管通过上述的分析技术可发现某个变量很重要，但是具体实战中也可能会选择放弃，个中的原因可能会涉及环境因素，比如说该变量的收集要花费太长的时间，或者花费过多的成本，那么权衡下来，就有可能放弃该变量。毕竟，只要最终的模型能满足初期的业务需求就可以了，模型的优化和提升是需要兼顾和权衡其他因素的制约的。

既要贯彻落实上述种种有效的筛选输入变量的方法和原理，又要在数据挖掘商业实战中综合考虑诸多环境因素和制约条件，并加以权衡和折中，这就是筛选输入变量的方法和原理中最后的准则。这个准则体现了筛选变量的过程是个辩证的、丰富多彩的、充满活力的过程，体现了数据分析挖掘强大的生命力和勃勃生机。

8.7　共线性问题

共线性问题是困扰模型预测能力的一个常见问题。所谓共线性，又叫多重共线性，是指自变量之间存在较强的，甚至完全的线性相关关系。当自变量之间高度相关时，模型参数会变得不稳定，模型的预测能力会降低。同时，严重的共线性增加了对于模型结果的解释成本，因为它致使很难确切分辨每个自变量对因变量的影响。所以，在建模前期的变量筛选环节，就要对共线性问题引起足够的重视，并采取有效措施尽量加以避免。

需要强调的是，理论上来讲，输入变量之间除了存在共线性之外，完全可能存在其他各种非线性的关系，这些非线性的关系也很可能如共线性一样影响模型的预测能力。但是，我们无法完全掌握这些非线性关系，所以，只能以考察它们之间的线性关系为基础来排除一些主要的线性关系的变量。

8.7.1　如何发现共线性

常见的识别共线性的方法如下：

❑相关系数的方法。最常见的就是皮尔逊相关系数（Pearson Correlation），详细内容请参考 8.6.3 节，对于线性相关指标的详细讨论。

❑通过模型结论的观察。比如，在回归模型中，如果回归系数的标准差过大，就可能意味着变量之间存在着共线性问题。

❑主成分分析方法。在主成分分析方法中，主成分里的系数，也就是主成分载荷大小能从一定程度上反映出各个变量的相关性。比如，第一主成分中，某几个原始变量的主成分载荷系数较大，且数值相近，就有可能在其中隐藏着共线性问题。

❑根据业务经验判断的原本应该没有预测作用的变量突然变得有很强的统计性，那其中就有可能隐藏着共线性问题。

❑对变量进行聚类。通过对区间型变量进行聚类，同一类中的变量之间具有较强的相似性，也就可能隐藏着共线性问题。

8.7.2　如何处理共线性

水至清则无鱼，人至察则无徒，对于数据挖掘实战中出现的共线性问题，也需要本着中庸之道灵活处理。轻微的共线性是可以容忍的。比如说模型拟合度较高，样本量大的时候，轻微的共线性可以适当的采用视而不见的方法。但是，当样本量较少，很轻微的共线性问题都有可能导致参数的不稳定。如果发生严重的共线性问题，一般采取以下措施：

❑对相关变量进行取舍。高度共线性的相关变量，可以选择保留对业务方最有价值、最有意义的变量，而过滤掉相关变量。

❑对相关变量组合，生成一个新的综合性变量。

❑当我们利用相关变量通过线性的方式衍生出新的变量时，要记得两者之间的共线性问题，并且及时删除相关的原始变量，不要将其投入到模型中。在实践应用中这种情况会经常出现，也很容易被人忽视。

❑尝试对相关变量进行一些形式的转换（参考 8.5 节），恰当的转换可以在一定程度上减少甚至去除共线性关系。