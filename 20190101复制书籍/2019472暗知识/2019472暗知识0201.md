# 第四章 逐鹿硅谷——AI 产业争霸战

导读

这一章不谈理论和技术，只谈AI的产业生态。对于想抓住AI时代投资机会的人，这章提供了对AI产业和商业的一个基础理解。没有读过前面章节的读者也可以直接读这一章。





最新技术巨浪


人工智能毫无疑问是继移动互联网之后的一次超级大浪，其规模和影响至少是互联网级别的。这次创新大浪启动的标志性事件是2012年的ImageNet比赛。ImageNet目前有1 400万张图片，其中上百万张有文字标注。标注的文字通常是用短语描述该图片的内容（例如“草地上卧着的一条黄狗”）。ImageNet的比赛主要是看谁的程序能够最准确地识别出图片的内容。在2012年以前，识别主要是人工选择物体特征并且写出识别这些特征的程序，准确率的最高水平一直在74%左右徘徊。2012年，亚历克斯·克里捷夫斯基使用多层神经网络AlexNet一举把识别率提高了10个百分点，达到84%。

这个突破性结果立即引起了产业界的强烈兴趣和关注。谷歌大脑的负责人杰夫·迪恩（Jeff Dean）敏锐地发现了这个重大机遇，他用了一年的时间说服了谷歌当时的CEO（首席执行官）兼创始人拉里·佩奇（Larry Page），开始全公司大举转型AI，随后Facebook、微软、百度等科技巨头纷纷跟进。如图4.1所示，在今后几年，神经网络不断提高识别的准确率，终于在2015年达到96%的准确率，超过了人类所能达到的95%。这些突破证明了机器学习可以开始解决实际问题，也让工业界认识到了巨大的商业潜力。但这个突破来之不易，AI走过了60年的艰辛道路。

图4.1 2010—2015年ImageNet大赛历年识别准确率

图片来源：http://yann.lecun.com/。





AI突破三要素


AI发展了60年，为什么到今天能够突破？这是由于长期积累的三个条件成熟了。

第一个条件是计算能力。计算能力和半导体的集成度（在单位半导体材料面积上可以集成的晶体管的数量）直接相关。从第一个集成电路晶体管诞生以来，在过去的50年中，半导体的集成度的增加速度基本遵循“摩尔定律”。1965年4月19日，《电子学》杂志（Electronics）发表了仙童半导体公司工程师戈登·摩尔（Gordon Moore，后成为英特尔的创始人之一）撰写的文章《让集成电路填满更多的组件》，文中预言半导体芯片上集成的晶体管和电阻数量将每年增加一倍。1975年，摩尔在IEEE（电竞和电子工程师协会）国际电子组件大会上提交了一篇论文，根据当时的实际情况对摩尔定律进行了修正，把“每年增加一倍”改为“每两年增加一倍”，而这个定律经过传播演化，变成今天普遍流行的说法“计算机运算速度每18个月提升一倍，价格每18个月下降一半”。1970年一个芯片上的晶体管数量约为1 000个，今天一个芯片上的晶体管数量达到100亿个，不到50年中提高了1 000万倍。相应地，计算能力也提高了1 000万倍。目前虽然单个芯片的晶体管数量增加速度放缓，但人们开始把成百上千个芯片封装在一起以便提高总的计算速度。

计算能力对人工智能的巨大推动还体现在一个标志性事件上——GPU（图形处理器）被用于训练AI算法。2009年，斯坦福大学计算机系教授吴恩达和他的博士生拉加特·蓝恩纳（Rajat Raina）第一次提出由于神经网络中大量计算可以并行，用一个GPU可以比双核CPU快70倍，原来需要几周完成的计算一天就可以完成。之后纽约大学、多伦多大学及瑞士人工智能实验室纷纷在GPU上加速其深度神经网络。赢得2012年ImageNet竞赛的AlexNet同样用的也是GPU。从此之后，GPU就在神经网络的训练和识别中树立了公认的王者地位。再到后来AlphaGo发威战胜人类顶级围棋手，背后则是谷歌自行研发的专为深度学习使用的TPU发挥了重要支撑，每个TPU可以提供10倍于GPU的计算能力。在本章中将会详细分析为什么TPU比GPU快。

第二个条件是数据。如果说算法是火箭发动机，那么数据就是燃料。由于互联网的发展和各类传感器（例如在各种环境中的温度、位置、压力等物理化学变量的测量，社会中大量摄像头的存在）成本的大幅下降和广泛安装，根据IDC（互联网数据中心）的监测统计，2011年全球数据总量已经达到1.8ZB（1ZB=1万亿GB），相当于18亿个1TB的移动硬盘，而这个数值还在以每两年翻一番的速度增长，预计到2020年全球将总共拥有35ZB的数据量，增长近20倍。

这比从人类出现到计算机出现前产生的所有数据都多。以目前的传感器技术发展速度，若干年后回头看今天的数据量，不仅量小而且数据采集的密度和广度都远远不够。

第三个条件就是那批甘愿坐“冷板凳”的科学家经过了几十年的积累，终于从2006年开始在算法上有了重大突破。当时在多伦多大学任教的辛顿教授在美国《科学》杂志和相关的期刊上发表了论文，证明了深度神经网络的能力和实用性。从此，基于多层神经网络的深度学习理论成为本轮人工智能发展的重要推动力，相当于过去飞机从达·芬奇设计的扇翅膀的飞行器变成有螺旋桨的发动机，人工智能的概念和应用开始一路攀升，语音识别、机器视觉技术在几年间便超过了人类的水平。

正是算力、数据、算法这三个要素同步成熟，形成合力，终于带来了今天AI的爆发。这三个要素中最重要的是计算能力的发展和算法的互相促进。





金字塔形的产业结构


一个产业的生态主要是指这个产业有哪些环节和这些环节之间的关系，例如哪个环节是生态的瓶颈并掌握最强的砍价能力。更深入的产业生态分析还包括各个环节的未来发展以及对整个生态的影响。AI的产业生态如图4.2所示，是一个金字塔形的结构。

图4.2AI产业生态的金字塔形结构



金字塔的下层对上层有依赖性，但反之不成立。也就是说上层是驱动力，是自变量，下层是驱动结果，是因变量。金字塔的宽度大致对应市场规模和公司的数量。所以越上层对整个行业的影响越大但市场规模越小，越下层市场规模越大但影响越小。





产业的皇冠：算法


我们前面说过，AI近年的突破性发展的三个驱动因素之一是神经网络算法的突破。其实这是三个因素中最重要的因素，因为其他两个因素（计算能力和数据量）属于“搭便车”。目前研究算法主要集中在美国的一流大学和几家超级互联网公司（谷歌、Facebook、亚马逊、微软、IBM、百度等）。大学的算法研究大部分都是学术性和公开的，而大公司的算法研究最核心的只留给自己用。专门研究算法的私人企业屈指可数，一家著名的算法公司就是被谷歌收购的大胜围棋世界冠军的DeepMind。另一家是由硅谷老将，曾经做出世界上第一台掌上电脑PalmPilot的杰夫·霍金斯（Jeff Hawkins）创办的Numenta（公司名来自拉丁文mentis，意为“心灵”）。Numenta是一个由私人资助的研究所，他们过去十几年专注于发展一种叫作层级时序记忆（Hierarchical Temporal Memory，HTM）的算法。这种算法受大脑新皮质中锥体细胞的启发，网络结构远比各种神经网络复杂。这种算法的一个特点是可以连续学习。神经网络都有一个缺陷，在模型训练完毕后，如果有新数据可以用，就必须把新数据和原来的老数据合并在一起重新训练模型。而HTM的连续学习没有这个缺陷，当新数据来了以后，只要继续把新数据喂给模型即可。HTM的第二个优势在于可以将物理世界的基本常识融入模型。Numenta并不寻求直接提供商业解决方案，而是仅仅提供算法的许可，让合作伙伴用自己的算法来解决商业问题。Numenta还提供了开源的平台，让更多的开发者在这个平台上完善HTM算法。从Numenta出来创业的威德罗教授的博士生迪利普·乔治（Dileep George）基于HTM创办了一家做机械手通用软件的公司Vicarious。相对于应用，纯粹做算法的公司少得可怜，原因主要是缺乏商业模式。





技术制高点：芯片


半导体芯片是一切信息技术的基础，有了芯片才有电脑和存储，有了电脑和存储才有互联网，有了互联网才有大数据，有了大数据才有人工智能。在这每一波的发展中，芯片都是最关键的环节，芯片厂商总是处在霸主地位。在大型机时代，能够自己开发芯片的IBM独占鳌头。在个人电脑时代，能够生产出最好的CPU的英特尔成为新的霸主。在移动通信时代，高通（Qualcomm）几乎垄断了手机芯片，直接挑战英特尔的霸主地位。在云计算大数据时代，三星（Samsung）凭借自己在存储芯片方面的优势成为世界半导体第一大厂家。在人工智能时代，谁将是新的霸主？

这个新霸主的桂冠很可能落在硅谷的半导体公司英伟达（Nvidia）头上。英伟达成立于1993年，创始人是出生于中国台湾，小时候随父母来到美国的斯坦福大学毕业生黄仁勋（Jen-sen Huang）。公司最初是做电脑图形显示卡，20多年来一直在研发销售图形显卡和图形处理芯片GPU。除了工业应用之外，图形显卡的最大市场是电脑游戏，今天高端电脑游戏里面几乎清一色用英伟达的显卡。当电脑游戏市场开始成熟后，英伟达也曾经想进入手机市场并收购过相应的公司，但是并不成功。直到2012年上天为准备好了的英伟达掉下一块“大馅饼”，这个馅饼就是我们前面提到过的2012年的ImageNet比赛。在这个比赛中取得突破的AlexNet的发明人亚历克斯就使用了英伟达的GPU，证明了GPU非常适合用于有许多并行计算的神经网络，比CPU快得多。在这之后的几年，其他人都开始采用GPU，比谁能将网络做得更大，层数更多。从此以后，GPU成了神经网络计算的引擎，相当于CPU对电脑的作用一样。

为什么GPU会成为神经网络计算的引擎？训练神经网络就相当于调黑盒子上的旋钮，调旋钮是通过数学的算法调的，这些旋钮动辄几十亿个，需要大量的计算。传统电脑用的是CPU，用CPU去调旋钮相当于调完第一个再调第二个，一个一个按顺序来，虽然现在CPU很快，但神经网络的旋钮实在太多了，连CPU都招架不住了，这时候GPU就出现了。

GPU和CPU不一样的地方是它一次可以同时调成千上万个旋钮，原来CPU几年才能调完的活GPU几天就干完了（有兴趣的读者可以看附录3中关于GPU的技术描述）。GPU的出现，让神经网络可以更大，因而处理能力更强，从一个纯学术的研究项目变为有巨大商业价值的工具。

深度学习需要用GPU的主要有两类：模型训练和识别。前者不光要处理大量训练数据，还要不断地试验不同的模型和参数，因此运算量巨大，一个训练模型可能要成百上千个GPU来算。识别的计算量少很多，但是用户多（例如谷歌、Facebook的用户都以10亿计），所以总的运算量更大，通常是模型训练的几十倍甚至上百倍。由于几乎所有的深度学习都从英伟达买GPU，所以英伟达芯片一直供不应求，其股票从2015到2017年涨了10倍。

面对如此大的权力和利润，其他公司都心有不甘。首先是英特尔不甘心被摘下霸主桂冠，开始在CPU里集成更多的核心，2017年的Xeon Phi（处理器）里面多达72个核。但CPU毕竟还要做许多其他事情，单论深度学习还是远不如同档次的GPU。先进微器件公司（Advanced Micro Device，AMD）发扬一贯的“宁做老二”的传统，在CPU上紧盯英特尔，在GPU上紧盯英伟达，永远走“功能类似，价格便宜”的路线。

其他几家互联网巨头也不想眼睁睁地看着英伟达控制着深度学习的命脉。谷歌就撸起袖子自己做了一款自用的TPU。TPU的设计思路是这样的：既然GPU通过牺牲通用性换取了在图形处理方面比CPU快15倍的性能，为什么不能进一步专注于只把神经网络需要的矩阵运算做好，进一步提高速度呢？所以TPU设计的第一个诀窍是比GPU更专注于神经网络里面计算量最大的矩阵计算，而不需要像GPU一样去顾及图形处理的许多需求。TPU的第二个诀窍是采用低精度的计算。图形与图像处理需要很高的精度（通常用32比特浮点精度），而用于识别的神经网络的参数并不需要很高的精度。

所以谷歌的第一款TPU就专门为识别设计，在运算上放弃32比特的浮点运算精度，全部采用8比特的整数精度。由于8比特的乘法器比32比特的简单4×4=16倍，所以在同等芯片面积上可以多放许多运算单元。谷歌的第一款TPU就有65 000个乘加运算单元，而最快的GPU只有5 300个单元。有兴趣的读者可以看附录3中对于CPU、GPU和TPU的详细技术分析和比较。

对于机器学习的芯片市场，不仅各大半导体厂商在攻城略地，美国、中国、欧洲至少有几十家新创公司也在摩拳擦掌。新创公司有机会吗？

神经网络芯片大致可以分成三大类。第一类是数据中心里面使用的用于训练模型和识别的芯片。目前这类芯片几乎被英伟达垄断。这类芯片客户不惜成本，不太计较耗电，只要计算速度快。这些芯片通常都用最新的半导体工艺以便集成最多的晶体管。截至本书成稿时，最先进的开始成熟商用的半导体工艺是7纳米晶圆线，使用最新的工艺成本也最高，一个芯片从研发、设计、流片、测试到量产的过程耗资动辄数十亿美元，新创公司通常没有这么多钱，风险资本也不愿意冒这么大风险。即使能生产出来，销售也是一个极大的问题，要说服大客户们使用一家没有验证过的芯片极为艰难，除非新创公司的芯片能比现有芯片快至少10倍以上。如果要比现有芯片快10倍以上，那么新创公司在设计阶段至少要比市场最快芯片快100~1 000倍，因为现有厂商也不会原地不动，也在不断地设计新一代的芯片。即使一个新芯片能快10倍，编程软件环境包括编译器、程序库等完善也需要若干年的时间。综上原因，一家新创公司想觊觎数据中心市场风险非常大。

第二类是用于汽车自动驾驶或机器人中的芯片，这类芯片的耗电不能太大，例如在电动汽车里，芯片耗电不能使电池巡航距离降低超过1%。对成本有一定的要求，计算速度也要比较快。目前英伟达的GPU是各大汽车厂商的首选，英伟达也将自动驾驶作为最重要的布局领域。

AMD、高通、英特尔都在竞争这块市场。高通曾经想以440亿美元收购荷兰半导体厂商恩智浦（NXP）公司，因为恩智浦的芯片已经广泛用在汽车的各个控制系统里。2017年3月，英特尔以153亿美元收购以色列汽车视觉公司Mobileye，从而在高级辅助驾驶系统（ADAS）市场实现领先。AMD联合了原本是英伟达合作伙伴的特斯拉开发适用于自动驾驶的AI芯片。在巨大的市场潜力的吸引下，一些新创公司也进入这个领域。但整体而言新创公司在这个市场的机会也比较小，原因类似：研发成本太高，客户（汽车公司和它们的一级供应商）大而保守等。

第三类是用于各类终端的芯片，例如用于摄像头、手机、医疗设备、小型机器人等。这类芯片要求耗电非常低（例如手机），成本非常低，在机器学习类的运算中速度只要能比CPU快10倍以上即可。新创公司在这一类芯片中机会最大。在这类终端芯片中，以手机市场最大，但进入一流手机厂商的机会也最小。例如苹果、三星、华为等一流手机厂商几乎都用自己的芯片以便提供差异化用户体验，对它们来说设计一个神经网络加速器的难度并不高。2017年苹果、华为已将AI元素融入自家芯片，发布内置AI芯片的iPhone X（苹果手机）、Mate10（华为手机）等。另外，手机芯片厂商绝不会放弃这个未来市场，高通、三星、联发科等计划在2018年也推出融入人工智能的芯片产品，其他手机厂商也会陆续使用神经网络芯片。即使是二三流的手机厂商也一定要求AI加速芯片被集成到CPU里而不是增加一个新的芯片（这样几乎不增加成本）。新创公司根本不可能开发一个全新的带AI加速器的CPU，最多只能把自己的AI加速器的设计给CPU芯片公司使用，但CPU芯片厂商都有自己设计AI加速器的能力，所以AI芯片的新创公司想进入手机市场几乎没有可能。

摄像头市场规模非常大，尤其是许多国家或城市正在打造智慧城市，AI芯片的需求潜力很难估量，很多AI芯片和算法创业公司都将安防作为最重要的落地场景之一，但该领域的集中度也很高，给安防设备厂商提供视频处理芯片的公司一定会把AI功能集成进去，安防设备龙头厂家也会研发自家的AI芯片。所以新创公司的最大机会在于为那些还不存在的或者目前规模很小的应用提供芯片，例如各种小机器人、物联网的应用。投资人要赌的是这些目前大厂看不上的市场会在短短几年内爆发。

许多读者会关心，中国的新创公司在AI芯片上有没有机会？对这个问题的回答是：上面的分析对任何一个国家的新创公司都是适用的。





生态大战——编程框架的使用和选择


在AI领域经常听到一个新技术名词叫作“编程框架”。这个编程框架和过去我们熟悉的“编程语言”“操作系统”是什么关系？简单讲，所谓编程框架就是一个程序库。这个库里有许多常用的函数或运算（例如矩阵乘法等）。这样的程序库可以大大节省编程人员的时间。这些编程框架的程序库大多都是“面向对象”的高级编程语言，例如C++（计算机程序设计语言）、Python（一种面向对象的解释型计算机程序设计语言）等。高级编程语言易于编程但效率低，低级编程语言例如汇编语言编程复杂但效率高。用这些高级编程语言写的程序库可以在各种不同的操作系统上运行，例如Linux（一种开源操作系统）、Windows（微软电脑系统）、Mac OS（苹果电脑系统）等。

作为机器学习的一个早期现象，现在有许多种不同的编程框架在竞争。其中比较著名的有TensorFlow、theano、Caffe、MXNet、CNTK、torch等。这些不同的编程框架的本质类似，通常都由以下五部分组成。





张量对象


张量就是一组多维的数据。例如一组24小时每小时平均温度的数据就是一个有24个数据的一维张量，也叫向量。一张480×640像素的黑白图像就是一个二维张量，也叫矩阵。第一个维度有480个数据，第二个维度有640个数据，共有480×640 =307 200个数据，每个数据的值就是这张图中一个像素的灰度。如果是一张彩色图像，每个像素又可以分解为红、绿、蓝三原色，这张图像就变成一个三维的张量，一共有480×640×3=921 600个数据。张量是一种能涵盖各种数据的形式，不论要处理的数据是天气、股票、人体生理指标、语音、图像还是视频，都可以用不同维度的张量表示。这样数据的统一的表达使数学运算形式的表达也能够统一：都是张量运算。所以在机器学习计算时（不论是训练模型还是识别特征）都要先把数据转化为张量形式，计算结果出来以后，再从张量形式转化为原来的数据形式。在所有的张量里，最常用的就是二维张量，即矩阵。下面为了直观易懂，我们在讨论过程中都用矩阵作为张量的代表。





对张量的运算


基于神经网络的机器学习在本质上是对输入数据的一系列矩阵运算、卷积运算和非线性运算（例如只取正值，负值一律等于零的运算）。作为模型训练，通过反向传播不断地调整加权系数（即矩阵的各个元素）使最后的输出与目标值的差达到最小。作为特征识别，将输入数据经过一系列矩阵和非线性运算后提取出某个训练过的特征，然后再拿这个特征和一类已知特征比较进行分类。这些常用的运算例如矩阵乘法、非线性处理等都可以成为程序库里的一个运算或函数。当我们调用这个函数时，只需要把该填的参数填进去，例如矩阵的大小和内容，而不必再自己写矩阵的具体运算。





运算流程图和程序优化


许多编程框架都提供可视运算流程图，这个工具可以把一个神经网络的全部运算用框图的方法画出来，框图中的每个节点就是程序库中的一个函数或运算（用编程语言说叫作一个对象），这样整个运算非常直观，也容易找到程序漏洞。当运算流程图画出来以后，编程框架就可以自动把流程图变成可执行程序。对于一个复杂的程序，用可视流程图方法画出来后有全局观，很容易优化（改动流程图比改程序容易）。编译器（把用编程语言写的程序转化成计算机底层指令的内置程序）可以根据流程图优化底层资源（内存、计算等资源）的分配。





自动求导器


在神经网络训练时最复杂的计算就是把输出误差通过反向传播，用最陡梯度法来调整网络各层的加权系数直至输出误差最小。这个计算基本是一个连锁的在网络各层对权重系数集求导数的计算。在大部分的编程框架中，这个连锁求导被打包成一个运算函数（在Python程序库里是一种“类”）。在有些提供高级应用接口的编程框架中，例如TensorFlow，甚至把整个“训练”打包成一个运算（“类”）。在把运算流程图画出来以后（等于定义好了神经网络的大小和结构），只要调用“训练”这个运算开始运算训练数据，就可以得出训练好的网络参数。





针对GPU的线性代数运算优化


传统的许多线性代数的函数是在CPU上运算的。前面介绍GPU时讲过，GPU的特点是大量的并行计算。线性代数中的运算许多都是矩阵运算，而矩阵运算中有大量可并行的计算。由于目前AI的计算平台以GPU为主，所以许多原来常用的线性代数函数和运算包要重新写，让这些运算充分并行。但这部分工作不影响编程框架用户的使用方式，这些优化后的接口和使用方式保持不变。

这些编程框架都是早期的机器学习编程者为了自己方便使用而积累出来的程序库。它们形成的时间不同，目的相异，解决的问题的侧重面也不同。有些框架提供的可调用程序属于底层（每个函数或运算相对基本，相当于盖房子的砖头），需要编程人员透彻理解神经网络，这些底层程序库的好处是编出来的程序灵活性高、适用性强、运行效率高。有些框架提供的调用程序属于高层（每个函数或运算复杂，相当于盖房子的预制板，如同一面墙）。不太懂神经网络的人也能很容易编程，但程序灵活性和适应性受限制，运行效率低。另外一个主要不同之处是程序库内的函数和运算的具体实现方法不同，有些效率高，有些效率低。关于各主要编程框架在使用单个和多个GPU进行机器学习中最常见的矩阵运算和卷积运算的效率比较，可以参见香港浸会大学褚晓文教授的论文《基准评测TensorFlow、Caffe、CNTK、MXNet、Torch在三类流行深度神经网络上的表现》。想更多地了解这些编程框架的读者可以参阅附录4，其列出了目前业界的主要编程框架。





开源社区与AI生态


我们从程序库的介绍可以看出，几乎任何早期的机器学习的编程者都会自己编写一些常用的函数和运算，建一个自己用的或自己的公司内部用的程序库。发源于大学的这些程序库，例如发源于加州伯克利大学（UC Berkeley）的Caffe，通常从一开始就是“开源”的，也就是这些程序库里面的源代码都是公开的。近年来谷歌、Facebook、微软、亚马逊等公司也将自己的程序库开源。为什么它们会如此“大公无私”？当然这些公司内部推动开源的技术人员中不无理想主义者，希望通过开源来推动机器学习的快速发展。但公司作为一个营利主体，开源的主要商业动机是吸引更多的软件编程人员使用自己的编程框架，滋养一个围绕着自己的编程框架的生态系统。那么“开源”这个游戏是怎么玩的呢？程序源代码开放后随便什么人都可以改，如何控制质量？下面我们就介绍一下“软件开源”这个游戏的历史和规则。

软件开源在美国有悠久的历史，最成功的开源项目就是互联网的开发。从1962年兰德公司提出互联网的概念，到1968年头三个网络节点（斯坦福大学、斯坦福研究所、加利福尼亚州洛杉矶大学）的连接，再到网络协议TCP/IP的开发和成熟，其间没有任何政府部门出面组织，也不受任何一家公司控制，完全靠社区志愿者。美国这个国家就是从社区到小镇，到州，再到联邦这样自下而上建立起来的。当1620年从英国驶往新大陆的“五月花”号轮船被季风从原来的英王特许地纽约吹到北边的荒无人烟的今天叫作波士顿的地方时，船上的102名男性清教徒在一起订下了“五月花号圣约”。这份圣约就是美国第一个社区的“乡民公约”。之后美国的各个移民社区都是这种自治管理模式，没有官，也没有“上级”，所有的社区都由社区居民自己定规则，自己管理。“政府”就是自己订的乡民公约。所以在美国独立战争后许多人认为根本没有必要成立一个联邦政府。这种强大的自治传统是开源软件的文化基础。所以像互联网这样无中心，没“领导”的“怪物”只能在美国成长起来，在世界任何其他地方都不太可能。这和技术无关，而和文化、历史有关。互联网的技术很简单，只要有一台显示终端、电话线和提供信息的计算机就可以搞互联网了。其实世界上联网最早的国家不是美国，而是法国。法国1968年就开发出了“Minitel”（法国自行建立的国家网络，建成早于互联网），由法国邮电部开发、控制。1982年就在全国铺开，给每个居民家里面安装一个统一的终端，通过电话线连接到邮电部的数据库里，可以查天气、订票等。法国这样的欧洲大陆国家有悠久的皇权传统，从一开始就和美国的思路完全相反，一切由邮电部控制。在互联网的冲击下，法国Minitel和其他国家政府控制管理的互联网门户一样早就荡然无存。回过头看，孰优孰劣一目了然。

软件开源也是一个社区，这个社区也有乡民公约，这个公约的主要内容就是鼓励每个人分享对开源软件的改进。经过多年的演进，目前开源社区的公约大多使用“Apache2.0协议”。这个协议的主要规则如下：任何人都可以使用Apache 2.0协议许可下的软件，并且可以用于商业；任何人都可以任意修改原有的软件，并将修改后的软件申请商标和专利，但修改的软件必须注明使用了Apache 2.0的许可，必须明确标示修改的部分。

开源社区允许在开源软件的基础上开发自己的商用软件，而大多数商用软件是不愿意公开的。那么开源社区如何解决“搭便车”的问题呢？这个问题就像问为什么总有人愿意出头为社群出力，为什么人会有利他的动机。答案在于种群的竞争和演化。设想远古两个邻近的部落，第一个部落里面的所有人都很自私，另一个部落里面有些人愿意为大家冒风险和做事，第二个部落的合作能力和战斗力就会比第一个强，两个部落发生战争时第二个部落就会把第一个部落消灭了。那些“纯自私”的人的基因就无法遗传下来，而获胜存活下来的基因中就会有利他成分。就像我们在日常生活中看到的一样，一个社群中愿意牺牲自己的利益为大家服务的人虽然总是极少数但永远存在。开源社区其实是同一个道理，在里面免费干活不断改进软件的人也是少数，所以许多开源社区都是一路艰难。但是对于那些逐渐成为重要基础设施的开源软件，例如互联网协议、操作系统Linux等，就会有更多的人来关心和付出。例如许多商业公司使用了这些开源软件，一旦公司做大，这些公司就非常关心这些开源软件的改进、更新和安全。这些公司就会出钱出力。还有一些用户也会捐献，许多常年依赖维基百科学习和检索的读者，因为希望这个工具越来越有用，就会定期捐款支持。笔者已经不能想象离开维基百科这样的工具该怎么工作和生活。但以上这些因素仍然没能彻底解决开源社区“搭便车”的问题。这和我们在一个社群社区的情况完全相同，总有人会“搭便车”。区块链技术的出现也许能彻底解决这个问题，这又是一个很长的话题。笔者也许会在下一本关于区块链对社会的冲击的书中详述。

所以有了开源的编程框架以后，大量的AI应用开发公司就可以使用现成的程序库而不必从头开始。这就大大降低了AI应用的技术门槛。一个不懂机器学习的有经验的软件工程师，可以用一个月时间在网上学一门机器学习的基础课程，再花一周时间就可以掌握像谷歌的TensorFlow这样的编程框架。所以今天融资的新创技术型公司都可以说自己是“AI公司”。如果这些公司的技术都使用开源编程框架，它们的技术差别就很小。因此这些公司比拼的是对某个行业的理解和在该行业的营销能力，以及对该行业数据的占先和占有程度。有AI技术实力的公司通常不完全依赖开源的编程框架，而是自己开发很多自己专用的底层程序库，甚至有自己的编程框架。

在AI的开源运动中，除了各大学和各大技术公司的开源编程框架，还有一些纯粹的公益组织。其中最著名的就是由“钢铁侠”马斯克和迄今最成功的孵化器之一Y Combinator的创始人山姆·艾特曼（Sam Atman）创建的Open AI。创办Open AI的动机有两个：一是不能让大公司控制人类未来最重要的技术之一。这一点和当年乔布斯、盖茨发起个人电脑革命时的驱动力相同：不能让IBM这样的大公司垄断计算机技术。二是现在就开始警惕AI对人类社会的潜在威胁（我们将在第七章专门讨论此问题）。Open AI的使命是“AI民主化”。意思是要让更多的人掌握AI技术，让更多的人受惠于AI。也有人反驳说让每个人都掌握一种威力无比强大的技术是否使人类更安全？应该说对AI的担心是明确的，但如何使AI更安全的路径是模糊的。但这就是那些创业家的性格，只要大方向对了就先干起来再说。Open AI的最大挑战是如何吸引第一流的人才。在硅谷，顶级的AI人才的工资、奖金、期权加起来每年可达数百万美元，而Open AI作为一家非营利性机构，只能给出一般的市场价（例如每年20万~30万美元）。即使如此，也曾经吸引了重量级的AI大神，例如发明了生成对抗网络的岩·古德菲勒（Ian Goodfellow）等人。Open AI的主要研究工作集中在通用人工智能，使用开源社区的方法吸引全世界的AI人才来贡献。Open AI是否能够最终成为AI生态中的一支重要力量，还要看他们的研究结果是否能被广泛应用，要看这些为理想而来的年轻人能否禁得住市场以10倍以上的待遇把他们挖走。





乱世枭雄


今天的AI就像20年前的互联网，是兵家必争之地。几乎所有的大公司都在争夺高地。其中最有代表性的就是世界上几大科技和互联网巨头：谷歌、Facebook、亚马逊、微软、IBM、百度、阿里巴巴、腾讯等。这些巨头可以分为三类：第一类是掌握大量用户数据的互联网公司；第二类是微软和IBM这样的技术公司；第三类是华为、小米等这类缺乏数据，但有应用场景，又希望通过AI提升自身产品的公司。第一类公司首先在自己的数据上全面使用AI技术，例如图片搜索、用户行为预测、智能推荐等。第二类公司则希望打造AI云计算让客户使用。第三类公司积极和前两者进行合作，或快速将它们的AI开源能力运用到自身的产品中。

在互联网公司中，技术驱动型的公司例如谷歌和百度都是在AI战略上最激进的公司。谷歌放弃了“移动第一”的战略，提出“AI第一”的新战略。百度也宣称自己是一家AI公司，要全力以赴投入AI（all in AI）。谷歌和百度除了在自己的数据优势上全面使用AI以外，也四面出击，企图进入自己不占有数据优势的垂直行业，例如自动驾驶和医疗健康行业。中美的搜索公司在进入垂直行业上比社交网络和电子商务公司更激进的另外一个原因是搜索公司的用户数据比社交和商务的用户数据“浅”，挖掘的价值没有社交和商务用户高。

这些互联网和科技巨头都在云计算方向上激烈竞争，每家都希望自己的AI云计算占有最大的市场份额，亚马逊、谷歌、百度、IBM、腾讯等都在其云计算平台推出了计算机视觉、语音识别、自然语言处理、翻译等能力。谷歌传统的云计算市场份额远不如亚马逊，但AI为其提供了一个翻身的机会。谷歌开发自己的TPU，除了降低成本以外，更重要的是TPU能够对TensorFlow编程框架下的计算提供更快的计算。相对于亚马逊的云服务AWS等竞争对手，谷歌不仅可以提供成本更低、拥有更强AI能力的云计算服务，还可以进一步吸引更多的人使用谷歌的编程框架TensorFlow。这样TensorFlow和谷歌的云计算服务的绑定就越紧密，TensorFlow和基于TPU的云计算就形成了正循环。目前TensorFlow的SDK（开发工具包）在全球有1 000万次下载，遍布180个国家和地区。同时谷歌云AI团队正在快速降低AI技术上的门槛。2018年初，谷歌发布全新的“自动机器学习云”（Cloud AutoML），不会用谷歌编程框架或任何编程框架的人也可以创建机器学习模型，用户只需上传数据便能自动创建机器学习模型，包括训练和调试。目前已经有上万家企业使用谷歌的自动机器学习云服务。

华为在通信设备、移动终端领域有领先优势，虽然该公司很早就部署了AI，但是在AI技术上整体还是大幅落后于谷歌、百度这类公司。因此该公司采用了全面开放合作的态度，构建自身的AI能力，以便升级现有的产品与服务能力。华为通过开放平台的搭建，在芯片层一开始采用了寒武纪的NPU（嵌入式神经网络处理器），在语音交互方面采用了科大讯飞和自主研发相结合，并通过开放的架构和战略合作将谷歌的TensorFlow、百度的Paddle Paddle等深度学习能力便捷地提供给开发者与合作伙伴，并和微软联合开发内置于系统层的机器翻译功能，以及和商汤等公司联合开发AI技术，以便为旗下产品打造更多的AI型应用，拉开和竞争对手的距离。





大卫和哥利亚


大公司既有技术又有数据，那新创公司怎么活？简单来讲，新创的AI公司要进入大公司不占有数据优势的那些垂直的行业。这样的行业又可以分为两类：一类是新兴领域，以前完全没有人做，一切从头开始；另一类是原有行业，例如金融、保险、能源等。新创公司进入第一类行业最容易，因为大公司通常不会进入一个全新的、市场还未知的领域，自动驾驶和人脸识别都是这样的新兴领域。目前在美国和中国做自动驾驶的新创公司有上百家，在中国做人脸识别的公司也有数百家。与任何新兴领域一样，这些新创公司的大部分将被淘汰或并购，尤其是自动驾驶，其属于未来汽车厂商之间竞争的核心技术，又牵涉到安全。大车厂不可能把这样的技术交给一家创业公司（二流以下车厂有可能，所以还是有市场的）。它们要么自己建研发队伍，要么收购最好的自动驾驶软件公司，目前几乎所有的一线国际大车厂都已经这么做了。自动驾驶软件公司遇到的第二个问题是它们的软件通常要通过一级供应商（Tier1）的集成后进入车厂。但一流的一级供应商也认为自动驾驶是它们未来的核心竞争力，也是要么自建研发，要么收购，也不会把人命关天的事交给一家新创公司。所以那些没能被收购的自动驾驶软件公司最后要么去攻比较简单的、限定的场景，例如校园、景区、小区等，要么去攻垂直市场，例如卡车、港口、仓库等。而人脸识别由于应用场景不同，客户要求不同（例如发现犯罪分子和刷脸支付要求完全不同），所以可以容纳更多的厂家存活。

现在难以看清楚的是那些既有钱又有数据的传统行业，例如金融、能源、医疗等。在这些领域有三类竞争者：第一类是本行业内部的团队，例如许多证券公司已经开始大规模自建AI交易和理财团队。第二类是各大互联网公司企图挟巨大技术优势进入或者颠覆传统行业，例如谷歌和腾讯都企图进入医疗领域。第三类是企图进入垂直行业的新创AI公司。两个有意思的行业是证券交易和医疗图像。前者几乎都在建立自己的团队和能力，而后者则基本都和外面合作。对于证券公司来讲，AI算法是未来交易的核心技术，证券公司必须掌握。交易算法必须由技术团队和交易团队紧密配合快速迭代，算法和数据必须严格保密，所以很难外包。而医疗成像识别目前主要是提高X光读片效率，不是生死攸关的技术，X光读片对于一家医院来说只是很小的一部分业务。美国的大连锁医疗集团例如凯撒（Kaiser Permanente）等还有一些内部的技术资源，大部分美国和中国的医院基本没有这样的技术能力。医疗图片即使泄露到同行手里，对医院本身也不会造成致命伤害。所以它们愿意承包给外面做。新创AI企业是否能顺利进入传统行业，要看AI技术在这个行业中的作用和对数据的敏感程度。预计银行和保险行业将和证券业类似，不会愿意分享数据外包AI，大银行和保险公司都将以自己为主。所以AI公司只能去攻那些中小规模的企业，它们自己没有技术能力，又面临被淘汰的危险。还有一种做法就是与新业态合作，例如新兴互联网银行、互联网保险。这些公司有互联网和大数据基因，属于行业新进入者，做法激进，天然拥抱AI技术。但对这类公司来说，AI将是核心技术，它们最终还是要自己做，也许会收购外包团队。

总体来讲，今天AI创业公司进入传统行业的商业模式还都不清晰，如果有选择，那么在为传统行业增加效率和从外部颠覆传统行业两者之间，前者更容易，但后者利益更大。

许多人今天对AI新创公司的一个担忧就是用户数据都在互联网巨头手里。这是一种静态的看法，今天互联网公司的数据主要是人们使用电脑和手机产生的浏览数据，它们并不掌握下列几大类对人类有用的，AI也需要用的数据。

（1）人类本身的数据，例如身体数据和心理数据。

（2）环境数据，其中包括自然环境、社会环境。

（3）各种人类劳动过程数据，例如农业、工业、服务业的过程数据。

人类劳动过程中的数据是未来最重要的数据，劳动过程无非是对一个给定的环境施加一组行为，让这个给定的环境变得对人类更有利（例如给一块地播种、灌水、施肥，使之长出庄稼，冶炼铁矿石变成钢，给患者打针吃药治愈疾病）。只要这个环境能够被测量（庄稼亩产、矿石和钢铁质量、人体健康程度），这组行为能够被控制（浇水施肥量、高炉温度、药的种类剂量），机器学习就可以被用来优化这个过程。所以一切能够被测量的环境和过程都将产生机器学习需要的数据。几十年以后回头再看，人类上网和玩手机产生的那点数据根本就不叫数据。如果把数据比作金矿，那么互联网巨头今天拥有的无非是地表面沙土里一层浅浅的金沙，真正的金疙瘩都还埋在迄今没发现的地方。这些地方有些是我们前面提到的现有行业，有些是我们今天还没看到的环境和过程。随着各类传感器成本的降低，越来越多的环境被更细密地感知。随着物联网的普及，这些无所不在的传感器将搜集到比今天互联网大许多数量级的数据。

新创公司和大公司竞争的最大的优势还是人才和激励机制。创业者通常都是最优秀、最有激情、敢于冒险的一批人。新创公司在一个全新领域可以随时掉头，快速迭代，迅速摸清市场需求，它们所有的脑筋都会放在如何满足用户需求上。而在大公司很少有人愿意冒风险尝试新东西，有些项目会牵涉许多部门利益，想做一件事要花大量时间去协调，有时还会打得不可开交。凡是在大公司待过的人对此都深有体会。以半自动驾驶功能为例，特斯拉率先推出自动线道保持功能，使驾驶员开车时可以不扶方向盘。这个线道识别技术最初是以色列公司Mobileye提供的，其他使用Mobileye方案的汽车厂家按理说都可以推出这个功能但却没有，因为这个功能风险很大。传统车厂的中高层经理打死都不会签字发布这个功能，后来也确实出现了交通事故的例子。特斯拉发布这样的功能极可能就是老板自己拍的板，因为除了利益之外，创始人天天泡在产品上，对细节非常了解，拍板时心里多少有数。一个新功能谁都无法打包票，不冒这样的风险就无法在自动驾驶技术上领先，无法领先新创公司就存活不下去。而传统大公司的CEO都是职业经理人，对某个具体功能不可能了解得那么细，要依赖一层一层的建议，如果下面没人愿意担这个风险，CEO就不敢随便签字。传统大公司的第二个问题是激励机制无法和新创公司比。大公司那点奖金和期权没法与创业公司的期权比（如果成功），大公司内部的人事斗争和协调成本都会把那些智商高的技术天才吓跑，即使招来也会气走。传统车厂有资金、渠道，甚至也掌握了相应的技术，由于上述种种原因也只能眼睁睁地看着特斯拉这样的公司冒出头来把它们甩得越来越远。





AI的技术推动力


许多人都在关心AI这波行情还能走多高、走多远。如果说算法是AI引擎的设计，算力是引擎的马力，数据是引擎的燃料，那么让我们分别看看这些技术推动力的发展。





算法


前面介绍过，目前新算法层出不穷，有些在继续沿着神经网络的方向走；有些开始探讨其他路径，例如贝叶斯网络、支持向量机；有些在把不同的算法融合起来；有的干脆另辟蹊径，提出新的人脑认知模型。算法的研究目前非常活跃，在未来5~10年还会有大量新的算法涌现。





算力


算力的增加基于摩尔定律。目前除了芯片的线宽继续变窄（最新半导体工艺线宽4纳米）导致集成度继续变高以外，还有各种封装技术，例如三维芯片封装可以将64个芯片摞在一起。目前芯片的耗电比人脑耗电还大几个数量级，在许多数据中心能耗成为制约瓶颈，大幅度降低耗电也是芯片设计的重要方向。虽然单个芯片计算能力的增长变缓，但是现在倾向于用越来越多的芯片。对于AI计算来讲，不论是训练还是识别，重要的不仅是单个芯片的能力，更是能够把多少芯片有效地组织在一起来完成一个计算任务。2012年以前还很少使用GPU，现在一个计算任务动辄使用成千上万个GPU或专用计算芯片，例如TPU。2018年Open AI发布的一份报告显示，自2012年以来，在AI训练运行中所使用的计算能力呈指数级增长，每3.5个月增长一倍。2012—2018年，这个指标已经增长了30万倍以上。具体说就是2018年谷歌的AlphaGo Zero比2012年ImageNet大赛获胜的AlexNet快了30万倍。





数据


数据的增加基于传感器或存储器越来越便宜，几乎所有传感器和存储器的成本都是由芯片成本决定的。当芯片集成度提高，芯片需求量增大时，传感器和存储器的成本会大幅度下降，更多的传感器会产生更多的数据。

综上所述，推动AI的三个技术要素都在快速发展，所以目前AI只是莱特兄弟刚刚把飞机飞离地面，离5马赫超音速还很远。

从市场看，目前受到AI冲击的传统行业还很少，大部分行业还没有开始被改造、被颠覆，因为AI从业者都在忙乎进入那些没有传统巨头的行业，例如人脸识别和自动驾驶。

图4.3是一个很著名的“新技术成熟曲线”。互联网过去20多年的发展就非常符合这条曲线：一个新的重大技术创新一开始没多少人相信，但超过一个转折点后就开始被热炒，大家的期望值都很高，大量的资金盲目进入，很快发现技术还不成熟，远不能达到期望值，大家都很失望，行业跌落到谷底。但这个技术其实只是需要时间，经过一段时间成熟起来，就会重新站上高地。就互联网来说，今天的发展远远超过了2000年泡沫时最狂野的想象。

图4.3新技术成熟曲线

图片来源：https://www.gartner.com/smarterwithgartner/whats-new-in-gartners-hype-cycle-foremerging-technologies-2015/。



那么AI处在这条曲线的什么位置呢？大约在峰值刚过，还远未到低谷。注意这条曲线只是个一般性规律，并不准确，也不一定适用所有新技术，即许多新技术都会经历这么两个起伏，但不同的技术起伏幅度不同。笔者预测AI有“冷静期”而没有“幻灭期”，因为AI在许多行业都已经证明有用。回头看过去几年还是有一些泡沫，泡沫体现在某些领域出现大量同质化公司和这些领域融资的估值上面。据不完全统计，中美两国的自动驾驶公司已经超过100家，中国号称做人脸识别的公司有数百家。显然市场不需要这么多家企业。在自动驾驶领域一个没有任何收入，也看不到清晰商业模式的公司可以喊价到数亿美元的估值。估值泡沫通常都是由于一次估值离谱的收购造成的。通用汽车公司在2016年宣称以10亿美元的估值收购了位于旧金山的Cruise Automation，以后所有的自动驾驶公司都以这次收购作为自己估值的对标。所有投资自动驾驶公司的投资者都赌自己投资的公司也会被高价收购。但是即使所有大的整车厂商都收购一家自动驾驶软件公司，市场也不需要上百家同质化的公司。当供大于求时，收购价格也会大幅度降低。笔者预计在今后2~3年中，大部分同质化公司在资金耗尽后将因为无法进一步融资而死亡，与此同时，少数几家技术独特或市场能力强且资金雄厚的公司将快速发展。

目前许多融资的新创公司都宣称自己是AI公司或者使用AI技术。但这些公司的技术大多是用几大公司的开源编程框架，例如谷歌的TensorFlow和英伟达的GPU或者市场上的云计算服务。技术高度同质化，没有任何壁垒。就像当年任何公司都说自己是.com公司一样。那么我们如何判断一家AI创业公司的价值呢？首先，应该看是否能够拿到别人拿不到的数据。做到这一点很难，你能拿到的数据别人通常也能拿到。如果不能独占数据，那就要看有多大先发优势。如果进入一个行业早，通过快速迭代，让自己的模型在这个行业中变得有用，就可以得到更多的数据和资源，后进者即使拿到同样的数据，模型质量差也打不进去。其次，要看该企业对所进入行业的独到理解和业务开发、落地能力。当然如果能够针对本行业在算法上有突破，就能够大大提高进入壁垒。





AI与互联网的三个区别


这次AI创新浪潮堪比互联网，但是AI浪潮和互联网浪潮有三个区别。

第一个区别是AI从一开始就要颠覆传统行业。互联网1994年起步时从经济的边缘开始，和传统产业似乎一点关系都没有，没有人懂一个网站能干什么。互联网20多年来逐步从边缘蚕食中心，直至今日影响每个行业。但即使是今天，互联网对制造业、农业、建筑业、交通运输等搬运原子的行业的影响也局限在媒体和营销方面，没有进入制造业的核心。而AI的特点是从第一天起就从传统产业中心爆炸，自动驾驶对汽车行业的颠覆就是一个典型的例子。

第二个区别是技术驱动。互联网除了搜索以外基本没有太多技术，主要是应用和商业模式。互联网创业者完全可以是不懂技术的人。目前为止AI创业者以技术大拿居多。当然随着AI技术的普及，许多有商业头脑的人只要看明白AI在一个行业的价值也可以拉起一家公司，但目前最稀缺的是AI的高级技术人才。

第三个区别是可能不会出现平台性公司或赢家“通吃”的局面。互联网的一个特点是连接供需双方，一旦用户超过一个门限，后来者就很难赶上，所以很容易形成赢家“通吃”的局面。但在AI产业里目前还没有看到这样的机会，不论是自动驾驶还是人脸识别都是一个一个山头去攻，无法在短期内形成垄断。造成融资泡沫的一个重要原因就是有些投资人还以为AI和互联网一样赢家“通吃”：只要投中第一名，多贵都值。

简单用一句话说就是互联网是to C（对用户）的生意，AI是to B（对企业）的生意。AI中to C的生意都会被现有互联网巨头吸纳，创业者的机会在于to B。