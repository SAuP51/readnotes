## 附录 1：一个经典的 5 层神经网络 LeNet-5

图附 1.1 是由人工智能大神之一岩·拉孔（CNN 的发明人）提出的典型的卷积网络 LeNet-5。这个网络的左边是输入的待识别的图像，最右边有 10 个输出，因此可以识别 10 种不同的图像，例如手写体的 0，1，2，…，9。

图附 1.1 岩·拉扎提出的卷积网络 LeNet-5

图片来源：http://yann.lecun.com/exdb/lenet/。

输入图像的大小是 32×32。网络的第一层是卷积层，所谓「卷积」就是拿着各种不同的小模板（图附 1.1 的模板大小是 5×5）在一张大图上滑动找相似的图形（每个模板是一个特定的图形，例如三角、方块、直线、弧线等）。模板在大图中一个一个像素滑动，在每一个滑动位置，都把大图上的像素值和模板对应的像素值相乘再全部加起来，把这个加总之和作为一个新的输出像素。当模板滑动过所有像素时，这些新的像素就构成一张新的大图。在模板滑动过程中，每当碰上大图中有类似模板上的图形（即在大图上这个区域的图形和模板重合度高时），新的像素值就会很大。图附 1.1 的模板尺寸是 5×5，因此在 32×32 的输入图像上横竖都只能滑动 28 次，所以卷积的结果是一张 28×28 的图片。在这个网络里，第一层使用了 6 个模板，所以第一层卷积后出来 6 张 28×28 的图片。

卷积的结果出来后，对每一个像素还要做一个「非线性」处理。一种「非线性」处理方法就是把所有小于零的像素值用零来代替，这样做的目的是让网络增加复杂性，可以识别更复杂的图形。

网络的第二层叫采样层（down-sampling）或汇集层（pooling），简单讲采样层就是把大图变小图。在这个网络里，采样的方法是用一个 2×2 的透明小窗在图片上滑动，每次挪动 2 个像素。每挪动一个位置，把小窗内的 4 个像素的最大值取出来作为下一层图片中的一个像素（除了取最大值，也可以有取平均值等其他方法）。经过这样的采样，就产生了下一层 6 张 14×14 的图片。

这个网络的第三层又是一个卷积层，这次有 16 个 5×5 大小的模板，在 14×14 的图片上产生 16 张 10×10 的图片（5×5 的模板在 14×14 的图片的横竖方向上都只能滑动 10 次）。

第三层做完非线性处理后，第四层又是一个采样层，把 16 张 10×10 的图片变成 16 张 5×5 的图片。

第五层再次用 120 个 5×5 模板对 16 张 5×5 的图片做卷积。每个模板同时对所有 16 张图片一起卷积，每个模板产生一个像素，一共产生 120 个像素，这 120 个像素就成为下一层的输入。从这里开始，原始的一张 32×32 的输入图片变成了 120 个像素。每个像素都代表着某个特征。从这一层开始下面就不再做卷积和采样了，而是变为「全连接」的标准神经网络，即第五层的 120 个神经元和第六层的 80 个神经元中的每一个都连接。（不要问为什么第六层是 80 个而不是 79 个或 81 个，答案是没太多道理。）

第六层的 80 个神经元和第七层（最后一层，即输出层）的 10 个神经元的每一个都相连接。

以上卷积 + 非线性 + 采样可以看成卷积网络的一个单元，这个单元的作用就是「提取特征 + 压缩」，在一个卷积网络中可以不断地重复使用这个单元。今天大型的卷积网络可能有几百次这样的重复，在这个例子里，只重复了一次。

卷积网络为什么这样设计？为什么分为卷积和全连接两个不同的部分？简单说是省了计算。如果用全连接多层网络，那么输入层要有 32×32=1 024 个神经元，在 LeNet 中最后压缩到 120 个。压缩的原理就在于对识别一张图像来说，只有图像中的某些特征是相关的。例如识别人脸，主要是五官的特征有用，至于背景是蓝天白云还是绿树红花都无所谓。卷积的模板就是要提取出那些最相关的特征。注意，这里模板并不需要事先设计好，模板中的各像素的数值，就是神经网络各层的加权系数。根据训练数据的不同，通过前述的最陡下降法，模板各像素取值最终也不同（即训练数据都是人脸和训练数据都是动物最终演化出来的模板不同）。每一层使用多少个模板要在识别率和计算量之间找一个平衡，模板越多识别越准确，但计算量也越大。

## 附录 2：循环神经网络 RNN 和长 - 短时记忆网络 LSTM

循环神经网络 RNN

CNN 比较适合处理图像，因为每幅图像之间都没有太多相关性，所以可以一幅一幅地输入 CNN 里。但许多非图像信息例如语音、天气预报、股市等是一个连续的数值序列，不仅无法截成一段一段地输入，前后之间还有很强的相关性。例如我们在下面句子里猜词填空：「我是上海人，会讲 ———— 话。」在这里如果我们没有看到第一句「我是上海人」就很难填空。处理这一类问题最有效的神经元网络叫循环神经网络。图附 2.1 是一个最简单的 RNN。

图附 2.1 最简单的 RNN

这里 Xt 是 t 时刻的输入向量，A 是神经元网络，ht 是 t 时刻的输出。注意到方框 A 有一个自我反馈的箭头，网络 A 下一时刻的状态依赖于上一时刻的状态，这是 RNN 和 CNN 及全连接网络的最大的区别。为了更清楚地理解图附 2.1，我们可以把每个离散时间网络的状态都画出来生成图附 2.2。

图附 2.2 把 RNN 从时间上展开看

这个链状结构展现出 RNN 与列表、数据流等序列化数据的密切关系，而这类数据的处理也确实在使用 RNN 这样的神经网络。在过去几年中，将 RNN 应用于很多问题后，已经取得了难以想象的成功，例如语音识别、语言建模、翻译、图像抓取等，而它所涉及的领域还在不断地增加。

现在我们已经知道了 RNN 是什么，以及基本的工作原理。下面我们通过一个有趣的例子来加深对 RNN 的理解：训练一个基于字符的 RNN 语言模型。我们的做法是，给 RNN「喂」大段的文字，要求它基于前一段字符建立下一个字母的概率分布，这样我们就可以通过前一个字母预测下一个字母，这样可以一个字母一个字母地生成新的文字。

举一个简单的例子，假设我们的字母库里只有 4 个字母可选：「h」「e」「l」「o」，我们想训练出一个能产生「hello」顺序的 RNN，这个训练过程事实上是 4 个独立训练的组合。

（1）给出字母「h」后，后面大概率是字母「e」。

（2）给出字符「he」后，后面大概率是字母「l」。

（3）给出字符「hel」后，后面大概率还是字母「l」。

（4）给出字符「hell」后，后面大概率是字母「o」。

具体来说，我们可以将每个字母编码成一个向量，这个向量除了该字母顺序位是 1，其余位置都是 0（例如对于「h」而言，第一位是 1，其余位置都是 0，而「e」则第二位是 1，其余位置是 0）。之后我们把这些向量一次一个地「喂」给 RNN，这样可以得到一个四维输出向量序列（每个维度对应一个字母），这个向量序列我们可以认为是 RNN 目前认为每个字母即将在下一个出现的概率。

图附 2.3 是一个输入和输出层为 4 维，隐含层为 3 个单元（神经元）的 RNN。图中显示了当 RNN 被「喂」了字符「hell」作为输入后，前向传递是如何被激活的。输出层包括 RNN 为下一个可能出现的字母（字母表里只有 h、e、l、o 4 个字母）所分配的概率，我们希望输出层加粗的数越大越好，输出层其他的数越小越好。例如，我们可以看到在第一步时，RNN 看到字母「h」，它认为下一个字母是「h」的概率是 1.0，是「e」的概率是 2.2，是「l」的概率是 - 3.0，是「o」的概率是 4.1。由于在我们的训练数据（字符串「hello」）中，正确的下一个字母是「e」，所以我们希望提升「e」的概率，同时降低其他字母的概率。同样地，对于 4 步中的每一步，我们都希望网络能提升对某一个目标字母的概率。由于 RNN 完全由可分的操作组成，因此我们可以采用反向传播算法来计算出我们应该向哪个方向调整每一个权量，以便提升正确目标的概率（输出层里的加粗数字）。

图附 2.3 RNN 学习「hello」语句的训练过程

接下来我们可以进行参数调整，即将每一个权值向着刚才说的梯度方向微微调整一点，调整之后如果还将刚才那个输入字符「喂」给 RNN，我们就会看到正确字母（例如第一步中的「e」）的分值提高了一点（例如从 2.2 提高到 2.3），而其他字母的分值降低了一点。然后我们不断地重复这个步骤，直到整个网络的预测结果最终与训练数据一致，即每一步都能正确预测下一个字母。需要注意的是，第一次字母「l」作为输入时，目标输出是「l」，但第二次目标输出就变成了「o」，因此 RNN 不是仅仅根据输入判断，而是使用反复出现的关系来了解语言环境，以便完成任务。

LSTM：记忆增强版 RNN

RNN 可以根据前一个字母预测出下一个字母。但有时候可能需要更前面的信息，例如我们要填空「我在上海出生，一直待到高中毕业，所以我可以讲 ———— 话」。此时如果没有看到第一句「我在上海出生」就猜不到填空的内容。为了解决这个问题，人们对 RNN 进行了改造，使其可以有更长的记忆。这就是长 - 短时记忆网络（LSTM）。

所有的 RNN 都是链状的、模块不断重复的神经网络，标准的 RNN 中，重复的模块结构简单。如图附 2.4 所示，每个模块由单个的 tanh 层组成。

图附 2.4 在标准的 RNN 中，重复的模块是单层结构

LSTM 也是链状的，和 RNN 相比，其重复模块中的神经网络并非单层，而是有 4 层，并且这 4 层以特殊的形式相互作用。（见图附 2.5）

图附 2.5 在 LSTM 中，重复模块中含有相互作用的 4 层神经网络

在图附 2.5 中，σ（希腊字母 sigma）是 sigmoid 函数，其定义如下：

这个 sigmoid 函数的图形如图附 2.6 所示。

图附 2.6 sigmoid 函数的图形

tanh 函数是 sigmoid 函数的放大平移版，它们之间的关系如下：

tanh（x）=2σ（2·x）-1

图附 2.7 sigmoid 函数和 tanh 函数曲线

不要被上面这么多复杂的东西吓着，我们等一下会详述 LSTM 的结构。从现在开始，请试图熟悉和适应我们将使用的一些符号。

在图附 2.8 中左起，长方形表示已经学习的神经网络层；圆圈代表逐点运算，就像向量加法；单箭头代表一个完整的向量，从一个节点的输出指向另一个节点的输入；合并的箭头表示关联；分叉的箭头表示同样的内容被复制并发送到不同的地方。

图附 2.8 LSTM 中使用的符号

LSTM 背后的核心

LSTM 的关键是单元状态，即图附 2.9 中上方那条贯穿单元的线。单元状态有点像传送带，它在整个链条一直传送下去，只有少数节点对它有影响，信息可以很方便地传送过去，保持不变。

图附 2.9 LSTM 的单元状态

LSTM 对单元状态有增加或移除信息的能力，但需要在一个叫作「门」的结构下按照一定的规范进行。「门」是有条件地让信息通过的路径，它由一个 sigmoid 神经网络层和一个逐点乘法运算构成。

图附 2.10 LSTM 的单元「门」结构

sigmoid 层输出 0 ~1 的数值，表示每个要素可以通过的程度，0 表示「什么都过不去」，1 表示「全部通过」。一个 LSTM 有 3 个这样的「门」，用来保护和控制单元状态。

附录 3：CPU、GPU 和 TPU

GPU 是神经网络计算的引擎。图附 3.1 是一个典型的神经网络，用输出误差来调节各层的权重系数，输入阵列 X 通过参数矩阵运算进入下一层运算，每一层运算都是一次这样的矩阵运算。所谓矩阵运算，就是先把数字相乘再相加。

图附 3.1 典型的深度神经网络

为了帮助那些没有学过矩阵的读者加深理解，现在我们只看图附 3.2 中左边输入和第一层中一个神经元的关系：每一个输入数字和圆圈里的权重系数相乘，然后把所有的乘积加起来就得到一个值。例如，输入有三个单元，分别是（2，5，8），对应的权重系数分别是（2，-1，0.5），它们相乘后再相加是 2×2+5×（-1）+8×0.5=4-5+4=3。这个值再经过一个「非线性」处理：这个值如果大于 0 就取原值，小于 0 就取值为 0。注意这里所有的相乘运算都可以同时进行，这就是所谓的可「并行运算」。我们刚才描述的仅仅是一个单元的计算方法，其他单元的计算方法也都一样，也就是说不仅一个单元的计算是可以并行的，所有单元的计算都是可以同时进行的。

图附 3.2 神经网络的一个单元内的计算

而 GPU 与 CPU 相比的优点正是在这里。当年设计 CPU 时主要为了执行计算机程序，绝大部分计算机程序都是「串行」的，也就是后一个命令或计算要等前一个命令或计算的结果出来后才能执行。而 GPU 最初是为图形处理用的，图形处理的一个特点就是可以并行。例如，从一张图中把所有的黑点找出来，就可以把一张图分割成许多小图同时来找黑点。图附 3.3 是 CPU 和 GPU 的结构对比，图中左边的 CPU 通常由一个控制器（Control）来给少数几个功能强大的算术逻辑运算器（ALU）分配任务，而右边的 GPU 通常由许多简单的控制器（右图最左边一列方块）来控制更多的算术运算单元组成（那些小格子）。在图形和图像处理中大量的运算都是矩阵运算，所以 GPU 从第一天起就是为矩阵运算设计的，没想到几十年后的深度学习也主要是矩阵运算，这就是「天上的馅饼」。

图附 3.3CPU 结构和 GPU 结构对比

图片来源：https://www.quora.com/Does-CPU-vendors-feel-the-competition-from-GPUs-computational-power。

那么在神经网络深度学习计算中 GPU 比 CPU 能好多少呢？一个极端的例子是在深度学习使用 GPU 之前，谷歌使用 16 000 个 CPU 建造了一个超级深度学习网络，如图附 3.4 所示，成本为数百万美元。

几年后，斯坦福大学只用几个 GPU 就可以达到同样的性能，成本只有 3 万美元！即使考虑到芯片本身在几年内的发展，这个比较也是惊人的。当然这个比较仅仅是比较深度学习的矩阵运算，谷歌大脑还可以做很多其他的运算，例如强化学习等。总的来说，CPU 适合串行运算，可以胜任从航天到手机的各种不同复杂运算和处理，而 GPU 主要用于简单的并行运算，并不会取代 CPU。但是在图形处理和深度神经网络计算中，GPU 可以比 CPU 快 10 倍甚至百倍。英伟达 2017 年推出的用于自动驾驶的芯片 Xavier 已经达到每秒 20 万亿次浮点计算。

图附 3.4 谷歌用 16 000 个 CPU 搭建的深度学习「谷歌大脑」

图片来源：https://amp.businessinsider.com/images/507ebdd2ecad045603000001- 480-360.jpg。

2006—2017 年，单片 CPU 的处理能力提高了 50 倍。50 倍的增长不是来自时钟速度的提高（即单次运算变快），而是来自在芯片中塞进更多的处理器。它的内核数量从 4 个变到 28 个，是原来的 7 倍。另外一个性能的提升来源于指令的宽度，2006 年一条指令只能处理 2 个单精度的浮点运算，今天 512 位的指令集中，一条指令可以同时处理 16 个单精度的浮点运算，这就相当于 8 倍的性能提升。7×8=56，约 50 倍的运算速度提升就是靠更多的处理器得来的。

图附 3.5 2006—2017 年 CPU 运算速度的进展

图片来源：香港浸会大学褚晓文教授「深度学习框架大 PK」中的五大深度学习框架三类神经网络全面测评。

再看 GPU 在近十年的发展，图附 3.6 是 GPU 和 CPU 的性能对比。2006 年英伟达第一次发布通用计算的 GPU8800GTX，当时它的性能已经达到 0.5 万亿次浮点运算（500 GFlops），接下来的十年，大家可以看到 GPU 相对 CPU 的计算能力一直维持在 10~15 倍的比例。GPU 从过去的 128 个核心变成 5 376 个核心，这个增长速度与 CPU 相同，所以 GPU 运算速度与 CPU 运算速度的比值一直保持不变。

图附 3.6 2006—2017 年 CPU 和 GPU 计算能力对比

图片来源：香港浸会大学褚晓文教授「深度学习框架大 PK」中的五大深度学习框架三类神经网络全面测评。

其他几家互联网巨头也不能眼睁睁地看着英伟达控制着深度学习的命脉。谷歌就撸起袖子做了一款自用的 TPU，设计思路是这样的：既然 GPU 通过牺牲通用性换取了在图形处理方面比 CPU 快 15 倍的性能，为什么不能进一步专注于只把神经网络需要的矩阵运算做好，进一步提高速度呢？TPU 设计的核心诀窍有以下四点。

第一，图形与图像处理需要很高的精度（通常用 32 比特浮点精度），而用于识别的神经网络的参数并不需要很高的精度。所以谷歌的第一款 TPU 就专门为识别设计，在运算上放弃 32 比特的浮点运算精度，全部采用 8 比特的整数精度。

第二，由于 8 比特的乘法器比 32 比特简单 4×4=16 倍，所以在同等芯片面积上可以多放许多运算单元。谷歌的第一款 TPU 就有 65 000 个乘加运算单元，相比最快的 GPU 只有 5 300 个单元，多了不止 10 倍。

第三，不论在多核的 CPU 还是 GPU 中，目前的运算速度瓶颈都是内存的读和写。因为要被运算的数据都存在中央存储器里，而这些数字在运算时要分配到几百上千个运算单元中，从存储器到运算单元可谓「千里迢迢」，往返很费时间。在 TPU 里为了解决这个问题，干脆把运算单元摆成矩阵一样，让被运算的数据（例如神经网络的输入数据）流淌过这些运算单元，从内存中提取一个数据就让它和所有的权重系数都做完乘法，而不是像传统 CPU 或 GPU 那样提取一个数据只做一次运算就放回到存储器，做下一次运算再千里迢迢从存储器去取。这样数据像波浪一样一波一波涌来，所以叫脉动式计算。

第四，一个专注于矩阵运算的芯片不用考虑图形处理时要考虑的许多其他事情，例如多线程、分叉预测、跳序执行等，这是由于脉动式计算省掉了许多暂存、缓存等。整个运算的指令只有矩阵运算和取非线性那么几个，例如读数据、读参数、相乘、累加、非线性、写数据等。整个芯片和软件都变得非常简单，这样可以做到每个时钟周期执行一次指令。

图附 3.7 向量（一维）计算、矩阵（二维）计算和张量（三维）计算

图片来源：https://mp.weixin.qq.com/s/e333KjLavEvvpNIL3u1Y4Q。

现在我们可以算一下 TPU 到底比 GPU 快多少了。谷歌第一代的 TPU 有 256×256=65 536 个 8 比特乘加器，时钟是 700MHz，每秒能做的 8 比特乘加运算 = 65 536×700×106 =46×1012 次乘加运算，即 92 万亿次整数运算（92 TOPS，一次乘加运算是两次运算）。所以当谷歌宣称比 GPU 快时，是用整数运算次数 OPS 和浮点运算次数 FLOPS 比。而 GPU 是以浮点运算为测量单位的，前面说过最新的英伟达 Xavier 运算速度可达 20 TFLOPS，这两个不可直接相比，但如果在神经网络用于识别时（而不是用于训练），浮点和整数运算造成的识别准确率差别不大，就可以说这款 TPU 比 GPU 快了 92/20=4.6 倍。对于谷歌这样需要大量矩阵运算的公司可以节省许多买 GPU 的钱，并且加快了识别速度（例如谷歌翻译、图片识别等的几亿用户非常在意处理速度），更重要的是把核心能力控制在了自己手里。谷歌在云服务方面没有亚马逊做得好，正在奋起直追，有了 TPU 则可以给用户提供更快、更便宜的深度学习云服务，所以谷歌的 TPU 目前只给自己用，不卖给别人。谷歌的第二代 TPU 已经能够进行 32 比特的单精度浮点运算，这样从训练到识别都不需要买别人的 GPU 了。用浮点运算做识别还有一个好处就是通过浮点运算训练出来的模型可以直接用于识别，而不是像第一代 TPU 那样先要把那些 32 比特的参数集都量化为 8 比特。但是通过刚才的讨论，我们知道 TPU 更快的一个重要原因是放弃浮点运算，当 TPU 也需要浮点运算时，相比 GPU 的性能提高就不会那么大了。谷歌的第二代（TPU2.0）可以达到每秒 45 万亿次单精度浮点运算，和英伟达 Xavier 芯片比只快了一倍（TPU2.0 在 Xavier 之后出来，快一倍就不算什么）。在 2018 年谷歌开发者大会上，谷歌又宣布了第三代（TPU3.0），宣称比 TPU2.0 快 8 倍。由于功耗的提高，所以第三代芯片已经需要液体制冷。一个第三代的 TPU 集群（一个机柜）有 64 块板卡，每块板卡上有 4 个 TPU，总运算速度可以达到每秒 8×45×4×64=92 160 万亿次浮点计算。

附录 4：机器学习的主要编程框架

TensorFlow 是由谷歌大脑团队开发的，主要用于机器学习和深度神经网络的研究。2016 年 5 月，谷歌从 Torch（一种编程框架）转移到 TensorFlow，这对其他编程框架造成了打击，特别是 torch 和 theano。许多人将 TensorFlow 描述成一个比 theano 更现代化的版本，吸取了这些年在新领域 / 技术的许多重要的经验教训。

TensorFlow 以智能、灵活的方式而闻名，是一种高度可扩展的机器学习系统，使其更容易适应不同的新旧产品和研究，并且比较容易安装，还针对初学者提供了教程，涵盖神经网络的理论基础和实际应用。TensorFlow 比 theano 和 torch 慢，但谷歌和开源社区正在解决这个问题。TensorBoard 是 TensorFlow 的可视化模块，它提供了一个计算路径的直观视图。深度学习库 Keras 被移植到 TensorFlow 上运行，这意味着任何用 Keras 编写的模型现在都可以运行在 TensorFlow 上。最后，值得一提的是 TensorFlow 可以在各种硬件上运行。其特点如下。

（1）GPU 加速：支持。

（2）语言 / 界面：Python、Numpy、C++。

（3）平台：跨平台。

（4）维护者：谷歌。

theano 起源于 2007 年在蒙特利尔大学的知名 MILA（学习算法研究所），是用 Python 编写的 CPU/GPU 符号表达式的深度学习编译器。theano 功能强大，速度极快，并且灵活，但通常被认为是一个底层框架。因此，原生 theano 更像是一个研究平台和生态系统，而非深度学习库，它经常被用作高级程序库的底层平台，而这些高级库给用户提供简单的 API。theano 提供一些比较受欢迎的库包括 Keras、Lasagne 和 Blocks。theano 的缺点之一是仍然需要一个支持多 GPU 的方案。theano 的特点如下。

（1）GPU 加速：支持。

（2）语言 / 界面：Python，Numpy。

（3）平台：Linux、Mac OS X 和 Windows。

（4）维护者：蒙特利尔大学 MILA 实验室。

在所有常见的框架中，torch 可能是最容易启动和运行的，特别是在使用 Ubuntu（一种开源电脑操作系统）的情况下。它允许基于神经网络的算法在 GPU 硬件上运行，而不需要在硬件级别进行编码。torch 在 2002 年由纽约大学开发，被 Facebook 和 Twitter 等大型科技公司广泛使用，并得到英伟达的支持。Torch 是用一种叫作 Lua 的脚本语言编写的，这种语言很容易阅读，但并不像 Python 那样通用。有用的错误提示消息、大量的示例代码 / 教程以及 Lua 的简单性让 torch 很容易上手。其特点如下。

（1）GPU 加速：支持。

（2）语言 / 界面：Lua。

（3）平台：Linux、Android、Mac OS X、iOS 和 Windows。

（4）维护者：Ronan、Clément、Koray 和 Soumith。

Caffe 被开发用于利用卷积神经网络的图像分类 / 机器视觉，由 1 000 多名开发人员推动其发展。Caffe 最出名的可能是 ModelZoo 模型，开发者无须编写任何代码就可以直接使用。

Caffe 主要针对工业应用，而 torch 和 theano 是为研究量身打造的。Caffe 不适用于非计算机视觉深度学习应用，如文本、声音或时间序列数据。Caffe 可以在各种硬件上运行，并且 CPU 和 GPU 之间的切换可以通过设置单个标志来完成。Caffe 的运行速度比 theano 和 torch 要慢。其特点如下。

（1）GPU 加速：支持。

（2）语言 / 界面：C、C++、Python、MATLAB、CLI。

（3）平台：Ubuntu、Mac OS X、Windows 实验版。

（4）维护者：伯克利视觉和学习中心（BVLC）。

CNTK 是微软深度学习工具包，是微软的开源深度学习框架。CNTK 在语音社区中比在一般深度学习社区中更为著名，可以用于图像和文本训练。CNTK 支持多种算法，例如 Feed Forward、CNN、RNN、LSTM 和 Sequence-to-Sequence。它可以运行在许多不同的硬件类型上，包括多个 GPU。其特点如下。

（1）GPU 加速：支持。

（3）语言 / 界面：Python、C++、C＃和 CLI。

（4）平台：Windows、Linux。

（5）维护者：微软研究院。

H2O 也称为 H2O.ai，是世界上使用最广泛的开源深度学习平台之一。它被全球超过 8 万名数据科学家和研究人员以及超过 9000 家企业和组织所用，包括为全球最有影响力的一些公司开发关键任务数据产品。H2O 提供基于 Web 的用户界面，同时可以访问机器学习软件库，并开启机器学习的过程。

维基百科中有一张表详细列出了各主要编程框架的参数和特点，链接如下：

https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software。