# 0701. 度量复杂性

这本书讲的是复杂性。但是到现在书中还没有严格定义这个术语，也没有明确回答以下问题：人类大脑比蚂蚁的大脑复杂吗？人类基因组是不是比酵母菌的基因组复杂？生物的复杂性在进化过程中是不断增加吗？直观上这些问题的答案太明显不过了。然而，要想得出一个公认的复杂性定义，来回答这些问题，其中的困难却超乎想象。

2004 年我曾在圣塔菲复杂系统暑期班上组织过一个研究小组。那一年有点特别，因为是圣塔菲研究所创建 20 周年。小组中有圣塔菲一些最杰出的学者，包括法墨尔（Doyne Farmer）、克鲁奇菲尔德（Jim Crutchfield）、弗瑞斯特（Stephanie Forrest）、史密斯（Eric Smith）、米勒（John Miller）、胡伯勒（Alfred Hubler）和艾森斯坦（Bob Eisenstein）—— 都是物理、计算机、生物、经济和决策论等领域的知名学者。暑期班的学生 —— 研究生和博士后层次的青年科学家 —— 在讨论班上可以提问。第一个问题就是，「复杂性该怎样定义？」听到后大家都笑了起来，因为这个问题是如此直截了当，如此让人期待，然而又是如此难以回答。然后多位学者对这个术语给出了各种不同的定义，接着彼此之间又产生了一些争论。学生们都一头雾水。

就连圣塔菲这个复杂系统领域最著名研究所的学者对复杂性的定义都达不成共识，复杂性科学又是如何产生的呢？ 答案是复杂性科学不止一个，而是有好几个，每个对复杂性的定义都不一样。其中一些定义很正式，一些则不那么正式。如果想要有统一的复杂性科学，就得弄清楚这些正式或非正式概念之间的关联。要对过于复杂的复杂性概念进行尽可能地提炼。这项工作目前还远未结束，也许还要等待那些被搞得一头雾水的下一代科学家来完成。

我希望那些同学不要对此太过诧异。只要了解一点科学史就能明白，核心概念缺乏公认的定义是很普遍的。牛顿对力的概念就没有很好的定义，事实上他不是很喜欢这个概念，因为它需要一种魔术般的「远距离作用」，而这在对自然的机械论解释中是不允许的。遗传学作为生物学领域发展最快和最大的学科，对于如何在分子层面上定义基因的概念 [87] 也没有达成一致。天文学家发现宇宙 95% 都是由暗物质和暗能量组成，却不清楚暗物质和暗能量到底是什么。心理学家对思维和概念也没有明确的定义，更不知道它们在大脑中对应的是什么。

这还只是部分例子。科学的进步往往就是通过为尚未完全理解的现象发明新术语实现的：随着科学逐渐成熟，现象逐渐被理解，这些术语也逐渐被提炼清晰。例如，物理学家现在就理解了自然界中所有的力都是四种基本力的组合：电磁力、强相互作用、弱相互作用、引力。基本粒子「远距离作用」的现象也已经被理论化。在量子力学中发展出描述四种基本力的统一理论是物理学现在面临的最大挑战。也许将来我们也会将「复杂性」分解成几个基本方面，并最终将这几个方面结合起来，形成对复杂现象的全面理解。

2001 年，物理学家劳埃德（Seth Lloyd）发表了一篇文章，[88] 提出了度量一个事物或过程的复杂性的三个维度： 描述它有多困难？ 产生它有多困难？ 其组织程度如何？ 劳埃德列出了 40 种度量复杂性的方法，这些方法分别是从动力学、热力学、信息论和计算等方面来考虑这三个问题。我们已经了解了这些概念的背景，现在我们可以来看看其中一些定义。下面我会通过比较人类与酵母菌基因组的复杂性来阐释这些定义。

人类基因组大约有 30 亿组碱基对（即核苷酸对）。据估计人类大约有 25000 个基因 —— 也就是对蛋白质进行编码的区域。让人吃惊的是，只有 2% 的碱基对组成了基因；其余的非基因部分被称为非编码区。非编码区有几个功能：其中一些用来防止染色体解体；一些则帮助调控真正基因的运作；有一些则可能是没有任何作用的「垃圾」或者功能还没有被发现。你肯定听说过人类基因组计划，但你可能不知道还有一个酵母菌基因组计划，这个计划的目标是测定几种酵母菌的完整 DNA 序列。测出的第一种被发现大约有 1200 万组碱基对和 6000 个基因。

## 7.1 用大小度量复杂性

复杂性的一个简单度量就是大小。根据这个度量，如果比较碱基对数量，人类比酵母复杂 250 倍，如果比较基因数量，人类则只比酵母复杂 4 倍。250 倍还是蛮多的，看来人类还是挺复杂，至少比酵母复杂。不过单细胞变形虫的碱基对是人类的 225 倍，拟南芥的基因与人类的大致一样多。

人类显然要比变形虫或芥菜复杂，至少我希望是这样。这就表明用基因组的规模来度量复杂性并不合适；我们的复杂性应该是某种比碱基对或基因的绝对数量更深刻的东西（图 7.1）。

图 7.1 从左上角依顺时针分别是：酵母、变形虫、人类、拟南芥哪个最复杂？如果用基因组长度度量复杂性，那变形虫毫无疑问会跑冠军（如果它有腿的话）

## 7.2 用熵度量复杂性

另一种直接的复杂性度量就是香农熵，在第 3 章曾将香农熵定义为信息源相对于信息接收者的平均信息量或「惊奇度」。举个例子，假设消息由符号 A、C、G 和 T 组成。如果序列高度有序，很容易描述，例如「A A A A A A A……A」，则熵为零。完全随机的序列则有最大可能熵。用香农熵度量复杂性有一些问题。

首先，所针对的对象或过程必须像上面一样转换成某种「消息」的形式。这并不总是那么容易做到，例如，人类大脑的熵该怎么度量呢？另外，随机消息的熵最高。我们可以随机排列 A、C、G 和 T 来人工构造一个基因组，这个随机的基因组几乎不可能有用，却会被认为比人类基因组更复杂。很显然，正是因为基因组不是随机的，而是不断进化从而让基因更有利于我们的生存，例如控制我们的眼睛和肌肉发育，才使得人类如此复杂。最复杂的对象不是最有序的或最随机的，而是介于两者之间。简单的香农熵不足以抓住我们对复杂性的直观认识。

## 7.3 用算法信息量度量复杂性 

人们提出了许多改进方法来用熵度量复杂性。其中最著名的方法由柯尔莫哥洛夫（Andrey Kolmogorov）、查汀（Gregory Chaitin）和索罗蒙洛夫（R ay Solomonoff）分别独立提出，他们将事物的复杂性定义为能够产生对事物完整描述的最短计算机程序的长度。这被称为事物的算法信扇量。[89] 

例如，考虑一个很短的（人工）DNA 序列： `A C A C A C A C A C A C A C A C A C A C`（序列 1） ，一个很短的计算机程序，「打印  AC 10 次」，就能输出这个序列。因此这个序列的算法复杂度很低。作为比较，下面是我用伪随机数发生器生成的一个序列： `A T C T G T C A A G A C G G A A C A T`（序列 2） ，如果我的随机数发生器没有问题，这个序列就不会有可识别的特征，因此程序要长一些，比如「打印字符串 A T C T G T C A A A A C G G A A C A T」。显然序列 1 可以压缩，而序列 2 则不能，因而包含更多算法信息。与熵类似，随机对象的算法信息量也会比我们直观上认为复杂的事物的信息量更大。

物理学家盖尔曼（Murray Gell-Mann）提出了一种称为「有效复杂性」（effective complexity）的相关度量，[90] 更符合我们对复杂性的直观认识。盖尔曼认为任何事物都是规则性和随机性的组合。例如，序列 1 就有非常简单的规则性：重复的 AC 模式。序列 2 则没有规则性，因为它是随机产生的。与之相比，生物的 DNA 则有一些规则性（例如，基因组不同部分之间存在重要关联），也有一些随机性（例如 DNA 中的垃圾）。

为了计算有效复杂性，首先要给出事物规则性的最佳描述；有效复杂性定义为包含在描述中的信息量或规则集合的算法信息量，两者等价。序列 1 具有规则性，即 AC 不断重复。描述这个规则性所需的信息量就是它的算法信息量：程序「打印 AC 数次」的长度。因此，事物的结构可预测性越大，有效复杂性就越低。序列 2 处于另一个极端，因为是随机的，所以没有规则性。因而也不需要信息来描述，虽然序列本身的算法信息量是最大的，序列规则性的算法信息量 —— 其有效复杂性 —— 却为零。

简而言之，就如我们希望的，最有序和最随机的事物有效复杂性很低。能够发育的生物的 DNA 具有许多独立和相关的规则性，这些规则需要可观的信息才能描述，因此会有很高的有效复杂性。显然，问题是我们如何给出这些规则？如果不同观察者对于系统的规则不能达成一致又怎么办？ 

盖尔曼用科学理论的形成做了类比，科学理论的形成实际就是寻找自然现象规律的过程。对于任何现象，都有多个描述其规律的可能理论，但显然理论有好有差，一些更加简洁优雅。盖尔曼对这个很有经验，他极为优雅的理论厘清了当时让人混淆的基本粒子类型及其相互作用，这让他获得了 1969 年的诺贝尔物理学奖。类似的，对于提出的一个事物的各种不同规则集，我们可以利用奥卡姆剃刀（Occam's Razor）来决定哪个是最好的。最好的规则集是能描述事物的最小规则集，同时还能将事物的随机成分最小化。例如，生物学家们现在已经发现了人类基因组的许多规律，包括基因、基因之间的相互作用，等等。但还有许多似乎不遵循任何规则的随机方面 —— 也就是所谓的垃圾 DNA。如果生物学的盖尔曼出现，他也许会找到约束更多基因组的极为简单的规则集。

有效复杂性是很有吸引力的思想，虽然同其他许多度量复杂性的提议一样，很难实际操作。也有批评意见指出，这个定义中的主观性也有待解决。91 

## 7.4 用逻辑深度度量复杂性

为了更加接近我们对复杂性的直觉，数学家班尼特在 20 世纪 80 年代初提出了逻辑深度（logical depth）的概念。

一个事物的逻辑深度是对构造这个事物的困难程度的度量。高度有序的 A、C、G、T 序列（例如前面的序列 1）显然很容易构造。同样，如果我要你给我一个 A、C、G、T 的随机序列，你也很容易就可以做出来，用个硬币或骰子就可以了。但如果我要你给我一个能够生成可发育的生物的 DNA 序列，如果不偷看真正的基因组序列，别说你，任何一个生物学家都会觉得很难办到。

用班尼特的话说，「有逻辑深度的事物 [92] …… 从根本上必须是长时间计算或漫长动力过程的产物，否则就不可能产生。」或是像劳埃德说的，「用最合理的方法生成某个事物时需要处理的信息量 [93] 等同于这个事物的复杂性，这是一个很吸引人的想法。」为了更精确地定义逻辑深度，班尼特将对事物的构造换成了对编码事物的 0/1 序列的计算。例如，我们可以用两位二进制数来编码核苷酸符号：A=00，C=01，G=10，T=11。用这个编码，我们就能将 A、C、G、T 转换成 0/1 序列。然后编写一个图灵机，用编写好的图灵机在空白带子上产生出这个序列，所需要的时间步就是其逻辑深度。

一般而言，多个图灵机都能产生出这个序列，所需的时间也可能不一样多。班尼特还必须说明应该用哪一个图灵机。他提出，应该根据前面提到的奧卡姆剃刀原则，选最短的那个（也就是状态和规则最少的那个）。逻辑深度具有很好的理论特征，符合我们的直觉，但是也没有具体给出度量实际事物复杂性的方法，因为没有寻找生成指定事物的最小图灵机的可操作方法，更不要说如何确定机器运算所需的时间。此外也没有考虑将事物表示成 0/1 序列的困难。

## 7.5 用热力学深度度量复杂性

20 世纪 80 年代末，劳埃德和裴杰斯（Heinz Pagels）提出了一种新的复杂性度量 [94] —— 热力学深度（thermodynamic depth）。劳埃德和裴杰斯的思想与班尼特的思想很相似：越复杂的事物越难构造。不过与图灵机生成对事物的描述所需的时间步不同，热力学深度首先是确定「产生出这个事物最科学合理的确定事件序列」，[95] 然后测量「物理构造过程所需的热力源和信息源的总量」。

例如，要确定人类基因组的热力学深度，我们得从最早出现的第一个生物的基因组开始，列出直到现代人类出现的所有遗传演化事件（随机变异、重组、基因复制，等等）。可以想象，人类进化出来的时间比变形虫要长 10 亿年，热力学深度肯定也大得多。

同逻辑深度一样，热力学深度也只是在理论上有意义，要真的用来度量复杂性也存在一些问题。首先，我们要能列出事物产生过程中的所有事件。另外，也有批评意见指出，[96] 劳埃德和裴杰斯的定义中没有明确界定什么是「事件」。一次遗传变异到底是单个事件还是在原子和亚原子层面导致变异发生的上百万次事件呢？两个祖先的基因重组应当视为单个事件吗？还是应当将导致它们相遇、交配和产生后代的所有微观事件都包括进来呢？用更专业一点的话说，是不清楚如何将系统的状态「粗粒化」—— 也就是说，在列出事件时，如何确定哪些是相关的宏观状态。

## 7.6 用计算能力度量复杂性

如果复杂系统能够执行计算，不管系统是天然的还是人工的，也许有可能用它们的计算的复杂程度来度量它们的复杂性。像物理学家沃尔夫勒姆（Stephen Wolfram）就提出，[97] 系统的计算能力如果等价于通用图灵机的计算能力，就是复杂系统。不过，班尼特等人则认为，[98] 具有执行通用计算的能力并不意味着系统本身就是复杂的；我们应当测量的是系统处理输入时的行为的复杂性。譬如，通用图灵机本身并不复杂，但是有了程序和输入，进行了繁复的计算，就能产生复杂的行为。

## 7.7 统计复杂性

物理学家克鲁奇菲尔德和卡尔·杨（Karl Young）定义了一个称为统计复杂性 [99] （statistical complexity）的量，度量用来预测系统将来的统计行为所需的系统过去行为的最小信息量。物理学家格拉斯伯杰（Peter Grassberger）也独立给出了很类似的定义，称为 有效度量复杂性 。

统计复杂性与香农熵相关，定义中系统被视为「消息源」，其行为以某种方式量化为离散的「消息」。对统计行为的预测需要观测系统产生的信息，然后根据信息构造系统的模型，从而让模型的行为在统计上与系统本身的行为一致。例如，序列 1 的信息源模型可以很简单：「重复 AC」；因此其统计复杂性很低。然而，与熵或算法信息量不同，对于产生序列 2 的信息源也可以有很简单的模型：「随机选择 A、C、G 或 T。」这是因为统计复杂性模型允许包含随机选择。统计复杂性的度量值是预测系统行为的最简单模型的信息量。

因此，与有效复杂性一样，对于高度有序和随机的系统，统计复杂性的值都很低，介于两者之间的系统则具有高复杂性，与我们的直觉相符。同前面描述的度量一样，度量统计复杂性也不容易，除非面对的系统可以解读为信息源。不过克鲁奇菲尔德、杨和他们的同事实际测量了一系列真实世界现象的统计复杂性，比如复杂晶体的原子结构 [100] 和神经元的激发模式 [101] 。

## 7.8 用分形维度量复杂性

前面讨论的复杂性度量都是基于信息论和计算理论的概念。但这并不是复杂性度量的唯一可能来源。还有人提出用动力系统理论的概念度量事物复杂性的方法。其中一个是用事物的分形维 （fractal di-mension）。要解释什么是分形维，还要先解释一下什么是分形。

分形最经典的例子是海岸线。从空中俯瞰下去，海岸线崎岖不平，有许多大大小小的海湾和半岛（图 7.2，上图）。如果你下去沿着海岸线游览，它似乎还是一样的崎岖不平，只是尺度更小（图 7.2，下图）。如果你站在沙滩上，或是以蜗牛的视角近距离观察岩石，相似的景象还是会一次又一次出现。海岸线在不同尺度上的相似性就是所谓的「自相似性」。

1『又见分形，《规模》那本书里专门有讲到，而且规模的作者我记得也是圣塔菲研究所的大牛。（2021-01-24）』

图 7.2 上图：爱尔兰岛鸟瞰图片，其海岸线有自相似（分形）特征。下图：爱尔兰海岸线的局部图片。这个尺度上的崎岖结构与更大尺度上的崎岖结构类似

分形一词是由法国数学家曼德布罗特（Benoit Mandelbrot）提出来的，曼德布罗特认识到自然界到处都有分形 —— 现实世界中许多事物都有自相似结构。海岸线、山脉、雪花和树是很典型的例子。曼德布罗特甚至提出宇宙也是分形的，[102] 因为就其分布来说，有星系、星系团、星系团的聚团，等等。图 7.3 展示了自然界中一些自相似性的例子。虽然人们对分形一词的意义有时候有不同理解，但一般来说分形指的是「在任何尺度上都有微细结构」的几何形状。[103] 许多让人感兴趣的分形具有自相似特性，海岸线就是这样的例子。

第 2 章中逻辑斯蒂映射的分叉图（图 2.10）也具有一定程度的自相似性，事实上，许多系统的混沌域（在逻辑斯蒂映射中是 R 大于 3.57 的部分）常被称为分形吸引子。

图 7.3 自然界中一些分形结构的例子：树、雪花（显微镜放大）、星系团

曼德布罗特等数学家为自然界中的分形设计了各种数学模型。其中一个很有名的例子是科赫曲线（Koch Curve，以发现这种分形的瑞典数学家命名）。科赫曲线是通过不断应用一条规则得出： 

1、从一条直线段开始。

2、应用科赫曲线规则：「将每段线段等分成三段，中间一段替换为一个三角形的两条边，每一边都等于原线段的 1/3。」因为只有一条线段，应用这个规则后变为：

3、对生成的图形再次应用科赫曲线规则，不断继续。下面是迭代了两次、三次和四次之后的情形： 

最后一张图有点像一条理想化的海岸线。（如果你向左旋转 90 度，然后斜着看，还真有点像阿拉斯加西海岸。）不过它是严格自相似的：曲线的部分，以及部分的部分，都与曲线整体是一样的形状。如果我们将科赫曲线规则应用无数次，图形在无数尺度上都将是自相似的 —— 完美的分形。而真正的海岸线并不严格自相似。如果你观察海岸线的一小段，它并不与整段海岸线的形状完全一样，而是在许多方面相似（例如，蜿蜒崎岖）。另外，在真实世界中，自相似在无穷小的尺度上并不成立。为了简单起见，海岸线这类真实世界的结构通常被称为「分形」，但更严格的叫法应该是「类分形」（fmctal-like），特别是有数学家在场的时候。

我们熟悉的空间维度的概念对于分形完全不适用。直线是 1 维，平面是 2 维，立方体是 3 维。那科赫曲线是几维呢？ 

首先我们来看看直线、正方形和立方体这些常规几何对象的维数到底指的是什么。先来看看直线段。将其一分为二。然后将得到的线段再二分，每次都将各段线段一分为二： 

每一次得到的图形都是由两个上次缩小一半的拷贝组成。再来看看正方形。从各边将其二分。然后将得到的正方形继续从各边二分，这样不断二分下去。每次得到的图形都是由上次四分之一大小的 4 个拷贝组成。你可能已经猜到下面做什么了，将立方体从各边二分。将得到的立方体不断二分： 每次得到的都是由上次八分之一大小的 8 个拷贝组成。

这里已经能够看出维度的意义。一般而言，每次得到的图形都是由上次缩小的拷贝组成，而拷贝的数量则是 2 的维数次幂（2 维数）。对于直线，是 `2^1=2` 个拷贝；对于正方形，是 `2^2=4` 个拷贝；对于立方体是 `2^3=8` 个拷贝。类似的，如果不是二分，而是将各边三分，则每次得到的图形是上次的 3 维数个拷贝。

2『这里阐述的「维度意义」真巧妙，做一张信息数据卡片。（2021-01-24）』——已完成

由此可以总结出一个规律： 将几何结构从各边分成 X 等份，不断重复这个过程。每次得到的将是前一次的个拷贝。根据维数的这种定义，直线是 1 维，正方形是 2 维，立方体是 3 维。都没有问题。现在将这个定义类推到科赫曲线。每次直线段都是之前的 1/3 长，而得到的则是之前的 4 个拷贝。根据前面的定义，应该是 3 维数 = 4。维数是多少呢？这里我们直接给出结果 104 （计算过程在注释中给出），根据前面的规律，维数约为 1.26。也就是说，科赫曲线既不是 1 维也不是 2 维，而是介于两者之间。太奇怪了，分形的维数居然不是整数。这正是分形的奇特之处。简而言之，分形维数 105 决定了物体的自相似拷贝的数量。同样，分形维也决定了随着层次的变化，物体总的大小（或者面积、体积）会如何改变。例如，如果你在每次应用规则后测量科赫曲线的总长度，你会发现每次长度增加为原来的 4/3。只有完美的分形 —— 可以缩小直至无穷 —— 才有精确的分形维数。

像海岸线这类真实世界的有穷类分形事物，我们只能测量近似的分形维数。为了解释分形维数的直观意义，有过很多尝试。譬如，认为分形维表示了物体的「粗糙度」、「凸凹度」、「不平整度」或「繁杂度」；物体的「破碎」度；还有物体的「结构致密」程度。例如，比较爱尔兰（图 7.2）和南非（图 7.4）的海岸线，前者的分形维数比后者更高。还有一种说法挺有诗意的，我很喜欢，即认为分形维数「量化了物体细节的瀑流」。[106] 也就是说，当你沿着自相似的瀑流越走越深时，它决定了你能看到多少细节。如果结构不是分形的，譬如平滑的大理石，你将它的结构不断放大，将不会出现有意思的细节。而分形则在所有层面上都有有趣的细节，分形维数一定程度上量化了细节的有趣程度与你观察的放大率之间的关系。这也就是为何人们对用分形维数度量复杂性感兴趣，许多科学家都用其来度量真实世界的现象。不过，除了崎岖度和细节瀑流，还有许多其他种类的复杂性我们也希望能进行度量。

图 7.4 南非海岸线

## 7.9 用层次性度量复杂性

1962 年，西蒙（Herbert Simon）发表了一篇著名的文章 —《复杂性的结构》 [107] ，文中西蒙提出一个系统的复杂性可以用层次度（degree of hierarchy）来刻画：「复杂系统由子系统组成，[108] 子系统下面又有子系统，不断往下。」西蒙是位杰出的学者，他博学多识，既是政治学家、经济学家，又是心理学家，他的成就用一章的篇幅来讨论也不为过。

1-2『又见赫伯特·西蒙，好亲切。已下载论文「2021004The Architecture of Complexity」并存入 Zotero。（2021-01-24）』

西蒙认为，复杂系统最重要的共性就是层次性和不可分解性。西蒙列举了一系列层次结构的复杂系统 —— 例如，身体由器官组成，器官又是由细胞组成，细胞中又含有细胞子系统，等等。某种程度上，这个观念与分形在所有尺度上都自相似类似。不可分解性指的是，在层次性复杂系统中，子系统内部的紧密相互作用比子系统之间要多得多。例如，细胞内部的新陈代谢网络就比细胞之间的作用要复杂得多。

西蒙还认为，进化之所以能设计出自然界中的复杂系统，正是因为它们能像砖块一样被结合到一起 —— 也就是说，具有层次性和不可分解性。细胞能够进化，从而成为高一级器官的建筑模块，组成的器官又可作为更高一级器官的建筑模块。西蒙认为复杂系统研究需要有一个「层次理论」。

许多人探讨了用层次性度量复杂性的可能途径。例如进化生物学家麦克西（Daniel McShea）就一直想厘清生物随着进化复杂性增加的意义，他提出了一种层次标度，[109] 可以用来度量生物的层次度。麦克西的标度是用嵌套层次定义：高一级的对象嵌有低一级的对象作为组分。麦克西提出了以下嵌套的生物学层次： 

层次 1：原核细胞（最简单的细胞，例如细菌）； 层次 2：层次 1 生物的聚合，例如真核细胞（更复杂的细胞，由原核细胞合并进化而来）； 层次 3：层次 2 生物的聚合，例如所有多细胞生物； 层次 4：层次 3 生物的聚合，例如昆虫群落和僧帽水母这样的群体生物。

随着嵌套继续，每一层可以说都比上一层更复杂。不过，就像麦克西所指出的，嵌套仅仅描述了生物的结构，而不涉及其功能。麦克西用化石和现代生物的数据揭示了生物的最高层次随着进化而不断增加。因此可以用此说明复杂性随着进化不断增加，虽然在度量具体生物的层次度时对于何为「组成」或「层次」存在一些主观性。还有很多度量复杂性的方法，在这里无法 一一 赘述。各种度量都抓住了复杂性思想的一些方面，但都存在理论和实践上的局限性，还远不能有效刻画实际系统的复杂性。度量的多样性也表明复杂性思想具有许多维度，也许无法通过单一的度量尺度来刻画。