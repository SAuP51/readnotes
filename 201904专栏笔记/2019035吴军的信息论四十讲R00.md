# 2019035吴军的信息论四十讲

## 记忆时间

## 卡片

### 1. 反常识卡——

这本书的主题核心，就是最大的反常识卡，并且注意时间脉络。

### 2. 术语卡——

根据反常识，再补充三个证据——就产生三张术语卡。

### 3. 术语卡——

### 4. 术语卡——

### 5. 人名卡——香农

根据这些证据和案例，找出源头和提出术语的人是谁——产生一张人名卡，并且分析他为什么牛，有哪些作品，生平经历是什么；

香农

### 6. 金句卡——

最后根据他写的非常震撼的话语——产生一张金句卡。

### 7. 行动卡——

行动卡是能够指导自己的行动的卡。

### 8. 任意卡——

最后还有一张任意卡，记录个人阅读感想。

## 模板

### 1. 逻辑脉络

用自己的话总结主题，梳理逻辑脉络，也就是在这个专栏的整个地图里，这一章节所在的节点。

### 2. 摘录及评论

1『自己的观点』

2『行动指南』

3『与其他知识的连接』

## 00. 发刊词

信息时代的最大特征是不确定性。而信息论就是这半个世纪以来，人类对抗不确定性，最重要和有效的方法论。今天的人，已经无法通过掌握几条不变的规律，工作一辈子；也难以通过理解几条简单的人生智慧，活好一辈子。一个通用规律就能解决一切问题，一个标准答案就能让人一劳永逸的时代，一去不复返了。

当前的整个世界充满了不确定性。自从 20 世纪之后，当确定性的规律被人类认识得差不多了之后，世界本身所固有的不确定性，给大家带来的麻烦就越来越凸显出来了。量子力学给物理学带来的危机，其实就是这种不确定性影响的体现。

近代科学的发展，又给人类掌握有关不确定性的规律提供了钥匙，这就如同 300 多年前牛顿等人找到了通过确定性消除无知的方法一样。20 世纪初概率论和统计学的成熟，使人们得以把握随机性。在此基础上，1948 年，香农博士找到了不确定性和信息的关系，从此为人类找到了面对不确定性世界时的方法论，也就是利用信息消除不确定性。可以说，这是随后的半个多世纪里，最重要的方法论。

世界上的知识，可以分为道和术两个层面，我们这门课讲的是道的层面的知识，它不会讲述任何具体的方法，比如信息的采集、处理或者传输的理论细节。这样，我们就能够把重点放在讲述用信息论指导做事的方法上，以便让我们能够在不断变化，而且充满不确定性的世界里把握住机会，立于不败之地。

2『去了解一下有关信息的采集、处理、传输的理论细节。』

课程主要包括三个模块的内容，它们分别围绕着信息的产生，信息的传输和信息的应用展开。基本都会采用两种方式讲解：一种方式是从分析事件或现象背后的信息论原理出发，讲解信息论的概念；另一种是从信息论的原理出发，分析我们今天的做事方法。

模块一信息产生：在面对大量信息时，排除噪音，提取利用有效信息，科学做决策的能力；模块二信息传播：向外界传递信息时，平衡分配有限资源，增加沟通带宽，放大影响力的能力；模块三信息应用：看懂信息应用的逻辑和通信发展的趋势，提前抓住新机遇的能力。

世界上任何一个探索者都需要清楚三件事：我们现在的位置，我们的目标，以及通向目标的道路。我们知道哲学是一门生活的艺术，它帮助我们认清自己，它回答了第一个问题。至于每一个人的目标，我相信大家比我更清楚。而第三件事其实是方法论或者说是理性的工具，也正是我想要给你的，相信大家有了信息论这个工具，就能更好地应对当下充满不确定性的世界，达成自己的目标。清楚了这三件事，我们便不再需要焦虑，内心就得到安宁。

1、根据信息论对信息量单位比特的定义，如果存在两种情况，它们出现的可能性相同，都是 50%，这时要消除其不确定性所需要的信息是 1 比特。但是如果一种情况发生的可能性大，另一种发生的可能性小，所需要的信息就不到 1 比特。比如说，一种情况出现的概率是 1/3，另一种是 2/3，这种情况下消除不确定性的信息量则降低到 0.9 比特。

2、为什么盟军骗过了德军，却还是损失惨重？因为德军采用了信息论中一种非常好的对策，也就是不把鸡蛋放在一个篮子中，他们在诺曼底也严密设防了。因此，德军在得到信息前和得到信息后，策略差不多，这 1 比特的信息作用就不大了。改变世界的情报，信息量可能 1 比特都不到，而且这 1 比特的信息很可能还会遇到更高明的应对策略，失去价值。

3、用很少的信息驱动很大的能量。在控制理论中有一种开关电路，控制这个开关只需要一比特的信息或者极低的能量，但是经过它的电流（可以被认为是能量）却能近乎无限大，今天我们很多电器中那些弱电控制强电的元器件就是利用这个特点工作的。

对一个未知系统（黑盒子）所作出的估计和真实情况的偏离，就是信息的损失，偏离越多损失越大。

[You Can Always Get What You Want — But Not What You Need (Graduation 2016) - YouTube](https://www.youtube.com/watch?v=QCw_D7dr7Rw)

2『上面的视频已下载，2019001。』

信息和能量一样，都是宇宙本身固有的特性。

世界上没有绝对的可靠，只有可靠性的高和低，信号和噪音比率的高和低。从信息论上说，信息的可靠性就叫做置信度。

提出问题比解决问题更重要，因为提出问题的人，开创了一个重要的却是完全未知的领域，回答问题并且给出最初解答的人，由于通常只是在某种认识阶段上对未知的领域提供一些有限的信息，因此贡献有限，被认可的程度不高。

其次，沃森和克里克这篇论文的逻辑很清晰，他们首先否定了鲍林之前的三螺旋结构，因为和一些已知的信息不相符。然后，他们提出了自己的双螺旋模型，那一段关键描述只有 105 个单词（包括 the，a ，so 这些虚词）。最后，他们说明了自己的模型同威尔金森、富兰克林观察到的信息相吻合。沃森和克里克等人提供了一个「利用能够看到的信息，解释看不到的信息」的绝佳案例。他们的论文和理论也验证了信息论中一条被称为「奥卡姆剃刀」的原则。

你可能会奇怪蛋白质不是基本的生命单元，怎么还能通信？其实我们的细胞膜上有 G 蛋白偶联受体（GPCRs），它们可以探测荷尔蒙、气味、化学神经传递，和其它外界刺激，在解码后激活细胞内不同的 G 蛋白，转而引发各种生命活动，这便是生物体工作的原理之一。

下面这一张图，是 G 蛋白偶联受体的工作示意图，G 蛋白偶联受体（蓝色）被外界信号，也就是紫色的小球入侵时，激活相应的 G 蛋白（金色的和红色的）。

G 蛋白是蛋白质中最大的家族，有多达 800 种蛋白质，在搞清楚了它们的通信原理之后，对了解人的生命现象，治疗疾病有极大意义。事实上，世界上 1/3 到一半的药物靶标都和 G 蛋白有关。科比尔卡在 2007 年完成了上述研究，并且在《科学》杂志上发表了一篇 8 页纸的论文，并因此获得了 2012 年的诺贝尔奖。从这个例子我们可以看出，控制生命活动的信息，其实并不被我们主观控制，但是我们可以发现它们。

给大家举了四类大数据思维应用在商业上的成功案例：第一类是解决人工智能问题，是利用数据（信息）消除不确定性，这是香农信息论的本质，也是大数据思维的科学基础；第二类是利用大数据进行精准服务，从中你可以看出一个商业趋势：公司从重研究方法到重数据收集的转变；第三类是动态调整做事策略，足够多的数据可以帮助我们动态匹配最佳结果；最后一类是利用大数据发现未知规律，这背后涉及互信息的理论，是我们后面课程的重点内容。

这四类大数据应用，都在传达一个信息，那就是大数据的关键是思维方式的变化。

先说说第一类，解决人工智能问题。世界上利用大数据解决的第一个智能型的问题是语音识别，接下来是机器翻译。语音识别的历史正好和电子计算机一样长，可以追溯到 1946 年。但是一直做得非常不成功。

到了 60 年代末，计算机已经进入到第三代了（基于集成电路的），语音识别才只能做到识别十个数字加上几十个单词，而且错误率高达 30%。这样水平的系统是不可用的，因为如果每十个词就错三个，你就无法复原原来的意思了。因此，在 60 年代初，就有人认为语音识别和治愈癌症、登月、水变油一样，近乎不可能。到了 70 年代，康奈尔大学著名的信息论专家贾里尼克来到 IBM，负责该公司的语音识别项目。贾里尼克是一位天才，他从博士毕业到成为讲席教授，教科书的作者，也就是十年时间。

在贾里尼克之前，人们觉得识别语音是一个智力活动，比如我们听到一串语音信号，脑子会把它们先变成音节，然后组成字和词，再联系上下文理解它们的意思，最后排除同音字的歧义性，得到它的意思。为了做这件事，科学家们就试图让计算机学会构词法，能够分析语法，理解语义。但这件事证明是不可行的。贾里尼克在到 IBM 之前并没有做过语音识别，他也不懂得传统的人工智能。事实上，直到死他都不认为他是人工智能专家。由于不受到传统的人工智能思想的约束，他得以用信息论的思维方式来看待语音识别问题。他认为语音识别是一个通信问题。

贾里尼克是这样考虑问题的。当说话人讲话时，他是用语言和文字将他的想法编码，这就变成了一个信息论的问题。语言和文字无论是通过空气传播，还是电话线传播，都是一个信息传播问题，在通信中有一套对应的信道编码理论。在听话人，也就是接收方那里，他再做解码的工作，把空气中的声波变回到语言文字，再通过对语言文字的解码，得到含义。

于是，贾里尼克就用通信的编解码模型，以及有噪音的信道传输模型，构建了语音识别的模型。但是这些模型里面有很多参数需要计算出来，这就要用到大量的数据，于是，贾里尼克就把上述问题又变成了数据处理的问题了。

在这样的思想指导下，贾里尼克裁掉了 IBM 全部的语言学家，并且对各种仿生学，比如研究人耳蜗的模型完全不感兴趣，他只注重收集数据，训练各种统计模型。贾里尼克思想的本质，是利用数据（信息）消除不确定性，这就是香农信息论的本质，也是大数据思维的科学基础。这就是第一类应用，把人工智能问题变成数据问题带给我们的启示。

今天，微软的搜索效果没有 Google 的好，不是技术不行，而是数据量不够。对于那些常见的搜索，大家其实水平差不多，微软差就差在了那些很少见到的长尾搜索关键词上。但是不管怎样，这两家公司已经从重视方法研究，转为重视信息收集和处理了。这是我们透视大数据思维的第二类应用，利用大数据进行精准服务，得出的一个趋势，供大家借鉴。

接下来说说动态调整做事策略的问题，也就是第三类服务。其实，滴滴公司远不是第一家网约车公司，之前一些公司，过分强调司机和乘客之间的固定，比如 A 乘客坐 B 司机的车子比较满意，他下次依然希望提前预订 B 司机的服务。这件事在优步和滴滴都是不允许的，但是一些网约车公司是允许的。这两种做法有什么差别呢？虽然没有人都能举出各自的利弊，但是，对于一个不断变化的打车人群分布和车辆分布，利用数据做动态调整是效率最高的策略。当然，如果你没有足够多的数据，一共只有 200 辆车，5000 个人的数据，你是做不到这一点的。反过来，如果有了足够多的数据，是否在理论上有保障，只要调整的次数足够多，就能收到最佳匹配呢？答案是肯定的，具体为什么，我们在后面会讲到。

最后说说如何利用大数据发现不知道的规律。我在之前好几次课上都讲过，今天研制一款新药需要 20 年时间，20 亿美元的投入，这是惊人的投入。能否减少这方面的研发成本，缩短研发周期呢？如果按照过去的做法工作，即使再努力，能提升的空间也有限。后来大家换了一个思路想问题，那就是让处方药和各种疾病重新匹配。

比如斯坦福大学医学院发现，过去一种治疗心脏病的药治疗胃病效果很好，于是他们直接进入小白鼠试验，然后进入了临床试验。由于这种药的毒性已经试验过了，因此临床试验的周期短了很多。这样，找到一种新的治疗方法平均只需要 3 年时间，投资 1 亿美元。当然，找到药和病的配对，本身是一个大数据问题。这种做事的方法能够成立，背后是有信息论理论依据的，即所谓的互信息理论。

## 01. 信息的产生

### 1. 逻辑脉络

1、信息的定义是消除不确定性（信息的度量单位是比特，一个充满不确定性的系统是个信息源，这个系统里的不确定性是信息熵，而信息就是用来消除这些信息熵的）；

2、如何对信息进行编码（各种编码系统本质上都是在编码复杂性和长度之间寻找平衡，它们数学上是等价的。香农第一定律告诉我们，只要编码设计的足够巧妙，就可以找到最短编码）；

3、如何设计一个好的编码系统（信息的编码要同时具有易辨识和有效性）；

4、理解哈夫曼编码（将最短的编码给出现概率最大的信息，一条信息编码的长度和出现概率的对数成正比），学会把哈夫曼编码运用到生活中，融入血肉里；

5、信息矢量化的原理及其意义；

### 2. 摘录及评论

佐尔格给斯大林的信息作用很大，但是信息量其实不到 1 比特，那到底怎么去衡量信息量的大小呢？在香农之前，人们并不认为信息还能像重量、体积、电流一样可以用什么单位去衡量。

人们过去绞尽脑汁试图从信息的内容出发，通过对比重要性，度量信息。香农说，这条路其实走错了。对于一条信息，重要的是找出其中有多少信息量，要搞清楚「信息量」，就要对信息进行量化的度量。但人们始终没找到量化度量信息的桥梁，也就是缺少一个合适的「衡量单位」，比如你用天平称重，需要在另一边摆放相应重量的砝码，那衡量信息的砝码是什么呢？

香农最大的贡献在于找到了这个「砝码」，也就是将信息的量化度量和不确定性联系起来。他给出一个度量信息量的基本单位，就是我们第一讲所讲的「比特」。

「比特」是这样定义的：如果一个黑盒子中有 A 和 B 两种可能性，它们出现的概率相同，那么要搞清楚到底是 A 还是 B，所需要的信息量就是一比特。如果我们对这个黑盒子有一点知识，知道 A 的概率比 B 大，那么解密它们所需要的信息就不到一比特。

那么如果是多于 A、B 这两种可能性，更复杂的黑盒子，要消除它的不确定性需要多少信息呢？比如我们做选择题四选一，或者猜世界杯足球赛的冠军是谁，他们想知道结果需要多少信息呢？如果我们对选择题的答案一无所知，去向一个知道答案的预言家请教，他每给你一个是非的答案，收取你一块钱。对我们来讲，有效的提问方式不是问他「是否答案是 A，或者是否答案是 B」，而应该先问他，「是否答案在 A、B 中」。如果他回答「是」，我们就圈定答案的范围是 A 或者 B，与 C、D 无关。接下来，再问一个问题就能确定是 A 还是 B 了。反之，当我们知道答案不在 A、B 中，我们也可以用第二个问题确定是 C 还是 D。这样我们一共付 2 块钱就可以了。

1『二分法的思路。』

我们把这样充满不确定性的黑盒子就叫做「信息源」，它里面的不确定性叫做「信息熵」，而「信息」就是用来消除这些不确定性的（信息熵），所以搞清楚黑盒子里是怎么一回事，需要的「信息量」就等于黑盒子里的「信息熵」。

熵其实是一个热力学的概念，表示一个系统的无序状态，或者说随机性。比如把冰水倒进一杯开水中，它们会彼此融合，杯子里的「熵」，也就是混乱程度会增加；在信息系统中也是如此，信息熵则表示一个系统内部的不确定性。

我们都知道，一个系统中的状态数量，也就是可能性，越多，不确定性就越大；在状态数量保持不变时，如果各个状态的可能性相同，不确定性就很大；相反，如果个别状态容易发生，大部分状态都不可能发生，不确定性就小。这段原理其实很简单，你先记住它，接下来我给你详细讲解。

香农把这个原理呢，用公式表示出来了，从此信息不仅可以度量了，信息熵也可以计算了。信息熵的公式：

我们大家不用搞懂公式，但要明白这个公式的原理。我先解释什么叫「一个系统中的状态数量（即可能性）越多，不确定性就越大」。比如，你买彩票，只有两个号，其中一个必中彩，不确定性就小，那么这个问题的信息熵就小。如果有 10000 个号，也是其中有一个必中彩，那不确定性就大了。

接下来，我再解释这半句：「在状态数量保持不变时，如果各个状态的可能性相同，不确定性就很大……」我们现在假定可能性的数量是固定的，比如在只有两种情况时，也就是非 A 即 B 的情况，信息熵的变化图类似一个抛物线：

图中的横轴是 A 发生的概率，它从 0 到 1 分布，纵轴就是熵，也就是确定它发生，你需要的信息量。你会发现，当 A 发生的概率正好是 1/2 时需要的信息熵达到顶峰，是一比特。这就类似抛一枚均质的硬币，谁也猜不好结果，因为正反两种结果发生的概率一样，都是 1/2。但是，如果这枚硬币没造好，一面重，一面轻，那就大概率是重的那面朝下，需要确定它哪面朝下的信息量就小。这告诉我们，永远不要听那些正确率总是 50% 的专家的建议，因为那相当于什么都没说，没有提供能够减少「信息熵」的「信息量」。

最后半句：「相反，如果个别状态容易发生，大部分状态都不可能发生，不确定性就小。」其实是这个意思：如果你买彩票要从 10000 个号里选出一个中奖的，不确定性就大多了。不过，如果其中一个号中彩的可能性是 99%，剩下所有的号加起来的可能性只有 1%，这个问题就比较确定，熵就小。

你知道了信息有单位，还可以通过公式计算，那又有什么用呢？大家都知道赌球的庄家总是稳赚不赔，就觉得里面猫腻很多，这次我带你从信息论的角度来看清这个问题。你会发现其实很多类似的复杂难题都是信息熵的计算问题。

假如，我们能提前确定各个球队获得世界杯冠军的概率，设定它们分别是 P1，P2，……，P32。那么我们套用上面的公式，就可以算出这件事需要多少信息，或者说这个问题的信息熵。我们假定为 3.4 比特，或者说对应于 3.4 块钱。如果有一个人提一次问题支付一块钱，从理论上讲，所有参加赌局的人只要平均支付 3.4 块钱就能得到谁是冠军这个信息。但是如果设定赌局的人将收费标准略微提高，提高到一个人平均 4 元。这里面的盈余就被设赌局的人拿走了。

那你会说，我们不可能提前知道概率，那每个球队得冠军的概率是如何预估的？其实这是我们这些下注的人告诉设赌局的人的。如果大家都往德国队身上下注，结果预测德国获冠军的概率就很高，所以押注的多少其实就是大家给出的概率。而开赌局的，只要收费比信息实际的价值高，都是稳赚不赔的。这里面的细节大家不用太在意，总之记住一点，就是开赌局的从来不是拿自家的钱和你对赌，而是让你们彼此互相赌，他通过变相多收费盈利。

你可能听说过「结构化的投资证券」（Structured Notes），比如说石油的价格上涨到 100 美元以上，每 1 美元高盛就付给你 1.5 美元。但是，如果没有到 100 美元，你需要每个月付给高盛 1 美元。这种投资工具，就被做成一种结构化的投资证券。像航空公司或者运输公司因为害怕油价浮动太高，会购买这样的投资产品。那么你以为是高盛在和石油公司，或者其他人对赌么？不是的，因为高盛转手就将和它完全相反的投资产品，卖给了希望油价波动的人。当然，高盛会包装得很好，让两边都感谢它，其实它才是真正挣钱的一方。

你可能听说过金融数学这个专业，那里面的人天天做的事情就是设计这种不容易为人所看懂的，自己永远不赔钱的金融产品。而所谓的基金经理，很多就是把这样的产品卖给你的人。因此，多了解信息论和基本的数学常识，可以在生活中省下不少冤枉钱。希望你知道，很多交易和产品都是利用了信息的可度量性，知道了这点，就可以看清很多复杂交易背后的原理。

掌握了信息量化度量的原理，你还可以用它来对付当今「信息过载」的问题，比如如何判断一篇报道里到底有多少信息量。信息说到底是用于消除不确定性的。如果讲的事情大部分大家都知道，信息量就很少。这也是为什么那些心灵鸡汤的文章大家不愿意读，并非是它们说的不对，而是没有信息量。

在非洲的草原上，食草动物在发现狮子或者狼狗接近它们的时候，会发出预警的声音，然后大家一起逃命。这其实就是使用和传播信息，而那声怪叫，就是一种信息编码。

在没有信息论之前，信息编码的复杂度通常和要传播的信息种类数量有关。早期人类了解和需要传播的信息是很少的，因此他们并不需要语言和数字，只需要发出不同的叫声，或者做些不同的手势和肢体接触即可。

但是随着人类的进步和文明的进展，需要表达的信息也越来越多，不再是几种不同的声音就能完全覆盖，语言就此产生。人们生活的经验，作为一种特定的信息，其实是那个时代最宝贵的财富，他们通过口述的语言传给了后代。同时，由于人类开始拥有一些食物和物件，便有了多和少的概念，因此数字也就产生了。

早期人类对信息的编码，基本上是每一种信息，都有一种相应的编码。要想表达 5 这个数字，就伸五个指头，但是很快人的十个指头就不够用了，于是早期不少文明就把脚给用上了。在历史上一些文明采用 20 进制，比如玛雅文明。另一些文明多少留有了 20 进制的痕迹，比如英语中 20（score）这个词，就是如此。再后来，手脚并用也不够了，于是人类就在石头和骨头上划道道。再往后，当数字多到划道道也无法表达时，就有了对数字的编码，也就是各种文明的数字——用有限数字的组合可以表示更多的数。

我们如果要表达 100 个数字，一个办法是设计 100 个不同的编号，让它们一对一对应，另一种是只设计几种编号，然后相互组合，来表达 100 个数。你可能觉得第二种方法更简洁，但这两种方法，在信息论中是等价的。假定我们有 100 个数，从中挑出一个，不确定性是 100 选 1，用上节课学得信息论的公式表达，它所代表的信息熵为：

	log100=6.65

6.65 比特。也就是说，如果我们有 6.65 比特的信息，就可以确定 100 个数中的一个。接下来，我们看看刚才说的两种编码需要的信息量是否一样。

我们先来试第一种编码，也就是一一对应，比如用 100 种奇形怪状的符号对应这 100 个数字，这种编码所能表示的信息量，其实就是 100 选一的问题，也就是 log100=6.65 比特。由于一个编码正好表示一个数，因此编码的长度为一。第二种编码方法是采用十进制编码，也就是用 10 种符号，每个符号所代表的信息量只有「log10=3.325」比特，但是 10 个符号想表示 100 个数字，就需要两两组合。也就是说，一个符号无法消除 100 个数中的不确定性，这样两个符号的信息量加起来还是 6.65 比特，正好可以消除 100 个数的不确定性。这样的编码系统比较简单，但是编码的长度是前一种的两倍。这个十进制的做法呢，就类似我们现在用到的阿拉伯数字 0~9。

当然，我们还可以用二进制编码，就是只有 0 和 1 这两个符号，它们所包含的信息只有「log2=1」比特，如果我们想用它们来表达 100 个数，则需要 6.65 个码。进位取整以后，也就是 7 位的码长，才能表示 100 个数字。符号越少，意味着码位越长，所以你看到二进制通常是一长串的 0101……由此可见，对数字的各种编码其实是等价的，无非是平衡编码复杂性和编码长度之间的关系。

对于数字，如果采用很多个符号，编码长度就短，但是系统就复杂。比如我们如果采用的是 20 进制，编码长度短了，但这就意味着它的编码系统很复杂，要记住的符号很多，大家学数学就太麻烦了。在历史上即使有这样的文明，在竞争中也会被淘汰。玛雅文明发展不快的一个原因，就和它的计数和书写系统太复杂有关。

相反，如果采用很少的符号编码，比如采用二进制，编码的长度就长。比如 100 在二进制中的编码是 1100100。所幸的是，各种编码系统在数学上是等价的，我们可以为人类找一个自己方便使用的，也可以为计算机找一个它方便使用的。

但是要说明的是，由于它们是等价的，在一个编码系统中解决不了的问题，换一个系统同样解决不了。一些媒体讲，由于量子计算不是二进制的，因此它能解决今天计算机解决不了的问题，这个说法显然缺乏常识，因为任何进制都是等价的。

当然对数字的编码不能有半个，因此如果我们采用二进制对 100 个数编码，刚才计算出来是需要 6.65 个码，那就要取下一个整数，编码的长度也就是 7 了。于是我们就得到了信息论中一个重要的公式：

	编码长度 ≥ 信息熵（信息量）/ 每一个码的信息量

香农对此作出了严格的数学证明，他同时还证明，只要编码设计得足够巧妙，上面的等号是成立的，这就是著名的香农第一定律。至于如何找到最巧妙的编码（或是说最短的编码）。

说完了数字的编码，接下来我们说说文字的诞生的过程。它和数字的诞生也很相似，早期无论是苏美尔人、古埃及人、古中国人，还是印度河文明的古印度人，都采用的是象形文字。一个图画就是一个意思。但是后来要表达的意思实在太多了，总不能无限制地发明文字，于是就出现了用几个文字表达一个复杂的含义。

那么这些原始的编码背后的信息论原理是什么呢？我们还是回到消除不确定性这件事来看待这个问题。假如一个原始人家里有 10 样东西，他给每个东西起一个名字，这就是最简单的编码，而且早期起的那些名字都容易让人联想起东西的特性，就如同把狗叫成汪星人，把猫叫成喵星人一样。

当然，家里的东西多了，要做的动作多了，就做不到把每一件事单独编码，就需要利用一些编码进行组合了。比如说我们有对一些东西的编码，又有了一些对动作的编码，这就形成了可以表达复杂意思的简单的句子。比如说一个原始人让孩子把家里的石斧拿来，他就可以告诉他采用「拿来」这个动作，而要拿的对象是「石斧」。人类使用动词，标志着文明的一大进步，这不仅意味着他们能够把动作进行分类，编码了，而且这样才能表达复杂的意思，才有可能形成知识。

有了象形文字和动词之后，人类就有了书写系统，各种信息就通过文字这种编码记录下来，这才让我们了解到过去的历史。但是，从此人类的不平等也开始加剧，因为能够认识编码的人，就掌握了其他人所没有的信息。信息太重要了。于是，这些能够读写的人就成了精英甚至是统治阶级。在任何历史阶段，谁控制了信息，谁就是世界的主人。

一个最有说服力的例证就是：在马丁·路德之前，关于上帝的信息是由教士和主教们控制的，因此农民们只好受人摆布。在中国虽然大家不信上帝，情况也是类似。过去在农村，不能识文断字的人，哪怕再有钱，也不过是土财主，家业很难长期兴旺；能够读书写字的人，哪怕穷，在宗族里也很有地位。今天，虽然大家都能识文断字，但是有的人掌握的信息多，有的人掌握的少，这就造成了很大的不平等。对于个体来讲，改变自身获取信息的能力，要比改变整个社会的不平等容易得多。

古代文字难以普及的一个重要的原因，就是基于各种象形文字的编码系统太复杂，要记忆的东西太多，学习的成本太高。于是全世界的语言都在沿着简化这条路发展。

我通过讲人类创造数字和文字语言的过程，告诉大家，其实它们都是人类用来消除信息不确定性的编码手段。各种编码系统，其实都是在编码复杂性和编码长度之间作平衡，它们在数学上是等价的；由于它们是等价的，所以，在一个编码系统中解决不了的问题，换一个系统同样解决不了；香农第一定律告诉我们，只要编码设计得足够巧妙，就可以找到最短编码。

各种编码系统本身在信息论上是等价的，但是，不同的编码系统可以有好有坏。比如今天使用的阿拉伯数字（其实是印度人发明的）0～9 就是一个很好的编码系统，对于描述数字信息来讲，它们的数量不多不少，形状差异大。如果采用一个小圆点「∙」代表一，两个「∙∙」代表二，三个「∙∙∙」代表三，十个「∙∙∙∙∙∙∙∙∙∙」代表十，就不太好，因为大家容易看花眼。因此好的编码第一个特点就是要便于区分不同的信息。

我国在文革后曾经推行过一版过于简单的简化字，但是很快就停止使用了，这里面主要的原因是将汉字的笔画简化得过少后，使得近形字大量出现，不易辨识，非常容易搞混，因此很快就废止了，从信息论上讲，它违反了好编码要便于信息辨识的原则。

从信息论上讲，它违反了好编码要便于信息辨识的原则。德国著名的营销专家和演说家多米尼克·穆特勒提出的清晰表达的五个原则：明确、诚实、勇气、责任和同理心，前四条就和信息编码要便于识别有关。信息编码的第一个基本原则：「易识别」，应用在我们个人沟通中，也是如此。第二个信息编码的原则：有效性。如何组合信息，保证它高效传递，还能不违背第一条「易辨识」的原则。这就需要我们主动思考了。

这依然不是最有效的编码，如果我们考虑采用二进制，而不是十进制进行编码，则能表示 1024 个不同的数字。具体的做法是这样的，我们把十个指头伸开：从左边的小拇指到大拇指编号为 0～4，再从右边的大拇指到小拇指，编号为 5～9。这十个指头，每一个都有伸出、收起两种状态。每一种状态对应于一位二进制，十个指头能表示 10 位 2 进制，因为 10 个指头，每个指头有两种情况，就是 2 的 10 次方，也就是 1024 种可能性。

1『信息的编码要具有易辨识和有效性，吴军用小老鼠试药的例子说明的有效性的理论上限问题。』

比如在一个产品中，有两种可用的方案，A 和 B，哪种更好呢？过去常常是工程师们和产品经理们拍脑袋想，有些时候某些人的「眼光」很好，正好蒙对了，选了一个用户也喜欢的方案，但是这种「眼光好」是无法复制的，一个公司将自己的商业成功寄托在「眼光好」上早晚要失败。

这时，就可以利用用户大数据评判 A、B 方案的好坏，通常的做法是随机选取 1% 的用户作对比实验。比如 Google 在改进搜索算法或者其它产品体验后，会先做这样不公开的测试，一般会持续一周左右。但是像 Google 这样有好几万工程师的大公司，每天的各种改进是很多的，如果每个项目都用掉 1% 的用户，把全部用户都用上也不够。

这就回到了我们刚才学过的高效编码问题，用少量用户同时进行很多个实验的方法，就类似上面这种让小白鼠试毒药的方法，也就是将各种不会发生冲突的实验用二进制进行编码，几组实验者，就可以同时进行几十个不同的实验。

如何对信息进行编码才最有效？这个问题一直困扰着人们，莫尔斯电码时讲到，他根据常识对经常出现的字母采用较短的编码，对不常见的字母用较长的编码，这样就可以降低编码的整体长度。

如果对英语 26 个字母采用等长度的编码，比如进行二进制编码，需要 log26。 log26 就是约 5 比特信息。而采用莫尔斯的编码方法，平均只需要 3 比特，这个效率就高了很多，这样发报，时间就能节省大约 1/3 左右。

无独有偶，全世界除美国之外，各国在设计长途电话区位码的时候，也充分考虑了每一个城市和地区的电话机数量，比如在中国北京、上海等重要城市就是两位，小城市就使用 3 位，这样做的目的是为了减少平均的编码长度。那么是否能够证明，越常出现的信息采用较短的编码，不常出现的信息采用较长的编码，就能比采用同样码长的信息总体上更合算呢？答案是肯定的。

我们不妨看一个具体的例子。我们假定有 32 条信息，每条信息出现的概率分别为 1/2、1/4、1/8、1/16…… 依次递减，最后 31、32 两个信息出现的概率是 1/2^31、1/2^31（这样 32 个信息的出现概率加起来就是 1 了）。现在需要用二进制数对它们进行编码。等长度和不等长度两种编码方法，我们来对比一下：

方法一：采用等长度编码，码长为 5。因为是 log32=5 比特。

方法二：不等长度编码，如果出现概率高就短一些，概率低就长一些。

我们把第一条信息用 0 编码，第二条用 10 编码，第三条用 110 编码…… 最后 31、32 两条出现概率相同，都很低，码长都是 31。第 31 条信息就用 1111……110（30 个 1 加 1 个 0）编码，第 32 条信息，就用 1111……111（31 个 1）来编码。这样的编码虽然大部分码的长度都超过了 5，但是乘以出现概率后，平均码长只有 2，也就是说节省了 60% 的码长。如果利用这个原理进行数据压缩，可以在不损失任何信息的情况下压缩掉 60%。

事实上，这种最短编码方法等于香农第一定律的继续，它最早是由 MIT 的教授哈夫曼发明的，因此也被称为「哈夫曼编码」。关于哈夫曼编码有三个要点值得一提：

1、如果你还记得第 5 讲的香农第一定律，一定知道编码长度是有个理论最小值的，从数学上可以证明哈夫曼的这种编码方法是最优化的。

2、哈夫曼编码从本质上讲，是将最宝贵的资源（最短的编码）给出现概率最大的信息。至于资源如何分配，哈夫曼给出了一个原则，也就是一条信息编码的长度和出现概率的对数成正比。

注：比如在上面的例子中，第一条消息出现的概率为 1/2，我们知道 1/2（以二为底）的对数等于 - 1，因此它的编码长度就是 1（即码 0）。最后两条消息出现的概率为 1/2^31 次方，取对数后等于 - 31，因此它们的编码长度就是 31。

如果我们回顾一下莫尔斯电码，就会发现它是不自觉地采用了哈夫曼编码的原理。只是它没有严格统计各个字母的频率，没有完全做到最优化。在一个极端的情况下，如果所有的信息出现的概率相同，采用哈夫曼编码，每一条信息的码长都一样，这时哈夫曼编码就变成了等长编码，没有优势了。

3、在现实生活中，很多信息的组合，比单独一条信息，其概率分布差异更大，因此对它们使用哈夫曼编码进行信息压缩，压缩比会更高。比如说，在汉语中，如果对汉字的频率进行统计，然后压缩，一篇文章通常能压缩掉 50% 以上，但是如果按照词进行频率统计，再用哈夫曼编码压缩，可以压缩掉 70% 以上。

哈夫曼编码又是怎么应用到我们的工作生活中呢？其实，但凡需要分配资源的工作，它都有指导意义。我在《浪潮之巅》一书中介绍凯鹏华盈时讲，虽然换了三代掌门人，但它能在四十多年，20 多期基金中，平均每一期基金的回报总是有 40 倍左右，这说明它不是靠一两个人天才的眼光，而是有一整套系统的方法，保证投资的成功率。那么它投资方法中的秘诀是什么呢？其实就是哈夫曼编码的原理，即通过每一次双倍砸钱（double down），把最多的钱投入到最容易成功的项目上。

我们还假设如果投资的公司最后能上市，将获得 50 倍的回报；如果上不了市，只是在下一轮融资被收购，将获得 3～5 倍的回报。在硅谷地区，获得投资的公司最终能上市的概率大约是 1%，大家不要觉得这个比例低，它已经比世界其他地区，包括美国硅谷以外的地区和中国，高很多了。至于被收购的概率，在硅谷地区大约是 20%，比中国要高很多。

如果使用第一种方法，基本上是拿到一个市场的平均回报，也就是一轮基金下来大约是 31% 到 71% 的回报，如果扣除管理费和基金本身拿走的分红，出资人大约能得到 20%～50% 左右的回报。通常一期风险投资基金投资的时间是 2～5 年（持续的时间可以长达 7～10 年），这样年化回报大约是 5%～20% 之间。这是硅谷风险投资的平均水平，大家不要觉得风险投资一定能挣钱，在中国，大部分风险投资基金是赔钱的，而在硅谷赔钱的基金的比例也高达 40%。

第二种方法，只投一家，这其实是赌博，如果碰上这家公司上市，有 50 倍的回报，碰上被收购的有 2～5 倍的回报，但是绝大多数情况则血本无归。如果所有的基金都玩这样的赌博，虽然平均回报率和第一种情况相似，但是投资风险高达 500%。根据投资领域普遍采用的夏普比率（请回到《硅谷来信》查看第 145—148 封信）来衡量，这是极为糟糕的投资方式。

第三种方法是按照哈夫曼编码的原理，可以先把钱分成几部分逐步投入下去，每一次投资的公司呈指数减少，而金额倍增。具体操作方法如下：第一轮，选择 100 家公司，每家投入 25 万美元，这样用掉 2500 万美元；第二轮，假定有 1/3 的公司即 33 家表现较好，每家再投入 75 万美元左右，也用掉 2500 万美元。至于剩下了的 2/3 已经死掉或者不死不活的公司，千万不要救它们，更不要觉得便宜去抄底；第三轮，假定 1/10 的公司，即 10 家表现较好，每家投入 250 万美元，再用掉 2500 万美元；第四轮，假定 3% 的公司，即 3 家表现较好，每家投入 800 万美元左右，用掉最后的 2500 万美元。

当然大部分人不会去参与风险投资，但是这种分配资源的原则在哪儿都适用。我在之前《Google 方法论》中介绍 Google 和 Facebook 等公司的管理方法时讲到，它们内部其实是一个大风投，各个项目一开始都有获得资源（主要是人力和财力）的可能性。但是很快，通常是三个月到半年，类似的项目就要开始整合，资源开始集中到更有希望的项目上去。最后能够变成产品上市的，是少数项目，但是大量的资源投入在其中了。这样既不会失去新的机会，也不会浪费资源。

今天的华为养了一个拥有几万人的庞大的预研部门，很多人觉得这是有了钱之后嘚瑟浪费，但是你可以把它看成是一个内部的大风投，每一个前期研究，都得到一定的发展机会，而投入的资源并不需要太多，最后能够进入到获得巨大资源攻坚阶段的项目，终究是少数。

这个道理对个人来讲也是适用的。美国有名的私立学校哈克学校的前校长尼克诺夫博士讲，在孩子小时候，要让他们尝试各种兴趣爱好，但是最终他们要在一个点上实现突破，他将这比做用圆规画圆，一方面有一个扎得很深的中心，另一方面有足够广的很浅的覆盖面。

一方面我从来不排斥尝试新东西，这样不会失去机会，我尝试过的各种事情远比外界知道的多，只是绝大部分失败了，我没有继续罢了，大家也就无从知晓了；但是，另一方面对于花了一些精力，看样子做不成的事情，我是坚决做减法止损，这样可以把最多的资源投入到我擅长的，有兴趣的，可能也是成功率最高的事情上。

就说英文单词吧，用 5 个字母就能拼出 1200 万个单词，即便扣除掉 iiiii、jjjjj 这种不合理的，也能拼出几十万个看起来很「合理」的英文单词，要是这样的话，所有单词只有 5 个字母，大家不就不需要背那些很长的英文单词了吗？对这个问题简单的回答是，语言和文字是慢慢演化过来的，而不是人为利用信息论的编码原理刻意构造的，因此不可能只照顾易辨识和有效性，而不考虑人类接受它们的难度，以及演化的过程。

相反，人们给计算机识别的单词，比如汇编语言的指令代号，基本上就是很短的、等长的字母组合，因为那是完全利用编码原理人工设计的。当然，在人类文字演化的过程中，也无意间用了一个信息论的原理 —— 信息的矢量数字化（也被称为 VQ），或者简单地讲就是矢量化。我们就从文字和语言的演化过程，来谈谈这个原理以及它们的意义。

人类在进入到文明社会时，活动的范围越来越大，需要记录的信息越来越多，人类就开始通过动词和名词的组合来表达复杂的意思。但是新概念、新事物还是不断地涌现，人类只好造出更多的象形文字，这就如同今天人们不断创造新词一样。信息越多，需要的编码越多，这是文明自然演变不可避免的过程。

太多不同的编码（文字）出现后，就要对编码进行简化，否则大家就没法学习了。而简化的自然过程，就是矢量化的过程。那什么是矢量化呢？你一定有这样的经历，就是把一张图片放大再放大，通常就会模糊，出现马赛克甚至锯齿。学计算机的人知道，计算机中使用的字体有位图（bitmap）和矢量图两种。位图一经放大就会出现锯齿，而矢量图随便放大，都很清晰。这是怎么做到的呢？我们先从信息的矢量化说起。我们假定有一些几何形状，它们具有不同的颜色。比如下面这张图：

这些基本的图形彼此有一些相似性，但是又不完全一样。我只画了十四个不同的形状，当然真实的情况是它们可能有成千上万个。这么多图形，我一个个描述太复杂，于是我们就把这成千上万个彩色的形状，按照颜色和形状两个维度各四种情况，分到了 16 个格中。这样，所有的图形，就被归为了 16 类。当然，其中还有四个格子没有信息，因此可以看成是不存在的。这便是矢量化的原理。

这个用坐标分类，概括多种形状，就是形状的矢量化过程。当然，如果我们分类所概括的是信息，不是图形，道理是一样的。为什么这种特殊的归类过程，我们称之为矢量化呢？因为当我们把杂乱无章的信息投射到两个维度之后，两个维度坐标可以决定平面上的一个矢量。

比如在上面的例子中，要找圆形和椭圆形的蓝色图形，就用下面这个从原点出发到（4，2）坐标的矢量来表述：

当然，通常将信息投射到两个维度是不够的，根据应用场景会投射到多个维度中，这样的过程就被称为矢量化。人类象形文字的演化，实际上就是这样一个矢量化的过程。我们不妨先看看各种象形文字演化的过程。文字演化的第一步是抽象化。下面一张图描述了美索不达米亚文字（上）、古埃及文字（中）和古代中国的文字（下），对「鱼、鸟、戚、矢（「有的放矢」的「矢」，也就是箭）、壶」这五个字抽象化的过程。

你可以看出最初的文字和真实的物体非常相似，但是这些象形文字彼此之间缺乏共性。但是后面逐渐地，它们就被抽象化成一些直线或者弧线了。在中国和美索不达米亚，由于早期的文字是刻在金石、竹木和泥板上，因此，更多被抽象化成点和线的组合，这样便于刻写。而古埃及是写在莎草纸上的，能够使用曲线书写，因此多用曲线进行抽象化。但不管怎么样，抽象化之后，就可以总结出共性了。接下来第二步，我们就以汉字为例来说明矢量化的过程。绝大多数汉字被映射到两个维度上，即一个表意的偏旁维度和一个提示读音的发音维度，有些时候，提示读音的维度本身也表意。再往后，表达含义的偏旁已经和原来的图画不太像了。而这些偏旁就构成了文字的基本单元，而且慢慢固定下来了。以后有新的概念需要创作出新字时，使用那些基本单元，即偏旁部首，重新组合就可以了。

拼音文字的简化主要是围绕读音进行的。在美索不达米亚人发明了楔形文字后，它很快就由象形文字变成了拼音文字。但是那些拼音文字并不简单，每一个表达意思的拼音其实是一堆很复杂的小箭头（很像楔子，所以也被成为楔形文字）。后来楔形文字被当地的闪米特人学会了，他们中间有一支非常善于远洋经商的族群 —— 就是腓尼基人。

腓尼基人将美索不达米亚的文字传播到地中海各岛屿。但是，在经商途中，商人们可没有闲情逸致刻写精美漂亮的楔形文字，于是他们对这种复杂的拼音文字进行了进一步简化，就剩下几十个字母了。可以讲，从复杂的楔形文字，变成简单的几十个字母，是一个巨大的进步，它使得人类学习读写变得很容易。再后来希腊人从腓尼基字母中总结成 24 个希腊字母，而罗马人又将它们变成 22 个拉丁字母。

随着罗马的扩张，征服了很多外国土地，吸纳了很多外国人，有些外国的人名和地名就无法表示了，于是罗马人在字母表中加入了 x，代表所有那些无法表示的音和词，这既是英语里包含 x 的单词特别少的原因，也是后来人们用 x 表示未知数的原因。再后来拉丁文里的 i 被拆成了 i 和 j 两个字母，v 被拆成了 u,v,w 三个字母，最终就形成了今天英语的 26 个字母。

今天欧洲其它的拼音文字大多源于拉丁语，虽然它们字母表的多少略有区别，而且读音不同，但是写法上相似，因为同一种写法表达的是同一条信息。虽然象形文字和拼音文字的形成和进化代表了两种不同的信息编码方式，但是它们都利用了信息论中矢量化的原理。

在欧洲的拼音文字中，虽然没有表达意思的偏旁部首，但是有很多词根，前缀和后缀起到了表达意思的作用，也就是说这些语言实际上将表达信息的基本单元（单词）用一个词根、前缀、后缀这样三维的矢量表示了。于是，稍微有些语言基础的人，可以猜出一些没见过的单词的含义。正因为这个原因，拼音文字比汉语容易学。

在近代史上，曾经有不少学者提出过将汉字改为拼音文字，但其实这是不可行的。比如你把计算机变成 jisuanji 这几个罗马字母，它完全没有词根、前缀和后缀，因此猜不出意思。信息的矢量化这件事应用的场景非常广，前面提到的矢量字体就是一个，它的原理是将字体的轮廓映射到一组曲线上。在显示（和打印）时，经过一系列的数学运算，恢复字体的形状。这一类字库不仅占用空间小，而且从理论上可以被无限地放大，笔划轮廓仍然能保持圆滑，非常美观。

此外，矢量化在生活中也有应用，比如我们通过高考成绩录取大学生，或者通过身高选拔篮球运动员，其实就是利用矢量化的原理，只不过是将所有的人映射到了一维的空间中。这种做法给工作带来了极大的便利性，但是显然没有全面地考察每一个人，或者说有信息的损失。所以，在信息论中，一个更有普遍意义的问题就是，矢量化会带来多大的信息损失，关于这一点，在信息论中有一套理论计算这种损失。而在工程中大家要做的事就是，如何平衡便利性和信息上的损失。人在年轻的时候，总是会想两者兼而有之，学习了各种科学知识后，就知道这种事情在理论上是办不到的。

从文字的演变，介绍了信息的矢量化这个概念，以及它的应用。我们进而讲述了，无论是象形文字还是天然形成的拼音文字，都通过两到三个维度的矢量化兼顾了读音和达意的关系。但是，如果强制将中文拼音化，它将失去达意的功能，这不符合信息论的原则，因此做不下去。世界上人为想做的，但违背规律的事情，做起来总是困难重重。在生活中其实也有很多矢量化的例子，它们让问题变得简单，但是会丢失信息，而平衡便利性和信息的完整性，就成为了艺术。

矢量化通过老师图形的例子和金助教画圆的例子，我觉得矢量化应该是提炼的一种算法吧，尽管不能确保信息的完整性，却能在简洁的算法下，确保能再次按照这样的规则再次重现信息。

















