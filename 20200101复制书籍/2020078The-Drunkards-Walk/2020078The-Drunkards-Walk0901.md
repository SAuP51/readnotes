# 0901. Illusions of Patterns and Patterns of Illusion

IN 1848 TWO TEENAGE GIRLS, Margaret and Kate Fox, heard unexplained noises, like knocking or the moving of furniture. Their house, it happened, had a reputation for being haunted. As the story goes,1 Kate challenged the source of the noises to repeat the snap of her fingers and to rap out her age. It rose to both challenges. Over the next few days, with their mother's and some neighbors' assistance, the sisters worked out a code with which they could communicate with the rapper (no pun intended). They concluded that the rapping originated with the spirit of a peddler who had been murdered years earlier in the home they now occupied. With that, modern spiritualism  —  the belief that the dead can communicate with the living  —  was born. By the early 1850s a particular type of spiritual contact, called table rapping, and its cousins, table moving and table turning, had become the rage in the United States and Europe. The enterprise consisted of a group of individuals arranging themselves around a table, resting their hands upon it, and waiting. In table rapping, after some time passed, a rap would be heard. In table moving and table turning, after time passed, the table would begin to tilt or move about, sometimes dragging the sitters along with it. One pictures serious bearded men with jackets reaching their midthigh and ardent women in hoop skirts, eyes wide in wonder as their hands followed the table this way or that.

Table moving became so popular that in the summer of 1853 scientists began to look into it. One group of physicians noted that during the silent sitting period a kind of unconscious consensus seemed to form about the direction in which the table would move.2 They found that when they diverted the sitters' attention so that a common expectation could not form, the table did not move. In another trial they managed to create a condition in which half the sitters expected the table to move to the left and half expected it to move to the right, and again it did not move. They concluded that「the motion was due to muscular action, mostly exercised unconsciously.」But the definitive investigation was performed by the physicist Michael Faraday, one of the founders of electromagnetic theory, inventor of the electric motor, and one of the greatest experimental scientists in history.3 Faraday first discovered that the phenomenon would occur even with just one subject sitting at the table. Then, enrolling subjects who were both「very honorable」and accomplished table movers, he conducted a series of ingenious and intricate experiments proving that the movement of the sitters' hands preceded that of the table. Further, he designed an indicator that alerted the subjects in real time whenever that was occurring. He found that「as soon as the…[indicator] is placed before the most earnest [subject]…the power [of the illusion] is gone; and this only because the parties are made conscious of what they are really doing.」4

Faraday concluded, as the doctors had, that the sitters were unconsciously pulling and pushing the table. The movement probably began as random fidgeting. Then at some point the sitters perceived in the randomness a pattern. That pattern precipitated a self-fulfilling expectation as the subjects' hands followed the imagined leadership of the table. The value of his indicator, Faraday wrote, was thus「the corrective power it possesses over the mind of the table-turner.」5 Human perception, Faraday recognized, is not a direct consequence of reality but rather an act of imagination.6

Perception requires imagination because the data people encounter in their lives are never complete and always equivocal. For example, most people consider that the greatest evidence of an event one can obtain is to see it with their own eyes, and in a court of law little is held in more esteem than eyewitness testimony. Yet if you asked to display for a court a video of the same quality as the unprocessed data captured on the retina of a human eye, the judge might wonder what you were trying to put over. For one thing, the view will have a blind spot where the optic nerve attaches to the retina. Moreover, the only part of our field of vision with good resolution is a narrow area of about 1 degree of visual angle around the retina's center, an area the width of our thumb as it looks when held at arm's length. Outside that region, resolution drops off sharply. To compensate, we constantly move our eyes to bring the sharper region to bear on different portions of the scene we wish to observe. And so the pattern of raw data sent to the brain is a shaky, badly pixilated picture with a hole in it. Fortunately the brain processes the data, combining the input from both eyes, filling in gaps on the assumption that the visual properties of neighboring locations are similar and interpolating.7 The result  —  at least until age, injury, disease, or an excess of mai tais takes its toll  —  is a happy human being suffering from the compelling illusion that his or her vision is sharp and clear.

We also use our imagination and take shortcuts to fill gaps in patterns of nonvisual data. As with visual input, we draw conclusions and make judgments based on uncertain and incomplete information, and we conclude, when we are done analyzing the patterns, that our「picture」is clear and accurate. But is it?

Scientists have moved to protect themselves from identifying false patterns by developing methods of statistical analysis to decide whether a set of observations provides good support for a hypothesis or whether, on the contrary, the apparent support is probably due to chance. For example, when physicists seek to determine whether the data from a supercollider is significant, they don't eyeball their graphs, looking for bumps that rise above the noise; they apply mathematical techniques. One such technique, significance testing, was developed in the 1920s by R. A. Fisher, one of the greatest statisticians of the twentieth century (a man also known for his uncontrollable temper and for a feud with his fellow statistics pioneer Karl Pearson that was so bitter he continued to attack his nemesis long after Pearson's death, in 1936).

To illustrate Fisher's ideas, suppose that a student in a research study on extrasensory perception predicts the result of some coin tosses. If in our observations we find that she is almost always right, we might hypothesize that she is somehow skilled at it, for instance, through psychic powers. On the other hand, if she is right about half the time, the data support the hypothesis that she was just guessing. But what if the data fall somewhere in between or if there isn't much data? Where do we draw the line between accepting and rejecting the competing hypotheses? This is what significance testing does: it is a formal procedure for calculating the probability of our having observed what we observed if the hypothesis we are testing is true. If the probability is low, we reject the hypothesis. If it is high, we accept it.

For example, suppose we are skeptics and hypothesize that the student cannot accurately predict the results of coin tosses. And suppose that in an experimental trial she predicts the coin tosses correctly in a certain number of cases. Then the methods we analyzed in chapter 4 allow us to calculate the probability that she could have accomplished the predictions by chance alone. If she had guessed the coin-toss results correctly so often that, say, the probability of her being that successful by chance alone is only 3 percent, then we would reject the hypothesis that she was guessing. In the jargon of significance testing, we would say the significance level of our rejection is 3 percent, meaning that the chances are at most 3 percent that by chance the data has led us astray. A 3 percent level of significance is fairly impressive, and so the media might report the feat as new evidence of the existence of psychic powers. Still, those of us who don't believe in psychic powers might remain skeptical.

This example illustrates an important point: even with data significant at, say, the 3 percent level, if you test 100 nonpsychic people for psychic abilities  —  or 100 ineffective drugs for their effectiveness  —  you ought to expect a few people to show up as psychic or a few ineffective drugs to show up as effective. That's one reason political polls or medical studies, especially small ones, sometimes contradict earlier polls or studies. Still, significance testing and other statistical methods serve scientists well, especially when they can conduct large-scale controlled studies. But in everyday life we don't conduct such studies, nor do we intuitively apply statistical analysis. Instead, we rely on gut instinct. When my Viking stove turned out to be a lemon and by chance an acquaintance told me she'd had the same experience, I started telling my friends to avoid the brand. When the flight attendants on several United Airlines flights seemed grumpier than those on other airlines I'd recently flown with, I started avoiding United's flights. Not a lot of data there, but my gut instinct identified patterns.

Sometimes those patterns are meaningful. Sometimes they are not. In either case, the fact that our perception of the patterns of life is both highly convincing and highly subjective has profound implications. It implies a kind of relativity, a situation in which, as Faraday found, reality is in the eye of the beholder. For example, in 2006 The New England Journal of Medicine published a `$`12.5 million study of patients with documented osteoarthritis of the knee. The study showed that a combination of the nutritional supplements glucosamine and chondroitin is no more effective in relieving arthritis pain than a placebo. Still, one eminent doctor had a hard time letting go of his feeling that the supplements were effective and ended his analysis of the study on a national radio program by reaffirming the possible benefit of the treatment, remarking that,「One of my wife's doctors has a cat and she says that this cat cannot get up in the morning without a little dose of glucosamine and chondroitin sulfate.」8

When we look closely, we find that many of the assumptions of modern society are based, as table moving is, on shared illusions. Whereas chapter 8 is concerned with the surprising regularities exhibited by random events, in what follows, I shall approach the issue from the opposite direction and examine how events whose patterns appear to have a definite cause may actually be the product of chance.

IT IS HUMAN NATURE to look for patterns and to assign them meaning when we find them. Kahneman and Tversky analyzed many of the shortcuts we employ in assessing patterns in data and in making judgments in the face of uncertainty. They dubbed those shortcuts heuristics. In general, heuristics are useful, but just as our manner of processing optical information sometimes leads to optical illusions, so heuristics sometimes lead to systematic error. Kahneman and Tversky called such errors biases. We all use heuristics, and we all suffer from biases. But although optical illusions seldom have much relevance in our everyday world, cognitive biases play an important role in human decision making. And so in the late twentieth century a movement sprang up to study how randomness is perceived by the human mind. Researchers concluded that「people have a very poor conception of randomness; they do not recognize it when they see it and they cannot produce it when they try,」9 and what's worse, we routinely misjudge the role of chance in our lives and make decisions that are demonstrably misaligned with our own best interests.10

Imagine a sequence of events. The events might be quarterly earnings or a string of good or bad dates set up through an Internet dating service. In each case the longer the sequence, or the more sequences you look at, the greater the probability that you'll find every pattern imaginable  —  purely by chance. As a result, a string of good or bad quarters, or dates, need not have any「cause」at all. The point was rather starkly illustrated by the mathematician George Spencer-Brown, who wrote that in a random series of 101,000,007 zeroes and ones, you should expect at least 10 nonoverlapping subsequences of 1 million consecutive zeros.11 Imagine the poor fellow who bumps into one of those strings when attempting to use the random numbers for some scientific purpose. His software generates 5 zeros in a row, then 10, then 20, 1,000, 10,000, 100,000, 500,000. Would he be wrong to send back the program and ask for a refund? And how would a scientist react upon flipping open a newly purchased book of random digits only to find that all the digits are zeros? Spencer-Brown's point was that there is a difference between a process being random and the product of that process appearing to be random. Apple ran into that issue with the random shuffling method it initially employed in its iPod music players: true randomness sometimes produces repetition, but when users heard the same song or songs by the same artist played back-to-back, they believed the shuffling wasn't random. And so the company made the feature「less random to make it feel more random,」said Apple founder Steve Jobs.12

One of the earliest speculations about the perception of random patterns came from the philosopher Hans Reichenbach, who remarked in 1934 that people untrained in probability would have difficulty recognizing a random series of events.13 Consider the following printout, representing the results of a sequence of 200 tosses of a coin, with X representing tails and O representing heads: ooooxxxxoooxxxooooxxooxoooxxxooxxoooxxxxoooxooxoxoooooxooxoooooxxooxxxoxxoxoxxxxoooxxooxxoxooxxxooxooxoxoxxoxoooxoxooooxxxxoooxxooxoxxoooxoooxxoxooxxooooxooxxxxooooxxxoooxoooxxxxxxooxxxooxooxoooooxxxx. It is easy to find patterns in the data  —  for instance, the four Os followed by four Xs at the beginning and the run of six Xs toward the end. According to the mathematics of randomness, such runs are to be expected in 200 random tosses. Yet they surprise most people. As a result, when instead of representing coin tosses, strings of Xs and Os represent events that affect our lives, people seek meaningful explanations for the pattern. When a string of Xs represents down days on the stock market, people believe the experts who explain that the market is jittery. When a string of Os represents a run of accomplishments by your favorite sports star, announcers sound convincing when they drone on about the player's「streakiness.」And when, as we saw earlier, the Xs or Os stood for strings of failed films made by Paramount and Columbia Pictures, everyone nodded as the industry rags proclaimed just who did and who did not have a finger on the pulse of the worldwide movie audience.

Academics and writers have devoted much effort to studying patterns of random success in the financial markets. There is much evidence, for instance, that the performance of stocks is random  —  or so close to being random that in the absence of insider information and in the presence of a cost to make trades or manage your portfolio, you can't profit from any deviations from randomness.14 Nevertheless, Wall Street has a long tradition of guru analysts, and the average analyst's salary, at the end of the 1990s, was about `$`3 million.15 How do those analysts do? According to a 1995 study, the eight to twelve most highly paid「Wall Street superstars」invited by Barron's to make market recommendations at its annual roundtable merely matched the average market return.16 Studies in 1987 and 1997 found that stocks recommended by the prognosticators on the television show Wall `$`treet Week did much worse, lagging far behind the market.17 And in a study of 153 newsletters, a researcher at the Harvard Institute of Economic Research found「no significant evidence of stock-picking ability.」18

By chance alone, some analysts and mutual funds will always exhibit impressive patterns of success. And though many studies show that these past market successes are not good indicators of future success  —  that is, that the successes were largely just luck  —  most people feel that the recommendations of their stockbrokers or the expertise of those running mutual funds are worth paying for. Many people, even intelligent investors, therefore buy funds that charge exorbitant management fees. In fact, when a group of savvy students from the Wharton business school were given a hypothetical `$`10,000 and prospectuses describing four index funds, each composed in order to mirror the S&P 500, the students overwhelmingly failed to choose the funds with the lowest fees.19 Since paying even an extra 1 percent per year in fees could, over the years, diminish your retirement fund by as much as one-third or even one-half, the savvy students didn't exhibit very savvy behavior.

Of course, as Spencer-Brown's example illustrates, if you look long enough, you're bound to find someone who, through sheer luck, really has made startlingly successful predictions. For those who prefer real-world examples to mathematical scenarios involving 101,000,007 random digits, consider the case of the columnist Leonard Koppett.20 In 1978, Koppett revealed a system that he claimed could determine, by the end of January every year, whether the stock market would go up or down in that calendar year. His system had correctly predicted the market, he said, for the past eleven years.21 Of course, stock-picking systems are easy to identify in hindsight; the true test is whether they will work in the future. Koppett's system passed that test too: judging the market by the Dow Jones Industrial Average, it worked for eleven straight years, from 1979 through 1989, got it wrong in 1990, and was correct again every year until 1998. But although Koppett's predictions were correct for a streak of eighteen out of nineteen years, I feel confident in asserting that his streak involved no skill whatsoever. Why? Because Leonard Koppett was a columnist for Sporting News, and his system was based on the results of the Super Bowl, the championship game of professional football. Whenever the team from the (original) National Football League won, the stock market, he predicted, would rise. Whenever the team from the (original) American Football League won, he predicted, the market would go down. Given that information, few people would argue that Koppett was anything but lucky. Yet had he had different credentials  —  and not revealed his method  —  he could have been hailed as the most clever analyst since Charles H. Dow.

As a counterpoint to Koppett's story, consider now the story of a fellow who does have credentials, a fellow named Bill Miller. For years, Miller maintained a winning streak that, unlike Koppett's, was compared to Joe DiMaggio's fifty-six-game hitting streak and the seventy-four consecutive victories by the Jeopardy! quiz-show champ Ken Jennings. But in at least one respect these comparisons were not very apt: Miller's streak earned him each year more than those other gentlemen's streaks had earned them in their lifetimes. For Bill Miller was the sole portfolio manager of Legg Mason Value Trust Fund, and in each year of his fifteen-year streak his fund beat the portfolio of equity securities that constitute the Standard & Poor's 500.

For his accomplishments, Miller was heralded「the Greatest Money Manager of the 1990s」by Money magazine,「Fund Manager of the Decade」by Morningstar, and one of the top thirty most influential people in investing in 2001, 2003, 2004, 2005, and 2006 by SmartMoney.22 In the fourteenth year of Miller's streak, one analyst was quoted on the CNNMoney Web site as putting the odds of a fourteen-year streak by chance alone at 372,529 to 1 (more on that later).23

Academics call the mistaken impression that a random streak is due to extraordinary performance the hot-hand fallacy. Much of the work on the hot-hand fallacy has been done in the context of sports because in sports, performance is easy to define and measure. Also, the rules of the game are clear and definite, data are plentiful and public, and situations of interest are replicated repeatedly. Not to mention that the subject gives academics a way to attend games and pretend they are working.

Interest in the hot-hand fallacy began around 1985, in particular with a paper by Tversky and his co-workers that was published in the journal Cognitive Psychology.24 In that paper,「The Hot Hand in Basketball: On the Misperception of Random Sequences,」Tversky and his colleagues investigated reams of basketball statistics. The players' talent varied, of course. Some made half their shots, some more, some less. Each player also had occasional hot and cold streaks. The paper's authors asked the question, how do the number and length of the streaks compare with what you would observe if the result of each shot were determined by a random process? That is, how would things have turned out if rather than shooting baskets, the players had tossed coins weighted to reflect their observed shooting percentages? The researchers found that despite the streaks, the floor shots of the Philadelphia 76ers, the free throws of the Boston Celtics, and the experimentally controlled floor shots of the Cornell University men's and women's varsity basketball teams exhibited no evidence of nonrandom behavior.

In particular, one direct indicator of「streakiness」is the conditional probability of success (that is, making a basket) if on the prior attempt the player had achieved success. For a streaky player, the chance of a success on the heels of a prior success should be higher than his or her overall chance of success. But the authors found that for each player a success following a success was just as likely as a success following a failure (that is, a missed basket).

A few years after Tversky's paper appeared, the Nobel Prize–winning physicist E. M. Purcell decided to investigate the nature of streaks in the sport of baseball.25 As I mentioned in chapter 1, he found, in the words of his Harvard colleague Stephen Jay Gould, that except for Joe DiMaggio's fifty-six-game hitting streak,「nothing ever happened in baseball above and beyond the frequency predicted by coin-tossing models.」Not even the twenty-one-game losing streak experienced at the start of the 1988 season by Major League Baseball's Baltimore Orioles. Bad players and teams have longer and more frequent streaks of failure than great players and great teams, and great players and great teams have longer and more frequent streaks of success than lesser players and lesser teams. But that is because their average failure or success rate is higher, and the higher the average rate, the longer and more frequent are the streaks that randomness will produce. To understand these events, you need only to understand the tossing of coins.

What about Bill Miller's streak? That a streak like Miller's could result from a random process may seem less shocking in light of a few other statistics. For instance, in 2004 Miller's fund gained just under 12 percent while the average stock in the S&P gained more than 15 percent.26 It might sound like the S&P trounced Miller that year, but actually he counted 2004 in his「win」column. That is because the S&P 500 is not the simple average of the prices of the stocks it comprises; it is a weighted average in which stocks exert influence proportional to each company's capitalization. Miller's fund did worse than the simple average of S&P stocks but better than that weighted average. Actually, there were more than thirty twelve-month periods during his streak in which he lost to the weighted average, but they weren't calendar years, and the streak was based on the intervals from January 1 to December 31.27 So the streak in a sense was an artificial one to start with, one that by chance was defined in a manner that worked for Miller.

But how can we reconcile these revelations with those 372,529-to-1 odds against him? In discussing Miller's streak in 2003, writers for The Consilient Observer newsletter (published by Credit Suisse–First Boston) said that「no other fund has ever outperformed the market for a dozen consecutive years in the last 40 years.」They raised the question of the probability of a fund's accomplishing that by chance and went on to give three estimates of that probability (the year being 2003, they referred to the chances of a fund's beating the market for only twelve consecutive years): 1 in 4,096, 1 in 477,000, and 1 in 2.2 billion.28 To paraphrase Einstein, if their estimates were correct, they would have needed only one. What were the actual chances? Roughly 3 out of 4, or 75 percent. That's quite a discrepancy, so I'd better explain.

Those who quoted the low odds were right in one sense: if you had singled out Bill Miller in particular at the start of 1991 in particular and calculated the odds that by pure chance the specific person you selected would beat the market for precisely the next fifteen years, then those odds would indeed have been astronomically low. You would have had the same odds against you if you had flipped a coin once a year for fifteen years with the goal of having it land heads up each time. But as in the Roger Maris home run analysis, those are not the relevant odds because there are thousands of mutual fund managers (over 6,000 currently), and there were many fifteen-year periods in which the feat could have been accomplished. So the relevant question is, if thousands of people are tossing coins once a year and have been doing so for decades, what are the chances that one of them, for some fifteen-year period, will toss all heads? That probability is far, far higher than the odds of simply tossing fifteen heads in a row.

To make this explanation concrete, suppose 1,000 fund managers  —  certainly an underestimate  —  had each tossed a coin once a year starting in 1991 (the year Miller began his streak). After the first year about half of them would have tossed heads; after two years about one-quarter of them would have tossed two heads; after the third year one-eighth of them would have tossed three heads; and so on. By then some who had tossed tails would have started to drop out of the game, but that wouldn't affect the analysis because they had already failed. The chances that, after fifteen years, a particular coin tosser would have tossed all heads are then 1 in 32,768. But the chances that someone among the 1,000 who had started tossing coins in 1991 would have tossed all heads are much higher, about 3 percent. Finally, there is no reason to consider only those who started tossing coins in 1991  —  the fund managers could have started in 1990 or 1970 or any other year in the era of modern mutual funds. Since the writers for The Consilient Observer used forty years in their discussion, I calculated the odds that by chance some manager in the last four decades would beat the market each year for some fifteen-year period. That latitude increased the odds again, to the probability I quoted earlier, almost 3 out of 4. So rather than being surprised by Miller's streak, I would say that if no one had achieved a streak like Miller's, you could have legitimately complained that all those highly paid managers were performing worse than they would have by blind chance!

I've cited some examples of the hot-hand fallacy in the context of sports and the financial world. But in all aspects of our lives we encounter streaks and other peculiar patterns of success and failure. Sometimes success predominates, sometimes failure. Either way it is important in our own lives to take the long view and understand that streaks and other patterns that don't appear random can indeed happen by pure chance. It is also important, when assessing others, to recognize that among a large group of people it would be very odd if one of them didn't experience a long streak of successes or failures.

No one credited Leonard Koppett for his lopsided successes, and no one would credit a coin tosser. Many people did credit Bill Miller. In his case, though the type of analysis I performed seems to have escaped many of the observers quoted in the media, it is no news to those who study Wall Street from the academic perspective. For example, the Nobel Prize–winning economist Merton Miller (no relation to Bill) wrote,「If there are 10,000 people looking at the stocks and trying to pick winners, one in 10,000 is going to score, by chance alone, and that's all that's going on. It's a game, it's a chance operation, and people think they are doing something purposeful but they're really not.」29 We must all draw our own conclusions depending on the circumstances, but with an understanding of how randomness operates, at least our conclusions need not be naive.

IN THE PRECEDING I've discussed how we can be fooled by the patterns in random sequences that develop over time. But random patterns in space can be just as misleading. Scientists know that one of the clearest ways to reveal the meaning of data is to display them in some sort of picture or graph. When we see data exhibited in this manner, meaningful relationships that we would likely have missed are often made obvious. The cost is that we also sometimes perceive patterns that in reality have no meaning. Our minds are made that way  —  to assimilate data, fill in gaps, and look for patterns. For example, look at the following arrangement of grayish squares in the figure below.

Photo from Frank H. Durgin,「The Tinkerbell Effect,」Journal of Consciousness Studies 9, nos. 5–6 (May to June 2002)

The image does not literally look like a human being. Yet you can make enough sense of the pattern that if you saw in person the baby pictured, you would probably recognize it. And if you hold this book at arm's length and squint, you might not even perceive the imperfections in the image. Now look at this pattern of Xs and Os:

Here we see rectangular clusters, especially in the corners. I have put them in boldface. If the Xs and Os represented events of interest, we might be tempted to wonder if those clusters signified something. But any meaning we assigned them would be misconceived because these data are identical to the earlier set of 200 random Xs and Os, except for the geometric 5-by-40 arrangement and the choice of which letters to put in boldface.

This very issue drew much attention toward the end of World War II, when V2 rockets started raining down on London. The rockets were terrifying, traveling at over five times the speed of sound, so that one heard them approach only after they had hit. Newspapers soon published maps of the impact sites, which seemed to reveal not random patterns but purposeful clusters. To some observers the clusters indicated a precision in the control of the rockets' flight path that, given the distance the rockets had to travel, suggested that German technology was much more advanced than anyone had dreamed possible. Civilians speculated that the areas spared were home to German spies. Military leaders worried that the Germans could target crucial military sites, with devastating consequences.

In 1946 a mathematical analysis of the bombing data was published in the Journal of the Institute of Actuaries. Its author, R. D. Clarke, divided the area of interest into 576 parcels half a kilometer on each side. Of these, 229 parcels sustained no hits while, despite their minuscule size, 8 parcels had four or five hits. Still, Clarke's analysis showed that, like the coin-toss data above, the overall pattern was consistent with a random distribution.30

Similar issues arise frequently in reports of cancer clusters. If you divide any city or county into parcels and randomly distribute incidents of cancer, some parcels will receive less than average and some more. In fact, according to Raymond Richard Neutra, chief of the Division of Environmental and Occupational Disease Control in California's Department of Health, given a typical cancer registry  —  a database on local rates for dozens of different cancers  —  for California's 5,000 census tracts, you could expect to find 2,750 with statistically significant but random elevations of some form of cancer.31 And if you look at a large enough number of such parcels, you'll find some regions in which cancer occurred at many times the normal rate.

The picture looks even worse if you draw the parcel boundaries after the cancers are distributed. What you get then is called the sharpshooter effect, after the apocryphal fellow who excels in his aim because he shoots at blank paper and draws the target afterward. Unfortunately that is how it usually happens in practice: first some citizens notice neighbors with cancer; then they define the boundaries of the area at issue. Thanks to the availability of data on the Internet, America these days is being scoured for such clusters. Not surprisingly, they are being found. Yet the development of cancer requires successive mutations. That means very long exposure and/or highly concentrated carcinogens. For such clusters of cancer to develop from environmental causes and show themselves in concert and before the victims have moved away from the affected area is quite a long shot. According to Neutra, to produce the kind of cancer clusters epidemiologists are typically called on to investigate, a population would have to be exposed to concentrations of carcinogens that are usually credible only in patients undergoing chemotherapy or in some work settings  —  far greater concentrations than people receive in contaminated neighborhoods and schools. Nevertheless, people resist accepting the explanation that the clusters are random fluctuations, and so each year state departments of health receive thousands of residential cancer-cluster reports, which result in the publication of hundreds of exhaustive analyses, none of which has convincingly identified an underlying environmental cause. Says Alan Bender, an epidemiologist with the Minnesota Department of Health, those studies「are an absolute, total, and complete waste of taxpayer dollars.」32

So far in this chapter we have considered some of the ways in which random patterns can fool us. But psychologists have not contented themselves to merely study and categorize such misperceptions. They have also studied the reasons we fall prey to them. Let's now turn our attention to some of those factors.

PEOPLE LIKE TO EXERCISE CONTROL over their environment, which is why many of the same people who drive a car after consuming half a bottle of scotch will freak out if the airplane they are on experiences minor turbulence. Our desire to control events is not without purpose, for a sense of personal control is integral to our self-concept and sense of self-esteem. In fact, one of the most beneficial things we can do for ourselves is to look for ways to exercise control over our lives  —  or at least to look for ways that help us feel that we do. The psychologist Bruno Bettelheim observed, for instance, that survival in Nazi concentration camps「depended on one's ability to arrange to preserve some areas of independent action, to keep control of some important aspects of one's life despite an environment that seemed overwhelming.」33 Later studies showed that a prior sense of helplessness and lack of control is linked to both stress and the onset of disease. In one study wild rats were suddenly deprived of all control over their environment. They soon stopped struggling to survive and died.34 In another study, in a group of subjects who were told they were going to take a battery of important tests, even the pointless power to control the order of those tests was found to reduce anxiety levels.35

One of the pioneers in the psychology of control is the psychologist and amateur painter Ellen Langer, now a professor at Harvard. Years ago, when she was at Yale, Langer and a collaborator studied the effect of the feeling of control on elderly nursing home patients.36 One group was told they could decide how their rooms would be arranged and were allowed to choose a plant to care for. Another group had their rooms set up for them and a plant chosen and tended to for them. Within weeks the group that exercised control over their environment achieved higher scores on a predesigned measure of well-being. Disturbingly, eighteen months later a follow-up study shocked researchers: the group that was not given control experienced a death rate of 30 percent, whereas the group that was given control experienced a death rate of only 15 percent.37

Why is the human need to be in control relevant to a discussion of random patterns? Because if events are random, we are not in control, and if we are in control of events, they are not random. There is therefore a fundamental clash between our need to feel we are in control and our ability to recognize randomness. That clash is one of the principal reasons we misinterpret random events. In fact, inducing people to mistake luck for skill, or pointless actions for control, is one of the easiest enterprises a research psychologist can engage in. Ask people to control flashing lights by pressing a dummy button, and they will believe they are succeeding even though the lights are flashing at random.38 Show people a circle of lights that flash at random and tell them that by concentrating they can cause the flashing to move in a clockwise direction, and they will astonish themselves with their ability to make it happen. Or have two groups simultaneously compete in a similar enterprise  —  one strives for clockwise motion along the circle, and the other attempts to make the lights travel counterclockwise  —  and the two groups will simultaneously perceive the lights traveling around the circle in the direction of their intention.39

Langer showed again and again how the need to feel in control interferes with the accurate perception of random events. In one of her studies, participants were found to be more confident of success when competing against a nervous, awkward rival than when competing against a confident one even though the card game in which they competed, and hence their probability of succeeding, was determined purely by chance.40 In another study she asked a group of bright and well-educated Yale undergraduates to predict the results of thirty random coin tosses.41 The experimenters secretly manipulated the outcomes so that each student was correct exactly half the time. They also arranged for some of the students to have early streaks of success. After the coin tosses the researchers quizzed the students in order to learn how they assessed their guessing ability. Many answered as if guessing a coin toss were a skill they could cultivate. One quarter reported that their performance would be hampered by distraction. Forty percent felt that their performance would improve with practice. And when asked directly to rate their ability at predicting the tosses, the students who achieved the early streaks of success rated themselves better at the task than did the others even though the number of successes was the same for all the subjects.

In another clever experiment, Langer set up a lottery in which each volunteer received a sports trading card with a player's picture on it.42 A card identical to one of the distributed cards was placed in a bag with the understanding that the participant whose card it matched would be declared the winner. The players were divided into two groups. Those in one group had been allowed to choose their card; those in the other had been handed a card at random. Before the drawing each participant was given the opportunity to sell his or her card. Obviously, whether participants chose their cards or were handed them had no effect on their chances of winning. Yet those who had chosen their own cards demanded more than four times as much money for them as those selling the randomly assigned cards.

The subjects in Langer's experiments「knew,」at least intellectually, that the enterprises in which they were engaging were random. When questioned, for example, none of the participants in the trading-card lottery said they believed that being allowed to choose their card had influenced their probability of winning. Yet they had behaved as if it had. Or as Langer wrote,「While people may pay lip service to the concept of chance, they behave as though chance events are subject to control.」43

In real life the role of randomness is far less obvious than it was in Langer's experiments, and we are much more invested in the outcomes and our ability to influence them. And so in real life it is even more difficult to resist the illusion of control.

One manifestation of that illusion occurs when an organization experiences a period of improvement or failure and then readily attributes it not to the myriad of circumstances constituting the state of the organization as a whole and to luck but to the person at the top. That's especially obvious in sports, where, as I mentioned in the Prologue, if the players have a bad year or two, it is the coach who gets fired. In major corporations, in which operations are large and complex and to a great extent affected by unpredictable market forces, the causal connection between brilliance at the top and company performance is even less direct and the efficacy of reactionary firings is no greater than it is in sports. Researchers at Columbia University and Harvard, for example, recently studied a large number of corporations whose bylaws made them vulnerable to shareholders' demands that they respond to rough periods by changing management.44 They found that in the three years after the firing there was no improvement, on average, in operating performance (a measure of earnings). No matter what the differences in ability among the CEOs, they were swamped by the effect of the uncontrollable elements of the system, just as the differences among musicians might become unapparent in a radio broadcast with sufficient noise and static. Yet in determining compensation, corporate boards of directors often behave as if the CEO is the only one who matters.

Research has shown that the illusion of control over chance events is enhanced in financial, sports, and especially, business situations when the outcome of a chance task is preceded by a period of strategizing (those endless meetings), when performance of the task requires active involvement (those long hours at the office), or when competition is present (this never happens, right?). The first step in battling the illusion of control is to be aware of it. But even then it is difficult, for, as we shall see in the following pages, once we think we see a pattern, we do not easily let go of our perception.

Suppose I tell you that I have made up a rule for the construction of a sequence of three numbers and that the sequence 2, 4, 6 satisfies my rule. Can you guess the rule? A single set of three numbers is not a lot to go on, so let's pretend that if you present me with other sequences of three numbers, I will tell you whether or not they satisfy my rule. Please take a moment to think up some three-number sequences to test  —  the advantage of reading a book over interacting in person is that in the book the author can display infinite patience.

Now that you have pondered your strategy, I can say that if you are like most people, the sequences you present will look something like 4, 6, 8 or 8, 10, 12 or 20, 24, 30. Yes, those sequences obey my rule. So what's the rule? Most people, after presenting a handful of such test cases, will grow confident and conclude that the rule is that the sequence must consist of increasing even numbers. But actually my rule was simply that the series must consist of increasing numbers. The sequence 1, 2, 3, for example, would have fit; there was no need for the numbers to be even. Would the sequences you thought of have revealed this?

When we are in the grasp of an illusion  —  or, for that matter, whenever we have a new idea  —  instead of searching for ways to prove our ideas wrong, we usually attempt to prove them correct. Psychologists call this the confirmation bias, and it presents a major impediment to our ability to break free from the misinterpretation of randomness. In the example above, most people immediately recognize that the sequence consists of increasing even numbers. Then, seeking to confirm their guess, they try out many more sequences of that type. But very few find the answer the fast way  —  through the attempt to falsify their idea by testing a sequence that includes an odd number.45 As philosopher Francis Bacon put it in 1620,「the human understanding, once it has adopted an opinion, collects any instances that confirm it, and though the contrary instances may be more numerous and more weighty, it either does not notice them or else rejects them, in order that this opinion will remain unshaken.」46

To make matters worse, not only do we preferentially seek evidence to confirm our preconceived notions, but we also interpret ambiguous evidence in favor of our ideas. This can be a big problem because data are often ambiguous, so by ignoring some patterns and emphasizing others, our clever brains can reinforce their beliefs even in the absence of convincing data. For instance, if we conclude, based on flimsy evidence, that a new neighbor is unfriendly, then any future actions that might be interpreted in that light stand out in our minds, and those that don't are easily forgotten. Or if we believe in a politician, then when she achieves good results, we credit her, and when she fails, we blame circumstances or the other party, either way reinforcing our initial ideas.

In one study that illustrated the effect rather vividly, researchers gathered a group of undergraduates, some of whom supported the death penalty and some of whom were against it.47 The researchers then provided all the students with the same set of academic studies on the efficacy of capital punishment. Half the studies supported the idea that the death penalty has a deterrent effect; the other half contradicted that idea. The researchers also gave the subjects clues hinting at the weak points in each of the studies. Afterward the undergraduates were asked to rate the quality of the studies individually and whether and how strongly their attitudes about the death penalty were affected by their reading. The participants gave higher ratings to the studies that confirmed their initial point of view even when the studies on both sides had supposedly been carried out by the same method. And in the end, though everyone had read all the same studies, both those who initially supported the death penalty and those who initially opposed it reported that reading the studies had strengthened their beliefs. Rather than convincing anyone, the data polarized the group. Thus even random patterns can be interpreted as compelling evidence if they relate to our preconceived notions.

The confirmation bias has many unfortunate consequences in the real world. When a teacher initially believes that one student is smarter than another, he selectively focuses on evidence that tends to confirm the hypothesis.48 When an employer interviews a prospective candidate, the employer typically forms a quick first impression and spends the rest of the interview seeking information that supports it.49 When counselors in clinical settings are advised ahead of time that an interviewee is combative, they tend to conclude that he is even if the interviewee is no more combative than the average person.50 And when people interpret the behavior of someone who is a member of a minority, they interpret it in the context of preconceived stereotypes.51

The human brain has evolved to be very efficient at pattern recognition, but as the confirmation bias shows, we are focused on finding and confirming patterns rather than minimizing our false conclusions. Yet we needn't be pessimists, for it is possible to overcome our prejudices. It is a start simply to realize that chance events, too, produce patterns. It is another great step if we learn to question our perceptions and our theories. Finally, we should learn to spend as much time looking for evidence that we are wrong as we spend searching for reasons we are correct.

Our journey through randomness is now almost at its end. We began with simple rules and went on to learn how they reflect themselves in complex systems. How great is the role of chance in that most important complex system of all  —  our personal destiny? That's a difficult question, one that has infused much of what we have considered thus far. And though I can't hope to answer it fully, I do hope to shed light on it. My conclusion is evident from the following chapter's title, which is the same as that of this book:「The Drunkard's Walk.」

## Notes

1 See Arthur Conan Doyle, The History of Spiritualism (New York: G. H. Doran, 1926); and R. L. Moore, In Search of White Crows: Spiritualism, Parapsychology, and American Culture (London: Oxford University Press, 1977).

2 Ray Hyman,「Parapsychological Research: A Tutorial Review and Critical Appraisal,」Proceedings of the IEEE 74, no. 6 (June 1986): 823–49.

3 Michael Faraday,「Experimental Investigation of Table-Moving,」Athenaeum, July 2, 1853, pp. 801–3.

4 Michael Faraday, quoted in Hyman,「Parapsychological Research,」p. 826.

5 Faraday, quoted ibid.

6 See Frank H. Durgin,「The Tinkerbell Effect: Motion Perception and Illusion,」Journal of Consciousness Studies 9, nos. 5–6 (May–June 2002): 88–101.

7 Christof Koch, The Quest for Consciousness: A Neurobiological Approach (Englewood, Colo.: Roberts, 2004), pp. 51–54.

8 The study was D. O. Clegg et al.,「Glucosamine, Chondroitin Sulfate, and the Two in Combination for Painful Knee Osteoarthritis,」New England Journal of Medicine 354, no. 8 (February 2006): 795–808. The interview was「Slate's Medical Examiner: Doubts on Supplements,」Day to Day, NPR broadcast, March 13, 2006.

9 See Paul Slovic, Howard Kunreuther, and Gilbert F. White,「Decision Processes, Rationality, and Adjustment to Natural Hazards,」in Natural Hazards: Local, National, and Global, ed. G. F. White (London: Oxford University Press, 1974); see also Willem A. Wagenaar,「Generation of Random Sequences by Human Subjects: A Critical Survey of Literature,」Psychological Bulletin 77, no. 1 (January 1972): 65–72.

10 See Hastie and Dawes, Rational Choice in an Uncertain World, pp. 19–23.

11 George Spencer-Brown, Probability and Scientific Inference (London: Longmans, Green, 1957), pp. 55–56. Actually, 10 is a gross underestimate.

12 Janet Maslin,「His Heart Belongs to (Adorable) iPod,」New York Times, October 19, 2006.

13 Hans Reichenbach, The Theory of Probability, trans. E. Hutton and M. Reichenbach (Berkeley: University of California Press, 1934).

14 The classic text expounding this point of view is Burton G. Malkiel, A Random Walk Down Wall Street, now completely revised in an updated 8th ed. (New York: W. W. Norton, 2003).

15 John R. Nofsinger, Investment Blunders of the Rich and Famous — and What You Can Learn from Them (Upper Saddle River, N.J.: Prentice Hall, Financial Times, 2002), p. 62.

16 Hemang Desai and Prem C. Jain,「An Analysis of the Recommendations of the ‘Superstar' Money Managers at Barron's Annual Roundtable,」Journal of Finance 50, no. 4 (September 1995): 1257–73.

17 Jess Beltz and Robert Jennings,「Wall $treet Week with Louis Rukeyser's Recommendations: Trading Activity and Performance,」Review of Financial Economics 6, no. 1 (1997): 15–27; and Robert A. Pari,「Wall $treet Week Recommendations: Yes or No?」Journal of Portfolio Management 14, no. 1 (1987): 74–76.

18 Andrew Metrick,「Performance Evaluation with Transactions Data: The Stock Selection of Investment Newsletters, Journal of Finance 54, no. 5 (October 1999): 1743–75; and「The Equity Performance of Investment Newsletters」(discussion paper no. 1805, Harvard Institute of Economic Research, Cambridge, Mass., November 1997).

19 James J. Choi, David Laibson, and Brigitte Madrian,「Why Does the Law of One Price Fail? An Experiment on Index Mutual Funds」(working paper no. W12261, National Bureau of Economic Research, Cambridge, Mass., May 4, 2006).

20 Leonard Koppett,「Carrying Statistics to Extremes,」Sporting News, February 11, 1978.

21 By some definitions, Koppett's system would be judged to have failed in 1970; by others, to have passed. See CHANCE News 13.04, April 18, 2004–June 7, 2004, http://www.dartmouth.edu/-chance/chance_news/recent_news/chance_news_13.04.htm.

22 As touted on the Legg Mason Capital Management Web site, http://www.leggmasoncapmgmt.com/awards.htm.

23 Lisa Gibbs,「Miller: He Did It Again,」CNNMoney, January 11, 2004, http://money.cnn.com/2004/01/07/funds/ultimateguide_billmiller_0204.

24 Thomas R. Gilovich, Robert Vallone, and Amos Tversky,「The Hot Hand in Basketball: On the Misperception of Random Sequences,」Cognitive Psychology 17, no. 3 (July 1985): 295–314.

25 Purcell's research is discussed in Gould,「The Streak of Streaks.」

26 Mark Hulbert,「Not All Stocks Are Created Equal,」www.MarketWatch.com, January 18, 2005, accessed March 2005 (site now discontinued).

27 Kunal Kapoor,「A Look at Who's Chasing Bill Miller's Streak,」Morningstar, December 30, 2004, http://www.morningstar.com.

28 Michael Mauboussin and Kristen Bartholdson,「On Streaks: Perception, Probability, and Skill,」Consilient Observer (Credit Suisse–First Boston) 2, no. 8 (April 22, 2003).

29 Merton Miller on「Trillion Dollar Bet,」NOVA, PBS broadcast, February 8, 2000.

30 R. D. Clarke,「An Application of the Poisson Distribution,」Journal of the Institute of Actuaries 72 (1946): 48.

31 Atul Gawande,「The Cancer Cluster Myth,」The New Yorker, February 28, 1998, pp. 34–37.

32 Ibid.

33 Bruno Bettelheim,「Individual and Mass Behavior in Extreme Situations,」Journal of Abnormal and Social Psychology 38 (1943): 417–52.

34 Curt P. Richter,「On the Phenomenon of Sudden Death in Animals and Man,」Psychosomatic Medicine 19 (1957): 191–98.

35 E. Stotland and A. Blumenthal,「The Reduction of Anxiety as a Result of the Expectation of Making a Choice,」Canadian Review of Psychology 18 (1964): 139–45.

36 Ellen Langer and Judith Rodin,「The Effects of Choice and Enhanced Personal Responsibility for the Aged: A Field Experiment in an Institutional Setting,」Journal of Personality and Social Psychology 34, no. 2 (1976): 191–98.

37 Ellen Langer and Judith Rodin,「Long-Term Effects of a Control-Relevant Intervention with the Institutionalized Aged,」Journal of Personality and Social Psychology 35, no. 12 (1977): 897–902.

38 L. B. Alloy and L. Y. Abramson,「Judgment of Contingency in Depressed and Nondepressed Students: Sadder but Wiser?」Journal of Experimental Psychology: General 108, no. 4 (December 1979): 441–85.

39 Durgin,「The Tinkerbell Effect.」

40 Ellen Langer,「The Illusion of Control,」Journal of Personality and Social Psychology 32, no. 2 (1975): 311–28.

41 Ellen Langer and Jane Roth,「Heads I Win, Tails It's Chance: The Illusion of Control as a Function of Outcomes in a Purely Chance Task,」Journal of Personality and Social Psychology 32, no. 6 (1975): 951–55.

42 Langer,「The Illusion of Control.」

43 Ibid., p. 311.

44 Raymond Fisman, Rakesh Khurana, and Matthew Rhodes-Kropf,「Governance and CEO Turnover: Do Something or Do the Right Thing?」(working paper no. 05-066, Harvard Business School, Cambridge, Mass., April 2005).

45 P. C. Wason,「Reasoning about a Rule,」Quarterly Journal of Experimental Psychology 20 (1968): 273–81.

46 Francis Bacon, Novum Organon, trans. by P. Urbach and J. Gibson (Chicago: Open Court, 1994), p. 57 (originally published in 1620).

47 Charles G. Lord, Lee Ross, and Mark Lepper,「Biased Assimilation and Attitude Polarization: The Effects of Prior Theories on Subsequently Considered Evidence,」Journal of Personality and Social Psychology 37, no. 11 (1979): 2098–109.

48 Matthew Rabin,「Psychology and Economics」(white paper, University of California, Berkeley, September 28, 1996).

49 E. C. Webster, Decision Making in the Employment Interview (Montreal: Industrial Relations Centre, McGill University, 1964).

50 Beth E. Haverkamp,「Confirmatory Bias in Hypothesis Testing for Client-Identified and Counselor Self-generated Hypotheses,」Journal of Counseling Psychology 40, no. 3 (July 1993): 303–15.

51 David L. Hamilton and Terrence L. Rose,「Illusory Correlation and the Maintenance of Stereotypic Beliefs,」Journal of Personality and Social Psychology 39, no. 5 (1980): 832–45; Galen V. Bodenhausen and Robert S. Wyer,「Effects of Stereotypes on Decision Making and Information-Processing Strategies,」Journal of Personality and Social Psychology 48, no. 2 (1985): 267–82; and C. Stangor and D. N. Ruble,「Strength of Expectancies and Memory for Social Information: What We Remember Depends on How Much We Know,」Journal of Experimental Social Psychology 25, no. 1 (1989): 18–35.

0901模式的错觉与错觉的模式

1848 年，两个十来岁的小女孩玛格丽特和凯特·福克斯，听到了一种无法解释的声音。这声音听起来像有人在敲打或移动家具。而更凑巧的是，她们的住处原本就背着鬼屋的名头。之后 ，凯特向那个声音的来源发起了挑战。她屈着自己的手指关节发出噼啪声，然后让那个声音照着重复一遍，接着又突然让那个怪声说出她的名字。那个奇怪的声音接受了挑战并做出回应。在随后的几天里，在母亲和几个邻居的帮助下，两姐妹找到了一种「密码」与这个敲击者（我可不是故意语带双关）沟通。姐妹俩最终的结论是，这敲击声是一个小贩的鬼魂发出来的，这个小贩几年前在她们现在住的屋子里被人谋杀了。相信死者能与生者沟通交流的现代招魂术伴随着这个故事诞生了。到 19 世纪 50 年代初，被称为「敲桌子」的这种与灵魂接触的方法，以及技出同门的「移桌子」和「翻桌子」在欧美风靡一时。这些通灵方法需要由一群人参与，参与者围坐在桌旁，手放在桌上，然后就是等待。在「敲桌子」方法中，经过一段时间的等待，参与者就会听到一声敲打声；而在「移桌子」和「翻桌子」方法中，桌子会移动或翘起，有时甚至连桌旁的人都一起被拽着跑。有人还画下了当时的场景，在画中，那些表情严肃、穿着下摆拖到大腿中部的夹克的大胡子男人，以及穿着带裙撑的长裙的女性爱好者，当他们的手随着桌子移来移去时，他们的眼睛因惊奇而瞪得老大。

「移桌子」方法一度非常流行，科学家们在 1853 年夏天时，开始觉得有必要对这个通灵方法进行研究。一群医生注意到，在参与者静静坐着的时间里，有一种无意识的共识好像在逐渐成形，最终决定着桌子移动的方向。他们发现，如果通过分散在座者的注意力阻止这个共识形成，桌子就不会移动。在另一项实验中，他们成功地制造出这样一种情况，让一半参与者希望桌子往左移，而另一半参与者希望它往右移，桌子也因而保持静止。他们得出的结论是：「桌子移动的原因是肌肉反应，且主要是通过无意识训练形成的反应。」最终给通灵术这个问题画上句号的，是电磁理论的创始人之一，电机的发明者，历史上最伟大的实验科学家之一，物理学家迈克尔·法拉第的研究。法拉第首先发现，哪怕桌旁只坐了一个人，桌子移动的现象也会发生。然后，在招募了「道德高尚」且事业有成的「移桌者」后，他进行了一系列复杂而富有创造性的实验，证明参与者双手的移动发生在桌子移动之前。他进而设计了一个指示器，一旦出现上述双手移动的情况，指示器就立刻向受试者发出警报。他发现，「一旦（指示器）…… 放到甚至是最认真的（受试者）之前……（这种错觉的）力量就消失了。这一情况产生的原因只能是，所有参与者都被迫意识到他们真正在做些什么」。

和那些医生一样，法拉第的结论是，桌子是被这些坐着的参与者无意识地拉动或推动的。这种运动也许是由于不耐烦而随机发生的，然后在某一时刻，参与者们随机运动中感知到某种模式，当参与者的手跟随桌子的这个想象中的移动而移动时，该模式便成为某种自我实现的期望。法拉第继续写道，他设置的那些指示器的价值，就在于「它能够纠正这些移桌者的意识」。法拉第认识到，人类的感知并不是客观实在造成的直接结果，而是人类对这些活动进行想象的产物。

感知需要想象，因为生活中的数据从来都是不完整且模棱两可的。大部分人都会认为亲眼所见的东西比其他证据更可靠，而在法庭上，没有什么比目击者的证言更受尊重的了。但如果在法庭上展示一段视频，视频的成像质量跟我们人类视网膜形成的未经处理的影像数据完全一样，那么法官可能会想，这个家伙是在玩拖延战术吗？因为在这段视频中，首先会有一个盲点，即视觉神经与视网膜的连接点；其次，在整个视野中，唯一具有足够高分辨率的区域，不过是围绕视网膜中心大约 1 度视角的范围，这个范围大致相当于将手臂伸直时我们看到的自己大拇指所对应的宽度。在这个区域之外，分辨率急剧下降。为了补偿这种成像质量下降的缺点，我们需要不停地移动眼球，让那个高成像分辨率区域能扫过我们希望观察的场景。因此总的来说，送往我们大脑的原始视觉数据，是一幅幅晃来晃去、让人头昏脑涨的图像，而且图像里面还有一个洞。幸运的是，大脑会对这些数据进行处理，将双眼传来的输入数据加以组合，并假设相邻位置具有相似的视觉特征，然后用内插值的方法填充图像中的空隙。大脑的这些辛劳带给我们的最终结果，就是一个开开心心的人会盲目地确信他 / 她的视觉既敏锐又清晰，至少在年老、受伤、疾病或过量的 mai tais 鸡尾酒造成严重的视力损伤之前是如此。但这不过是一种错觉。

我们还投机取巧地通过想象填补了非视觉数据模式中的空白。如同视觉输入一样，我们依据不确定和不完整的信息得出结论和做出判断，而当我们对那些数据中的模式进行全完分析之后，就能得到一个我们所见的「画面」清晰又准确的结论。但事实果真如此吗？

其实科学家在帮助我们预防发现虚假模式的方向上已经前进了一步。他们建立了统计分析方法，用来判断一组观测值是否可以对某个假设提供足够的支持，还是它们那看起来的支持完全是偶然因素造成的。例如，当物理学家希望确定超级对撞机所得数据的显著性时，他们并不是把所得到的曲线翻来覆去地看了又看，以试图找到从噪声中冒出的波峰，而是使用数学技巧进行判断。显著性检验就是这样一种统计分析方法，它是由 20 世纪最伟大的统计学家之一 —— 费歇尔于 20 世纪 20 年代建立的。（费歇尔还因他那无法控制的火暴脾气而著称，他与同道的统计学先驱皮尔逊长期不和，甚至在 1936 年皮尔逊死后很久，还对这个死对头进行攻击。）

为了说明费歇尔的思路，现在我们来设想一项超感官知觉的研究。在这个研究里，我们让一名学生预测扔硬币的结果。我们发现如果她几乎每次都能正确地预测结果，我们就有理由假设她确实有这方面的超能力，比如她可以通过意念力预测结果。另一方面，如果她只能正确预测差不多一半的结果，这个数据所能支持的假设就是她只不过是在瞎猜。但如果结果落在两者之间，或者根本就没有足够多的数据，那么结论又该如何呢？在接受与拒绝这两个对立的假设时，我们决策的分界线应该画在哪儿呢？这就是显著性检验所做的事情：它是一个形式化的计算方法，能够计算出当所检验的假设为真时，我们观测到实际所得的数据的概率。如果这个概率小，我们就拒绝假设，反之就接受假设。

举例来说，假定我们都是怀疑论者，并且这名学生不能完全准确地预测出扔硬币的结果。我们进一步假设在某次实验中，她正确预测了若干次扔硬币的结果。那么利用第 4 章给出的分析方法，我们可以算出她仅凭运气就能达到这个预测水平的概率是多少。如果她频繁地猜出准确结果，并且我们算出她纯靠运气做到这一点的可能性仅为 3%，我们就应当拒绝她完全是在瞎猜的假设。用显著性检验的术语来说，拒绝该假设的显著性水平为 3%，即由于纯粹偶然性而得出错误判断的可能性最多为 3%。3% 的显著性水平应该算是相当令人印象深刻了，因此媒体以此作为精神力存在的证据，而对学生的这个表现大加报道。但不相信有精神力的人则可能继续持保留态度。

这个例子说明了一个重要的问题：即使数据的显著性达到类似 3% 这样的水平，如果我们对 100 名不具有精神力的人进行精神力测试，或者对 100 种无效药物进行有效性测试，我们也应该预计到，总会有那么几个人表现得好像确实具有精神力，或者总有那么几种无效药物表现得好像确实有效。这就是为什么政治方面的民意调查或医学领域的研究结果，有时会与之前的结果相矛盾，尤其是在数据量较小时。尽管如此，显著性检验与其他统计方法还是为科学家提供了良好的帮助，特别是在大规模的受控研究中。不过，我们平时并不会进行这样的研究，也做不到下意识地用统计分析加以处理。我们进行判断时依靠的是本能。如果我的维京牌炉子有些毛病，而凑巧一个熟人告诉我她也有同样的经历，那么我会告诫朋友们不要再买这个牌子的炉子；如果在乘坐联合航空的班机时，有那么几次，跟我同机的乘客表现得似乎总比最近坐过的其他公司航班的乘客更为乖戾，我就会避免继续乘坐联合航空的班机。在这些情况中，数据量都不大，但我们内在的本能找到了某种模式。

这些模式有时有意义，有时没有。不论是哪种情况，事实都是我们对生活中各种模式的感知既有很强的说服力，又有高度的主观性。这一事实有着深远的影响。它实际上隐含了某种相对性。正如法拉第发现的那样，在某些情况下，现实只存在于相信它存在的人的眼中。比如，2006 年《新英格兰医学期刊》发表了一项耗资 1250 万美元、对膝关节炎登记患者开展的研究。研究表明，过量的葡萄糖胺与软骨素组成的复合营养成分，在减轻关节炎疼痛方面并不比安慰剂更有效。尽管如此，某位著名医生还是难以舍弃这些营养成分确实有效的感觉。因此，他在一个全国性广播节目中重申了该疗法的潜在好处，而且不再对上面那项研究进行讨论。他说的是：「我妻子的一位医生养了只猫，她说如果这只猫不吃一点儿葡萄糖胺加软骨素硫酸盐的话，早上连站都站不起来。」

更仔细些观察我们就能发现，现代社会中的许多假设，就像「移桌子」一样，建立在共享的错觉之上。在第 8 章中，我们关注了随机事件所展现的令人吃惊的规律性。而接下来的内容，我们将从相反的方面进行叙述，即那些似乎具有确定原因的事件模式，其实有可能纯粹是偶然因素的产物。

寻找模式并赋予其意义是人类的天性。卡尼曼和特沃斯基对许多简化方法进行了分析，而我们在评价数据模式以及在不确定性的环境中进行决策时，常常会用到这些简化方法。他们称这些简化方法为「启发式方法」。一般来说，启发式方法十分有用，但正如我们的视觉信息处理方式有时会带来视觉错觉一样，启发式方法有时也会导致系统性错误。卡尼曼和特沃斯基称这种错误为「偏误」。我们都在使用启发式方法，也都受到偏误的影响。不过，在我们的日常体验中，尽管视觉错觉并不常见，但认知偏误在我们的决策中却扮演着重要的角色。因此，20 世纪晚期兴起一股热潮，研究人类思维究竟是如何认知随机性的。研究者的结论是，「人们对于随机性基本没什么概念：他们对随机性视而不见，而当他们想要制造随机性时，也无法做到」。更糟的是，我们总是习惯性地误判机遇在生活中的角色，并总是做出与我们的最大利益并不相符的决定。

设想存在一个事件序列，这些事件可以是季度的盈利，或是通过互联网约会服务获得的一串或好或糟的约会。但不管是什么样的事件，序列的长度越长，或者观察的序列数量越多，越有可能发现任何一种很容易想象到的模式，但这些「模式」完全是碰巧的结果，根本就不需要有什么「原因」带来那一连串盈利或好或糟的季度，或者或好或糟的约会。数学家斯潘塞 - 布朗向我们充分展示了这一点。他指出，在 10 1000007 个 0 或 1 构成的随机序列中，我们有超过五成的把握，在里面找到至少 10 个互不重叠的、由 100 万个连续的 0 构成的子序列。假设有个可怜的家伙恰巧就掉进这样一个子序列中，那么当他想在科学研究中使用这个随机数序列所产生的随机数时，就会发现程序先是连续蹦出 5 个 0，然后是 10 个，然后是 20 个、1000 个、1 万个、10 万个、50 万个。那么这时他是不是有合理的理由把这个程序退货呢？如果某科学家打开一本新买的关于随机数表的书，却发现所有的数字都是 0，那么他会有怎样的反应呢？斯潘塞 - 布朗这个例子的关键点在于，一个过程本身是随机的，并不等同于这个过程产生的结果也是随机的。苹果公司最初的 iPod（音乐播放器软件），在随机播放歌曲时就碰到这个问题：真正的随机性有时会造成重复，但使用者如果连着听到同一首歌或同一个人唱的歌，他们就会认为这个歌曲的随机排列实际上并不是随机的。因此，公司修改了软件，让它变得「不那么随机，让人觉得它更加随机」（苹果公司创始人史蒂夫·乔布斯语）。

针对随机模式感知问题最早期的思索之一，出自哲学家汉斯·赖欣巴哈。他在 1934 年写道，未受过概率论训练的人们很难识别出随机事件序列。看看下面这个连扔 200 次硬币得到的结果序列，其中 X 表示反面朝上，而 O 表示正面朝上：

```
OOOOXXXXOOOXXXOOOOXXOOXOOOXXXOOXXOOOXXXXOOOXOOXOXOOOOOXOOXOOOOOXXOOXXXOXXOXOXXXXOOOXXOOXXOXOOXXXOOXOOXOXOXXOXOOOXOXOOOOXXXXOOOXXOOXOXXOOOXOOOXXOXOOXXOOOOXOOXXXXOOOOXXXOOOXOOOXXXXXXOOXXXOOXOOXOOOOOXXXX。
```

我们可以很容易找到数据中的模式，比如一开始跟在 4 个 X 后面的 4 个 O，以及快结束时出现的连续 6 个 X。数学计算告诉我们，这类模式出现在 200 次扔硬币的结果序列中的可能性是很高的，但仍有许多人对这种结果感到吃惊。如果这些 X 和 O 的字符串代表的不是扔硬币的结果，而是会对我们产生实际影响的事情，那么人们会希望为它们找到一个有意义的解释。如果 X 代表股市下跌的日子，那么一连串的 X 会让人们相信那些认为股市即将崩溃的观点；如果一连串的 O 代表的是你最喜爱的运动明星连续获得的好成绩，对这位运动员的「稳定表现」大加赞扬的评论听起来就十分令人信服；而如果像我们之前的例子那样，这些 X 或 O 代表的是派拉蒙或哥伦比亚电影公司的失败作品，电影行业的小报所宣称的某人能够或不能够把握全世界电影观众脉搏的言论就足以让每个人点头称是。

学者和作家们投入了大量精力，研究金融市场中的随机成功案例中的模式。比如，许多证据都表明，股票的表现是随机的，或者它如此接近随机，如果没有内幕消息，或者股票交易或账户管理要收取费用，那么人们根本无法从股票价格的随机波动中得到任何获利。尽管如此，华尔街长期以来却对股票分析大师有着一种不成文的信仰。20 世纪 90 年代末，这些分析师的平均年薪约为 300 万美元。他们做得怎样呢？1995 年的一项研究发现，《巴伦周刊》每年都会邀请 8～12 名薪金最高的「华尔街超级明星」到一个圆桌会议上推荐股票。但实际上这些股票的收益，仅仅和市场的平均回报率相当。1987 年和 1997 年进行的研究表明，预言家们在《华尔街一周》这个电视节目上推荐的股票，表现甚至还要差更多：它们远远落后于市场平均盈利水平。哈佛经济研究所的一名研究人员对 153 条时事通讯进行了分析，发现「并没有显著证据证明分析师们确实具有挑选绩优股的能力」。

哪怕纯粹靠运气，也会有某些分析师或基金能表现出令人印象深刻的成功模式。许多研究都表明，这些市场中以往的成功事例，并不能作为未来继续成功的一个良好指标，也就是说，这些成功很大程度上不过是走运而已。尽管如此，大多数人还是觉得，他们的股票经纪人或基金运营专家提供的建议是值得他们的要价的。包括那些足够聪明的投资者在内的许多人，都因此购买了管理费过高的基金。实际上，有一项实验，实验者给一群来自沃顿商学院的聪明的学生每人提供了 1 万美元的虚拟资金，以及反映了标准普尔 500 指数的 4 只指数型基金的汇总报告。绝大多数学生都未能以最低成本选择基金。由于哪怕仅仅是多付 1% 的年费，也可能让你多年后的退休基金缩减 1/3 甚至 1/2，因此这些聪明学生的表现可一点儿都算不上聪明。

当然，斯潘塞 - 布朗的例子已经表明，如果我们观察足够长的时间，就肯定会碰到某个家伙，仅仅靠好运也能做出令人称奇的成功预测。相比于包含 10 1000007 个随机数字的数学问题，如果我们更喜欢现实世界中的例子，就来看看专栏作家伦纳德·科佩特吧。1978 年，科佩特公开了一个股市涨跌的预测体系。按他本人的话来说，这种预测方法能够在每年 1 月底确定当年股市的涨跌，对之前的 11 个年头，该方法都正确地给出了当年股市涨跌的结果。当然，事后对股市涨跌进行「预测」，对于股票选择系统而言是很容易的。所以真正的考验在于，这种方法能否预测股市未来的发展。科佩特的这套东西同样通过了后一个考验：以道琼斯指数来衡量，该系统从 1979 年到 1989 年，连续 11 年都做出了正确的预测；1990 年的预测失败了，但从 1991 年到 1998 年，方法再次连续给出正确答案。虽然科佩特在 19 年的预测中命中了 18 次，但我可以很有把握地说，方法的这个稳定表现与什么能力之类的东西根本无关，不管是什么能力。为什么呢？因为科佩特是《体育新闻》的一名专栏作家，而他的方法不过是根据超级碗（职业橄榄球大联盟的年度冠军赛）的比赛结果进行预测。如果在比赛中获胜的是（前）美国国家橄榄球联合会的球队，他就预测股市上涨；而如果获胜的是（前）美国橄榄球联合会的球队，他就预测股市下跌。要是人们知道这个方法是怎么回事，估计没几个人会说科佩特的成绩不是靠运气得来的。但如果他换个身份，而且不公开他的方法，恐怕人们就会向他欢呼致敬，称他是自查尔斯·道以来最聪明的分析师了。

相较于科佩特的故事，现在来看看比尔·米勒这个确实有着正确身份的家伙的故事。不同于科佩特的成就，米勒可是在许多年里一直保持着连胜，这个连胜完全可以与乔·迪马吉奥连续 56 场比赛击球成功，或者肯·詹宁斯在智力问答节目《危险边缘！》（ Jeopardy !）中 74 次连胜的纪录相媲美。不过这也不是非常贴切的对比：米勒在他那些连胜的年头里，一年赚的钱比后两位先生一辈子赚的钱还要多。因为米勒是美盛价值信托基金唯一的投资组合经理，在他 15 年连续成功的投资中，他的基金的表现每年都好过标准普尔 500 指数的股票证券投资组合。

米勒因这个成就被《金钱》杂志誉为「20 世纪 90 年代最伟大的基金经理」，而晨星网则称他为「年代基金经理」，《精明理财》杂志也在 2001 年、2003 年、2004 年、2005 年和 2006 年把他选为对投资最有影响力的 30 人之一。在米勒连胜的第 14 年中，美国有线电视新闻网财经频道曾引用某位分析师的话，称纯粹靠运气获得连续 14 年成功的机会是 1/372529（后面我们会对这个值多说几句）。

这种认为随机序列中出现的重复是某种超优异表现的错误认识，被学术界称为「热手谬误」。大多数针对热手谬误的研究都集中于体育运动领域，因为在体育运动中，我们可以很容易定义和衡量运动员的表现；同时，体育比赛的规则清晰而确定，数据充足而公开，我们感兴趣的重复情况能一再出现，更不用说这类课题还能让这些学者一边观看比赛一边假装工作。

学术界是在 1985 年前后开始对热手谬误产生兴趣的。更具体点儿说，这个领域的研究始于特沃斯基与同事发表在《认知心理学》上的一篇论文。在名为「篮球运动中的热手：论对随机序列的错误感知」的文章中，特沃斯基和同事们研究了大量的篮球比赛统计数据。当然，每名球员的天赋都不同：有的投篮命中率为 50%，有的更多些，有的则更少些；每名球员也都有过一段时间手热或手冷的经历。文章作者考虑的问题是，假设每一次投篮的投中与不中是随机的，那么我们通过随机序列能观察到的手热或手冷时期的次数和长度，跟实际中的次数及长度相比会怎样？也就是说，如果球员们并不是去投篮，而是扔一枚能够反映其命中率的不完美硬币，所得的结果会怎样？研究者发现，尽管存在着连续命中或连续投失的情况，在费城 76 人队的投篮，波士顿凯尔特人队的罚篮，以及受控实验中康奈尔大学男子和女子篮球队的投篮中，我们都找不到任何证据说明这些情况不能由随机序列的行为解释。

对运动员表现「稳定性」的一个具体而直接的衡量标准，就是在上一次投篮成功（即投中一球）的前提下，本次投篮也成功的条件概率。我们认为的这种发挥稳定的球员，他 / 她的这个条件概率应该比他 / 她的总成功率要高。但论文作者发现，对于每名球员，紧跟在一次成功之后的成功，与紧跟在一次失败（即投篮不中）之后的成功，两者的可能性相等。

特沃斯基的论文发表几年后，诺贝尔奖得主、物理学家珀塞尔决定也来研究一下棒球运动中稳定发挥的本质。而他的发现如果用他在哈佛的同事古尔德的话来说，就是除了迪马吉奥连续 56 场都能击中球的这个例外，「在棒球运动中，从来都没有什么事情的发生频率，会超出由扔硬币模型所预测得到的结果」。甚至美国职业棒球大联盟的巴尔的摩金莺队在 1988 赛季一开始时遭遇的 21 连败，也在预测的频率内。相较于更好的球员和球队，更差一些的球员和球队会出现持续时间更久的连续失败，而且出现连败的频率更高；而相较于弱一些的对手，好的球员和球队会出现更长时间的连胜，同时出现连胜的频率也更高。但这是由于他们平均的败率或胜率要高一些。这个平均比率越高，随机产生的连败或连胜就越长且越频繁。我们只需要搞清楚在扔硬币时发生了什么，就能够理解上面的情况。

可是米勒的那个连续 15 年胜过标准普尔的成功又是怎么回事呢？如果我们知道了另外几个统计数字，那么对于米勒的稳定表现仍然是某随机过程产物的这一事实，可能就不会那样震惊了。比如在 2004 年，米勒的基金盈利差一点儿不到 12%，而标准普尔中的股票平均盈利为 15% 多一点儿。这么听起来，在这一年他应该输给了标准普尔。不过实际上，2004 年仍然被归入米勒「获胜」的一栏，因为标准普尔 500 指数不是股票价格的简单平均，而是按各公司资本额对股票加权后的均值。米勒的基金比标准普尔股票的简单平均要差，却比加权平均要好。事实上，在米勒连续成功的那些年中，有 30 多个「连续 12 个月时间段」，他做得还不如标准普尔 500 指数的加权平均好，但这些时间段都不是一个完整的日历年，也就是从 1 月 1 日到 12 月 31 日的区间，而他的连续成功是按日历年计算的。从某种意义上说，他的连续成功从一开始就有虚假的味道，因为这个成功的定义方式正好对他奏效。

但我们的这些新发现，与米勒要获得他的连胜所面对的那个 1/372529 的不利概率，是怎样互不矛盾的呢？在谈论米勒在 2003 年的表现时，为《融会贯通的观察者》（ The Consilient Observer ）内部通讯（由瑞士信贷第一波士顿出版）撰稿的作家们称，「在过去 40 年中，没有其他任何一只基金的表现，能连续 12 年优于市场」。他们在文中提出一个问题：纯靠运气做到这一点的概率是多少？接着，他们给出三个估计值（这一年是 2003 年，而他们考虑的，是一只基金在区区 12 年中连续击败市场的概率）：1/4096，1/477000，以及二十二亿分之一。按爱因斯坦的话来说，如果他们的估计是正确的，只要一个值就够了。所以真正的概率是多少呢？差不多是 3/4，或者 75%。嗯，这个差别实在是有些悬殊，看来我最好解释一下。

那些非常之低的概率值给出的，是在某个意义之下的正确答案：如果你在 1991 年开始时正好挑出了米勒这么个人，而完全出于偶然，这个你挑选的人不多也不少地在接下来的 15 年中表现得比市场更好，那么这个可能性的分母的确是个天文数字。这个概率也等于在 15 年中每年扔一次硬币且每次都扔出正面朝上的概率。但正如我们对马里斯的本垒打进行的分析那样，这个概率并不是与我们现在的问题真正有关的概率，因为除了米勒，还有几千名基金经理（目前有 6000 多名），同时，也有许多个连续 15 年的时间段可以用来完成这一丰功伟业。因此，我们真正要问的问题是，如果现在有几千人，每人每年都扔一次硬币，并且一直这样做了好几十年，那么其中会出现某个人，在这段时间中的某个连续的 15 年内，能够全部扔出正面朝上，这个概率是多少？而这时候的值远大于单纯地连续扔出 15 次正面朝上的概率。

我们解释得更形象点儿。假设有 1000 名（这个数字肯定低估了）基金经理，从 1991 年（米勒开始连胜的那一年）开始，每年都扔一次硬币。第一年，这些人中大约一半会扔出正面朝上；两年后，大概有 1/4 的人会扔出两次正面朝上；三年后，大概有 1/8 的人扔出三次正面朝上，依此类推。那些扔出反面朝上的人逐渐退出了游戏。这并不影响我们的分析，因为他们已经失败了。在 15 年后，某特定的扔硬币者一直扔出正面朝上的概率为 1/32768。但是，从 1991 年开始扔硬币的 1000 人中的某个人能一直扔出正面朝上的概率则大得多，大约为 3%。最后，我们没有理由只考虑那些从 1991 年才开始扔硬币的人，因为这些经理可以从 1990 年或 1970 年或现代基金时代的任意一年起开始扔硬币。既然《融会贯通的观察者》的撰稿人用的是 40 年这个值，我们就来算一算，在过去 40 年中，某个经理能在某个 15 年的时间段内，每年都击败市场的概率。那这个可能性就增加到了之前所给的值，也就是差不多 3/4。因此，与其说我对米勒的连胜感到惊讶，倒不如说要是没有人能够做到这一点，我们就大可以有理有据地嘲笑那些拿着高薪的经理，因为他们的表现实在比瞎碰运气还要糟糕！

到目前为止，我引用的都是体育和金融领域中热手谬误的例子。但是，在生活中的方方面面，我们其实都能碰到连续的成功或失败，或其他特定的成功与失败交织的模式。有时成功是主流，有时失败更多些。但不管碰到的是哪种模式，生活中重要的是我们要将眼光放长远，并理解不管是连续的成功或失败，还是其他看似不随机的模式，其实可能只是纯粹的运气。而在评价他人时同样重要的是，我们应该认识到对一大群人而言，如果没有人经历过长时期的连续成功或失败，就真是一件很奇怪的事了。

没有人会相信科佩特的成功，也没有人会相信一个靠扔硬币来出主意的家伙。但的确有许多人相信米勒。在这个个案中，尽管那些观察家似乎都忘了我的这类分析，但对于从学术角度研究华尔街的人来说，我说的那些根本不是什么新闻。比如，诺贝尔奖获得者、经济学家默顿·米勒（跟比尔·米勒没啥关系）写道：「如果有 1 万个人观察股票市场，并试图挑出最赚钱的那些股票，那么这 1 万人中总有一个能成功，而他所靠的仅仅是运气。所发生的事情不过如此。这只是场游戏，一个随机的操作。人们以为他们是在有目的地做着些什么，但实际并非如此。」每个人都必须根据所处的环境得出自己的结论，但如果你理解了随机性的运作方式，至少你得到的结论不会太过单纯和天真。

之前我们讨论，那些在一段时间内产生的随机序列的模式是如何愚弄我们的。不过空间中的随机模式同样具有误导性。科学家们知道，要揭示数据隐含的意义，最清晰明了的方式被之一，就是将其绘制成某种图像或图形。当数据用这种方式被呈现出来时，那些很可能被忽略的有意义的关系将会变得更明显。但这个做法的代价是，有时我们也会感知到一些实际上没有意义的模式。我们大脑的构造，就是为这种吸收数据、填补空隙、寻找模式的行事方式设计出来的。让我们看看图 9-1 中灰色方块的排列。

图 9-1

注：照片引自 Frank H. Durgin,「The Tinkerbell Effect」, Journal of Consciousness Studies, 9,nos. 5-6（May to June 2002）。

这张图看上去并不像一个真实的人，但我们可以从这个模式中得到足够的信息，根据这些信息，当我们看到图像中那个婴儿本人时，我们很可能认出他来。如果你拿起书，伸直手臂，从旁边一点儿的角度斜看过去，甚至有可能察觉不到图像中的不完美之处。现在，让我们看看下面的 X 和 O 组成的模式：

```
OOOOXXXXOOOXXXOOOOXXOOXOOOXXXOOXXOOOXXXXOOOXOOXOXOOOOOXOOXOOOOOXXOOXXXOXXOXOXXXXOOOXXOOXXOXOOXXXOOXOOXOXOXXOXOOOXOXOOOOXXXXOOOXXOOXOXXOOOXOOOXXOXOOXXOOOOXOOXXXXOOOOXXXOOOXOOOXXXXXXOOXXXOOXOOXOOOOOXXXX
```

其中可以看到一些矩形的相同字符的集合，特别是在角落上。我将它们加粗了。如果这些 X 和 O 代表着我们感兴趣的事件，那么我们可能会思考这些矩形是不是有什么特殊的含义。但给它们赋予的任何意义，都将是对这些数据的一种误读，因为这些数据与早先那 200 个随机的 X 和 O 的序列完全相同，只不过现在按 5 行 40 列排列出来，并选出了某些部分加粗显示而已。

这个空间模式的问题在二战即将结束时，曾引发了许多关注。当时，德军的 V2 火箭开始轰炸伦敦城。这些火箭非常可怕，飞行速度达到了 5 倍音速，因此当人们听到它们飞过来的声音时，火箭其实早已落地。很快，报纸就公布了火箭落点的分布图，在这些图中，人们似乎可以看到并非随机而像是有特定目的的成簇落点。在一些观察人士看来，这一簇簇落点，就是火箭飞行轨迹控制精度的一种体现。由于火箭的飞行距离很长，因此这些簇模式似乎表明，德国人的科技水平已经达到一个大家做梦都想不到的先进程度。公众怀疑那些躲过了 V2 攻击的区域，就是有德国间谍的地方。而军方高层担心德国人会把关键的军事地点作为目标，从而造成毁灭性的后果。

1946 年，《精算师学会杂志》（ Journal of the Institute of Actuaries ）发表了对上述轰炸数据的数学分析结果。作者克拉克将研究区域分为边长为 0.5 公里的 576 个正方形小格子。这些格子中有 229 个从未遭到轰击，而另一方面，尽管格子的面积很小，却有 8 个格子遭到了 4-5 次轰击。不过，克拉克的分析表明，跟之前那个扔硬币的数据一样，这些落点数据的总体分布模式，仍然符合随机分布。

1-3『这个例子在刘嘉的概率论专栏课里看到过，用泊松分布方程可以检验。（2021-03-01）』

在有关癌症集中高发区的报告中，我们也经常会发现类似问题。如果将任何一个城市或国家分成很多小格，然后让癌症病例在这些小格中随机分布，那么有些格子中的病例数将低于平均值，有些则高于平均值。实际上，根据加利福尼亚州职业安全和健康管理局主管雷蒙德·里查德·诺伊特拉的说法，根据记录几十种不同癌症本地发病率的常见癌症等级数据库，我们有超过 50% 的把握，在加利福尼亚州 5000 个普查区域的 2750 个区域中发现具有统计显著性却是随机结果的某种癌症的高发区。如果对足够多的格子进行观察，我们就能发现，有些区域的癌症发病率，甚至比正常水平高出很多倍。

如果将癌症病例分配好再来画格子，得到的图景会让人感觉更糟。这种做法造成的结果被称为神枪手效应，它得名于一个造假的家伙。这个家伙的枪法比别人的都要好，因为他先朝着一张白纸开枪，然后在纸上画出靶子。不幸的是，这种事情在现实中经常发生：某些市民首先注意到一些邻居得了癌症，然后确定他们认为的癌症高发区域的边界。现在的互联网能够为我们提供广泛且可获取的数据，因此最近就有一些人在美国境内寻找这样的团簇。这些团簇毫不意外一个接一个被找到。但癌症的发生需要细胞不断变异，因此需要长时间暴露在致癌物质中和或存在着高浓度的致癌物。如果这些高发区团簇的确是由环境因素引发的，而且其效果在这些受害者搬离这一地区之前就能够显现出来，那么还真需要一些运气。根据诺伊特拉的说法，人们需要暴露在通常仅能在接收化疗的病人体内或某些特定的工作环境中才能见到的高浓度致癌物质中，才能产生这种程度的、需要流行病学家进行调查的癌症高发区，而实际上所需的这种致癌物质的浓度，远比人们在受污染的住所周围或学校中接触到的浓度要高得多。但人们仍然不愿接受这些团簇是随机形成的这一解释，因此，加利福尼亚州健康部门每年都会收到数以千计的居民点癌症集中发作的报告，也因此生产了数百份巨细无遗的分析报告，但没有一份报告能令人信服地给出那个隐藏着的致癌环境因素。明尼苏达州健康部流行病学家艾伦·本德说，这些研究「是对纳税人金钱绝对、完全、彻底的浪费」。

我们现在已经考查了随机模式愚弄我们的几种方式。不过心理学家并不满足于仅仅对这些误解进行研究与分类，他们还希望了解人们被这些误解愚弄的原因。那么就让我们来看看这些原因的其中几个吧。

人们喜欢对周围的环境施加控制。有些人灌了半瓶苏格兰威士忌后照样开车，却在飞机遇上一点儿小小的颠簸时吓得举止失常，原因就在于他在前一种情况中认为自己能控制局面，而在后一种情况中控制权在他人手上。这种控制欲并非毫无缘由，因为事物在自己的控制之下的感觉，已经与自我认知和自尊融为一体了。实际上，我们能做的对自己最有好处的事情，就是找到各种方式控制自己的生活，或者至少是自以为控制了自己的生活。心理学家布鲁诺·贝特尔海姆就发现，要在纳粹集中营中幸存下来，「所依靠的是这样一种能力，即无论周围环境看起来如何具有压迫性，都能进行计划和安排，以保留一些独立行动的空间，并对生活中的某些至关重要的方面保持控制」。后来的研究表明，无来由的无助感以及在控制力上的缺乏，与情绪紧张和疾病发生这两者都有关系。在一项研究中，野鼠突然被剥夺了对环境的所有控制权，它们很快就失去了继续活下去的欲望，就此死亡。在另一项研究中，受试者被告知他们将要参与一组重要的测试，在这种情况下，即使仅仅让受试者控制这些测试的先后顺序，哪怕这种控制毫无意义，也能减少受试者的焦虑。

控制心理学先驱之一的心理学家兼业余画家埃伦·兰格是哈佛大学的一名教授。多年前还在耶鲁时，她曾与人合作，一同研究控制感在看护住家病人的老年人身上所造成的影响。他们告诉其中一组看护，他们可以自行决定房间的布置，还可以挑选一株植物来照料；而另一组的房间事先已经安排好，植物也已经选好并有人打理。几周后，他们发现能对环境施加控制的那一组，其快乐度得分更高。而更令人不安的，则是 18 个月后的跟进研究所得到的令研究者震惊的数据：没有控制权的那一组老年人，死亡率高达 30%；而有控制权的那一组，其死亡率仅为 15%。

人类对控制权的需求，与我们讨论的随机模式有什么关系呢？因为如果事情是随机的，我们就没有掌控权；而如果我们可以做主，事情就不会是随机的。因此，我们对掌握控制权的需求，和我们识别随机性的能力之间存在着根本的冲突，而这正是我们会错误地解释随机事件的主因之一。实际上，心理学研究者最容易做到的事情之一，就是诱导人们将幸运错当成能力，或将无目的的行动错当成在控制什么东西。比如我们可以让人们按一个按钮控制灯的闪烁，而实际上这个按钮是完全无用的。但哪怕灯的闪烁完全随机，人们仍然相信确实是他们在控制着灯。在另一个实验中，研究者给受试者看一圈随机闪烁着的灯，并告诉他们可以通过集中精神让灯泡沿顺时针方向依次被点亮，接着受试者真的觉得自己做到了这一点，并因此而惊讶得目瞪口呆。我们还可以让两个小组在类似的实验设置下相互竞争，其中一组努力让闪烁按顺时针方向进行，而另一组要让它按逆时针发生，结果这两组人都觉得闪烁方向与他们自己希望的方向是一致的。

兰格一次又一次地证明，这种对掌控感的需求，影响了我们对随机事件的准确感知。在她的一项研究中我们看到，相较于一个充满自信的对手，参与者在与一个神经兮兮、笨手笨脚的对手竞争时，对成功更有信心，尽管他们参加的不过是纸牌游戏，而且在游戏中获胜可能完全靠运气。在另一项研究中，她让一组聪明且受过良好教育的耶鲁大学本科生，去预测 30 次扔硬币的结果。实验设计者秘密地操纵了扔硬币的结果，使每个学生刚好猜对一半；另外对其中的某些学生，实验设计者让他们在开始时能够连猜连中。等这 30 次扔完后，研究者对学生们做了个小小的问卷调查，以了解他们是如何评价自己的猜测能力的。许多学生的答案显示，他们似乎认为猜硬币也是一种可以通过培养锻炼来获得和提高的技能。有 1/4 的学生称，他们的表现被一些分心的事情影响。40% 的学生觉得自己的表现能通过训练获得提高。当直接要求他们对自己猜硬币的能力打分时，开始时连续猜中的学生给自己打的分要更高些，尽管对所有受试者而言，他们猜对的次数是一样的。

在另一个巧妙的实验中，兰格设计了一个彩票抽奖。参与的志愿者每人都得到一张交易卡片，卡片上印着某个游戏者的卡片。抽奖的袋子里也放着一张卡片，并且与发下去的某张卡片一模一样。谁拿的卡片跟袋子里的一样，谁就是赢家。游戏者被分成两组，其中一组可以自行挑选卡片，而另一组的卡片是随机分发的。开奖前，每个参与者都可以出售自己的卡片。显然，不管参与者的卡片是自行挑选的，还是随机分发的，其获胜概率都不会受影响。但相较于随机分配得到卡片的参与者，那些自行挑选卡片的人将卡片的售价定到了前者的 4 倍。

兰格实验中的受试者至少在理智上都「知道」，他们参与的游戏是随机的。当我们直接询问受试者时，没有一个人会说自己相信自行挑选卡片会改变获胜的概率。但他们实际的行为却在说着完全相反的事情。或者正如兰格写的那样：「人们可能会在口头上认同偶然性的概念，但他们按偶然事件能被控制来行事。」

随机性在真实生活中扮演的角色，远不像在兰格的实验中那样明显。相比之下，我们对于结果以及控制结果的能力更为关注。因此，在真实生活中我们要抗拒这种控制的错觉，尤为困难。

这种错觉的一种表现方式，常常出现在哪些经历了一段时期的顺风或逆流的机构中，这时不管是成功还是失败，都很容易被归因于机构的领导，而不会被归因于运气以及造成机构目前状况的无数环境因素。在体育运动中，这一点表现得尤为明显。我们在前言中提到过，如果球员有那么一两年表现得十分糟糕，那么教练会被炒鱿鱼。大公司业务庞杂，而且很大程度上受到不可预料的市场因素的影响，因此高层管理者的聪明才智与公司业绩之间的关系，更说不上有多么直接了。如果因为公司糟糕的效益而条件反射式地解雇高管，这个效果就一点儿也不比解雇教练来得好。哥伦比亚大学和哈佛大学的研究者们最近就对许多公司进行了研究。这些公司设立的规章制度让它们很容易受到股东的影响，因此一旦遭遇困难处境，应对之策往往就是更换管理层。研究者发现，在旧的管理人员被解聘后的三年中，平均而言，公司的业绩（以盈利计算）并无改善。不管首席执行官们在个人能力上有什么差别，他们的个人能力总是受到整个体系中那些不可控因素的影响，就好像音乐家演奏水平的差别，在充斥着噪声的广播中会变得不那么明显了一样。但在决定补救措施的时候，公司董事会的所作所为，却好像是在说首席执行官是全公司唯一紧要的人。

研究还表明，对于那些实际结果由随机造成的任务，如果我们在结果发生之前，先进行一段时间的战略安排（比如那开不完的会议），或者在这些任务需要人们发挥主动性去参与的时候（那些待在办公室里的漫长时间），或者在任务中存在竞争的时候（哪有什么竞争啊！是吧？），对于偶然事件的可控错觉，就会在金融、体育，特别是商业环境中被进一步强化。与可控错觉做斗争的第一步，就是要认识到它的存在。但即使做到了这一点，前路仍然艰辛，因为一旦我们觉得自己发现了一个模式，就不会轻易放弃这个发现，就如同下面的例子要展示的那样。

假设我制定一条规则，然后用这条规则构造由 3 个数字组成的一个序列。如果现在我告诉你 {2,4,6} 这个序列是符合规则的，那么你是否能猜出这条规则是什么？只靠一个由 3 个数字组成的集合，当然不足以让游戏继续。因此，设想一下现在由你来告诉我一些其他的 3 个数字组成的序列，而我告诉你它们是否满足规则。好了，现在请你稍事停顿，想一些这样的数字序列出来。相较于直接和人互动，阅读的好处就在于，作者可以在书中表现出无限的耐心。

现在你已经考虑好你的策略。如果你跟大多数人一样，那么我敢说你设想的差不多就是 {4,6,8} 或 {8,10,12} 或 {20,24,30} 这一类的序列。是的，这些序列都满足我的规则。那么这个规则到底是什么呢？大多数人在列举了若干类似的测试例子后，会变得越来越有信心，并认为这个规则就是序列由递增的偶数构成。实际上，我的规则不过是要求序列必须由递增的数字构成。例如序列 {1,2,3} 也满足规则。这些数字不必为偶数。你设想的序列是否能揭示这一点呢？

一旦被错觉掌控，或者在本例中，当我们有了新想法时，我们通常不是去寻找方法证明这个想法错了，而是试图去证明它是正确的，心理学家称其为「确认偏误」。如果我们希望避免对随机性进行错误解释，它就构成了一个主要障碍。在上面的例子中，大多数人立刻认识到，我给出的序列是递增的偶数。然后为了证实这个猜测，他们测试了许多这一类型的序列。但很少有人能更迅速地找到答案，也就是通过测试包含奇数的序列来证伪他们的猜想。哲学家弗朗西斯·培根在 1620 年写道：「人类的理解力一旦采纳了某个想法，就会收集任何能证实该想法的例子，哪怕反例可能更多也更有分量，但人们要么不去注意它们，要么干脆拒绝接受它们，以此保证自己所采纳的观点仍能维持那不可动摇的地位。」

更糟的是，我们不仅会有倾向性地去寻找证实预设观念的证据，而且会将模棱两可的证据按有利于我们想法的方式进行解释。这可是个大问题，因为数据经常是模棱两可的。通过忽略某些模式并强调另外一些模式，我们聪明的大脑可以强化它的信念，即使在缺乏确有说服力的数据时也如此。比如，如果我们根据可信度不高的证据认为新邻居是个不友好的家伙，那么他今后的任何可以用「不友好」加以解释的行为，都会留在我们的脑海中，而其他行为很容易被我们遗忘。如果我们信任某个政治家，那么当她干得好时，功劳当然都归她。而当她做得不好时，该责怪的就是大环境或她的对手，但不管怎样，我们都只是在不断强化我们开始的想法。

一项研究生动地向我们展现了确认偏误的效果。研究者集中了一群本科生，其中某些人支持死刑，另一些人则反对。然后，研究者给所有学生提供了一些关于死刑实际效果的学术研究成果，两组所得的资料是相同的。在这些资料中，一半的结论支持死刑具有威慑力的观点，而另一半正好相反。研究者还为受试者提供了若干线索，这些线索表明上述研究都存在若干弱点。然后，研究者让这些学生独自给各项研究的质量评分，并说明他们对死刑的态度是否以及在多大程度上会受这些研究的影响。参与者对那些支持自己最初观点的研究给予了更高的分数，哪怕双方的研究是用同样的方法进行的。最后，尽管每个人都阅读了相同的研究报告，但开始时持不同观点的人都宣称这些资料强化了他们的信念。这些数据不但没有说服任何人，反而使双方观点的两极分化更加严重。因此，即使是一个随机模式，如果它与我们的成见相吻合，我们也会把它解释为具有说服力的证据。

确认偏误在实际中造成了许多不幸的后果。如果一名教师在开始时相信某个学生比另一个更聪明，他就会有选择性地将注意力集中在那些倾向于证实这一猜想的证据上。如果一名雇主对一个符合他的某些预设想法的应聘者进行面试，那么他通常会迅速形成第一印象，然后将剩下的面试时间用来寻找支持这种印象的信息。如果临床咨询顾问被提前告知某来访者生性好斗，那么他们将倾向于得出来访者确实好斗的结论，哪怕该来访者并不比普通人好斗。人们在解释少数族裔的一些行为时，也常常会根据一个预设的「样板」加以解释。

人类的大脑已经进化到能高效进行模式识别的水平。但正如确认偏误所显示的那样，我们的注意力主要放在了发现和确认这些模式上，而不是将错误结论降到最少。但我们也不需要对此感到悲观，因为我们仍然有可能克服这些偏见。能够认识到偶然事件也能产生模式，仅这一点就是一个开始了。如果我们能够学会质疑我们的观点和理论，那么又是前进了一大步。最后我们还应该学会，不但要寻找证明我们正确的原因，而且要用同样多的时间寻找证明我们错误的证据。

我们这段穿越随机性的旅程，现在快到终点了。我们从简单的规则开始我们的旅程，并了解了这些规则是如何在那些复杂的系统中表现自己的。那么对我们自身命运这个最为重要的复杂系统而言，偶然性扮演的角色到底有多重要呢？这可是个难题，其中包含了我们至此所考虑过的大部分内容。虽然我并不指望自己能够给出一个完整的答案，但我很希望能使这个问题变得更清晰。在下一章的标题，同时也是本书的书名中，读者可以很明显地看出我对这个问题的回答：醉汉的脚步。