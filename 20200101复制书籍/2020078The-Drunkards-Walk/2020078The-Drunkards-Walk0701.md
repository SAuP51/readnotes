# 0701. Measurement and the Law of Errors

ONE DAY not long ago my son Alexei came home and announced the grade on his most recent English essay. He had received a 93. Under normal circumstances I would have congratulated him on earning an A. And since it was a low A and I know him to be capable of better, I would have added that this grade was evidence that if he put in a little effort, he could score even higher next time. But these were not normal circumstances, and in this case I considered the grade of 93 to be a shocking underestimation of the quality of the essay. At this point you might think that the previous few sentences tell you more about me than about Alexei. If so, you're right on target. In fact, the above episode is entirely about me, for it was I who wrote Alexei's essay.

Okay, shame on me. In my defense I should point out that I would normally no sooner write Alexei's essays than take a foot to the chin for him in his kung fu class. But Alexei had come to me for a critique of his work and as usual presented his request late on the night before the paper was due. I told him I'd get back to him. Proceeding to read it on the computer, I first made a couple of minor changes, nothing worth bothering to note. Then, being a relentless rewriter, I gradually found myself sucked in, rearranging this and rewriting that, and before I finished, not only had he fallen asleep, but I had made the essay my own. The next morning, sheepishly admitting that I had neglected to perform a「save as」on the original, I told him to just go ahead and turn in my version.

He handed me the graded paper with a few words of encouragement.「Not bad,」he told me.「A 93 is really more of an A- than an A, but it was late and I'm sure if you were more awake, you would have done better.」I was not happy. First of all, it is unpleasant when a fifteen-year-old says the very words to you that you have previously said to him, and nevertheless you find his words inane. But beyond that, how could my material — the work of a person whom my mother, at least, thinks of as a professional writer — not make the grade in a high school English class? Apparently I am not alone. Since then I have been told of another writer who had a similar experience, except his daughter received a B. Apparently the writer, with a PhD in English, writes well enough for Rolling Stone, Esquire, and The New York Times but not for English 101. Alexei tried to comfort me with another story: two of his friends, he said, once turned in identical essays. He thought that was stupid and they'd both be suspended, but not only did the overworked teacher not notice, she gave one of the essays a 90 (an A) and the other a 79 (a C). (Sounds odd unless, like me, you've had the experience of staying up all night grading a tall stack of papers with Star Trek reruns playing in the background to break the monotony.)

Numbers always seem to carry the weight of authority. The thinking, at least subliminally, goes like this: if a teacher awards grades on a 100-point scale, those tiny distinctions must really mean something. But if ten publishers could deem the manuscript for the first Harry Potter book unworthy of publication, how could poor Mrs. Finnegan (not her real name) distinguish so finely between essays as to award one a 92 and another a 93? If we accept that the quality of an essay is somehow definable, we must still recognize that a grade is not a description of an essay's degree of quality but rather a measurement of it, and one of the most important ways randomness affects us is through its influence on measurement. In the case of the essay the measurement apparatus was the teacher, and a teacher's assessment, like any measurement, is susceptible to random variance and error.

Voting is also a kind of measurement. In that case we are measuring not simply how many people support each candidate on election day but how many care enough to take the trouble to vote. There are many sources of random error in this measurement. Some legitimate voters might find that their name is not on the rolls of registered voters. Others mistakenly vote for a candidate other than the one intended. And of course there are errors in counting the votes. Some ballots are improperly accepted or rejected; others are simply lost. In most elections the sum of all these factors doesn't add up to enough to affect the outcome. But in close elections it can, and then we usually go through one or more recounts, as if our second or third counting of the votes will be less affected by random errors than our first.

In the 2004 governor's race in the state of Washington, for example, the Democratic candidate was eventually declared the winner although the original tally had the Republican winning by 261 votes out of about 3 million.1 Since the original vote count was so close, state law required a recount. In that count the Republican won again, but by only 42 votes. It is not known whether anyone thought it was a bad sign that the 219-vote difference between the first and second vote counts was several times larger than the new margin of victory, but the upshot was a third vote count, this one entirely「by hand.」The 42-vote victory amounted to an edge of just 1 vote out of each 70,000 cast, so the hand-counting effort could be compared to asking 42 people to count from 1 to 70,000 and then hoping they averaged less than 1 mistake each. Not surprisingly, the result changed again. This time it favored the Democrat by 10 votes. That number was later changed to 129 when 700 newly discovered「lost votes」were included.

Neither the vote-counting process nor the voting process is perfect. If, for instance, owing to post office mistakes, 1 in 100 prospective voters didn't get the mailer with the location of the polling place and 1 in 100 of those people did not vote because of it, in the Washington election that would have amounted to 300 voters who would have voted but didn't because of government error. Elections, like all measurements, are imprecise, and so are the recounts, so when elections come out extremely close, perhaps we ought to accept them as is, or flip a coin, rather than conducting recount after recount.

The imprecision of measurement became a major issue in the mid-eighteenth century, when one of the primary occupations of those working in celestial physics and mathematics was the problem of reconciling Newton's laws with the observed motions of the moon and planets. One way to produce a single number from a set of discordant measurements is to take the average, or mean. It seems to have been young Isaac Newton who, in his optical investigations, first employed it for that purpose.2 But as in many things, Newton was an anomaly. Most scientists in Newton's day, and in the following century, didn't take the mean. Instead, they chose the single「golden number」from among their measurements — the number they deemed mainly by hunch to be the most reliable result they had. That's because they regarded variation in measurement not as the inevitable by-product of the measuring process but as evidence of failure — with, at times, even moral consequences. In fact, they rarely published multiple measurements of the same quantity, feeling it would amount to the admission of a botched process and raise the issue of trust. But in the mid-eighteenth century the tide began to change. Calculating the gross motion of heavenly bodies, a series of nearly circular ellipses, is a simple task performed today by precocious high school students as music blares through their headphones. But to describe planetary motion in its finer points, taking into account not only the gravitational pull of the sun but also that of the other planets and the deviation of the planets and the moon from a perfectly spherical shape, is even today a difficult problem. To accomplish that goal, complex and approximate mathematics had to be reconciled with imperfect observation and measurement.

There was another reason why the late eighteenth century demanded a mathematical theory of measurement: beginning in the 1780s in France a new mode of rigorous experimental physics had arisen.3 Before that period, physics consisted of two separate traditions. On the one hand, mathematical scientists investigated the precise consequences of Newton's theories of motion and gravity. On the other, a group sometimes described as experimental philosophers performed empirical investigations of electricity, magnetism, light, and heat. The experimental philosophers — often amateurs — were less focused on the rigorous methodology of science than were the mathematics-oriented researchers, and so a movement arose to reform and mathematize experimental physics. In it Pierre-Simon de Laplace again played a major role.

Laplace had become interested in physical science through the work of his fellow Frenchman Antoine-Laurent Lavoisier, considered the father of modern chemistry.4 Laplace and Lavoisier worked together for years, but Lavoisier did not prove as adept as Laplace at navigating the troubled times. To earn money to finance his many scientific experiments, he had become a member of a privileged private association of state-protected tax collectors. There is probably no time in history when having such a position would inspire your fellow citizens to invite you into their homes for a nice hot cup of gingerbread cappuccino, but when the French Revolution came, it proved an especially onerous credential. In 1794, Lavoisier was arrested with the rest of the association and quickly sentenced to death. Ever the dedicated scientist, he requested time to complete some of his research so that it would be available to posterity. To that the presiding judge famously replied,「The republic has no need of scientists.」The father of modern chemistry was promptly beheaded, his body tossed into a mass grave. He had reportedly instructed his assistant to count the number of words his severed head would attempt to mouth.

Laplace's and Lavoisier's work, along with that of a few others, especially the French physicist Charles-Augustin de Coulomb, who experimented on electricity and magnetism, transformed experimental physics. Their work also contributed to the development, in the 1790s, of a new rational system of units, the metric system, to replace the disparate systems that had impeded science and were a frequent cause of dispute among merchants. Developed by a group appointed by Louis XVI, the metric system was adopted by the revolutionary government after Louis's downfall. Lavoisier, ironically, had been one of the group's members.

The demands of both astronomy and experimental physics meant that a great part of the mathematician's task in the late eighteenth and early nineteenth centuries was understanding and quantifying random error. Those efforts led to a new field, mathematical statistics, which provides a set of tools for the interpretation of the data that arise from observation and experimentation. Statisticians sometimes view the growth of modern science as revolving around that development, the creation of a theory of measurement. But statistics also provides tools to address real-world issues, such as the effectiveness of drugs or the popularity of politicians, so a proper understanding of statistical reasoning is as useful in everyday life as it is in science.

IT IS ONE OF THOSE CONTRADICTIONS of life that although measurement always carries uncertainty, the uncertainty in measurement is rarely discussed when measurements are quoted. If a fastidious traffic cop tells the judge her radar gun clocked you going thirty-nine in a thirty-five-mile-per-hour zone, the ticket will usually stick despite the fact that readings from radar guns often vary by several miles per hour.5 And though many students (along with their parents) would jump off the roof if doing so would raise their 598 on the math SAT to a 625, few educators talk about the studies showing that, if you want to gain 30 points, there's a good chance you can do it simply by taking the test a couple more times.6 Sometimes meaningless distinctions even make the news. One recent August the Bureau of Labor Statistics reported that the unemployment rate stood at 4.7 percent. In July the bureau had reported the rate at 4.8 percent. The change prompted headlines like this one in The New York Times:「Jobs and Wages Increased Modestly Last Month.」7 But as Gene Epstein, the economics editor of Barron's, put it,「Merely because the number has changed it doesn't necessarily mean that a thing itself has changed. For example, any time the unemployment rate moves by a tenth of a percentage point…that is a change that is so small, there is no way to tell whether there really was a change.」8 In other words, if the Bureau of Labor Statistics measures the unemployment rate in August and then repeats its measurement an hour later, by random error alone there is a good chance that the second measurement will differ from the first by at least a tenth of a percentage point. Would The New York Times then run the headline「Jobs and Wages Increased Modestly at 2 P.M.」?

The uncertainty in measurement is even more problematic when the quantity being measured is subjective, like Alexei's English-class essay. For instance, a group of researchers at Clarion University of Pennsylvania collected 120 term papers and treated them with a degree of scrutiny you can be certain your own child's work will never receive: each term paper was scored independently by eight faculty members. The resulting grades, on a scale from A to F, sometimes varied by two or more grades. On average they differed by nearly one grade.9 Since a student's future often depends on such judgments, the imprecision is unfortunate. Yet it is understandable given that, in their approach and philosophy, the professors in any given college department often run the gamut from Karl Marx to Groucho Marx. But what if we control for that — that is, if the graders are given, and instructed to follow, certain fixed grading criteria? A researcher at Iowa State University presented about 100 students' essays to a group of doctoral students in rhetoric and professional communication whom he had trained extensively according to such criteria.10 Two independent assessors graded each essay on a scale of 1 to 4. When the scores were compared, the assessors agreed in only about half the cases. Similar results were found by the University of Texas in an analysis of its scores on college-entrance essays.11 Even the venerable College Board expects only that, when assessed by two raters,「92% of all scored essays will receive ratings within ± 1 point of each other on the 6-point SAT essay scale.」12

Another subjective measurement that is given more credence than it warrants is the rating of wines. Back in the 1970s the wine business was a sleepy enterprise, growing, but mainly in the sales of low-grade jug wines. Then, in 1978, an event often credited with the rapid growth of that industry occurred: a lawyer turned self-proclaimed wine critic, Robert M. Parker Jr., decided that, in addition to his reviews, he would rate wines numerically on a 100-point scale. Over the years most other wine publications followed suit. Today annual wine sales in the United States exceed `$`20 billion, and millions of wine aficionados won't lay their money on the counter without first looking to a wine's rating to support their choice. So when Wine Spectator awarded, say, the 2004 Valentín Bianchi Argentine cabernet sauvignon a 90 rather than an 89, that single extra point translated into a huge difference in Valentín Bianchi's sales.13 In fact, if you look in your local wine shop, you'll find that the sale and bargain wines, owing to their lesser appeal, are often the wines rated in the high 80s. But what are the chances that the 2004 Valentín Bianchi Argentine cabernet that received a 90 would have received an 89 if the rating process had been repeated, say, an hour later?

In his 1890 book The Principles of Psychology, William James suggested that wine expertise could extend to the ability to judge whether a sample of Madeira came from the top or the bottom of a bottle.14 In the wine tastings that I've attended over the years, I've noticed that if the bearded fellow to my left mutters「a great nose」(the wine smells good), others certainly might chime in their agreement. But if you make your notes independently and without discussion, you often find that the bearded fellow wrote,「Great nose」the guy with the shaved head scribbled,「No nose」and the blond woman with the perm wrote,「Interesting nose with hints of parsley and freshly tanned leather.」

From the theoretical viewpoint, there are many reasons to question the significance of wine ratings. For one thing, taste perception depends on a complex interaction between taste and olfactory stimulation. Strictly speaking, the sense of taste comes from five types of receptor cells on the tongue: salty, sweet, sour, bitter, and umami. The last responds to certain amino acid compounds (prevalent, for example, in soy sauce). But if that were all there was to taste perception, you could mimic everything — your favorite steak, baked potato, and apple pie feast or a nice spaghetti Bolognese — employing only table salt, sugar, vinegar, quinine, and monosodium glutamate. Fortunately there is more to gluttony than that, and that is where the sense of smell comes in. The sense of smell explains why, if you take two identical solutions of sugar water and add to one a (sugar-free) essence of strawberry, it will taste sweeter than the other.15 The perceived taste of wine arises from the effects of a stew of between 600 and 800 volatile organic compounds on both the tongue and the nose.16 That's a problem, given that studies have shown that even flavor-trained professionals can rarely reliably identify more than three or four components in a mixture.17

Expectations also affect your perception of taste. In 1963 three researchers secretly added a bit of red food color to white wine to give it the blush of a rosé. They then asked a group of experts to rate its sweetness in comparison with the untinted wine. The experts perceived the fake rosé as sweeter than the white, according to their expectation. Another group of researchers gave a group of oenology students two wine samples. Both samples contained the same white wine, but to one was added a tasteless grape anthocyanin dye that made it appear to be red wine. The students also perceived differences between the red and the white corresponding to their expectations.18 And in a 2008 study a group of volunteers asked to rate five wines rated a bottle labeled `$`90 higher than another bottle labeled `$`10, even though the sneaky researchers had filled both bottles with the same wine. What's more, this test was conducted while the subjects were having their brains imaged in a magnetic resonance scanner. The scans showed that the area of the brain thought to encode our experience of pleasure was truly more active when the subjects drank the wine they believed was more expensive.19 But before you judge the oenophiles, consider this: when a researcher asked 30 cola drinkers whether they preferred Coke or Pepsi and then asked them to test their preference by tasting both brands side by side, 21 of the 30 reported that the taste test confirmed their choice even though this sneaky researcher had put Coke in the Pepsi bottle and vice versa.20 When we perform an assessment or measurement, our brains do not rely solely on direct perceptional input. They also integrate other sources of information — such as our expectation.

Wine tasters are also often fooled by the flip side of the expectancy bias: a lack of context. Holding a chunk of horseradish under your nostril, you'd probably not mistake it for a clove of garlic, nor would you mistake a clove of garlic for, say, the inside of your sneaker. But if you sniff clear liquid scents, all bets are off. In the absence of context, there's a good chance you'd mix the scents up. At least that's what happened when two researchers presented experts with a series of sixteen random odors: the experts misidentified about 1 out of every 4 scents.21

Given all these reasons for skepticism, scientists designed ways to measure wine experts' taste discrimination directly. One method is to use a wine triangle. It is not a physical triangle but a metaphor: each expert is given three wines, two of which are identical. The mission: to choose the odd sample. In a 1990 study, the experts identified the odd sample only two-thirds of the time, which means that in 1 out of 3 taste challenges these wine gurus couldn't distinguish a pinot noir with, say,「an exuberant nose of wild strawberry, luscious blackberry, and raspberry,」from one with「the scent of distinctive dried plums, yellow cherries, and silky cassis.」22 In the same study an ensemble of experts was asked to rank a series of wines based on 12 components, such as alcohol content, the presence of tannins, sweetness, and fruitiness. The experts disagreed significantly on 9 of the 12 components. Finally, when asked to match wines with the descriptions provided by other experts, the subjects were correct only 70 percent of the time.

Wine critics are conscious of all these difficulties.「On many levels…[the ratings system] is nonsensical,」says the editor of Wine and Spirits Magazine.23 And according to a former editor of Wine Enthusiast,「The deeper you get into this the more you realize how misguided and misleading this all is.」24 Yet the rating system thrives. Why? The critics found that when they attempted to encapsulate wine quality with a system of stars or simple verbal descriptors such as good, bad, and maybe ugly, their opinions were unconvincing. But when they used numbers, shoppers worshipped their pronouncements. Numerical ratings, though dubious, make buyers confident that they can pick the golden needle (or the silver one, depending on their budget) from the haystack of wine varieties, makers, and vintages.

If a wine — or an essay — truly admits some measure of quality that can be summarized by a number, a theory of measurement must address two key issues: How do we determine that number from a series of varying measurements? And given a limited set of measurements, how can we assess the probability that our determination is correct? We now turn to these questions, for whether the source of data is objective or subjective, their answers are the goal of the theory of measurement.

THE KEY to understanding measurement is understanding the nature of the variation in data caused by random error. Suppose we offer a number of wines to fifteen critics or we offer the wines to one critic repeatedly on different days or we do both. We can neatly summarize the opinions employing the average, or mean, of the ratings. But it is not just the mean that matters: if all fifteen critics agree that the wine is a 90, that sends one message; if the critics produce the ratings 80, 81, 82, 87, 89, 89, 90, 90, 90, 91, 91, 94, 97, 99, and 100, that sends another. Both sets of data have the same mean, but they differ in the amount they vary from that mean. Since the manner in which data points are distributed is such an important piece of information, mathematicians created a numerical measure of variation to describe it. That number is called the sample standard deviation. Mathematicians also measure the variation by its square, which is called the sample variance.

The sample standard deviation characterizes how close to the mean a set of data clusters or, in practical terms, the uncertainty of the data. When it is low, the data fall near the mean. For the data in which all wine critics rated the wine 90, for example, the sample standard deviation is 0, telling you that all the data are identical to the mean. When the sample standard deviation is high, however, the data are not clustered around the mean. For the set of wine ratings above that ranges from 80 to 100, the sample standard deviation is 6, meaning that as a rule of thumb most of the ratings fall within 6 points of the mean. In that case all you can really say about the wine is that it is probably somewhere between an 84 and a 96.

In judging the meaning of their measurements, scientists in the eighteenth and nineteenth centuries faced the same issues as the skeptical oenophile. For if a group of researchers makes a series of observations, the results will almost always differ. One astronomer might suffer adverse atmospheric conditions; another might be jostled by a breeze; a third might have just returned from a Madeira tasting with William James. In 1838 the mathematician and astronomer F. W. Bessel categorized eleven classes of random errors that occur in every telescopic observation. Even if a single astronomer makes repeated measurements, variables such as unreliable eyesight or the effect of temperature on the apparatus will cause the observations to vary. And so astronomers must understand how, given a series of discrepant measurements, they can determine a body's true position. But just because oenophiles and scientists share a problem, it doesn't mean they can share its solution. Can we identify general characteristics of random error, or does the character of random error depend on the context?

One of the first to imply that diverse sets of measurements share common characteristics was Jakob Bernoulli's nephew Daniel. In 1777 he likened the random errors in astronomical observation to the deviations in the flight of an archer's arrows. In both cases, he reasoned, the target — true value of the measured quantity, or the bull's-eye — should lie somewhere near the center, and the observed results should be bunched around it, with more reaching the inner bands and fewer falling farther from the mark. The law he proposed to describe the distribution did not prove to be the correct one, but what is important is the insight that the distribution of an archer's errors might mirror the distribution of errors in astronomical observations.

That the distribution of errors follows some universal law, sometimes called the error law, is the central precept on which the theory of measurement is based. Its magical implication is that, given that certain very common conditions are satisfied, any determination of a true value based on measured values can be solved employing a single mathematical analysis. When such a universal law is employed, the problem of determining the true position of a heavenly body based on a set of astronomers' measurements is equivalent to that of determining the position of a bull's-eye given only the arrow holes or a wine's「quality」given a series of ratings. That is the reason mathematical statistics is a coherent subject rather than merely a bag of tricks: whether your repeated measurements are aimed at determining the position of Jupiter at 4 A.M. on Christmas Day or the weight of a loaf of raisin bread coming off an assembly line, the distribution of errors is the same.

This doesn't mean random error is the only kind of error that can affect measurement. If half a group of wine critics liked only red wines and the other half only white wines but they all otherwise agreed perfectly (and were perfectly consistent), then the ratings earned by a particular wine would not follow the error law but instead would consist of two sharp peaks, one due to the red wine lovers and one due to the white wine lovers. But even in situations where the applicability of the law may not be obvious, from the point spreads of pro football games25 to IQ ratings, the error law often does apply. Many years ago I got hold of a few thousand registration cards for a consumer software program a friend had designed for eight- and nine-year-olds. The software wasn't selling as well as expected. Who was buying it? After some tabulation I found that the greatest number of users occurred at age seven, indicating an unwelcome but not unexpected mismatch. But what was truly striking was that when I made a bar graph showing how the number of buyers diminished as the buyers' age strayed from the mean of seven, I found that the graph took a very familiar shape — that of the error law.

It is one thing to suspect that archers and astronomers, chemists and marketers, encounter the same error law; it is another to discover the specific form of that law. Driven by the need to analyze astronomical data, scientists like Daniel Bernoulli and Laplace postulated a series of flawed candidates in the late eighteenth century. As it turned out, the correct mathematical function describing the error law — the bell curve — had been under their noses the whole time. It had been discovered in London in a different context many decades earlier.

OF THE THREE PEOPLE instrumental in uncovering the importance of the bell curve, its discoverer is the one who least often gets the credit. Abraham De Moivre's breakthrough came in 1733, when he was in his mid-sixties, and wasn't made public until his book The Doctrine of Chances came out in its second edition five years later. De Moivre was led to the curve while searching for an approximation to the numbers that inhabit the regions of Pascal's triangle far beneath the place where I truncated it, hundreds or thousands of lines down. In order to prove his version of the law of large numbers, Jakob Bernoulli had had to grapple with certain properties of the numbers that appeared in those lines. The numbers can be very large — for instance, one coefficient in the 200th row of Pascal's triangle has fifty-nine digits! In Bernoulli's day, and indeed in the days before computers, such numbers were obviously very hard to calculate. That's why, as I said, Bernoulli proved his law of large numbers employing various approximations, which diminished the practical usefulness of his result. With his curve, De Moivre was able to make far better approximations to the coefficients and therefore greatly improve on Bernoulli's estimates.

The approximation De Moivre derived is evident if, as I did for the registration cards, you represent the numbers in a row of the triangle by the height of the bars on a bar graph. For instance, the three numbers in the third line of the triangle are 1, 2, 1. In their bar graph the first bar rises one unit; the second is twice that height; and the third is again just one unit. Now look at the five numbers in the fifth line: 1, 4, 6, 4, 1. That graph will have five bars, again starting low, rising to a peak at the center, and then falling off symmetrically. The coefficients very far down in the triangle lead to bar graphs with very many bars, but they behave in the same manner. The bar graphs in the case of the 10th, 100th, and 1,000th lines of Pascal's triangle are shown on chapter 07.

If you draw a curve connecting the tops of all the bars in each bar graph, it will take on a characteristic shape, a shape approaching that of a bell. And if you smooth the curve a bit, you can write a mathematical expression for it. That smooth bell curve is more than just a visualization of the numbers in Pascal's triangle; it is a means for obtaining an accurate and easy-to-use estimate of the numbers that appear in the triangle's lower lines. This was De Moivre's discovery.

Today the bell curve is usually called the normal distribution and sometimes the Gaussian distribution (we'll see later where that term originated). The normal distribution is actually not a fixed curve but a family of curves, in which each depends on two parameters to set its specific position and shape. The first parameter determines where its peak is located, which is at 5, 50, and 500 in the graphs on chapter 7. The second parameter determines the amount of spread in the curve. Though it didn't receive its modern name until 1894, this measure is called the standard deviation, and it is the theoretical counterpart of the concept I spoke of earlier, the sample standard deviation. Roughly speaking, it is half the width of the curve at the point at which the curve is about 60 percent of its maximum height. Today the importance of the normal distribution stretches far beyond its use as an approximation to the numbers in Pascal's triangle. It is, in fact, the most widespread manner in which data have been found to be distributed.

When employed to describe the distribution of data, the bell curve describes how, when you make many observations, most of them fall around the mean, which is represented by the peak of the curve. Moreover, as the curve slopes symmetrically downward on either side, it describes how the number of observations diminishes equally above and below the mean, at first rather sharply and then less drastically. In data that follow the normal distribution, about 68 percent (roughly two-thirds) of your observations will fall within 1 standard deviation of the mean, about 95 percent within 2 standard deviations, and 99.7 percent within 3.

The bars in the graphs above represent the relative magnitudes of the entries in the 10th, 100th, and 1,000th rows of Pascal's triangle (see chapter 04). The numbers along the horizontal axis indicate to which entry the bar refers. By convention, that labeling begins at 0, rather than 1 (the middle and bottom graphs have been truncated so that the entries whose bars would have negligible height are not shown).

In order to visualize this, have a look at the graph on chapter 07. In this table the data marked by squares concern the guesses made by 300 students, each observing a series of 10 coin flips.26 Along the horizontal axis is plotted the number of correct guesses, from 0 to 10. Along the vertical axis is plotted the number of students who achieved that number of correct guesses. The curve is bell shaped, centered at 5 correct guesses, at which point its height corresponds to about 75 students. The curve falls to about two-thirds of its maximum height, corresponding to about 51 students, about halfway between 3 and 4 correct guesses on the left and between 6 and 7 on the right. A bell curve with this magnitude of standard deviation is typical of a random process such as guessing the result of a coin toss.

The same graph also displays another set of data, marked by circles. That set describes the performance of 300 mutual fund managers. In this case the horizontal axis represents not correct guesses of coin flips but the number of years (out of 10) that a manager performed above the group average. Note the similarity! We'll get back to this in chapter 9.

A good way to get a feeling for how the normal distribution relates to random error is to consider the process of polling, or sampling. You may recall the poll I described in chapter 5 regarding the popularity of the mayor of Basel. In that city a certain fraction of voters approved of the mayor, and a certain fraction disapproved. For the sake of simplicity we will now assume each was 50 percent. As we saw, there is a chance that those involved in the poll would not reflect exactly this 50/50 split. In fact, if N voters were questioned, the chances that any given number of them would support the mayor are proportional to the numbers on line N of Pascal's triangle. And so, according to De Moivre's work, if pollsters poll a large number of voters, the probabilities of different polling results can be described by the normal distribution. In other words about 95 percent of the time the approval rating they observe in their poll will fall within 2 standard deviations of the true rating, 50 percent. Pollsters use the term margin of error to describe this uncertainty. When pollsters tell the media that a poll's margin of error is plus or minus 5 percent, they mean that if they were to repeat the poll a large number of times, 19 out of 20 (95 percent) of those times the result would be within 5 percent of the correct answer. (Though pollsters rarely point this out, that also means, of course, that about 1 time in 20 the result will be wildly inaccurate.) As a rule of thumb, a sample of 100 yields a margin of error that is too great for most purposes. A sample of 1,000, on the other hand, usually yields a margin of error in the ballpark of 3 percent, which for most purposes suffices.

Coin toss guessing compared to stock-picking success

It is important, whenever assessing any kind of survey or poll, to realize that when it is repeated, we should expect the results to vary. For example, if in reality 40 percent of registered voters approve of the way the president is handling his job, it is much more likely that six independent surveys will report numbers like 37, 39, 39, 40, 42, and 42 than it is that all six surveys will agree that the president's support stands at 40 percent. (Those six numbers are in fact the results of six independent polls gauging the president's job approval in the first two weeks of September 2006.)27 That's why, as another rule of thumb, any variation within the margin of error should be ignored. But although The New York Times would not run the headline「Jobs and Wages Increased Modestly at 2 P.M.,」analogous headlines are common in the reporting of political polls. For example, after the Republican National Convention in 2004, CNN ran the headline「Bush Apparently Gets Modest Bounce.」28 The experts at CNN went on to explain that「Bush's convention bounce appeared to be 2 percentage points…. The percentage of likely voters who said he was their choice for president rose from 50 right before the convention to 52 immediately afterward.」Only later did the reporter remark that the poll's margin of error was plus or minus 3.5 percentage points, which means that the news flash was essentially meaningless. Apparently the word apparently, in CNN-talk, means「apparently not.」

For many polls a margin of error of more than 5 percent is considered unacceptable, yet in our everyday lives we make judgments based on far fewer data points than that. People don't get to play 100 years of professional basketball, invest in 100 apartment buildings, or start 100 chocolate-chip-cookie companies. And so when we judge their success at those enterprises, we judge them on just a few data points. Should a football team lavish `$`50 million to lure a guy coming off a single record-breaking year? How likely is it that the stockbroker who wants your money for a sure thing will repeat her earlier successes? Does the success of the wealthy inventor of sea monkeys mean there is a good chance he'll succeed with his new ideas of invisible goldfish and instant frogs? (For the record, he didn't.)29 When we observe a success or a failure, we are observing one data point, a sample from under the bell curve that represents the potentialities that previously existed. We cannot know whether our single observation represents the mean or an outlier, an event to bet on or a rare happening that is not likely to be reproduced. But at a minimum we ought to be aware that a sample point is just a sample point, and rather than accepting it simply as reality, we ought to see it in the context of the standard deviation or the spread of possibilities that produced it. The wine might be rated 91, but that number is meaningless if we have no estimate of the variation that would occur if the identical wine were rated again and again or by someone else. It might help to know, for instance, that a few years back, when both The Penguin Good Australian Wine Guide and On Wine's Australian Wine Annual reviewed the 1999 vintage of the Mitchelton Blackwood Park Riesling, the Penguin guide gave the wine five stars out of five and named it Penguin Best Wine of the Year, while On Wine rated it at the bottom of all the wines it reviewed, deeming it the worst vintage produced in a decade.30 The normal distribution not only helps us understand such discrepancies, but also has enabled a myriad of statistical applications widely employed today in both science and commerce — for example, whenever a drug company assesses whether the results of a clinical trial are significant, a manufacturer assesses whether a sample of parts accurately reflects the proportion of those that are defective, or a marketer decides whether to act on the results of a research survey.

THE RECOGNITION that the normal distribution describes the distribution of measurement error came decades after De Moivre's work, by that fellow whose name is sometimes attached to the bell curve, the German mathematician Carl Friedrich Gauss. It was while working on the problem of planetary motion that Gauss came to that realization, at least regarding astronomical measurements. Gauss's「proof,」however, was, by his own later admission, invalid.31 Moreover, its far-reaching consequences also eluded him. And so he slipped the law inconspicuously into a section at the end of a book called The Theory of the Motion of Heavenly Bodies Moving about the Sun in Conic Sections. There it may well have died, just another in the growing pile of abandoned proposals for the error law.

It was Laplace who plucked the normal distribution from obscurity. He encountered Gauss's work in 1810, soon after he had read a memoir to the Académie des Sciences proving a theorem called the central limit theorem, which says that the probability that the sum of a large number of independent random factors will take on any given value is distributed according to the normal distribution. For example, suppose you bake 100 loaves of bread, each time following a recipe that is meant to produce a loaf weighing 1,000 grams. By chance you will sometimes add a bit more or a bit less flour or milk, or a bit more or less moisture may escape in the oven. If in the end each of a myriad of possible causes adds or subtracts a few grams, the central limit theorem says that the weight of your loaves will vary according to the normal distribution. Upon reading Gauss's work, Laplace immediately realized that he could use it to improve his own and that his work could provide a better argument than Gauss's to support the notion that the normal distribution is indeed the error law. Laplace rushed to press a short sequel to his memoir on the theorem. Today the central limit theorem and the law of large numbers are the two most famous results of the theory of randomness.

To illustrate how the central limit theorem explains why the normal distribution is the correct error law, let's reconsider Daniel Bernoulli's example of the archer. I played the role of the archer one night after a pleasant interlude of wine and adult company, when my younger son, Nicolai, handed me a bow and arrow and dared me to shoot an apple off his head. The arrow had a soft foam tip, but still it seemed reasonable to conduct an analysis of my possible errors and their likelihood. For obvious reasons I was mainly concerned with vertical errors. A simple model of the errors is this: Each random factor — say, a sighting error, the effect of air currents, and so on — would throw my shot vertically off target, either high or low, with equal probability. My total error in aim would then be the sum of my errors. If I was lucky, about half the component errors would deflect the arrow upward and half downward, and my shot would end up right on target. If I was unlucky (or, more to the point, if my son was unlucky), the errors would all fall one way and my aim would be far off, either high or low. The relevant question was, how likely was it that the errors would cancel each other, or that they would add up to their maximum, or that they would take any other value in between? But that was just a Bernoulli process — like tossing coins and asking how likely it is that the tosses will result in a certain number of heads. The answer is described by Pascal's triangle or, if many trials are involved, by the normal distribution. And that, in this case, is precisely what the central limit theorem tells us. (As it turned out, I missed both apple and son, but did knock over a glass of very nice cabernet.)

By the 1830s most scientists had come to believe that every measurement is a composite, subject to a great number of sources of deviation and hence to the error law. The error law and the central limit theorem thus allowed for a new and deeper understanding of data and their relation to physical reality. In the ensuing century, scholars interested in human society also grasped these ideas and found to their surprise that the variation in human characteristics and behavior often displays the same pattern as the error in measurement. And so they sought to extend the application of the error law from physical science to a new science of human affairs.

## Notes

1 Sarah Kershaw and Eli Sanders,「Recounts and Partisan Bickering Bring Election Fatigue to Washington Voters,」New York Times, December 26, 2004; and Timothy Egan,「Trial for Governor's Seat Set to Start in Washington,」New York Times, May 23, 2005.

2 Jed Z. Buchwald,「Discrepant Measurements and Experimental Knowledge in the Early Modern Era,」Archive for History of Exact Sciences 60, no. 6 (November 2006): 565–649.

3 Eugene Frankel,「J. B. Biot and the Mathematization of Experimental Physics in Napoleonic France,」in Historical Studies in the Physical Sciences, ed. Russell McCormmach (Princeton, N.J.: Princeton University Press, 1977).

4 Charles Coulston Gillispie, ed., Dictionary of Scientific Biography (New York: Charles Scribner's Sons, 1981), p. 85.

5 For a discussion of the errors made by radar guns, see Nicole Weisensee Egan,「Takin' Aim at Radar Guns,」Philadelphia Daily News, March 9, 2004.

6 Charles T. Clotfelter and Jacob L. Vigdor,「Retaking the SAT」(working paper SAN01-20, Terry Sanford Institute of Public Policy, Duke University, Durham, N.C., July 2001).

7 Eduardo Porter,「Jobs and Wages Increased Modestly Last Month,」New York Times, September 2, 2006.

8 Gene Epstein on「Mathemagicians,」On the Media, WNYC radio, broadcast August 25, 2006.

9 Legene Quesenberry et al.,「Assessment of the Writing Component within a University General Education Program,」November 1, 2000; http://wac.colostate.edu/aw/articles/quesenberry2000/quesenberry2000.pdf.

10 Kevin Saunders,「Report to the Iowa State University Steering Committee on the Assessment of ISU Comm-English 105 Course Essays,」September 2004; www.iastate.edu/-isucomm/InYears/ISUcomm_essays.pdf (accessed 2005; site now discontinued).

11 University of Texas, Office of Admissions,「Inter-rater Reliability of Holistic Measures Used in the Freshman Admissions Process of the University of Texas at Austin,」February 22, 2005; http://www.utexas.edu/student/admissions/research/Inter-raterReliability2005.pdf.

12 Emily J. Shaw and Glenn B. Milewski,「Consistency and Reliability in the Individualized Review of College Applicants,」College Board, Office of Research and Development, Research Notes RN-20 (October 2004): 3; http://www.collegeboard.com/research/pdf/RN-20.pdf.

13 Gary Rivlin,「In Vino Veritas,」New York Times, August 13, 2006.

14 William James, The Principles of Psychology (New York: Henry Holt, 1890), p. 509.

15 Robert Frank and Jennifer Byram,「Taste-Smell Interactions Are Tastant and Odorant Dependent,」Chemical Senses 13 (1988): 445–55.

16 A. Rapp,「Natural Flavours of Wine: Correlation between Instrumental Analysis and Sensory Perception,」Fresenius' Journal of Analytic Chemistry 337, no. 7 (January 1990): 777–85.

17 D. Laing and W. Francis,「The Capacity of Humans to Identify Odors in Mixtures,」Physiology and Behavior 46, no. 5 (November 1989): 809–14; and D. Laing et al.,「The Limited Capacity of Humans to Identify the Components of Taste Mixtures and Taste-Odour Mixtures,」Perception 31, no. 5 (2002): 617–35.

18 For the rosé study, see Rose M. Pangborn, Harold W. Berg, and Brenda Hansen,「The Influence of Color on Discrimination of Sweetness in Dry Table-Wine,」American Journal of Psychology 76, no. 3 (September 1963): 492–95. For the anthocyanin study, see G. Morrot, F. Brochet, and D. Dubourdieu,「The Color of Odors,」Brain and Language 79, no. 2 (November 2001): 309–20.

19 Hilke Plassman, John O'Doherty, Baba Shia, and Antonio Rongel,「Marketing Actions Can Modulate Neural Representations of Experienced Pleasantness,」Proceedings of the National Academy of Sciences, January 14, 2008; http://www.pnas.org.

20 M. E. Woolfolk, W. Castellan, and C. Brooks,「Pepsi versus Coke: Labels, Not Tastes, Prevail,」Psychological Reports 52 (1983): 185–86.

21 M. Bende and S. Nordin,「Perceptual Learning in Olfaction: Professional Wine Tasters Versus Controls,」Physiology and Behavior 62, no. 5 (November 1997): 1065–70.

22 Gregg E. A. Solomon,「Psychology of Novice and Expert Wine Talk,」American Journal of Psychology 103, no. 4 (Winter 1990): 495–517.

23 Rivlin,「In Vino Veritas.」

24 Ibid.

25 Hal Stern,「On the Probability of Winning a Football Game,」American Statistician 45, no. 3 (August 1991): 179–82.

26 The graph is from Index Funds Advisors,「Index Funds.com: Take the Risk Capacity Survey,」http://www.indexfunds3.com/step3page2.php, where it is credited to Walter Good and Roy Hermansen, Index Your Way to Investment Success (New York: New York Institute of Finance, 1997). The performance of 300 mutual fund managers was tabulated for ten years (1987–1996), based on the Morningstar Principia database.

27 Polling Report,「President Bush — Overall Job Rating,」http://pollingreport.com/BushJob.htm.

28.「Poll: Bush Apparently Gets Modest Bounce,」CNN, September 8, 2004, http://www.cnn.com/2004/ALLPOLITICS/09/06/presidential.poll/index.htm.

29「Harold von Braunhut,」Telegraph, December 23, 2003; http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2003/12/24/db2403.xml.

30 James J. Fogarty,「Why Is Expert Opinion on Wine Valueless?」(discussion paper 02.17, Department of Economics, University of Western Australia, Perth, 2001).

31 Stigler, The History of Statistics, p. 143.

0701测量与误差定律

不久前，我的儿子阿列克谢回到家中，向我通报了他最近一次作文的成绩 —— 93 分。我一般会祝贺他得了个 A，然后鼓励他争取下次得到更高的分数。所以我往往会再多句嘴，告诫他再努力一点点，但这次的情况有点儿不同。这个 93 分相较于文章的质量，实在是太低了。这个说法是不是让你觉得我是在维护自己的文章，而不是阿列克谢的作文？啊，你可说到点子上了：实际上，前面这段话的确完全是在说我自己，因为那篇作文是我写的……

对对对，我的做法真是可耻。不过我还是要为自己辩护一下，我一般不会帮阿列克谢写作文，我也不会到他的武术课用自己的脸替他挨那一脚。不过那天的情况有点儿不同。阿列克谢照例又是在交作业的头天深夜来找我，希望我点评一下他的作品。我答应帮他看看。最开始我只是在一两个地方做了一点儿小小的改动，但接着，我发现自己渐渐深陷无情的改写之中：这里的文字调一下顺序，那一段干脆重新写。等到修改完成后，儿子早已上床睡觉了，于是他的作文成了我自己的作品。第二天早上，我睡眼惺忪地承认，忘了把他原来的文章另存一份，所以我让他把我修改后的那版直接交上去。

他把成绩单递给了我。成绩单上写着几句鼓励的话语。「还不算糟，」他告诉我，「93 分确实更接近 A- 而不是 A，不过那天也确实是挺晚了。我相信，如果那天你更清醒一点儿，肯定会写得更好。」这些安慰的话可一点儿没让我更加高兴。首先，一个 15 岁的孩子把本该是你的台词甩到你的脸上，这种情况实在无法让人舒心，更何况我在他的话里听不出一星半点的真诚。另外，我，一个至少在我妈妈看来是个职业作家的人写的作文，怎么可能在高中英语课上拿不到高分？显然，在这个问题上我并不孤单。后来我听说另一个有着类似经历的作家的故事，不过他的女儿拿的可是个 B。很明显，这位拥有英语博士学位的作家，他的文笔好得至少能够满足《滚石》、《时尚先生》和《纽约时报》的要求，却对付不了英语 101 网络电台。阿列克谢还试着用另一个故事来安慰我：他有两个朋友，有一次把完全相同的两篇作文一起交了上去。他觉得这两个家伙很蠢，而且觉得他们肯定会被抓抄袭。但操劳过度的老师不仅没有注意到这两篇文章完全雷同，而且给其中一个打了 90 分（A），给另一个打了 79 分（C）。（听起来很怪，不过你如果跟我一样，要熬通宵给高高一摞卷子打分，而排遣这个枯燥无聊的工作的，只有旁边重播的《星际迷航》，你就会明白了。）

数字似乎总是自带权威性。人们几乎总是，或者至少是下意识地认为，如果老师按百分制打分，那么即使 1 分、2 分的微小差别，也一定意味着某种真实的差距。但如果连续 10 个出版商都相信第一部《哈利·波特》的手稿不值得出版的话，那么可怜的芬尼根太太（可不是我儿子英语老师的真名啊）怎么可能如此精确地区分两篇作文的好坏，给一篇打 92 分而另一篇打 93 分呢？即使我们接受作文质量可以在一定程度上被定义的观点，我们也应该认识到，分数并不是对作文质量的描述，更大程度上是对作文质量的测量。而随机性对测量的影响，正是它影响我们的最重要的方式之一。在作文的这个例子中，测量装置是教师，正如任何测量值一样，教师给出的分数很容易受随机变化和误差的影响。

投票也是一种测量。投票所测量的，并不是每位候选人在投票那天得到了多少人的支持，而是有多少人在乎这个选举，还不嫌麻烦地跑去投票。有些合法选民也许会发现，登记选民名册中并没有他们的名字；另外一些人则可能误将选票投给了他们并不支持的人。当然计票也有误差。有些选票不该被接受却被收下，有些选票不该被拒收却被拒之门外，还有些选票干脆凭空消失了。在大多数选举中，这些因素累积起来的总后果并不足以影响选举结果。但如果候选人得票数相差不大，这个后果就可能产生实质影响。这时，我们常常进行一次或多次的重新计票，就好像第二次或第三次计票，受到的随机影响会比第一次更少。

在 2004 年的华盛顿州州长竞选中，尽管最初的计票结果表明，共和党候选人靠着总数约 300 万张选票中多出的 261 张获胜，但最终获胜的却是民主党候选人。由于第一次的得票数很接近，根据该州法律，这时要进行重新计票。第二次计票仍然是共和党获胜，但领先差距缩小到 42 票。两次计票的这 219 票的差别，已经是新的领先票数的好几倍了。结果是不是让什么人产生了不祥的预感，我们不得而知。但头两次计票的结果，带来了第三次纯「手工」计票。这个 42 票的优势，相当于在每 7 万张选票中领先 1 票，因此，手工计票的作用，实际上可以被比拟为让 42 个人从 1 数到 7 万，并希望平均数错的次数为每人 1 次。所以并不令人吃惊的是，选举结果再次变化，民主党人反赢了 10 票。当新发现的 700 张「丢失选票」被加进来之后，这个优势变成了 129 票。

上面的计票和投票过程并非尽善尽美。比如，如果邮局犯错，每 100 个原本打算投票的选民，就有 1 个未能收到通知投票地点的邮件，因而没有去参加投票，那么在华盛顿州的这场选举中，仅这一项错误就能产生 300 名有投票意愿却因政府的过失而没有参加的选民。与所有的测量一样，选举并非精确无误，重新计票也是如此。因此，当选举结果极其接近时，也许我们更应该接受这个事实，或者干脆就靠扔硬币决定胜负，而不是把选票数了又数。

测量的不精确性是 18 世纪中叶学术界讨论的一个主要问题。那时，天体物理和数学领域的研究者的首要任务，就是让牛顿定律与所观测到的月球与行星的运行轨迹相吻合。如果对同一个量有若干彼此不同的观测值，而现在要利用这些不同值产生一个单一的值，那么方法之一是对这些观测值求平均，或者取它们的均值。现在我们认为是年轻的牛顿在做光学研究的时候，最早将这个算平均的方法用在了刚才所讲的那个问题上。但如同牛顿在许多其他事情上的做法一样，这种求平均的处理方式在当时可是个另类。从牛顿的时代到其后一个世纪的时间里，大多数科学家都不使用均值产生多个观测结果对应的最终结果，他们的做法是从这些观测结果中挑出一个「黄金数」，也就是他们的第六感认为最可靠的那个结果。他们之所以这么做，是因为同一个量的多次测量值的变化并未被他们看作测量过程中不可避免的副产品，而是被他们视为失败的象征，有时甚至还会给他们带来道德问题。实际上，他们很少公开同一个量的多个测量结果，因为如果这么做了，就等于承认他们对结果进行了修补，而这会给他们带来信任危机。但到了 18 世纪中期，情况有所变化。今天，计算天体的完整运行轨迹（一系列接近圆形的椭圆）是一件十分简单的事情，天赋高一点儿的高中生甚至可以一边戴着耳机听音乐，一边就把它给解决了。但是如果要更精细地描述行星的运动，不仅需要考虑太阳引力，还需要考虑其他行星的引力，以及行星和月球的形状与完美球体之间的偏差。时至今日，这仍是一个巨大的难题。为了完成这个任务，人们不得不将复杂的近似数学公式与不完美的观测结果加以调和。

18 世纪晚期出现的这个对于测量的数学理论的需求，还另有一个原因：18 世纪 80 年代，法国兴起一种新的、严密的实验物理学。在此之前，物理学包括两种相互分离的实践方式。一方面，数学家研究牛顿的运动和重力理论的精确推论；另一方面，一群有时被称为实验哲学家的人，采用经验主义的方式来研究电、磁、光和热。这些实验哲学家常常是一些业余爱好者，相较于关注数学的研究者，他们不那么看重严密的科学方法论。因此，一场对实验物理学进行改革并将其数学化的运动就此展开。而拉普拉斯再次扮演了主角。

拉普拉斯之所以对物理学产生兴趣，是由于被尊为现代化学之父的他的同胞安托万 - 洛朗·拉瓦锡的研究工作。拉普拉斯和拉瓦锡共事多年，但在幸免政治动乱这方面，拉瓦锡却不像拉普拉斯那样成功。为了赚钱支持自己的科学实验，拉瓦锡成为受国家保护的收税官这个享有特权的法国国王私人组织中的一员。这个职位的工作，本来也不可能让你的公民同胞们突然迸发出热情，并邀请你去他家享用美味可口的姜饼配咖啡。而等法国大革命到来时，它更有可能成为一项足以带来非常严重的后果的铁证。1794 年，拉瓦锡与组织的其他成员一同被捕，并很快被判处死刑。一直以来都是一名专注的科学家的拉瓦锡，请求法官再给他一点儿时间，以便他能把手头的几项研究完成，好留给后人。对他的这个请求，主审法官给出一个著名的答复：「共和国不需要科学家。」现代化学之父很快被处斩，尸体被扔进一个合葬墓。按传说所言，他在临刑前还指示他的助手要数一数，看他的头颅被砍下来之后还能说出几个字。

拉普拉斯、拉瓦锡和其他一些人，特别是进行电磁实验的法国物理学家查利 - 奥古斯丁·库仑的成果，改变了实验物理学的面貌。而在 18 世纪 90 年代，他们还对一种新的、合理的单位制的建立做出了贡献。这个新的单位制 —— 国际单位制 —— 的目的，是替代那些各自为政、阻碍科学发展还经常导致贸易纠纷的多个现有的单位制。国际单位制是由法国国王路易十六指派的一个小组建立的，在路易十六被推翻后被革命政府采用。具有讽刺意味的是，拉瓦锡就曾是这个小组的成员之一。

天文学和实验物理学方面的需求，意味着在 18 世纪晚期到 19 世纪早期，很大一部分数学家的任务，就是去理解和量化随机误差。他们的研究工作催生了一个新领域：数学统计学。这个分支学科提供了一整套工具，用于解释观测值和实验数据。统计学家有时认为，现代科学就是围绕着测量理论的建立成长起来的。统计学同样为解决诸如药物有效性或政治家受欢迎程度等现实问题提供了工具。正确地理解统计推理，不仅对科学研究十分有帮助，而且对我们在日常生活中碰到的很多问题作用很大。

尽管测量总是伴随着不确定性，但在给出测量结果时，测量中的不确定性很少被提及。这本身就是生活中众多的自相矛盾之一。如果一个挑剔的交警告诉法官，她的雷达枪测出你在限速 35 英里 / 小时的路段开到了 39 英里的时速，那么你一般是无法逃过被开超速罚单的命运的，尽管雷达枪的读数常常会发生多达数英里的变化。对许多学生（及其家长）而言，只要能让 SAT 数学考试成绩从 598 分提高到 625 分，就是让他们跳楼都没问题。不过却没有几位教育者告诉他们，有研究表明，你只需要多考几次，就有很大把握多拿 30 分。有时候，一些毫无意义的差别甚至能变成新闻。最近的某个 8 月，美国劳工统计局的就业统计数据显示失业率为 4.7%，而在 7 月，这个比例是 4.8%。这一改变马上就上了头条，比如《纽约时报》上的这个「上月的职位与薪水温和增长」。但用《巴伦周刊》经济版编辑吉恩·爱泼斯坦的话来说：「单纯的数字变化，并不意味着事情本身真的发生了变化。比如，失业率在任何时候都可能发生 0.1 个百分点的变化…… 这个变化如此之小，以至我们无法确定改变是否确实发生了。」换句话说，如果美国劳工统计局在 8 月统计了失业率，并在一小时后重新统计一次，那么仅仅由于随机误差，就很有可能使两次结果相差至少 0.1 个百分点。如果是这样的话，《纽约时报》的头条是不是该换成「下午两点的职位与薪水温和增长」？

当测量对象是一个如阿列克谢的英语课作文质量那样的主观量时，测量中的不确定性造成的问题就更大了。宾夕法尼亚州克莱瑞恩大学的一群研究者收集了 120 个学期的试卷，并对它们进行了非常仔细的审查。可以确定的是，我们自家孩子的作业永远享受不到如此细致程度的评价：每张试卷都由 8 名教师独立按 A 到 F 的七级分数制打分。对于同一张试卷，这 8 位教师给出的分数有时会相差 2 个甚至更多等级。而平均来说，不同教师给出的这个评分差别，差不多是 1 个等级。由于学生的前途常常取决于这类评价，因此，评分中存在的这个不精确性是十分不幸的。但我们应该知道，不论从方法上或理念上来看，任何一所大学中都有着从卡尔·马克思式到格劳乔·马克斯式的各式教授。如此一来，分数上的差别也就可以理解了。不过让我们再来考虑一下，如果我们在一定程度上控制这些影响打分的因素，比如固定打分的教师，并让他们按某固定的打分依据阅卷，结果又会如何呢？艾奥瓦州立大学的一名研究者，交给一群主修修辞与专业沟通的博士生大约 100 名学生的作文，并事先按评分标准对他们进行了高强度训练。每篇作文由两名独立评分者打分，等级为 1 等到 4 等。当比较他们的分数时，仅仅在大约一半的试卷上，两名打分者的意见相互一致。得克萨斯大学在本校的入学作文考试分数上也发现了类似的结果。即使是可敬的美国大学理事会，它所期望的评分稳定程度也不过是，当由两位打分者进行评判时，「在按 6 分制打分的 SAT 作文考试中，92% 的作文所得的两个分数分差在 ±1 分之内」。

另一个被赋予超出其合理可信程度的主观性测量，就是葡萄酒的评分。回溯 20 世纪 70 年代，当时的葡萄酒业还是个死气沉沉的行业，虽然有增长，但这个增长主要来自廉价的低等佐餐酒的销售。1978 年发生了一件通常被认为是葡萄酒业迅猛发展的事件：罗伯特·帕克这位律师出身的自封的酒评师，决定在文字的评语之外，再用 0 到 100 的数字给葡萄酒打分。之后，大多数其他酒类出版物都沿用这套体系。如今，美国的葡萄酒年销售额超过 200 亿美元，而那些数以百万计的葡萄酒迷，在没有看酒的评分之前，是绝不会把钱放到柜台上的。因此，比如那次《葡萄酒观察家》杂志给 2004 年份瓦伦丁比安奇酒庄的阿根廷赤霞珠打了 90 分而非 89 分时，这多出来的一分，就带来了瓦伦丁比安奇酒庄销售额上的巨大增长。实际上，如果你去看看你当地的葡萄酒店，那些因吸引力不够而常常沦为促销品和低价货的葡萄酒，得分一般都在 80 多分不到 90 分这一档。但如果我们过一个小时给那个 90 分的 2004 年份瓦伦丁比安奇酒庄的阿根廷赤霞珠重新打一次分，那么这个新分数是 89 分的可能性有多大呢？

威廉·詹姆斯在 1890 年出版的《心理学原理》中认为，品酒专家能将品酒能力发挥到如此的程度，他们甚至可以判断出某杯马德拉白葡萄酒是来自上半瓶还是下半瓶。我在这么多年来参加过的品酒会上也注意到，如果我左边那个长着大胡子的家伙嘟囔了一声「大酒香味」（这酒闻起来挺香），那么其他人肯定也会发出赞同的共鸣。但如果以各自独立品酒、不能相互讨论的方式各打各的分数，那么我们经常能看到，那个大胡子写的是「大酒香味」，另一个刚剃过头的家伙则草草写了个「没有酒香」，而这个烫发的金发美女写的却是「有意思的酒香，似乎有一丝欧芹和刚晒过的皮革的味道」。

根据理论观点，我们有许多理由质疑葡萄酒评分的统计显著性。首先，我们对味道的感知，依赖于味觉和嗅觉刺激复杂的相互作用。严格来说，味觉来自舌头上的 5 种感知细胞：咸、甜、酸、苦和鲜味感知细胞。最后一种细胞会对某些氨基酸成分（例如普遍存在于酱油中的某些成分）产发生反应。但如果这就是味觉的全部，我们就可以用食盐、蔗糖、醋、奎宁和味精，模仿你最爱的牛排、烤土豆、苹果派大餐，或美味的意大利肉酱面等任何东西。幸运的是，仅仅这些味道还不足以让我们胃口大开，还需要嗅觉发挥作用。饮用两杯浓度完全相同的糖水时，如果我们在其中一杯加入一些（无糖的）草莓香精，那么你会觉得这杯水更甜。这种情况可以通过嗅觉来解释。我们所感知的葡萄酒味道，是 600-800 种可挥发性的有机化合物构成的大杂烩，它们同时在舌头上和鼻子中产生混合效果。有研究证明，即使是受过品味训练的专业人员，也很少能够分辨出一种混合物中 3 到 4 种以上的组成成分。在这种情况下，要分辨产生葡萄酒味的大杂烩，可真是一个问题了。

2『人舌头上 5 类基本感知细胞，做一张信息数据卡片。（2021-02-28）』

我们对于味道的预期，同样能影响我们对味道的感知。1963 年，3 名研究者偷偷地在白葡萄酒中加入了一点儿红色食用色素，把酒弄成了玫瑰红色。然后，他们请一群专家给染了色的酒的甜度打分，再把这个分数和未加色素的酒进行比较。这些专家都觉得红葡萄酒应该比白葡萄酒更甜，因此他们打出的分数也都表明假的玫瑰红葡萄酒比白葡萄酒更甜。另一组研究者把两杯葡萄酒样品提供给一群品酒学专业的学生。这两杯样品是相同的白葡萄酒，但其中一杯加入了无味的葡萄花青素，看上去像红葡萄酒。同样，由于对不同品种的酒的味道抱有预期，这些学生觉得红葡萄酒和白葡萄酒在味道上存在差别。在 2008 年的一项研究中，研究者让受试者给 5 瓶酒打分。他们给标价 90 美元的酒打出的分数，要高于另一瓶标价 10 美元的酒，但实际上，狡猾的研究者在两个瓶子里灌的是完全相同的酒。研究者还在实验中对受试者的大脑活动进行了磁共振成像。结果表明，当受试者品尝他们相信是更贵的酒时，那个普遍被认为对快感产生响应的大脑区域，确实处于更加兴奋的状态。但如果我们现在想对这些品酒行家指指点点的话，就让我们再来看看下面这个例子吧。一位研究者首先询问了 30 个人各自对可乐的偏好，看他们是更喜欢可口可乐还是百事可乐。然后，这些人品尝了并排放着的两个牌子的可乐，看看他们对这些可乐口味的评价，是不是与他们事先的偏好吻合。品鉴结束后，有 21 个人都说试喝更加确认了他们的选择。可实际上，这个鬼祟的研究者把瓶子里的内容对调了：百事可乐瓶子里装的是可口可乐，而百事可乐则倒进了可口可乐的瓶子里。在进行评价或测量时，我们的大脑并非单纯依赖于直接的感知输入，而是额外结合了其他的信息源 —— 比如我们抱有的期望。

期望偏误的反面则是我们因为缺乏相关背景知识，对结果无法做出准确预估，品酒师也经常被这种期望偏误的另一面欺骗。你不大可能把一大块放在你的鼻子下面的山葵跟一瓣大蒜搞混；当然，你基本上也不会把大蒜的味道和你运动鞋里的味道搞混（仅仅是打个比方）。但如果现在你面对的是一杯清澈的液体，那么试图分辨它的气味，根本就是徒劳的。在缺乏其他相关因素时，你把气味搞混的可能性相当大。至少当两名研究者让专家们判断一系列随机的 16 种气味时，情况就是如此：大概每 4 种气味中，这些专家就会认错 1 种。

这些情况足以令人对品酒这件事起疑，因此受此推动，科学家设计了多种方式，直接测量品酒专家对味道的分辨力。一种方法是酒味三角。它并不是什么真正的三角形，而是个比拟：每位专家都得到 3 杯酒，其中 2 杯完全相同，他们的任务是找出那杯不同的。1990 年的一项研究表明，专家能正确识别这个不同样品的比例仅为 2/3，也就是说，差不多每 3 次中就会有 1 次，这些品酒界的宗师级人物，会在品酒挑战中区别不出比如有着「野草莓、甜黑莓和覆盆子的馥郁醇香」的黑皮诺，和有着「干李子、黄樱桃和丝滑黑醋栗的独特气味」的黑皮诺。同样，在这项研究中有若干专家被要求在包括酒精含量、是否含鞣酸、甜度和果味度等 12 个评分指标上，给若干种酒打分。专家们的意见在 9 个指标上存在明显差异。最后，在一项根据其他专家的描述指出相应的酒的测试中，受试者的正确率仅仅为 70%。

酒评家们其实很清楚上面这些困难。「从许多层面而言……（这个给酒评分的体系）是毫无意义的。」《葡萄酒与烈酒杂志》的编辑就这样说过。而按《葡萄酒爱好者》杂志某前编辑的话来说：「你对它了解越深，越能了解这个东西是多么被人误导又在误导别人。」但评分体系仍然蓬勃发展。为什么会这样呢？酒评家们发现，当他们用星级制或简单的好、坏、糟透了这样的词语描述酒的质量时，消费者对他们的意见并不十分信服；但如果他们给出的是数值的评定结果，那么购买者表现出的态度简直可以用崇拜来形容。数字评级尽管十分可疑，却能让购买者相信，他们从不同种类、不同酿酒商和不同年份的葡萄酒的大海中捞到了那枚金针（或银针，这就要看他们的预算了）。

如果一种葡萄酒或一篇文章的质量确实可以用一个数字来衡量，那么测量理论必须解决两个关键问题：如何根据一系列不同的测量值求出这个最后的数？给定一组有限的测量值时，如何评估这个所得的数就是正确答案的概率？我们现在就来看看这两个问题，无论问题中的测量值是通过主观还是客观方式获得的，对这两个问题的回答，都是测量理论希望达到的目标。

要理解测量，关键在于理解随机误差造成的数据变化的性质。我们可以把几种酒提供给 15 个酒评家，或者在不同时间重复提供给某位酒评家，或者把这两种方式结合使用，我们接着可以把每种酒所得的多个得分求平均值或取均值，这样就可以简单明了地得到酒评家对这些酒的总看法。但重要的不仅仅是均值：某种酒在所有 15 次酒评中都得到 90 分，这传达的是一种信息；而如果它的 15 个分数是 80、81、82、87、89、89、90、90、90、91、91、94、97、99 和 100，这传达的又是另一种信息。这两组数据的均值相同，但数据偏离均值的程度不同。数据点的分布方式非常重要，因此数学家创造了一个数值量度描述数据中的波动。这个数值被称为样本标准差。数学家有时还会使用这个值的平方，即样本方差。

样本标准差描述了一组数据与其均值的接近程度，或者实际上，描述了数据不确定性程度的高低。样本标准差较小时，数据都落在均值附近。例如那组所有酒评分都是 90 分的数据，其样本标准差为 0，而这个样本标准差就告诉我们，所有数据都与均值相同。但当样本标准差较大时，数据就没有密集地分布在均值附近。那个取值在 80 分到 100 分的酒评分数集，其样本标准差为 6。利用这个样本标准差，我们可以通过一个经验性规则，判断有超过半数的评分，它们与均值的差落在 6 分之内。在这种情况下，你真正能说的，是这种葡萄酒的分数可能在 84 分到 96 分之间。

18 世纪和 19 世纪的科学家在试图解读测量数据的真实意义时，也面临着持怀疑态度的酒评家所面临的相同问题。如果现在有一群研究人员对同一个量进行了一系列观测，那么他们得到的测量结果几乎总是不同的。一个天文学家可能碰到了不适合进行观测的气象条件；另一个人的望远镜有可能被微风给吹动了；而第三个人说不定刚与威廉·詹姆斯品完马德拉葡萄酒之后才到家。1838 年，数学家与天文学家贝塞尔就总结出每一次望远镜观测过程中可能会出现的 11 类随机误差。即使是同一名天文学家进行重复测量，诸如视力不佳或温度对测量仪器的影响之类的变数，也会导致测量结果发生改变。因此，天文学家必须知道，在给定了一系列不完全相同的测量结果时，如何才能确定天体的真实位置。尽管酒评家和科学家面对的问题是一样的，我们却不能仅凭这一点就认为解决方法也是相同的。那么我们能不能识别随机误差的一般特征？还是说随机误差的特征取决于环境？

雅各布·伯努利的侄子丹尼尔是最早认识到不同类型的测量方法具有共同特征的人之一。1777 年，他将天文观测中的随机误差，与弓箭手射箭时的偏差进行了类比。他推断，在两种情况下，目标（被测量的真实值或箭靶靶心）应该落在中心附近的某个位置，而观测结果应该围绕着它，而且，离目标较近的观测结果应该比远离目标的更多。他用来描述这个分布的定律并不正确，但重要的是他洞察了如下事实，即描述弓箭手误差的分布，也能用来描述天文观测误差。

测量理论的基本原理，就是误差分布遵循某种普遍规律，这个规律有时被称为误差定律。神奇的是，由它还可以得出如下推论，即当数据满足某些十分常见的条件时，通过单一的数学分析，就能根据测量值确定任意类型的真值。根据这一普遍规律，由天文学家的观测数据确定某天体真实位置的问题，跟仅知道箭支落点的位置确定靶心位置的问题，或是根据一系列评酒分数确定葡萄酒「品质」的问题，就是等价的。数学统计之所以是一个连贯体系的学科，并非在于它仅仅是一堆技巧，而在于，无论我们现在的测量对象是圣诞节凌晨 4 点时木星的位置，还是某条生产线生产的提子面包的重量，重复测量所得的结果中，误差的分布都是一样的。

这并不是说只有随机误差会影响测量结果。如果一群酒评家中的一半只爱红葡萄酒，而另一半对白葡萄酒情有独钟，但是除此之外他们的观点完全一致，那么一种特定葡萄酒所得的分数，其分布将不会遵循误差定律，而是会形成两个高峰，其中一个对应着红葡萄酒爱好者所给的分数，另一个则对应着白葡萄酒爱好者所给的分数。即使在一些定律的适用性不那么明显的场合，例如职业足球比赛中获胜方领先的分数 ，或是 IQ（智商）得分，误差定律也适用。多年以前，我曾经掌握了某软件几千名用户的注册数据。这个软件是一位朋友为 8 岁和 9 岁的孩子设计的，但软件的销售情况不如预期。到底是什么人购买了这个软件呢？我根据注册数据制作了一些表格，发现最大的购买人群是 7 岁的孩子。这个期望目标人群和实际目标人群之间的错位当然令人不快，却也并非完全出乎意料。真正令我震惊的是，当我绘制条形图想看看购买者数量随着年龄逐渐偏离 7 岁这个均值时的趋势时，我发现，画出来的条形图看上去十分眼熟 —— 它就是误差定律中的那条曲线。

能够质疑弓箭手和天文学家、化学家和市场销售经理遵从的是同一条误差定律是一回事，能找出定律的具体形式却是另一回事。在天文数据分析这个需求的驱动下，18 世纪晚期，丹尼尔·伯努利和拉普拉斯这样的科学家，提出了一系列假设。事实证明，正确描述误差定律的数学函数即钟形曲线，其实一直就在他们的眼皮子底下。而在几十年前，在一个不同的场合，它就已经在伦敦被发现了。

揭示钟形曲线重要性的有三个人，但被认为贡献最小的人，恰好就是钟形曲线的发现者。亚伯拉罕·棣莫弗是在 1733 年取得这个突破的，当时他 65 岁左右。但这个突破要为人所知，还得等到 5 年后他的《机会的学说》第二版的出版。之前我们把帕斯卡三角形在第 10 行就拦腰斩断了，如果这个三角形继续向下延伸，直到数百行甚至数千行，这时帕斯卡三角形区域的近似值是多少呢？这就是棣莫弗探索的东西。正是这个探索，让他发现了钟形曲线。雅各布·伯努利在证明自己的大数定律时，也必须研究这些数列的某些性质。这些数字可以非常大，例如，在帕斯卡三角形第 200 行中的一个系数，有 59 个数字！在伯努利那个时代，这样巨大的数字的计算显然异常困难。实际上，直到计算机出现之前，这都是一个无比艰巨的任务。我前面提到过，伯努利在证明他的大数定律的过程中使用了近似，而正是这些近似削弱了他的结果的实用性。但他之所以这样做，是因为他需要处理这些巨大的帕斯卡三角形的系数。不过，棣莫弗利用他发现的曲线，进行了更好的近似计算，从而极大地改进了伯努利的估计。

如果按照我在那个软件用户注册数据上的做法，把帕斯卡三角形中某行的数字，以条形图绘制出来，那么棣莫弗推导出的近似就是显而易见的。例如，帕斯卡三角形第 3 行中的 3 个数字分别是 1、2、1。绘制成条形图的话，第一个条形高 1 个单位；第二个条形的高度是第一个的一倍；而第三个条形的高度又变为 1 个单位。现在来看第 5 行中的 5 个数字：1、4、6、4、1。图中将有 5 个条形，且同样由最矮的条形开始，在中间上升到顶点，然后又对称地降下来。那些很靠下的行中的系数会形成有非常多条形的条形图，但这些条形的高矮变化方式是一样的。帕斯卡三角形的第 10 行、第 100 行和第 1000 行所对应的条形图见图 7-1。

图 7-1

注：以上图中的条形高度代表帕斯卡三角形（见图 4-1）中第 10、第 100 和第 1000 行中各系数的相对幅值。横轴上的数字代表对应系数的序号。按照惯例，这些序号从 0 而不是 1 开始。（中图和下图的横轴被截断了，省略了那些条形高度可以忽略的系数。）

如果用一条曲线将这些条形的顶部连起来，就会出现一个很有特征的形状，一个近似钟形的形状。如果把曲线再弄平滑些，你就可以写出它所对应的数学表达式。这条平滑的钟形曲线不仅仅是对帕斯卡三角系数的可视化描述，它更提供了一种既精确又易于使用的估计方法，让我们可以估计帕斯卡三角中那些较大行数中的系数。这就是棣莫弗的发现。

钟形曲线如今常被称为正态分布，有时也被称为高斯分布（我们将在后面看到这个名称的由来）。正态分布实际上并不是一条固定的曲线，而是一系列曲线，其具体位置与形状由两个参数来确定。第一个参数确定了曲线峰值出现的位置，图 7-1 中分别为 5、50 和 500。第二个参数则确定了曲线的延展程度。这个延展程度的现代名称 —— 标准差 —— 要到 1894 年才会出现，它也是我们早先提到的样本标准差这个概念的理论对应物。大体而言，在曲线最大高度的 60% 处，它是曲线宽度的一半。如今，正态分布的重要性已远远超出它作为帕斯卡三角形中数字的近似值的用途。实际上，它是我们发现的最常见的数据分布方式之一。

2『才知道正态分布并不是一条固定的曲线，而是一系列曲线。做一张反常识卡片。（2021-02-28）』

钟形曲线能描述数据分布这一点表明，在多次重复观测后，大多数结果将落在均值附近。曲线中的波峰就表明了这一点。不仅如此，当曲线对称地朝两边逐渐降低时，它同时也给出那些大于或小于均值的特定观测结果出现的频次逐渐减小的规律。在遵循正态分布的数据中，大约 68%（差不多 2/3）的观测值将落在均值 1 个标准差的范围内，大约 95% 落在 2 个标准差的范围内，而 3 个标准差的范围则囊括了 99.7% 的观测值。

我们用图 7-2 进行说明。我们扔了 10 次硬币，并让 300 名学生猜测每次的结果，图中的正方形标出的就是猜测的结果。水平轴上的数字代表在 10 次扔硬币中猜对的次数，分别是从 0 次到 10 次。垂直方向绘制的则是猜对的学生的数量。曲线的形状像一口钟，中心落在了猜中 5 次的位置上，这个位置的曲线高度对应了 75 名学生。在其左侧，差不多是 3 次到 4 次中间的地方，曲线高度约为 51 名学生，即大致为最高值的 2/3；而在右侧，这个高度所对应的点落在了 6 次与 7 次中间。对于猜测扔硬币的结果而言，具有这种标准差大小的钟形曲线，是典型的随机过程。

图 7-2 测量与误差定律

在同一张图上，我们还用圆圈标出了另一组数据。这是 300 名共同基金经理的业绩数据。此时，水平轴表示的不是 10 次扔硬币中猜对的次数，而是 10 年中某经理的表现高于这个群体平均水平的年份的数量。注意两条曲线之间的相似性！在第 9 章中，我们将回到这个问题上来。

要更直观地认识正态分布与随机误差的关系，一个好例子是民意调查或抽样的过程。读者也许还能记起第 5 章那个巴塞尔市长支持度的调查。有确定的一部分选民支持市长，也有确定的另一部分不支持他。为了简单起见，假设这两种人各占 50%。我们已经知道，被调查人群可能无法正好反映这种对半分的情况。实际上，如果询问 N 名选民，他们之中支持市长的人数为某个给定数量的可能性，就正比于帕斯卡三角第 N 行中对应的系数。因此棣莫弗的成果告诉我们，如果调查者对大量选民进行调查，那么各种调查结果的出现概率，就可以用正态分布来描述。换言之，在民意调查中观察到的 95% 的支持率会落在 50% 这个真实支持率 2 个标准差的范围内。民意调查方用误差幅度来表示这一不确定性。如果调查者对媒体说某次调查的误差幅度是正负 5%，他们的意思是指，如果将同样的调查重复多次，那么在 20 次中有 19 次（95%），所得结果会在正确结果的 5% 的误差范围内（尽管调查者很少明说，但实际上这句话也意味着，每 20 次调查中大概就有 1 次的结果错得离谱）。根据经验，样本容量为 100 时的误差幅度，对大多数调查目的而言都太大了；而样本容量为 1000 时的误差幅度大约是 3%，在大多数情况下就已足够了。

不管是什么类型的调查或民意测验，我们在对调查结果进行评价时，一个关键在于我们必须认识到，如果这个调查或测验重新来过的话，调查结果将会毫无意外地发生变化。举个例子，如果已登记选民对于总统的真实认可率为 40%，那么对选民进行的 6 次独立调查，更可能给出诸如 37%、39%、39%、40%、42% 和 42% 这样的结果，而不会是所有这 6 次调查得到的认可率都是 40%（前面这 6 个数字，实际上就是 2006 年 9 月的头两周，对总统任职情况认可率进行的 6 次独立调查的结果）。这也就是我们应该忽略所有那些没有超出误差幅度的差异的原因，这同样是一条经验性的规则。但是，《纽约时报》虽然没有登出「下午两点的职位与薪水温和增长」这样的头条，但在报道政治方面的民意调查时，类似的头条却随处可见。例如在 2004 年美国共和党全国代表大会之后，CNN（美国有线电视新闻网）的头条是「布什的支持度明显温和反弹」。CNN 的专家们接着解释道：「布什在大会上的支持率看来反弹了约 2 个百分点…… 潜在选民中表示要选他为总统的比例，从大会前的 50% 立刻上升到之后的 52%。」只是到了后面，报道者才指出，调查的误差幅度为正负 3.5 个百分点，实际上这等于宣告了上述新闻根本毫无意义。很明显，「明显」这个词在 CNN 评论节目中的真正含义是「明显没有」。

对于许多调查而言，大于 5% 的误差幅度都属于不可接受的范围。但在日常生活中，我们进行判断所依据的数据量往往远少于必需。人们不可能打 100 年的职业棒球，或投资 100 幢公寓楼，或办 100 家巧克力曲奇饼公司。因此，如果要评价别人在这些方面取得的成就，我们的判断依据就仅仅是少数几个数据点。橄榄球队是否应该大手笔甩出 5000 万美元，引诱那个上赛季刚刚取得了创纪录成绩的家伙呢？那个想要你掏钱投资的股票经纪人，她之前的成功能够复现的可能性有多高？而那个富有的发明者在「海猴子」这个发明上取得的成功，是不是意味着他那些「隐身金鱼」和「即活蛙」的新点子的成功率也挺大呢？（据考，他的这些新点子并不成功。）一次成功或失败，只是我们观察到的一个数据点，或者钟形曲线上的一个采样，它仅仅代表一直存在着的许多可能性中的一个。对于这一单独的观测值，我们无法知道它只是代表着均值，还是某「异常值」；也无法知道这个观测所对应的，到底是我们有把握赌一把的事情，还是某个不大可能再次出现的罕见情况。但是我们至少应该了解，一个采样点就是一个采样点，相较于简单地视其为真实值，我们更应该在产生该采样点的分布标准差或离散程度这个语境中看待它。一瓶葡萄酒的得分是 91 分，但如果没有足够的信息估计这种酒被多次评分或由多人评分时得分的波动，91 这个数字就毫无意义。下面的例子也许能帮助我们理解这一点：几年前，《企鹅版澳大利亚优质葡萄酒指南》（ The Penguin Good Australian Wine Guide ）以及《葡萄酒》（ Wine ）杂志的《澳大利亚葡萄酒年鉴》（ Australian Wine Annual ），都对 1999 年份米其顿黑森林公园雷司令酒进行了点评。「企鹅版指南」按五星制评分给了该酒五星，并称其为「企鹅版」年度最佳葡萄酒；而《葡萄酒》杂志则将其列在所有参评酒的末位，并确信这是十年来最差的酒。正态分布不仅可以帮助我们理解这样的矛盾，而且使无数的统计应用成为现实，而这些统计应用广泛出现在科学和商业领域，比如制药公司评判临床试验结果的显著性，或者制造商评判部件的一个抽样检测是否准确反映了不合格产品的比例，或者市场经销商是否应该接受一项研究调查的结果并据此对其行动做出决策，等等。

认识到正态分布描述了测量误差的分布，则是棣莫弗发现钟形曲线之后数十年的事了。发现这一点的人是德国数学家卡尔·弗里德里希·高斯，人们常常将他的名字与钟形曲线联系在一起。高斯在解决行星运动问题时开始认识到正态分布的这个性质，至少是认识到天文测量中的误差可以通过正态分布来描述。但高斯的「证明」是有问题的，他自己后来也承认了这一点。而且，他也未能发现正态分布的许多更加深刻的推论。因此，他将这条定律毫不起眼地插在了《天体运动论》结尾部分的一个小节中。该定律本来很可能就此被埋没，并最终成为那些数量不断增多的被摈弃的误差定律之一。

将正态分布从这个阴暗的角落中拉出来的人是拉普拉斯。他在 1810 年读到了高斯的著作，而此前不久，他刚刚在法兰西科学院宣读了一份备忘，证明了一条被称为中心极限的定理。该定理指出，大数量独立随机因素总和的值为任意给定值的概率，遵循正态分布。比如，你准备烤 100 条 1 千克重的面包，每条面包你都严格按照配方来做，但由于随机性，你有时可能会多加一点儿或少加一点儿面粉或牛奶，或者在烘烤时多烤掉一点儿或少烤掉一点儿水分。这些各种各样的因素，最终会让你烤出来的每条面包跟 1 千克的期望重量相比，都多几克或少几克。但中心极限定理告诉我们，这些面包的重量将遵循正态分布。读了高斯的著作后，拉普拉斯立刻意识到，他可以用正态分布改进自己的工作，并可以给出一个比高斯更好的论证，来说明正态分布的确就是人们孜孜以求的误差定律。拉普拉斯忙不迭地给备忘加了个小尾巴，而中心极限定理和大数定律现在也成为随机性理论中最负盛名的两个结论。

2『这里有关「中心极限定理」的信息做一张主题卡片，而且这里的信息还不够，需要补充。（2021-02-28）』

要理解中心极限定理是怎样说明正态分布就是正确的误差定律的，让我们再来看看丹尼尔·伯努利的弓箭手例子。我就曾扮演过这个弓箭手的角色。那是在一个晚上，在令人欣欣然的葡萄美酒与有大人们参与的幕间节目之后，小儿子尼古拉递给我一张弓和一支箭，问我敢不敢试试射落他头上放着的苹果。箭头是用柔软的泡沫塑料做成的，但如果先来分析一下我可能出现的失误和概率，应该也不可谓不合理。出于大家显然都能理解的原因，我主要考虑垂直方向的误差。我对这个误差建立了如下的简单模型：每个随机因素，比如瞄准的偏差、气流的影响等等，都会使我射出的箭在垂直方向上偏离目标，或者高些或者低些，两者出现的可能性相等。如果我走运的话，这些影响因素中大概有一半将使箭的落点偏高，而另一半使之偏低，总的来说，结果就是箭能准确命中目标。但如果我不走运（或者更准确点儿说，我儿子不走运）的话，所有这些误差都会使箭朝同一个方向偏离，从而使箭或高或低地远离目标。现在的问题是，当所有引起偏差的因素被综合起来后，误差正好相互抵消，或者正好都朝一个方向累加并形成最大误差，或者最终的总和落在前两者中间的任意一点时，各自的可能性有多大？这些影响因素构成了一个伯努利过程，因此上面的问题就等同于问，在扔多次硬币时得到特定次数的正面朝上的可能性有多大。帕斯卡三角形给出了问题的答案，或者当试验次数很多时，正态分布给出了答案。这也是中心极限定理所说的内容。（至于那次射箭挑战，最后我既没有射中苹果，也没有射中我的儿子，却射翻了一杯上好的红葡萄酒。）

19 世纪 30 年代，大多数科学家已经开始相信，每个测量值都是一个复合的产物，受到众多误差源的影响，并因此遵循误差定律。从此以后，利用误差定律和中心极限定理，我们便获得了对数据及其与物理现实之间的关系的崭新且更为深刻的理解。在紧接而来的下一个世纪中，研究兴趣放在人类社会本身的那些学者也掌握了这个观念，并惊奇地发现，个人的性格与行为的变化，也常常表现出跟测量误差一样的模式。因此，他们试图将误差定律的应用，从物理科学拓展到与人类本身的事务有关的一门新科学之中。