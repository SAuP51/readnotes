# 0401. Sample Surveys in the Real World

In this chapter you will:

Discover that sampling errors occur in even the best samples.

Learn that nonsampling errors may occur.

See how question wording may impact survey responses.

Understand that the results that are reported from samples are typically not from a simple random sample.

Case Study: Obtaining a Sample for a Survey

Consider an opinion poll that reports the results, along with a margin of error, from a recent question asked of 1000 randomly selected American adults. Should we be happy? Perhaps not. For example, how many people did the polling company have to contact to get the 1000 to participate?

Many polls don’t tell the whole truth about their samples. However, the Pew Research Center for the People and the Press did tell the whole truth about its sampling methods to get the 1507 American adults for a particular survey. Here it is.

Landline Cell

Noncontacts 2464 3114

Not eligible 18,427 6084

Other 104 56

Unknown eligibility 3305 361

Refusals, breakoffs, and partials 4719 4836

Complete interview 902 605

Total called 29,921 15,056

Most polls are taken by telephone (both landline and cell), dialing numbers at random (to get both listed and unlisted telephone numbers) to get a random sample of households. It may be reasonable to consider a landline telephone number to represent a household, but a cell number typically represents one person and may represent a person whose residence has a landline telephone. According to Pew,「landline and cell phone numbers were sampled to yield a ratio of approximately two completed landline interviews to each cell phone interview.」In addition to the balance of landline and cell numbers, Pew made sure to contact telephone numbers that would be representative of the United States geographically.

Once the collection of telephone numbers was identified, each sampled landline and cell phone number was called a minimum of seven times (assuming the phone was not answered the first six times). Calls were「staggered over times of day and days of the week (including at least one daytime call) to maximize the chances of making contact with a potential respondent.」The Pew Research Center interviewers had to call a total of 44,977 telephone numbers (29,921 landline and 15,056 cell) to get its sample of 1507 (902 landline and 605 cell) respondents.

The calculation to get the response rate is more complicated than taking the number of completed interviews for landline and cell phone numbers divided by the total number of calls. The combined response rate for the poll netting 1507 respondents was 9% (10% of landline numbers and 7% of cell numbers). A higher-effort survey conducted by the Pew Research Center during the same time period resulted in a 22% combined response rate.

We know from Chapter 3 that a sample of 1507 people will give us an approximate margin of error of percentage points. Although Pew did obtain a sample of reasonable size, can we trust the results of this poll to make conclusions about all American adults? Can the results of the poll be extended to people who only have cell phones?

How Sample Surveys Go Wrong

Random sampling eliminates bias in choosing a sample and allows control of variability. So once we see the magic words「randomly selected」and「margin of error,」do we know we have trustworthy information before us? It certainly beats voluntary response, but not always by as much as we might hope. Sampling in the real world is more complex and less reliable than choosing a simple random sample (SRS) from a list of names in a textbook exercise. Confidence statements do not reflect all the sources of error that are present in practical sampling.

Most sample surveys are afflicted by errors other than random sampling errors. These errors can introduce bias that makes a confidence statement meaningless. Good sampling technique includes the art of reducing all sources of error. Part of this art is the science of Statistics, with its random samples and confidence statements. In practice, however, good statistics isn’t all there is to good sampling. Let’s look at sources of errors in sample surveys and at how samplers combat them.

Sampling Errors

Random sampling error is one kind of sampling error. The margin of error tells us how serious random sampling error is, and we can control it by choosing the size of our random sample. Another source of sampling error is the use of bad sampling methods, such as voluntary response. We can avoid bad methods. Unfortunately, nonsampling errors are not so easy to handle.

Errors in sampling

Sampling errors are errors caused by the act of taking a sample. They cause sample results to be different from the results of a census of the population.

Random sampling error is the deviation between the statistic and the parameter caused by chance in selecting a random sample. The margin of error in a confidence statement includes only random sampling error.

Nonsampling errors are errors not related to the act of selecting a sample from the population. They can be present even in a census.

Undercoverage occurs when some groups in the population have no chance of being included the sample.

Nonsampling Errors

Nonsampling errors are those that can plague even a census. Sampling begins with a list of individuals from which we will draw our sample. This list is called the sampling frame. Ideally, the sampling frame should list every individual in the population of interest. Because a list of the entire population is rarely available, the sampling frame is often not an accurate or complete representation of the population. This leads to errors known as frame errors. A common frame error is undercoverage.

If the sampling frame leaves out certain groups of people, even random samples from that frame will be biased. Using telephone directories as the frame for a telephone survey, for example, would miss everyone with an unlisted landline telephone number, everyone who cannot afford a landline phone, everyone who has a voice over Internet Protocol (VoIP) phone, and everyone who has only a cell phone. Recall from the Case Study that the Pew Research Center tries to balance its calls geographically, but both cell and VoIP numbers may betray the region the person with the number is actually in. For example, consider a household in Michigan that has no landline but has two cell phone numbers associated with its residents, one a 614 (Columbus, Ohio) area code and one a 602 (Phoenix, Arizona) area code.

Example 1

Did we miss anyone?

Most opinion polls can’t afford to even attempt full coverage of the population of all adult residents of the United States. The interviews are done by telephone, thus missing the 2% of households without phones, either landline, VoIP, or cell. Additionally, those without regular access to phones, like prison inmates, or to reliable cell service, like many deployed members of the armed forces, are left out. In addition, many polls in the United States interview only in English and Spanish, which leaves some immigrants with telephone numbers out of their samples. Those left out of these polls include a growing number of people who speak Arabic or Asian languages.

The kinds of undercoverage found in most sample surveys are most likely to leave out people who are young or poor or who move often. Nonetheless, random digit dialing comes close to producing a random sample of households with phones. Sampling errors in careful sample surveys are usually quite small. The real problems start when someone picks up (or doesn’t pick up) the phone.

Frame errors can also arise from erroneous inclusions and multiple inclusions. Erroneous inclusions can occur if the frame includes units that are not in the population of interest so that invalid units have a chance of being in the sample. Multiple inclusions occur if some population members appear multiple times in the sampling frame so that they have a higher chance of being sampled. For example, if the sampling frame is a telephone book, some members of the population may have multiple listings because they have multiple telephone lines.

Nonsampling errors also include processing errors—mistakes in mechanical tasks such as doing arithmetic or entering responses into a computer. Fortunately, the wide availability and adoption of computers has made processing errors less common than in the past.

Example 2

Computer-assisted interviewing

The days of the interviewer with a clipboard are gone. Contemporary interviewers carry a laptop computer for face-to-face interviews or watch a computer screen as they carry out a telephone interview. Computer software manages the interview. The interviewer reads questions from the computer screen and uses the keyboard to enter the responses. The computer skips irrelevant items—once a respondent says that she has no children, further questions about her children never appear. The computer can check that answers to related questions are consistent with each other. It can even present questions in random order to avoid any bias due to always asking questions in the same order.

Computer software also manages the record keeping. It keeps records of who has responded and prepares a file of data from the responses. The tedious process of transferring responses from paper to computer, once a source of processing errors, has disappeared. The computer even schedules the calls in telephone surveys, taking account of the respondent’s time zone and honoring appointments made by people who were willing to respond but did not have time when first called.

Another type of nonsampling error is response error, which occurs when a subject gives an incorrect response. A subject may lie about her age or income or about whether she has used illegal drugs. She may remember incorrectly when asked how many packs of cigarettes she smoked last week. A subject who does not understand a question may guess at an answer rather than appear ignorant. Questions that ask subjects about their behavior during a fixed time period are notoriously prone to response errors due to faulty memory. For example, the National Health Survey asks people how many times they have visited a doctor in the past year. Checking their responses against health records found that they failed to remember 60% of their visits to a doctor. A survey that asks about sensitive issues can also expect response errors, as the next example illustrates.

Example 3

The effect of race

In 1989, New York City elected its first black mayor and the state of Virginia elected its first black governor. In both cases, samples of voters interviewed as they left their polling places predicted larger margins of victory than the official vote counts. The polling organizations were certain that some voters lied when interviewed because they felt uncomfortable admitting that they had not voted for the black candidate. This phenomenon is known as「social desirability bias」and「the Bradley effect,」after Tom Bradley, the former black mayor of Los Angeles who lost the 1982 California gubernatorial election despite leading in final-day preelection polls.

This effect attracted media attention during the 2008 presidential election. A few weeks before the election, polls predicted a victory, possibly a big one, for Barack Obama. Even so, Democrats worried that these polls might be overly optimistic because of the Bradley effect. In this case their fears were unfounded, but some political scientists claimed to detect the Bradley effect in polls predicting outcomes in primary races between Barack Obama and Hillary Clinton (for example, in the New Hampshire primary, polls predicted an 8 percentage point Obama victory but Clinton won by 3 percentage points).

Even the 2016 presidential election included effects due to race, despite both major party candidates being white. According to The Washington Post,「whites were more divided based on their perceptions of discrimination against whites than they were in 2012.」As early as the primary election of 2016, The Washington Post reported「both white racial identity and beliefs that whites are treated unfairly are powerful predictors of support for Donald Trump in the Republican primaries.」Unlike the Bradley effect, this racial divide was less about social desirability and more about white voters fearing for their rights.

Technology and attention to detail can minimize processing errors. Skilled interviewers greatly reduce response errors, especially in face-to-face interviews. There is no simple cure, however, for the most serious kind of nonsampling error, nonresponse.

Key Terms

Nonresponse is the failure to obtain data from an individual selected for a sample. Most nonresponse happens because some subjects can’t be contacted or because some subjects who are contacted refuse to cooperate.

Nonresponse is the most serious problem facing sample surveys. People are increasingly reluctant to answer questions, particularly over the phone. The rise of telemarketing, voicemail, caller ID, and spoofed phone numbers drives down response to telephone surveys. Gated communities and buildings guarded by doormen prevent face-to-face interviews. Nonresponse can bias sample survey results because different groups have different rates of nonresponse. Refusals are higher in large cities and among the elderly, for example. Bias due to nonresponse can easily overwhelm the random sampling error described by a survey’s margin of error.

Example 4

How bad is nonresponse?

The U.S. Census Bureau’s American Community Survey (ACS) is a monthly survey of almost 300,000 housing units and replaced the U.S. Census Bureau’s「long form」that in the past was sent to some households in the decennial national census. Participation in the ACS is mandatory, and the U.S. Census Bureau follows up by telephone and then in person if a household fails to return the mail questionnaire.

The ACS has the lowest nonresponse rate of any poll we know: in 2016, only about 2.1% of the households in the sample refused to respond; the overall nonresponse rate, including「never at home」and other causes, was only 5.3%. This is a stark contrast in nonresponse from 2013 when overall response rate was 10.1%. What happened in October 2013? There was a government shutdown. During that time, there were no follow-ups by mail, phone, or in person, and the overall panel response rate was about 7 percentage points lower than is usual for the ACS. If October 2013 is excluded from the ACS when considering nonresponse, the nonresponse rate for 2013 was only 2.9%, similar to previous years.

Another survey that has a remarkable response rate is the University of Chicago’s General Social Survey (GSS), the nation’s most important social survey. The GSS (Example 7 in Chapter 1) contacts its sample in person, and it is run by a university. Despite these advantages, its recent surveys have a 30% rate of nonresponse.

What about polls done by the media and by market research and opinion-polling firms? We often don’t know their rates of nonresponse because they won’t say. That itself is a bad sign. The Pew Poll we looked at in the Case Study suggests how bad things are. Pew got 1507 responses and 15,293 who were never at home, refused, or would not finish the interview. That’s a nonresponse rate of 15,293 out of 16,800, or 91%. We should keep in mind that Pew researchers are more thorough than many pollsters.

Sample surveyors know some tricks to reduce nonresponse. Carefully trained interviewers can keep people on the line if they answer at all. Calling back over longer time periods helps. So do letters sent in advance. Letters and many callbacks slow down the survey, so opinion polls that want fast answers to satisfy the media don’t use them. Even the most careful surveys find that nonresponse is a problem that no amount of expertise can fully overcome. That makes the reminder in the box to the right even more important.

What the margin of error doesn’t say

The announced margin of error for a sample survey covers only random sampling error. Undercoverage, nonresponse, and other practical difficulties can cause large bias that is not covered by the margin of error.

Careful sample surveys warn us about the other kinds of error. The Pew Research Center, for example, says,「In addition to sampling error, one should bear in mind that question wording and practical difficulties in conducting surveys can introduce error or bias into the findings of opinion polls.」How true it is.

Statistics in Your World

He started it! A study of deaths in bar fights showed that in 90% of the cases, the person who died started the fight. You shouldn’t believe this. If you killed someone in a fight, what would you say when the police asked you who started the fight? After all, dead men tell no tales. Now that’s nonresponse!

Does nonresponse make many sample surveys useless? Maybe not. We began this chapter with an account of a「standard」telephone survey done by the Pew Research Center. The Pew researchers also carried out a「rigorous」survey, with letters sent in advance, unlimited calls over eight weeks, letters by priority mail to people who refused, and so on. All this drove the rate of nonresponse down to 78%, compared with 91% for the standard survey. Pew then compared the answers to the same questions from the two surveys. The two samples were quite similar in age, sex, and race, though the rigorous sample was a bit more prosperous. The two samples also held similar opinions on all issues except one: race. People who at first refused to respond were less sympathetic toward the plights of blacks and other minorities than those who were willing to respond when contacted the first time. Overall, it appears that standard polls give reasonably accurate results. But, as in Example 3, race is again an exception.

Wording of Questions

A final influence on the results of a sample survey is the exact wording of questions. It is surprisingly difficult to word the questions so that they are completely clear. A survey that asks「Do you enjoy watching football?」will generate different answers based on the respondent’s understanding of「football」(American football or soccer).

Example 5

Words make a big difference

In May 2013, the Pew Research Center and the Washington Post/ABC News conducted polls asking whether people felt the U.S. Department of Justice had the right to subpoena Associated Press reporters’ phone records. Each survey phrased the question differently, and each survey found different results.

When the Pew Research Center asked,「Do you approve or disapprove of the Justice Department’s decision to subpoena the phone records of AP journalists as part of an investigation into the disclosure of classified information,」36% of respondents approved. The Washington Post/ABC News survey said,「The AP reported classified information about U.S. anti-terrorism efforts and prosecutors have obtained AP’s phone records through a court order. Do you think this action by federal prosecutors is or is not justified?」Fifty-two percent (52%) of respondents to this survey said that the action of federal prosecutors was justified.

Before asking about approval or disapproval of the Justice Department’s actions, Pew asked how closely respondents had followed the issues that led up to the subpoena, and 64% reported not following the news story too closely. Additionally, the difference in wording—「decision to subpoena」in the Pew Research survey versus「obtained... through a court order」used by the Washington Post/ABC News survey—could have led to legitimizing the actions of the Department of Justice and thus a higher「approval」for the Justice Department’s actions. Or, was the higher justification based on the inclusion of the mention of U.S. anti-terrorism efforts? Or, perhaps the difference is due to Pew using「the Justice Department」and the Washington Post/ABC News using「federal prosecutors.」We cannot begin to determine which part of the wording impacted the responses. As the Pew Research Center said in its article comparing the results of these two surveys,「each polling organization made good-faith efforts to describe the facts of the situation as accurately as possible, but the word choices and context make it impossible to identify one particular phrase or concept that tipped the public’s thinking.」

The wording of questions always influences the answers. If the questions are slanted to favor one response over others, we have another source of nonsampling error. A favorite trick is to ask if the subject favors some policy as a means to a desirable end:「Do you favor banning private ownership of handguns in order to reduce the rate of violent crime?」and「Do you favor imposing the death penalty in order to reduce the rate of violent crime?」are loaded questions that draw positive responses from people who are worried about crime. Here is another example of the influence of question wording.

Example 6

Paying taxes

In April 2018, a Gallup Poll asked two questions about the amount one pays in federal income taxes. Here are the two questions:

Do you consider the amount of federal income tax you have to pay as too high, about right, or too low?

Do you regard the income tax which you will have to pay this year as fair?

The first question had 48% of respondents say「about right」(45% said「too high」) while the second question resulted in 61% of respondents saying that the taxes they paid were「fair.」There appears to be a definite difference in opinions about taxes when people are asked about the amount they have to pay or whether it is fair.

Now it’s your turn

4.1 Should we recycle? Is the following question slanted toward a desired response? If so, how?

In view of escalating environmental degradation and incipient resource depletion, would you favor economic incentives for recycling of resource-intensive consumer goods?

How to Live with Nonsampling Errors

Nonsampling errors, especially nonresponse, are always with us. What should a careful sample survey do about this? First, substitute other households for the nonresponders. Because nonresponse is higher in cities, replacing nonresponders with other households in the same neighborhood may reduce bias. Once the data are in, all professional surveys use statistical methods to weight the responses in an attempt to correct sources of bias. If many urban households did not respond, the survey gives more weight to those that did respond. If too many women are in the sample, the survey gives more weight to the men. Here, for example, is part of a statement in the New York Times describing one of its sample surveys:

The results have been weighted to take account of household size and number of telephone lines into the residence and to adjust for variations in the sample relating to geographic region, sex, race, age and education.

The goal is to get results「as if」the sample matched the population in age, gender, place of residence, and other variables.

The practice of weighting creates job opportunities for statisticians. It also means that the results announced by a sample survey are rarely as simple as they seem to be. In April 2018, Gallup announced that it interviewed 1509 adults and found that 56% of them have a Facebook account. It would seem that because 56% of 1509 is 845, Gallup found that 845 people in its sample had a Facebook account. Not so. Gallup no doubt used some quite fancy statistics to weight the actual responses: 56% is Gallup’s best estimate of what it would have found in the absence of nonresponse. Weighting does help correct bias. It usually also increases variability. The announced margin of error must take this into account, creating more work for statisticians.

Sample Design in the Real World

The basic idea of sampling is straightforward: take an SRS from the population and use a statistic from your sample to estimate a parameter of the population. We now know that the statistic is altered behind the scenes to partly correct for nonresponse. The statisticians also have their hands on our beloved SRS. In the real world, most sample surveys use more complex designs.

Example 7

The Current Population Survey

The population that the Current Population Survey (CPS) is interested in consists of all households in the United States. The scientifically chosen sample of about 60,000 occupied households is chosen in stages. The U.S. Census Bureau divides the nation into 2025 geographic areas called Primary Sampling Units (PSUs). These are generally groups of neighboring counties. At the first stage, 824 PSUs are chosen. This isn’t an SRS. If all PSUs had the same chance to be chosen, the sample might miss Chicago and Los Angeles. So 446 highly populated PSUs are automatically in the sample. The other 1579 are divided into 378 groups, called strata, by combining PSUs that are similar in various ways. One PSU is chosen at random to represent each stratum.

U.S. Census Bureau/Photo by Syda Productions/Shutterstock

Each of the 824 PSUs in the first-stage sample is divided into census blocks (smaller geographic areas). The blocks are also grouped into strata, based on such things as housing types and minority population. The households in each block are arranged in order of their location and divided into groups, called clusters, of about four households each. The final sample consists of samples of clusters (not of individual households) from each stratum of blocks. Interviewers go to all households in the chosen clusters. The samples of clusters within each stratum of blocks are also not SRSs. To be sure that the clusters spread out geographically, the sample starts at a random cluster and then takes, for example, every 10th cluster in the list.

The design of the CPS illustrates several ideas that are common in real-world samples that use face-to-face interviews. Taking the sample in several stages with clusters at the final stage saves travel time for interviewers by grouping the sample households first in PSUs and then in clusters. Note that clustering is not an aspect of all sampling strategies but can be quite helpful in situations like the CPS.

The most important refinement mentioned in Example 7 is stratified sampling, which is described in the box to the right.

Stratified sample

To choose a stratified random sample:

Step 1. Divide the sampling frame into distinct groups of individuals, called strata. Choose the strata according to any special interest you have in certain groups within the population or because the individuals in each stratum resemble each other.

Step 2. Take a separate SRS in each stratum and combine these to make up the complete sample.

We must of course choose the strata using facts about the population that are known before we take the sample. You might group a university’s students into undergraduate and graduate students or into those who live on campus and those who commute. Stratified samples have some advantages over an SRS. First, by taking a separate SRS in each stratum, we can set sample sizes to allow separate conclusions about each stratum. Second, a stratified sample usually has a smaller margin of error than an SRS of the same size. The reason is that the individuals in each stratum are more alike than the population as a whole, so working stratum-by-stratum eliminates some variability in the sample.

It may surprise you that stratified samples can violate one of the most appealing properties of the SRS—stratified samples need not give all individuals in the population the same chance to be chosen. Some strata may be deliberately overrepresented in the sample.

Example 8

Stratifying a sample of students

A large university has 30,000 students, of whom 3000 are graduate students. An SRS of 500 students gives every student the same chance to be in the sample. That chance is

We expect an SRS of 500 to contain only about 50 grad students—because grad students make up 10% of the population, we expect them to make up about 10% of an SRS. A sample of size 50 isn’t large enough to estimate grad student opinion with reasonable accuracy. We might prefer a stratified random sample of 200 grad students and 300 undergraduates.

You know how to select such a stratified sample. Label the graduate students 0001 to 3000 and use Table A to select an SRS of 200. Then label the undergraduates 00001 to 27000 and use Table A a second time to select an SRS of 300 of them. These two SRSs together form the stratified sample.

In the stratified sample, each grad student has chance

to be chosen. Each of the undergraduates has a smaller chance,

Because we have two SRSs, it is easy to estimate opinions in the two groups separately. The quick and approximate method (page 43) tells us that the margin of error for a sample proportion will be about

for grad students and about

for undergraduates.

Because the sample in Example 8 deliberately overrepresents graduate students, the final analysis must adjust for this to get unbiased estimates of overall student opinion. Remember that our quick method works only for an SRS. In fact, a professional analysis would also take account of the fact that the population contains「only」30,000 individuals—more job opportunities for statisticians.

Now it’s your turn

4.2 A stratified sample. The statistics department at Cal Poly, San Luis Obispo, has 18 faculty members and 80 undergraduate majors. Use the Simple Random Sample applet, other software, or Table A, starting at line 111, to choose a stratified sample of 1 faculty member and 1 student to attend a reception being held by the university president.

Example 9

The woes of telephone samples

In principle, it would seem that a telephone survey that dials numbers at random could be based on an SRS. Telephone surveys have little need for clustering. Stratifying can still reduce variability, however, and so telephone surveys often take samples in two stages: a stratified sample of telephone number prefixes (area code plus first three digits) followed by individual numbers (last four digits) dialed at random in each prefix.

The real problem with an SRS of telephone numbers is that too few numbers lead to households. Blame technology. Fax numbers, cell phones, and Voice over Internet Protocols (VoIP) demand new phone numbers. Between 1997 and 2017, the number of households in the United States grew by 25%, but the number of possible phone numbers continues to grow as more individuals have access to cell phones. Some analysts believe that in the near future we may have to increase the number of digits for telephone numbers from 10 (including the area code) to 12. This will further exacerbate this problem. Telephone surveys now use「list-assisted samples」that check electronic telephone directories to eliminate prefixes that have no listed numbers before random sampling begins. Fewer calls are wasted, but anyone living where all numbers are unlisted is missed. Prefixes with no listed numbers are therefore separately sampled (stratification again), perhaps with a smaller sample size than if included in the list-assisted sample, to fill the gap.

The proliferation of cell phones has created additional problems for telephone samples. As of December 2017, about 53.9% of households had cell phones only. Random digit dialing using a machine is not allowed for cell phone numbers. Phone numbers assigned to cell phones are determined by the location of the cell phone company providing the service and need not coincide with the actual residence of the user. This makes it difficult to implement sophisticated methods of sampling, such as stratified sampling by geographic location.

It may be that the woes of telephone sampling prompted the Gallup Organization, in recent years, to drop the phrase「random sampling」from the description of their survey methods at the end of most of their polls. This presumably prevents misinterpreting the results as coming from simple random samples. In their detailed description of survey methods used for the Gallup World Poll and the Gallup Well-Being Index (available online at the Gallup website), the samples are described as involving random sampling.

This might be a good place to read the「What’s the verdict?」story on page 85, and to answer the questions. These questions involve material from this chapter and Chapter 3 to assess some puzzling results from the National Longitudinal Study of Adolescent Health.

The Challenge of Internet Surveys

The Internet is having a profound effect on many things people do, and this includes surveys. Using the Internet to conduct「Web surveys」is becoming increasingly popular. Web surveys have several advantages over more traditional survey methods. It is possible to collect large amounts of survey data at lower costs than traditional methods allow. Anyone can put survey questions on dedicated sites offering free services; thus, large-scale data collection is available to almost every person with access to the Internet. Furthermore, Web surveys allow one to deliver multimedia survey content to respondents, opening up new realms of survey possibilities that would be extremely difficult to implement using traditional methods. Some argue that eventually Web surveys will replace traditional survey methods.

Statistics in Your World

New York, New York New York City, they say, is bigger, richer, faster, and ruder. Maybe there’s something to that. The sample survey firm Zogby International says that as a national average, it takes 5 calls to reach a live person. When trying to reach respondents in New York City, it takes 12 calls. Survey firms assign their best interviewers to make calls to New York City and often pay them bonuses to cope with the stress.

Although Web surveys are easy to do, they are not easy to do well. The reasons include many of the issues we have discussed in this chapter. Three major problems are voluntary response, undercoverage, and nonresponse. Voluntary response appears in several forms. Some Web surveys invite visitors to a particular website to participate in a poll. Misterpoll.com is one such example. Visitors to this site can participate in several ongoing polls, create their own poll, and respond multiple times to the same poll. Other Web surveys solicit participation through announcements in newsgroups, email invitations, and banner ads on high-traffic sites. An example is a series of 10 polls conducted by Georgia Tech University’s Graphic, Visualization, and Usability Center (GVU) in the 1990s.

Although misterpoll.com indicates that the surveys on the site are primarily intended for entertainment, the GVU polls appear to claim some measure of legitimacy. The website www.cc.gatech.edu/gvu/user_surveys/ states that the information from these surveys「is valued as an independent, objective view of developing Web demographics, culture, user attitudes, and usage patterns.」

A third and more sophisticated example of voluntary response occurs when the polling organization creates what it believes to be a representative panel consisting of volunteers and uses panel members as a sampling frame. A random sample is selected from this panel, and those selected are invited to participate in the poll. A very sophisticated version of this approach is used by the Harris Poll Online.

Web surveys, such as the Harris Poll Online, in which a random sample is selected from a well-defined sampling frame, are reasonable when the sampling frame clearly represents some larger population or when interest is only in the members of the sampling frame. An example are Web surveys that use systematic sampling to select every th visitor to a site and the target population is narrowly defined as visitors to the site. Another example are some Web surveys on college campuses. All students may be assigned email addresses and have Internet access. A list of these email addresses serves as the sampling frame, and a random sample is selected from this list. If the population of interest is all students at this particular college, these surveys can potentially yield very good results. Here is an example of this type of Web survey.

Example 10

Doctors and placebos

A placebo is a dummy treatment like a salt pill that has no direct effect on a patient but may bring about a response because patients expect it to. Do academic physicians who maintain private practices sometimes give their patients placebos? A Web survey of doctors in internal medicine departments at Chicago-area medical schools was possible because almost all doctors had listed email addresses.

Send an email to each doctor explaining the purpose of the study, promising anonymity, and giving an individual a Web link for response. Result: 45% of respondents said they sometimes use placebos in their clinical practice.

Several other Web survey methods have been employed to eliminate problems arising from voluntary response. One is to use the Web as one of many alternative ways to participate in the survey. The Bureau of Labor Statistics and the U.S. Census Bureau have used this method. Another method is to select random samples from panels, but instead of relying on volunteers to form the panels, members are recruited using random sampling (for example, random digit dialing). Telephone interviews can be used to collect background information, identify those with Internet access, and recruit eligible persons to the panel. If the target population is current users of the Internet, this method should also potentially yield reliable results. The Pew Research Center has employed this method.

Perhaps the most ambitious approach, and one that attempts to obtain a random sample from a more general population, is the following. Take a probability sample (defined on the next page) from the population of interest. Provide all those selected with the necessary equipment and tools to participate in subsequent Web surveys. This methodology is similar in spirit to that used for the Nielsen TV ratings. It was employed by one company, InterSurvey, several years ago, although InterSurvey is no longer in business.

Several challenges remain for those who employ Web surveys. Even though Internet and email use is growing (according to a February 2018 Pew Research Center report, 89% of American adults aged 18 and older have Internet access), there is still the problem of undercoverage if Web surveys are used to draw conclusions about all American adults aged 18 and older. Weighting responses to correct for possible biases does not solve the problem because studies indicate that Internet users differ in many ways that traditional methods of weighting do not account for.

In addition, even if 100% of Americans had Internet access, there is no list of Internet users that we can use as a sampling frame, nor is there anything comparable to random digit dialing that can be used to draw random samples from the collection of all Internet users.

Finally, Web surveys often have very high rates of nonresponse. Methods that are used in phone and mail surveys to improve response rates can help, but they make Web surveys more expensive and difficult, offsetting some of their advantages.

Statistical controversies

The Harris Online Poll

The Harris Poll Online has created an online research panel of over 6 million volunteers. According to the Harris Poll Online website, the「panel consists of a diverse cross-section of people residing in the United States, as well as in over 200 countries around the world,」and「this multimillion member panel consists of potential respondents who have been recruited through online, telephone, mail, and in-person approaches to increase population coverage and enhance representativeness.」One can join the panel at https://www.harrispollonline.com/#homepage.

When the Harris Poll Online conducts a survey, this panel serves as the sampling frame. A probability sample is selected from it, and statistical methods are used to weight the responses. In particular, the Harris Poll Online uses propensity score weighting, a proprietary Harris Interactive technique, which is also applied (when applicable) to adjust for respondents’ likelihood of being online. They claim that「this procedure provides added assurance of accuracy and representativeness.」

Are you convinced that the Harris Poll Online provides accurate information about well-defined populations such as all American adults? Why or why not?

For more information about the Harris Poll Online and Web surveys in general, see https://theharrispoll.com/ and in a special issue of Public Opinion Quarterly available online at https://academic.oup.com/poq/issue/72/5.

Probability Samples

It’s clear from Examples 7, 8, and 9, and from the challenges of using the Internet to conduct surveys, that designing samples is a business for experts. Even most statisticians don’t qualify. We won’t worry about such details. The big idea is that good sample designs use chance to select individuals from the population. That is, all good samples are probability samples.

Key Terms

A probability sample is a sample chosen by chance. We must know what samples are possible and what chance, or probability, each possible sample has. Some probability samples, such as stratified samples, don’t allow all possible samples from the population and may not give an equal chance to all the samples they do allow. As such, not all probability samples are random samples.

A stratified sample of 300 undergraduate students and 200 graduate students, for example, allows only samples with exactly that makeup. An SRS would allow any 500 students. Both are probability samples. We need only know that estimates from any probability sample share the nice properties of estimates from an SRS. Confidence statements can be made without bias and have smaller margins of error as the size of the sample increases. Nonprobability samples such as voluntary response samples do not share these advantages and cannot give trustworthy information about a population. Now that we know that most nationwide samples are more complicated than an SRS, we will usually go back to acting as if good samples were SRSs. That keeps the big idea and hides the messy details.

Questions to Ask before You Believe a Poll

Opinion polls and other sample surveys can produce accurate and useful information if the pollster uses good statistical techniques and also works hard at preparing a sampling frame, wording questions, and reducing nonresponse. Many surveys, however, especially those designed to influence public opinion rather than just record it, do not produce accurate or useful information. Here are some questions to ask before you pay much attention to poll results.

Who carried out the survey? Even a political party should hire a professional sample survey firm whose reputation demands that they follow good survey practices.

What was the population? That is, whose opinions were being sought?

How was the sample selected? Look for mention of random sampling.

How large was the sample? Even better, find out both the sample size and the margin of error within which the results of 95% of all samples drawn as this one was would fall.

What was the response rate? That is, what percentage of the original subjects actually provided information?

How were the subjects contacted? By telephone? Mail? Face-to-face interview?

When was the survey conducted? Was it just after some event that might have influenced opinion?

What were the exact questions asked?

Academic survey centers and government statistical offices answer these questions when they announce the results of a sample survey. National opinion polls usually don’t announce their response rate (which is often low) but do give us the other information. Editors and newscasters have the bad habit of cutting out these dull facts and reporting only the sample results. Many sample surveys by interest groups and local newspapers and TV stations don’t answer these questions because their polling methods are in fact unreliable. If a politician, an advertiser, or your local TV station announces the results of a poll without complete information, be skeptical.

Chapter 4 Summary and Exercises

Chapter 4: Statistics in Summary

Sampling in the real world is complex. Even professional sample surveys don’t give exactly correct information about the population.

There are many potential sources of error in sampling. The margin of error announced by a sample survey covers only random sampling error, the variation due to chance in choosing a random sample.

Sampling errors come from the act of choosing a sample. Random sampling error or the use of bad sampling methods are common types of sampling error.

The most serious errors in most careful surveys, however, are nonsampling errors. These have nothing to do with choosing a sample—they are present even in a census.

Frame errors can occur because the sampling frame, the list from which the sample is actually chosen, is not an accurate representation of the population. One such error is undercoverage. Undercoverage occurs when some members of the population are left out of the sampling frame. Other frame errors occur if the frame includes units not in the population of interest and if the frame lists units multiple times.

The most challenging problem for sample surveys is nonresponse: subjects can’t be contacted or refuse to answer.

Mistakes in handling the data (processing errors) and incorrect answers by respondents (response errors) are other examples of nonsampling errors.

Finally, the exact wording of questions has a big influence on the answers.

People who design sample surveys use statistical techniques that help correct nonsampling errors, and they also use probability samples more complex than simple random samples, such as stratified samples.

You can assess the quality of a sample survey quite well by just looking at the basics: use of random samples, sample size and margin of error, the rate of nonresponse, and the wording of the questions.

This chapter summary will help you evaluate the Case Study.

Link It

In Chapter 3 we saw that random samples can provide a sound basis for drawing conclusions about a parameter. In this chapter, we learn that even when we take a random sample, our conclusions can be weakened by undercoverage, processing errors, response error, nonresponse, and wording of questions. We must pay careful attention to every aspect of how we collect data to ensure that the conclusions we make are valid. In some cases, more complex probability samples, such as stratified samples, can help correct nonsampling errors. This chapter provides a list of questions you can ask to help you assess the quality of the results of samples collected by someone else.

Case Study Evaluated

Use what you have learned in this chapter to evaluate the Case Study that opened the chapter. Start by reviewing the Chapter Summary. Then communicate clearly enough for any of your classmates to understand what you are saying.

Use what you have learned about what sampling errors are, how nonsampling errors and question wording may impact survey results, and how the results reported from samples are typically not from an SRS to evaluate the Case Study that opened the chapter. In particular, answer the questions given in the section「Questions to Ask before You Believe a Poll」on page 75. Are the results of the Pew poll useless? You may want to refer to the discussion on pages 59–60.

In this chapter you have:

Discovered that sampling errors occur in even the best samples.

Learned that nonsampling errors may occur.

Saw how question wording may impact survey responses.

Understood that the results that are reported from samples are typically not from a simple random sample.

Online Resources

The video technology manuals explain how to select an SRS using JMP, Excel, R, Minitab, CrunchIt!, SPSS, and the TI 83/84.

The Statistical Applet, Simple Random Sample, can be used to select a simple random sample when the number of labels is 144 or less.

Check the Basics

For Exercise 4.1, see page 67; for Exercise 4.2, see page 71.

4.3 What does the margin of error include? When a margin of error is reported for a survey, it includes

random sampling error and other practical difficulties like undercoverage and non-response.

random sampling error, but not other practical difficulties like undercoverage and nonresponse.

practical difficulties like undercoverage and nonresponse, but not random sampling error.

None of the above is correct.

4.4 What kind of sample? Archaeologists plan to examine a sample of 2-meter square plots near an ancient Greek city for artifacts visible in the ground. They choose separate samples of plots from floodplain, coast, foothills, and high hills. What kind of sample is this?

A simple random sample

A voluntary response sample

A stratified sample

A cluster sample

4.5 Sampling issues. A sample of households in a community is selected at random from the telephone directory. In this community, 4% of households have no telephone, 10% have only cell phones, and another 25% have unlisted telephone numbers. The sample will certainly suffer from

nonresponse.

undercoverage.

false responses.

all of the above.

4.6 Question wording. Which of the following represents wording that will most likely not influence the answers?

Do you think that all instances of academic misconduct should be reported to the dean?

Academic misconduct undermines the integrity of the university and education in general. Do you believe that all instances of academic misconduct should be reported to the dean?

Academic misconduct can range from something as minor as using one’s own work in two courses to major issues like cheating on exams and plagiarizing. Do you believe that all instances of academic misconduct should be reported to the dean?

None of the above will influence the answers.

4.7 Sampling considerations. A Statistics class has 10 graduate students and 40 undergraduate students. You want to randomly sample 10% of the students in the class. One graduate student and four undergraduate students are selected at random. Which of the following is not correct?

Because each student has a 10% chance of being selected, this is a simple random sample.

Because each sample includes exactly one graduate student and four undergraduate students, this is not a random sample.

It is possible to get a sample that contains only graduate students.

It is possible to get a sample that contains only undergraduate students.

Chapter 4 Exercises

4.8 What kind of error? Which of the following are sources of sampling error and which are sources of nonsampling error? Explain your answers.

The subject lies about past illegal drug use.

A typing error is made in recording the data.

Data are gathered by asking people to go to a website and answer questions online.

4.9 What kind of error? Each of the following is a source of error in a sample survey. Label each as sampling error or nonsampling error, and explain your answers.

The telephone directory is used as a sampling frame.

The subject cannot be contacted in five calls.

Interviewers choose people on the street to interview.

4.10 Not in the margin of error. According to a December 2017 Gallup Poll, 7% of American adults report soccer as their favorite sport, up from 4% in June 2013 and just 2% in April 1997. This may seem low to you, but the United States is catching up to the rest of the world in its interest in soccer. The survey methods section of the poll states:

For results based on the total sample of national adults, the margin of sampling error is ±4 percentage points at the 95% confidence level. All reported margins of sampling error include computed design effects for weighting.

Give one example of a source of error in the poll result that is not included in this margin of error.

4.11 Not in the margin of error. According to an April 2018 survey, a majority of employed American adults (59%) are confident about their job security, stating it is not at all likely for them to lose their job or be laid off in the next 12 months. The survey methods section of the poll states:

For results based on the total sample of employed adults, the margin of sampling error is ±5 percentage points at the 95% confidence level. All reported margins of sampling error include computed design effects for weighting.

Give one example of a source of error in the poll result that is not included in this margin of error.

4.12 College parents. An online survey of college parents was conducted during February and March 2007. Emails were sent to 41,000 parents who were listed in either the College Parents of America database or the Student Advantage database. Parents were invited to participate in the online survey. Out of those invited, 1727 completed the online survey. The survey protected the anonymity of those participating in the survey but did not allow more than one response from an individual IP address.

One of the survey results was that 33% of mothers communicate at least once a day with their child while at school.

What was the response rate for this survey? (The response rate is the percentage of the planned sample—that is, those invited to participate—who responded.)

Use the quick method (page 43) to estimate the margin of error for a random sample of size 1727.

Do you think that the margin of error is a good measure of the accuracy of the survey’s results? Explain your answer.

4.13 Polling customers. An online store chooses an SRS of 100 customers from its list of all people who have bought something from the store in the last year. It asks those selected how satisfied they are with the store’s website. If it selected two SRSs of 100 customers at the same time, the two samples would give somewhat different results. Is this variation a source of sampling error or of nonsampling error? Would the survey’s announced margin of error take this source of error into account?

4.14 Ring-no-answer. A common form of non-response in telephone surveys is「ring-no-answer.」That is, a call is made to an active number but no one answers. The Italian National Statistical Institute looked at nonresponse to a government survey of households in Italy during the periods January 1 to Easter and July 1 to August 31. All calls were made between 7 and 10 P.M., but 21.4% gave「ring-no-answer」in one period versus 41.5%「ring-no-answer」in the other period. Which period do you think had the higher rate of no answers? Why? Explain why a high rate of nonresponse makes sample results less reliable.

4.15 Race relations. Here are two recent opinion poll questions asked about race relations in the United States.

Would you say relations between whites and blacks are very good, somewhat good, somewhat bad, or very bad?

Do you think race relations in the United States are generally good or generally bad?

In response to the first question, 72% of non-Hispanic whites and 66% of blacks answered that relations between blacks and whites are「very good」or「somewhat good.」Sixty-one (61%) of those answering the second question responded「generally bad.」

The first question came from a poll that was conducted in March 2015. The second question came from a poll that was conducted between April 30 and May 3 of that same year. Between the two polls, a man named Freddie Gray died after being in the custody of the Baltimore police. In what ways do you think this event may have impacted the responses to the two different polls? Do you think the results would be different if the question from the second poll had been worded like the question from the first poll?

4.16 The environment and the economy. Here are two opinion poll questions asked about protecting the environment versus protecting the economy.

Often there are trade-offs or sacrifices people must make in deciding what is important to them. Generally speaking, when a trade-off has to be made, which is more important to you: stimulating the economy or protecting the environment?

Which worries you more: that the U.S. will NOT take the actions necessary to prevent the catastrophic effects of global warming because of fears those actions would harm the economy, or that the U.S. WILL take actions to protect against global warming and those actions will cripple the U.S. economy?

In response to the first question, 61% said stimulating the economy was more important. But only 46% of those asked the second question said they were afraid that the United States will take actions to protect against global warming and that those actions will cripple the U.S. economy. Why do you think the second wording discouraged more people from expressing more concern about the economy than about the environment?

4.17 Amending the Constitution. You are writing an opinion poll question about a proposed amendment to the Constitution. You can ask if people are in favor of「changing the Constitution」or「adding to the Constitution」by approving the amendment.

Why do you think the responses to these two questions will produce different percentages in favor?

One of these choices of wording will produce a much higher percentage in favor. Which one? Why?

4.18 Legal marijuana use. Recently, the issue of the legalization of marijuana has been appearing on more state ballots. In April 2018, a Quinnipiac University poll asked two questions about legal marijuana use. Here are the two questions:

Do you think that the use of marijuana should be made legal in the United States, or not?

Do you support or oppose allowing adults to legally use marijuana for medical purposes if their doctor prescribes it?

One of these questions drew 63% saying that marijuana use is okay; the other drew 93% with the same response. Which wording produced the higher percentage? Why?

4.19 Wording survey questions. Comment on each of the following as a potential sample survey question. Is the question clear? Is it slanted toward a desired response? (Survey questions on issues that one might regard as inflammatory are often prone to slanted wording.)

Which of the following best represents your opinion on gun control? The government should take away our guns.

We have the right to keep and bear arms.

In light of skyrocketing gasoline prices, we should consider opening up a very small amount of Alaskan wilderness for oil exploration as a way of reducing our dependence on foreign oil. Do you agree or disagree?

Do you think that the excessive restrictions placed on U.S. law enforcement agencies hampered their ability to detect the 9/11 terrorist plot before it occurred?

Do you use drugs?

4.20 Bad survey questions. Write your own examples of bad sample survey questions.

Write a biased question designed to get one answer rather than another.

Write a question that is confusing, so that it is hard to answer.

4.21 Appraising a poll. In May 2018, an NBC News/SurveyMonkey poll asked about racism in American society and American politics. The question asked respondents to choose one of the following:「racism remains a major problem in our society, racism exists today but is not a major problem, racism once existed but no longer exists in our society, racism has never been a major problem in our society」The article noted 64% of Americans think racism is still a major problem in society today. News articles tend to be brief in describing sample surveys. NBC’s description of this poll explained that the poll was conducted in May 2018 and included 6,518 American adults from all adults (about 3 million!) who take SurveyMonkey polls each day. Note that this was not a probability sample. Polls of this size have a margin of error of plus or minus 1.5 percentage points.

Pages 75–76 list several「questions to ask」about an opinion poll. What answers does NBC News give to each of these questions?

4.22 Appraising a poll. In May–June 2018, an NBC News/GenForward poll asked the question「Do you personally know someone who has dealt with an opioid addiction?」The article noted 42% of millennials know someone who has dealt with opioid addiction. The description of this poll on the main article page explained that the poll was conducted in May 2018 and included 1,886 American adults between the ages of 18 and 34 who were recruited by NORC at the University of Chicago. Polls of this size and type have a margin of error of plus or minus 3.78 percentage points.

More specific information about the poll methodology was available by clicking a link. That information stated that the 1,886 respondents represented all 50 U.S. states and the District of Columbia. The respondents included 525 African Americans, 256 Asian Americas, 502 Latinx Americans, and 553 white Americans (another 50 young adults had other racial and ethnic backgrounds). Additionally, the methodology stated that the survey was conducted in either English or Spanish and through the web (93%) or telephone (7%). Also of note was that only 26% of surveys were completed.

Pages 75–76 list several「questions to ask」about an opinion poll. What answers does NBC/GenForward give to each of these questions?

4.23 Closed versus open questions. Two basic types of questions are closed questions and open questions. A closed question asks the subject for one or more of a fixed set of responses. An open question allows the subject to answer in his or her own words; the interviewer writes down the responses and classifies them later. An example of an open question is

What do you believe about the afterlife?

An example of a closed question is

What do you believe about the afterlife? Do you believe

there is an afterlife and entrance depends only on your actions?

there is an afterlife and entrance depends only on your beliefs?

there is an afterlife and everyone lives there forever?

there is no afterlife?

I don’t know.

What are the advantages and disadvantages of open and closed questions?

4.24 Telling the truth? Many subjects don’t give honest answers to questions about activities that are illegal or sensitive in some other way. One study divided a large group of white adults into thirds at random. All were asked if they had ever used cocaine. The first group was interviewed by telephone: 21% said「Yes.」In the group visited at home by an interviewer, 25% said「Yes.」The final group was interviewed at home but answered the question on an anonymous form that they sealed in an envelope. Of this group, 28% said they had used cocaine.

Which result do you think is closest to the truth? Why?

Give two other examples of behavior you think would be underreported in a telephone survey.

4.25 Did you vote? When the Current Population Survey asked the adults in its sample of 60,000 households if they voted in the 2016 presidential election, 56% said they had. The margin of error was less than 0.3%. In fact, only 55% of the adult population voted in that election. Why do you think the CPS result missed by 3 times the margin of error?

4.26 A party poll. At a party there are 20 students over age 21 and 40 students under age 21. You choose at random 2 of those over 21 and separately choose at random 4 of those under 21 to interview about attitudes toward alcohol. You have given every student at the party the same chance to be interviewed: what is that chance? Why is your sample not an SRS?

4.27 A stratified sample. A club has 30 student members and 10 faculty members. The students are

Aguirre Cooper Kemp Peralta Stankiewicz

Butterfield Dobbs Kessler Risser Steele

Caporuscio Freeman Koepnick Rodriguez Tong

Carlson Girard Macha Ryndak White

Chilson Gonzales Makis Soria Williams

Clement Grebe Palacios Spiel Zhang

The faculty members are

Atchade Everson Hansen Nair Romero

Craigmile Fink Murphy Nguyen Turkmen

The club can send 3 students and 2 faculty members to a convention. It decides to choose those who will go by random selection.

Use the Simple Random Sample applet, other technology, or Table A to choose a stratified random sample of 3 students and 2 faculty members.

What is the chance that the student named White is chosen? What is the chance that faculty member Romero is chosen?

4.28 A stratified sample. A state university has 4900 in-state students and 2100 out-of-state students. A financial aid officer wants to poll the opinions of a random sample of students. In order to give adequate attention to the opinions of out-of-state students, the financial aid officer decides to choose a stratified random sample of 200 in-state students and 200 out-of-state students. The officer has alphabetized lists of in-state and out-of-state students.

Explain how you would assign labels and use random digits to choose the desired sample. Use the Simple Random Sample applet, other technology, or Table A at line 122 and give the first 5 in-state students and the first 5 out-of-state students in your sample.

What is the chance that any one of the 4900 in-state students will be in your sample? What is the chance that any one of the 2100 out-of-state students will be in your sample?

4.29 Sampling by accountants. Accountants use stratified samples during audits to verify a company’s records of such things as accounts receivable. The stratification is based on the dollar amount of the item and often includes 100% sampling of the largest items. One company reports 5000 accounts receivable. Of these, 100 are in amounts over $50,000; 500 are in amounts between $1000 and $50,000; and the remaining 4400 are in amounts under $1000. Using these groups as strata, you decide to verify all of the largest accounts and to sample 5% of the midsize accounts and 1% of the small accounts. How would you label the two strata from which you will sample? Use the Simple Random Sample applet, other technology, or Table A, starting at line 115, to select only the first 5 accounts from each of these strata.

4.30 A sampling paradox? Example 8 compares two SRSs of a university’s undergraduate and graduate students. The sample of undergraduates contains a smaller fraction of the population, 1 out of 90, versus 1 out of 15 for graduate students. Yet sampling 1 out of 90 undergraduates gives a smaller margin of error than sampling 1 out of 15 graduate students. Explain to someone who knows no Statistics why this happens.

4.31 Appraising a poll. Exercise 4.22 gives part of the description of a sample survey from the NBC News|GenForward. It appears that the sample was taken in several stages. Why can we say this? The first stage no doubt used a stratified sample, though the NBC survey does not say this. Explain why it would be bad practice to use an SRS from all possible telephone numbers and impossible to use an SRS of all possible Internet users rather than a stratified sample of telephone and Internet users.

4.32 Multistage sampling. An article in the journal Science looks at differences in attitudes toward genetically modified foods between Europe and the United States. This calls for sample surveys. The European survey chose a sample of 1000 adults in each of 17 European countries. Here’s part of the description:「The Eurobarometer survey is a multistage, random-probability face-to-face sample survey.」

What does「multistage」mean?

You can see that the first stage was stratified. What were the strata?

What does「random-probability sample」mean?

4.33 Online courses in high schools? What do adults believe about requiring online courses in high schools? Are opinions different in urban, suburban, and rural areas? To find out, researchers wanted to ask adults this question:

It has become common for education courses after high school to be taken online. In your opinion, should public high schools in your community require every student to take at least one course online while in high school?

Because most people live in heavily populated urban and suburban areas, an SRS might contain few rural adults. Moreover, it is too expensive to choose people at random from a large region. We should start by choosing school districts rather than people. Describe a suitable sample design for this study, and explain the reasoning behind your choice of design.

4.34 Systematic random samples. The last stage of the Current Population Survey (Example 7) uses a systematic random sample. The following example will illustrate the idea of a systematic sample. Suppose that we must choose 4 rooms out of the 100 rooms in a dormitory. Because , we can think of the list of 100 rooms as four lists of 25 rooms each. Choose 1 of the first 25 rooms at random, using Table A. The sample will contain this room and the rooms 25, 50, and 75 places down the list from it. If 13 is chosen, for example, then the systematic random sample consists of the rooms numbered 13, 38, 63, and 88. Use Table A to choose a systematic random sample of 5 rooms from a list of 200. Enter the table at line 120.

4.35 Systematic isn’t simple. Exercise 4.34 describes a systematic random sample. Like an SRS, a systematic sample gives all individuals the same chance to be chosen. Explain why this is true, then explain carefully why a systematic sample is nonetheless not an SRS.

4.36 Planning a survey of students. The student government plans to ask a random sample of students their opinions about on-campus parking. The university provides a list of the 20,000 enrolled students to serve as a sampling frame.

How would you choose an SRS of 200 students?

How would you choose a systematic sample of 200 students? (See Exercise 4.34 to learn about systematic samples.)

The list shows whether students live on campus (8000 students) or off campus (12,000 students). How would you choose a stratified sample of 50 on-campus students and 150 off-campus students?

4.37 Sampling students. You want to investigate the attitudes of students at your school toward the school’s policy on extra fees for lab courses. You have a grant that will pay the costs of contacting about 500 students.

Specify the exact population for your study. For example, will you include part-time students?

Describe your sample design. For example, will you use a stratified sample with student majors as strata?

Briefly discuss the practical difficulties that you anticipate. For example, how will you contact the students in your sample?

4.38 Mall interviews. Example 1 in Chapter 2 (page 22) describes mall interviewing. This is an example of a convenience sample. Why do mall interviews not produce probability samples?

4.39 Minorities and police. Here are three questions from an April 2018 Quinnipiac University Poll that deal with similar issues, along with the poll results:

Do you think the police in the United States are generally tougher on whites than on blacks, tougher on blacks than on whites, or do the police treat them both the same? Result: 1% tougher on whites, 48% tougher on blacks, 44% treat them the same.

When faced with a possible criminal situation, do you think police in the United States are more likely to shoot someone who is black, more likely to shoot someone who is white, or do you think police are equally likely to shoot someone of either race? website Result: 40% blacks, 2% whites, 53% either race

Is being the victim of police brutality something you personally worry about, or not? Result who worry about: 9% Republicans, 31% Democrats, 21% Independents; 12% whites, 64% blacks, 37% Hispanics

Using this example, discuss the difficulty of using responses to opinion polls to understand public opinion and come up with an overall conclusion.

Exploring the Web

Access these exercises on the text website: macmillanlearning.com/scc10e.

What’s the Verdict?

The following「What’s the verdict?」story explores unexpected results from a well-known mid-1990s survey and from a follow-up survey, as well as subsequent attempts by researchers to understand these results. The material in this chapter and Chapter 3 will help you appreciate issues discussed in these research papers.

In a well-known National Longitudinal Study of Adolescent Health in the mid-1990s, teenagers responded to survey questions about their health, including their sexual identity and behavior. When the results of this study were analyzed, researchers were surprised that 5 to 7% of teens were reporting that they identified as homosexual or bisexual, which was an increase from the 1% researchers had previously estimated.

Questions

WTV4.1. Researchers recognized that questions about sexuality are sensitive and thus worded their question in the study in terms of both-sex or same-sex romantic attraction. In general, what could be some challenges in asking teenagers about their sexual identity and behavior?

WTV4.2. How would you get a nationally representative sample of U.S. teenagers?

WTV4.3. How would you collect data about sexual behavior and identity to get the most accurate answers?

WTV4.4. Does your answer to the question above change if you want to follow up with these same teenagers as they become adults?

In follow-up surveys, researchers noticed that when these same teenagers were surveyed again as adults, over 70% of those teens (mostly males) who were identified as homosexual or bisexual replied that they were only heterosexual as adults. The researchers wondered why there would be such a large decrease. This would be the opposite of what other sexual development research had shown because people are more (not less) likely to be open about their non heterosexual identity as they get older.

A 2014 research paper (see Notes and Data Sources) discussed this issue. The authors of this paper proposed three explanations for the apparent inconsistency. First, perhaps gay adolescents went into the closet as young adults. Second, perhaps the teenagers were confused about the use of the phrase「romantic attraction」as a substitute for sexual orientation. Third, perhaps some of the teenagers「played a ‘jokester’ role by reporting same-sex attraction when none was present.」The authors of the paper believed that the first explanation was very unlikely but did not dismiss the other two explanations as possible.

WTV4.5. What type of error is it when the survey question is worded in a confusing way?

The researchers did look at some of the other health questions that the same teenagers (and later adults) were asked in the surveys, and they found something else interesting:「Most of the kids who first claimed to have artificial limbs [as teenagers] miraculously regrew arms and legs when researchers came back to interview them [as adults].」

WTV4.6. Based on this additional piece of evidence, what is a possible explanation for the sudden decrease in the number of people who identify as homosexual or bisexual from teenagers to adulthood in this survey?

What’s the verdict? Response errors can undermine the results of otherwise well-designed surveys.

CHAPTER 5 Experiments, Good and Bad

In this chapter you will:

Apply the language of experiments.

Apply the logic of experiments.

