

基础卡片：LLM的关键技术2023-11-29解释：参考：selfstudy => 003Paper => 2023046A-Survey-of-Large-Language-Models原文：Key Techniques for LLMs. It has been a long way that LLMs evolve into the current state: general and capable learners. In the development process, a number of important techniques are proposed, which largely improve the capacity of LLMs. Here, we briefly list several important techniques that (potentially) lead to the success of LLMs, as follows.• Scaling. As discussed in previous parts, there exists an evident scaling effect in Transformer language models: larger model/data sizes and more training compute typically lead to an improved model capacity [30, 34]. As two representative models, GPT-3 and PaLM explored the scaling limits by increasing the model size to 175B and 540B, respectively. Since compute budget is usually limited, scaling laws can be further employed to conduct a more compute-efficient allocation of the compute resources. For example, Chinchilla (with more training tokens) outperforms its counterpart model Gopher (with a larger model size) by increasing the data scale with the same compute budget [34]. In addition, data scaling should be with careful cleaning process, since the quality of pre-training data plays a key role in the model capacity.• Training. Due to the huge model size, it is very challenging to successfully train a capable LLM. Distributed training algorithms are needed to learn the network parameters of LLMs, in which various parallel strategies are often jointly utilized. To support distributed training, several optimization frameworks have been released to facilitate the implementation and deployment of parallel algorithms, such as DeepSpeed [74] and Megatron-LM [75–77]. Also, optimization tricks are also important for training stability and model performance, e.g., restart to overcome training loss spike [56] and mixed precision training [78]. More recently, GPT-4 [46] proposes to develop special infrastructure and optimization methods that reliably predict the performance of large models with much smaller models.• Ability eliciting. After being pre-trained on large-scale corpora, LLMs are endowed with potential abilities as general-purpose task solvers. These abilities might not be explicitly exhibited when LLMs perform some specific tasks. As the technical approach, it is useful to design suitable task instructions or specific in-context learning strategies to elicit such abilities. For instance, chain-of-thought prompting has been shown to be useful to solve complex reasoning tasks by including intermediate reasoning steps. Furthermore, we can perform instruction tuning on LLMs with task descriptions expressed in natural language, for improving the generalizability of LLMs on unseen tasks. These eliciting techniques mainly correspond to the emergent abilities of LLMs, which may not show the same effect on small language models.• Alignment tuning. Since LLMs are trained to capture the data characteristics of pre-training corpora (including both high-quality and low-quality data), they are likely to generate toxic, biased, or even harmful content for humans. It is necessary to align LLMs with human values, e.g., helpful, honest, and harmless. For this purpose, InstructGPT [66] 7designs an effective tuning approach that enables LLMs to follow the expected instructions, which utilizes the technique of reinforcement learning with human feedback [66, 79]. It incorporates human in the training loop with elaborately designed labeling strategies. ChatGPT is indeed developed on a similar technique to InstructGPT, which shows a strong alignment capacity in producing high-quality, harmless responses, e.g., rejecting to answer insulting questions.• Tools manipulation. In essence, LLMs are trained as text generators over massive plain text corpora, thus performing less well on the tasks that are not best expressed in the form of text (e.g., numerical computation). In addition, their capacities are also limited to the pre-training data, e.g., the inability to capture up-to-date information. To tackle these issues, a recently proposed technique is to employ external tools to compensate for the deficiencies of LLMs [80, 81]. For example, LLMs can utilize the calculator for accurate computation [80] and employ search engines to retrieve unknown information [81]. More recently, ChatGPT has enabled the mechanism of using external plugins (existing or newly created apps) 12 , which are by analogy with the “eyes and ears” of LLMs. Such a mechanism can broadly expand the scope of capacities for LLMs.In addition, many other factors (e.g., the upgrade of hardware) also contribute to the success of LLMs. Currently, we limit our discussion to the major technical approaches and key findings for developing LLMs.LLM 的关键技术。LLM 发展到目前这个阶段：成为通用且有能力的学习者，经历了漫长的过程。在发展过程中，提出了许多重要的技术，这些技术大大提高了 LLM 的能力。在这里，我们简要列出了一些可能导致 LLM 成功的重要技术，如下。·缩放。如前文所述，变压器语言模型中存在明显的缩放效应：更大的模型 / 数据规模和更多的训练计算通常会导致模型容量的提升 [30, 34]。作为两个代表性模型，GPT-3 和 PaLM 通过将模型规模分别增加到 175B 和 540B 来探索缩放极限。由于计算预算通常有限，缩放法则可以进一步用于更高效地分配计算资源。例如，Chinchilla（使用更多训练词汇）在相同的计算预算下通过增加数据规模，胜过了其对应模型 Gopher（具有更大的模型规模）[34]。此外，数据缩放应该伴随着仔细的清洗过程，因为预训练数据的质量在模型容量中起着关键作用。·训练。由于模型规模庞大，成功训练一个有能力的 LLM 非常具有挑战性。需要分布式训练算法来学习 LLM 的网络参数，其中通常会联合使用各种并行策略。为了支持分布式训练，发布了几个优化框架来促进并行算法的实现和部署，如 DeepSpeed [74] 和 Megatron-LM [75–77]。此外，优化技巧对于训练的稳定性和模型性能也很重要，例如，重启以克服训练损失突增 [56] 和混合精度训练 [78]。更近期，GPT-4 [46] 提出开发特殊基础设施和优化方法，以便使用更小的模型可靠地预测大型模型的性能。·能力引发。在大规模语料库上预训练后，LLM 被赋予了作为通用任务解决器的潜在能力。当 LLM 执行某些特定任务时，这些能力可能不会明确地表现出来。作为技术方法，设计适当的任务指令或特定的上下文学习策略来引发这些能力是有用的。例如，已经证明链式思维提示对于通过包含中间推理步骤来解决复杂推理任务是有用的。此外，我们可以通过用自然语言表达的任务描述对 LLM 进行指令调整，以提高 LLM 在未见任务上的泛化能力。这些引发技术主要对应于 LLM 的涌现，这些能力在小型语言模型上可能不会显示出相同的效果。·对齐调整。由于 LLM 被训练以捕捉预训练语料库的数据特征（包括高质量和低质量数据），它们可能会生成有害、有偏见甚至对人类有害的内容。因此，有必要将 LLM 与人类价值观（如有益、诚实和无害）对齐。为此，InstructGPT [66] 设计了一种有效的调整方法，使 LLM 能够遵循预期的指令，该方法利用了带有人类反馈的强化学习技术 [66, 79]。它在训练循环中加入人类，并采用精心设计的标注策略。ChatGPT 实际上是基于类似于 InstructGPT 的技术开发的，展现出在产生高质量、无害回应方面的强大对齐能力，例如拒绝回答侮辱性问题。·工具操控。从本质上讲，LLM 作为文本生成器在大量普通文本语料库上进行训练，因此在不以文本形式最佳表达的任务上表现不佳（例如，数值计算）。此外，它们的能力也受限于预训练数据，例如无法捕捉最新信息。为解决这些问题，最近提出的技术是使用外部工具来弥补 LLM 的不足 [80, 81]。例如，LLM 可以使用计算器进行精确计算 [80]，并利用搜索引擎检索未知信息 [81]。最近，ChatGPT 启用了使用外部插件（现有或新创建的应用）的机制 [12]，这类似于 LLM 的「眼睛和耳朵」。这样的机制可以广泛扩展 LLM 的能力范围。此外，许多其他因素（例如硬件的升级）也促成了 LLM 的成功。目前，我们将讨论限制在开发 LLM 的主要技术方法和关键发现上。