

基础卡片：大语言模型 XY 两个维度上的上下文2024-05-01原文：。评论：。参考：ITstudy => 001 大语言模型 => 13Article-LLM => 20240501RAG在长上下文大语言模型中的应用探讨原文：我们都知道，大语言模型 (LLM) 的上下文窗口正逐渐扩大。在 x 轴上，展示的是预训练时使用的 Token 数量，这一数字也在持续增长。专有模型的规模已达到约 2 亿亿个 Token。而小型模型，如 5.2，使用的 Token 数远少于此。更引人注目的是，在 y 轴上，可以看到一年前的顶尖模型处理能力大约在 4000 到 8000 Token，相当于几十页纸的内容。Claude 2 推出了一个可处理 20 万 Token 的模型，而 GPT-4 则有 12.8 万 Token，相当于几百页纸的内容。而现在，Claude 3 和 Gemini 推出的模型可以处理高达 100 万 Token，这相当于几百到几千页的内容。因此，随着这种技术的发展，人们开始质疑 RAG 的必要性 —— 如果一个 LLM 能够处理数千页的内容，为何还需要一个检索系统？这个问题在 Twitter 上激发了热烈的讨论。RAG，即基于检索的推理，涉及使用索引过的文档进行推理和检索。这些文档通过语义相似度搜索或关键词搜索等机制被检索出来，然后由 LLM 进行分析，以便在回答问题时能够引用这些文档。整个过程不仅涉及多个文档，还包括了复杂的推理活动。