

术语卡片：变换器（transformer）2023-11-24定义：解释：例子：参考：ITStduy => 001大语言模型 => Article => 20231122What-Is-ChatGPT-Doing-and-Why-Does-It-Work原文：OK, so we're finally ready to discuss what's inside ChatGPT. And, yes, ultimately, it's a giant neural net—currently a version of the so-called GPT-3 network with 175 billion weights. In many ways this is a neural net very much like the other ones we've discussed. But it's a neural net that's particularly set up for dealing with language. And its most notable feature is a piece of neural net architecture called a "transformer".In the first neural nets we discussed above, every neuron at any given layer was basically connected (at least with some weight) to every neuron on the layer before. But this kind of fully connected network is (presumably) overkill if one's working with data that has particular, known structure. And thus, for example, in the early stages of dealing with images, it's typical to use so-called convolutional neural nets ("convnets") in which neurons are effectively laid out on a grid analogous to the pixels in the image—and connected only to neurons nearby on the grid.The idea of transformers is to do something at least somewhat similar for sequences of tokens that make up a piece of text. But instead of just defining a fixed region in the sequence over which there can be connections, transformers instead introduce the notion of "attention"—and the idea of "paying attention" more to some parts of the sequence than others. Maybe one day it'll make sense to just start a generic neural net and do all customization through training. But at least as of now it seems to be critical in practice to "modularize" things—as transformers do, and probably as our brains also do.好的，我们终于准备好讨论 ChatGPT 内部的构造了。是的，归根结底，它是一个巨大的神经网络 —— 目前是所谓的 GPT-3 网络的一个版本，拥有 1750 亿个权重。在许多方面，这个神经网络非常类似于我们之前讨论过的其他网络。但它是一个专门为处理语言而设置的神经网络。它最显著的特征是一种被称为「变换器」（transformer）的神经网络架构。在我们上面讨论的第一个神经网络中，任何给定层上的每个神经元基本上都与前一层上的每个神经元相连（至少有一些权重）。但如果处理的数据具有特定的、已知的结构，这种完全连接的网络（可能）就过于复杂了。因此，例如，在处理图像的早期阶段，通常使用所谓的卷积神经网络（卷积网），在这种网络中，神经元有效地按照类似于图像中像素的网格布局，并且只与网格上附近的神经元相连。变换器的想法是为构成一段文本的词元序列做类似的事情。但变换器不是定义序列中可以建立连接的固定区域，而是引入了「注意力」的概念 —— 以及「更多关注」序列中某些部分而不是其他部分的想法。也许有一天只启动一个通用神经网络并通过训练进行所有定制是有意义的。但至少目前看来，在实践中「模块化」事物 —— 就像变换器所做的那样，也可能像我们的大脑所做的那样 —— 似乎是至关重要的。