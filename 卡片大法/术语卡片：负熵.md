

术语卡片：负熵2024-05-19解释：参考：selfstudy => 003Paper => 2024009What-Is-Information原文：Schrödinger [13] in his famous and highly influential book What is Life? first explicitly introduced the notion of negative entropy:Every process, event, happening—call it what you will; in a word, everything that is going on in Nature means an increase of the entropy of the part of the world where it is going on. Thus a living organism continually increases its entropy—or, as you may say, produces positive entropy—and thus tends to approach the dangerous state of maximum entropy, which is death. It can only keep aloof from it, i.e., alive, by continually drawing from its environment negative entropy—which is something very positive as we shall immediately see. What an organism feeds upon is negative entropy. Or, to put it less paradoxically, the essential thing in metabolism is that the organism succeeds in freeing itself from all the entropy it cannot help producing while alive (Chapter 6).Both Wiener [3] and Brillouin [14] both adopted Shannon's definition of information and its relation to entropy with the one exception of its sign, likely influenced by the arguments of Szilard and Schrödinger [13].Wiener [3] wrote,Messages are themselves a form of pattern and organization. Indeed, it is possible to treat sets of messages as having entropy like sets of states in the external world. Just as entropy is a measure of disorganization, the information carried by a set of messages is a measure of organization. In fact, it is possible to interpret the information carried by a message as essentially the negative of its entropy, and the negative logarithm of its probability. That is, the more probable the message, the less information it gives (p. 39)…. This amount of information is a quantity which differs from entropy merely by its algebraic sign and a possible numerical factor.Brillouin [14] also argued that a living system exports entropy in order to maintain its own entropy at a low level. Brillouin used the term negentropy to describe information rather than negative entropy.The reason that Wiener and Brillouin consider entropy and information as opposites or regard information as negative entropy follows from the tendency in nature for systems to move into states of greater disorder, i.e., states of increased entropy and hence states for, which we have less information. Consider a system, which is in a state for which there is a certain finite number of possible configurations or microstates all of which are equivalent to the same macro state. The tendency of nature according to the second law of thermodynamics is for the number of microstates that are equivalent to the macrostate of the system to increase. Because there are more possible microstates as time increases and we do not know which particular microstate the system is in, we know less about the system as the number of possible microstates increases. It therefore follows that as the entropy increases the amount of information we have about the system decreases and hence entropy is negative information or vice�versa information is the negative of entropy. In other words the second law of thermodynamics tell us that when system A evolves into system B that system B will have more possible redundant or equivalent micro states than system A and hence we know less about system B than system A since the uncertainty as to which state the system is in has increased.Wiener and Brillouin relate information to entropy with a negative sign whereas Shannon uses a positive sign. Hayles [6] notes that although this difference is arbitrary it had a significant impact. Observing that Shannon used the positive sign she also noted that "identifying entropy with information can be seen as a crucial crossing point, for this allowed entropy to be reconceptualized as the thermodynamic motor driving systems to self-organization rather than as the heat engines driving the world to universal heat death." For Wiener, on the other hand she wrote, "life is an island of negentropy amid a sea of disorder [6]."Despite the difference in the sign of information entropy assigned by Shannon and Wiener, Shannon was heavily influenced by Wiener's work as indicated by the way Shannon [2] credits Wiener for his contribution to his thinking in his acknowledgement: "Credit should also be given to Professor N. Wiener, whose elegant solution of the problems of filtering and prediction of stationary ensembles has considerably influenced the writer's thinking in this field." Shannon also acknowledges his debt to Wiener in footnote 4 of Part III:Communication theory is heavily indebted to Wiener for much of its basic philosophy and theory. His classic NDRC report, The Interpolation, Extrapolation and Smoothing of Stationary Time Series, contains the first clear-cut formulation of communication theory as a statistical problem, the study of operations on time series. This work, although chiefly concerned with the linear prediction and filtering problem, is an important collateral reference in connection with the present paper. We may also refer here to Wiener's Cybernetics [3], dealing with the general problems of communication and control.Schrödinger [13] 在他著名的《生命是什么？》一书中首次明确提出了负熵的概念：每个过程、事件，无论你如何称呼它，总之，自然界中发生的一切都会导致该部分世界的熵增加。因此，活的有机体会不断增加其熵，或者说产生正熵，从而趋向于最大熵的危险状态，即死亡。只有通过不断从环境中获取负熵，它才能避免这种情况，即保持活力。实际上，有机体所需的是负熵。换句话说，新陈代谢的本质在于有机体成功地摆脱了它在活着时不可避免产生的所有熵（第六章）。Wiener [3] 和 Brillouin [14] 都采用了 Shannon 对信息及其与熵关系的定义，唯一的例外是其符号，这可能受到 Szilard 和 Schrödinger [13] 论点的影响。Wiener [3] 写道：信息本身是一种模式和组织形式。实际上，可以将一组信息看作是具有熵的，就像外部世界的状态集一样。正如熵是无序的度量，信息量是有序的度量。事实上，可以将信息量解释为熵的负值，以及其概率的负对数。也就是说，消息越可能，提供的信息量就越少（第 39 页）…… 这种信息量在代数符号和可能的数值因子上仅与熵不同。Brillouin [14] 也认为，生命系统通过输出熵来保持其自身的低熵水平。Brillouin 使用「负熵」一词来描述信息，而不是负的熵。Wiener 和 Brillouin 认为熵和信息是相反的，或者说信息是负熵。这是因为自然界中的系统倾向于进入更无序的状态，即熵增加的状态，这也意味着我们掌握的信息减少。设想一个系统处于某个状态，该状态有一定数量的可能配置或微观状态，这些微观状态都对应同一个宏观状态。根据热力学第二定律，自然界倾向于增加系统宏观状态对应的微观状态数量。随着时间推移，可能的微观状态越来越多，而我们无法确定系统处于哪个特定的微观状态，因此我们对系统的了解会减少。因此，熵增加意味着我们掌握的信息减少，熵是负信息，反之亦然，信息是负熵。换句话说，热力学第二定律告诉我们，当系统 A 演变为系统 B 时，系统 B 将有更多可能的冗余或等效微观状态，因此我们对系统 B 的了解比系统 A 少，因为系统状态的不确定性增加了。Wiener 和 Brillouin 在将信息与熵联系起来时使用了负号，而 Shannon 则使用了正号。Hayles [6] 指出，尽管这种差异是任意的，但它产生了重大影响。她注意到 Shannon 使用正号，并评论道：「将熵与信息等同是一个重要的转折点，这使得熵被重新定义为驱动系统自组织的热力学引擎，而不是导致世界走向普遍热寂的热机。」对于 Wiener，她写道，「生命是无序海洋中的一个负熵岛屿 [6]。」尽管 Shannon 和 Wiener 在信息熵符号的使用上存在差异，但 Shannon 受到了 Wiener 工作的深刻影响。Shannon [2] 在他的致谢中提到，「也应感谢 N. Wiener 教授，他对静态集群的过滤和预测问题的优雅解决方案极大地影响了我的思考。」Shannon 还在论文第三部分的脚注 4 中提到：通信理论在很大程度上受到了Wiener的基本哲学和理论的影响。他的经典NDRC报告《静态时间序列的插值、外推和平滑》首次将通信理论明确为一个统计问题，即对时间序列的操作研究。尽管这项工作主要关注线性预测和过滤问题，但它是与本文相关的重要参考资料。我们还可以提到Wiener的《控制论》[3]，这本书讨论了通信和控制的一般问题。