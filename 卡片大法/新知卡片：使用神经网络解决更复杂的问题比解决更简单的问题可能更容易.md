

新知卡片：使用神经网络解决更复杂的问题比解决更简单的问题可能更容易2023-11-23已知：新知：解释：例子：参考：ITStduy => 001大语言模型 => Article => 20231122What-Is-ChatGPT-Doing-and-Why-Does-It-Work原文：The picture above shows the kind of minimization we might need to do in the unrealistically simple case of just 2 weights. But it turns out that even with many more weights (ChatGPT uses 175 billion) it's still possible to do the minimization, at least to some level of approximation. And in fact the big breakthrough in "deep learning" that occurred around 2011 was associated with the discovery that in some sense it can be easier to do (at least approximate) minimization when there are lots of weights involved than when there are fairly few.In other words—somewhat counterintuitively—it can be easier to solve more complicated problems with neural nets than simpler ones. And the rough reason for this seems to be that when one has a lot of "weight variables" one has a high-dimensional space with "lots of different directions" that can lead one to the minimum—whereas with fewer variables it's easier to end up getting stuck in a local minimum ("mountain lake") from which there's no "direction to get out".It's worth pointing out that in typical cases there are many different collections of weights that will all give neural nets that have pretty much the same performance. And usually in practical neural net training there are lots of random choices made—that lead to "different-but-equivalent solutions", like these:But each such "different solution" will have at least slightly different behavior. And if we ask, say, for an "extrapolation" outside the region where we gave training examples, we can get dramatically different results:But which of these is "right"? There's really no way to say. They're all "consistent with the observed data". But they all correspond to different "innate" ways to "think about" what to do "outside the box". And some may seem "more reasonable" to us humans than others.上面的图片展示了我们可能需要在只有 2 个权重的不切实际简单情况下进行的最小化。但事实证明，即使有更多的权重（ChatGPT 使用 1750 亿个），仍然可以进行最小化，至少在某种程度的近似中。实际上，「深度学习」在 2011 年左右发生的重大突破与发现在某种意义上，当涉及许多权重时，进行（至少是近似的）最小化可能比只涉及相对较少的权重时更容易。换句话说，有点违反直觉地，使用神经网络解决更复杂的问题比解决更简单的问题可能更容易。大致原因似乎是当一个人有很多的「权重变量」时，一个人拥有一个高维空间，其中有「许多不同的方向」可以引导一个人到达最小值 —— 而较少的变量则更容易最终被困在局部最小值（山间湖泊）中，从那里没有「走出去的方向」。值得指出的是，在典型情况下，有许多不同的权重集合都会给神经网络带来几乎相同的表现。而且在实际神经网络训练中通常会做出许多随机选择 —— 这导致了「不同但等效的解决方案」，如下所示：但每种「不同的解决方案」至少会有稍微不同的行为。如果我们要求在我们提供训练示例的区域之外的「外推」，我们可能会得到截然不同的结果：但哪一个是「正确的」？真的没有办法说。它们都是「与观察到的数据一致的」。但它们都对应于不同的「天生的」方式来「思考」如何「走出框架」。有些可能对我们人类来说看起来「更合理」一些。