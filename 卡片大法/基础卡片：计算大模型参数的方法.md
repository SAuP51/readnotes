

基础卡片：计算大模型参数的方法2023-12-14解释：参考：ITStduy => 001大语言模型 => 12宝玉的Twitter => 202312宝玉的Twitter原文：Barret李靖 @Barret_China 2023-12-06经常看到社区推出各种 LLM，有 7B、13B、60B 等等，这些数值代表着模型的参数量，大家都知道数值越大，意味着模型的规模越大，那这些参数量到底是如何被计算出来的呢？ChatGPT 中的 GPT 全称是 Generative Pre-trained Transformer，它是基于 Transformer 架构建设的，市面上的大模型基本也采用此架构，它们都是四层设计，分别是：1、Embedding 层：用于将输入的标记（token）映射到高维的词嵌入空间，参数的数量取决于词汇表的大小和词嵌入的维度。2、Transformer 层：每个 Transformer 层包括多个子层，例如自注意力层（self-attention layer）和前馈神经网络层（feedforward neural network layer）；参数的数量取决于每个子层的权重和偏置参数数量，以及模型中的 Transformer 层数。3、Layer Normalization：归一化层通常用于规范模型中间层的输出，这些层的参数数量取决于归一化层的输入维度。4、Output 层：大多数语言模型在最后添加一个用于分类的线性层，用于将模型的输出映射到分类标签。大模型的参数量就是上面四层的参数总量之和。为了一探究竟，我在 Hugging Face 上找了个稍小的模型，dolly-v2-3b，它是 Databricks 创建的一个 28 亿参数 LLM ，源自 EleutherAI 的 Pythia-2.8b，使用 model.named_parameters() 可以打印模型中的所有参数：可以清晰看到每一层的参数量，将其累加：128716800 + (2560 * 4 + 19660800 + 7680 + 6553600 + 2560 + 26214400 * 2 + 10240 + 2560) * 32 + 2560 * 2 + 128716800 = 2775086080结果是 27.7 亿参数，四舍五入就是 28 亿。这种计算方式非常精确，也有更快捷的方式估算出大模型的参数总量，看到一篇文章，https://zhuanlan.zhihu.com/p/624740065，提供了一个估算的方法：分析 transformer 模型的参数量、计算量、中间激活、KV cache - 知乎由于 Transformer 这一层的模式比较固定，而且参数量最大，因此仅计算这一层即可，文章推导出可以使用 12 * L * H^2 作为公式来进行估算，其中 L 为模型层数，H 为隐藏层维度。dolly-v2-3b 的层数是 32，隐藏层维度是 7680，按照公式计算出来的结果是 22 亿，相差不大。