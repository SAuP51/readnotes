

新知卡片：大模型一种是在压缩世界一种是在压缩人类的行为2024-03-18解释：。参考：001大语言模型 => 13Article-LLM => 20240318月之暗面杨植麟复盘大模型创业这一年原文：腾讯新闻《潜望》：你认可它那句话吗？——「扩展视频生成模型是构建物理世界通用模拟器的一条有前途的途径。」杨植麟：我非常认同，这两个东西优化的是同一个目标函数，没有太大疑问。腾讯新闻《潜望》：你怎么看杨立昆又跳出来反对生成式 AI？他的观点是：「通过生成像素对世界进行建模是一种浪费，并且注定会失败。生成恰好适用文本，因为文本是离散的具有有限数量的符号。这种情况下，处理预测中的不确定性很容易，处理高纬连续感官输入中的预测不确定性是非常棘手的。」杨植麟：我现在觉得，你通过对视频的边际概率去建模，本质是在做无损压缩，跟语言模型 next token predictions 没有本质区别。只要你压缩得足够好，就可以把这个世界可以被解释的东西去进行解释。但同时也有重要的还没做的事：它怎么跟已有的已经被压缩的能力结合起来？可以理解成有两种不同压缩。一种是压缩原始世界，这是视频模型在做的。另一种是压缩人类产生的行为，因为人类产生的行为经过了人的大脑，这是世界上唯一能产生智能的东西。你可以认为视频模型在做第一种，文本模型在做第二种，当然视频模型也一定程度包含了第二种，一些人创造出来的视频包含了创作者的智能。它最终可能会是 mix，需要通过这两种方式从不同角度学习，但最终对智能的增长都有帮助。所以，生成可能不是目的，它只是压缩这个函数。如果你压缩足够好，最后生成的效果就会很好。反过来，如果你这个模型本身没办法生成，是不是也存在可能把它压缩得非常好？这点存疑。有可能生成非常好，是压缩非常好的一个必要条件。