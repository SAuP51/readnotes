

基础卡片：缩放法则和涌现提供了理解大模型相较于小模型优势的两种视角2023-11-29解释：参考：selfstudy => 003Paper => 2023046A-Survey-of-Large-Language-Models原文：How Emergent Abilities Relate to Scaling Laws. In existing literature [30, 31, 34], scaling laws and emergent abilities provide two perspectives to understand the advantage of large models over small models. In general, scaling law (often measured by language modeling loss) describes predictable performance relation with the potential effect of diminishing returns, while emergent abilities (often measured by task performance) are unpredictable but very profitable once such abilities actually emerge. Since the two perspectives reflect different performance trends (continuous improvement v.s. sharp performance leap), they might lead to misaligned findings or observations. There are also extensive debates on the rationality of emergent abilities. A popular speculation is that emergent abilities might be partially attributed to the evaluation setting for special tasks (e.g., the discontinuous evaluation metrics) [70, 71]: when evaluation metrics are altered accordingly, the sharpness of the emergent ability curve would disappear. However, the performance of LLMs on most tasks are perceived by users naturally in a discontinuous way. For instance, end users prefer a reliable code generated by LLMs that can successfully pass the test case, but are less interested in selecting a better code with fewer errors between two failed ones. More recently, a study [72] proposes a new evaluation setting that can enlarge the resolution of task metrics, making task performance more predictable. Despite these efforts, more fundamental research (e.g., grokking 10 ) about the working mechanism of LLMs is still in need to understand the emergence of certain abilities. The subtle relation between scaling law and emergent abilities can be explained by analogy with the ability acquisition of human 11 . Take the speaking ability as an example. For children, language development (especially infants) can be also considered as a multi-level process where “emergent abilities” occur. Specially, the language ability would relatively stable within a time interval, but qualitative change only occurs when evolving into another ability level (e.g., from speaking simple words to speaking simple sentences). Such a learning process is essentially not smooth and stable (i.e., language ability does not develop at a constant rate over time), though a child actually grows every day. It is interesting that young parents would be often surprised by unexpected progress of the speaking ability exhibited by their babies.如何理解涌现与缩放法则的关系。在现有文献中 [30, 31, 34]，缩放法则和涌现提供了理解大模型相较于小模型优势的两种视角。一般来说，缩放法则（通常通过语言建模损失衡量）描述了与潜在收益递减效应相关的可预测性能关系，而涌现（通常通过任务性能衡量）虽然不可预测，但一旦这些能力真正出现，它们会带来很大的利益。由于这两种视角反映了不同的性能趋势（连续改进与急剧性能飞跃），它们可能导致不一致的发现或观察。关于涌现的合理性，也存在广泛的争论。一个流行的猜测是，涌现可能部分归因于特殊任务的评估设置（例如，不连续的评估指标）[70, 71]：当相应地改变评估指标时，涌现曲线的急剧性可能会消失。然而，用户自然地以不连续的方式感知大多数任务上 LLM 的性能。例如，最终用户更喜欢由 LLM 生成的能成功通过测试用例的可靠代码，而不太关心在两个失败的代码中选择错误较少的那个。最近的一项研究[72]提出了一种新的评估设置，可以提高任务指标的分辨率，使任务性能更加可预测。尽管做出了这些努力，但为了理解某些能力的出现，仍需要更多基础性研究（例如，深刻理解 10），以了解 LLM 的工作机制。缩放法则与涌现之间微妙的关系可以通过类比人类 11 的能力获取来解释。以说话能力为例，对儿童（尤其是婴儿）来说，语言发展也可以被视为一个多层次的过程，其中会出现「涌现」。特别是，语言能力在一段时间内相对稳定，但只有在进化到另一个能力水平（例如，从说简单单词到说简单句子）时才会发生质的变化。这样的学习过程本质上是不平稳的（即语言能力不会随时间以恒定速率发展），尽管儿童实际上每天都在成长。有趣的是，年轻父母经常会对他们的孩子所展示的说话能力的意外进步感到惊讶。