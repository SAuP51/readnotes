

基础卡片：目前的RAG 不适应于长上下文窗口的大语言模型2024-05-01原文：。评论：。参考：ITstudy => 001 大语言模型 => 13Article-LLM => 20240501RAG在长上下文大语言模型中的应用探讨原文：这不只是个不怎么样的笑话，音乐家 Frank Zappa 曾说：爵士乐没死，只是有点儿异味。我认为 rag 也是这样。Rag 没有消失，只是在转变。这是一个关键的观点。接下来说说现在的 rag，它主要专注于精确地检索相关的文档片段。这通常涉及以特定方式将文档分成块，使用的分块方法很有个性。比如，块的大小常常似乎是随机决定的。这些块被编码后存入一个索引中，把一个问题也编码后，通过 K 近邻 (KNN) 相似度搜索来找出相关的块。通常需要设置一个 K 值，即检索的块的数量。在找到的块上可能还会做些过滤或后处理，然后在这些块的基础上构建答案。这个过程非常注重精确地找到最合适的块。在长上下文模型越来越普遍的今天，我们不禁要问，这真的是最合理的方法吗？在这里你可以看到，有人可能会太过追求精确，这不仅会增加复杂性，还会因为块大小、K 值等奇特的参数而变得过于敏感，而且可能因为只选择极其精确的块而导致召回率低。这种方法很依赖特定的编码模型。因此，随着长上下文模型的进一步发展，我们确实需要重新思考这种极端精确的块检索方式是否还适合现在。另一方面，简单地将所有文档堆积到上下文中似乎也不是最好的选择，这会导致更高的延迟和更多的令牌消耗。值得一提的是，现在每生成一次 100,000 令牌的 GPT-4 要花费 1 美元。我在 LangChain 的账户上为这种多角度分析花了一大笔钱，我不太想告诉 Harrison 花了多少。显然，这种方式有其不便之处 —— 你无法对检索进行审计，如果不同用户需要对检索到的不同文档或块有不同的访问权限，还可能涉及安全性和认证问题。