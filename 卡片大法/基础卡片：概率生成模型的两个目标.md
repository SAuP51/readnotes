

基础卡片：概率生成模型的两个目标2023-11-18解释：参考：参考：王冠.(2023).2023012深度生成模型.中国工信出版社出版社 => 推荐序原文：P6无监督学习有很多种方法。本书主要介绍概率生成模型。这个子领域的一个目标是给输入数据估计一个概率模型。一旦有了这个模型，我们就可以从中采样出新的样本（比如那些并不存在的人脸图像）。另一个目标是通过输入来学习抽象的表征。这一领域被称为表征学习。这种更高层次的表征能将输入数据自组织成「解构」的概念群，而这些概念可能是我们所熟悉的物体，比如车、小猫，以及它们之间的关系（小猫坐在车里）。这种解构具有清晰的直觉上的意义，但也很难被合适地定义。20 世纪 90 年代，人们更多谈论的是统计独立的隐变量。我们大脑的目标就是把那些强关联的像素表征变换为更有效和更少冗余的独立隐变量的表征，这样就可以压缩输入，使得大脑能用更少的能量处理更多的信息。学习和压缩是紧密相关的。学习的过程需要有损压缩的数据，因为我们感兴趣的是知识的泛化能力而不是数据的存储。在数据集层面，机器学习本身就是把数据中一小部分的信息抽取为模型的参数，而不去考虑数据中与预测目标不相关的其他信息。类似地，当人类看一幅图像的时候，相比于单一数据点，大脑更感兴趣的是图像中抽象的更高层次的概念，比如不同的物体之间的关系。大脑内部形成的模型使我们可以理解这些物体，想象如何操纵这些物体，以及可能带来的后果。能够从像素级别的信息集中抽象出能够用于预测的信息，并能以合适的方式表达出对实际应用有作用的信息，即表征，这就是智慧。当然，我们日常生活中熟悉的那些物体并不是完全独立的。一只猫在追一只小鸟，两者就不再是统计性独立的了。因此，人们一直在试图定义解构。解构可以定义为变量的子空间，在变换输入数据时（即相对应的表征）可以有一些简单的变换特性；也可以是不同的变量，每个都可以独立控制，从而可以操纵所处环境；也可以是因果变量，激活一些特定的独立机制来描述周边环境，等等。不需要标签来训练模型，最简单的方法是学习关于输入数据的概率生成模型（或者密度）。概率生成模型的领域有不少技术可以直接最大化生成模型对应数据的对数概率（或者对数概率的边界）。除了 VAE 和 GAN，本书还讨论了标准化流模型、自回归模型、基于能量的模型，以及最新的深度扩散模型。不用训练生成模型也可以学习到一些表征，这些表征对很多下游的预测任务都会很有用。具体做法是将表征要完成的任务设计为并不需要标注数据就可以完成的任务。以时序数据为例，我们总可以通过在历史数据中设定时间点来训练模型去预测未来，比如预测一段数据是不是在另一些数据的左侧或右侧，或者一部电影是在正着播放还是倒着播放，或者用句子周围的词来预测中间的词。这种无监督学习方法被称为自监督学习。很多方法都可以被认为是这种无监督学习的「辅助任务」，如一些概率生成模型。比如，变分自动编码器（VAE）就是用信息瓶颈来返回预测自己本身的模型输入；生成对抗网络（GAN）就是预测输入数据是真实的图像（原数据点）还是假的图像（生成图像）；噪声对比估计就是在隐空间中预测输入数据的隐变量在时间或空间中是否相近。