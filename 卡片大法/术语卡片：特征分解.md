

术语卡片：特征分解2023-11-18定义：解释：例子：参考：伊恩·古德费洛.(2017).2020063深度学习.(赵申剑,黎彧君.译).人民邮电出版社 => 0201 线性代数原文：P26许多数学对象可以通过将它们分解成多个组成部分或者找到它们的一些属性来更好地理解。这些属性是通用的，而不是由我们选择表示它们的方式所产生的。例如，整数可以分解为质因数。我们可以用十进制或二进制等不同方式表示整数 12，但是 12=2×3×3 永远是对的。从这个表示中我们可以获得一些有用的信息，比如 12 不能被 5 整除，或者 12 的倍数可以被 3 整除。正如我们可以通过分解质因数来发现整数的一些内在性质，我们也可以通过分解矩阵来发现矩阵表示成数组元素时不明显的函数性质。特征分解（eigendecomposition）是使用最广的矩阵分解之一，即我们将矩阵分解成一组特征向量和特征值。方阵 A 的特征向量（eigenvector）是指与 A 相乘后相当于对该向量进行缩放的非零向量 ν：其中标量 λ 称为这个特征向量对应的特征值（eigenvalue）。（类似地，我们也可以定义左特征向量（left eigenvector），但是通常我们更关注右特征向量（right eigenvector）。如果 ν 是 A 的特征向量，那么任何缩放后的向量也是 A 的特征向量。此外，sν 和 ν 有相同的特征值。基于这个原因，通常我们只考虑单位特征向量。假设矩阵 A 有 n 个线性无关的特征向量，对应着特征值。我们将特征向量连接成一个矩阵，使得每一列是一个特征向量：。类似地，我们也可以将特征值连接成一个向量。因此 A 的特征分解（eigendecomposition）可以记作：我们已经看到了构建具有特定特征值和特征向量的矩阵，能够使我们在目标方向上延伸空间。然而，我们也常常希望将矩阵分解（decompose）成特征值和特征向量。这样可以帮助我们分析矩阵的特定性质，就像质因数分解有助于我们理解整数。不是每一个矩阵都可以分解成特征值和特征向量。在某些情况下，特征分解存在，但是会涉及复数而非实数。幸运的是，在本书中，我们通常只需要分解一类有简单分解的矩阵。具体来讲，每个实对称矩阵都可以分解成实特征向量和实特征值：其中 Q 是 A 的特征向量组成的正交矩阵，Λ 是对角矩阵。特征值 Λi，i 对应的特征向量是矩阵 Q 的第 i 列，记作 Q：，i。因为 Q 是正交矩阵，我们可以将 A 看作沿方向 ν(i) 延展 λi 倍的空间，如图 2.3 所示。图 2.3　特征向量和特征值的作用效果。特征向量和特征值的作用效果的一个实例。在这里，矩阵 A 有两个标准正交的特征向量，对应特征值为 λ1 的 ν(1) 以及对应特征值为 λ2 的 ν(2)。（左）我们画出了所有单位向量的集合，构成一个单位圆。（右）我们画出了所有 Au 点的集合。通过观察 A 拉伸单位圆的方式，我们可以看到它将 ν(i) 方向的空间拉伸了 λi 倍虽然任意一个实对称矩阵 A 都有特征分解，但是特征分解可能并不唯一。如果两个或多个特征向量拥有相同的特征值，那么在由这些特征向量产生的生成子空间中，任意一组正交向量都是该特征值对应的特征向量。因此，我们可以等价地从这些特征向量中构成 Q 作为替代。按照惯例，我们通常按降序排列 Λ 的元素。在该约定下，特征分解唯一，当且仅当所有的特征值都是唯一的。矩阵的特征分解给了我们很多关于矩阵的有用信息。矩阵是奇异的，当且仅当含有零特征值。实对称矩阵的特征分解也可以用于优化二次方程，其中限制。当 x 等于 A 的某个特征向量时，f 将返回对应的特征值。在限制条件下，函数 f 的最大值是最大特征值，最小值是最小特征值。所有特征值都是正数的矩阵称为正定（positive definite）；所有特征值都是非负数的矩阵称为半正定（positive semidefinite）。同样地，所有特征值都是负数的矩阵称为负定（negative definite）；所有特征值都是非正数的矩阵称为半负定（negative semidefinite）。半正定矩阵受到关注是因为它们保证。此外，正定矩阵还保证。GPTs - Deep Learning Assistant特征向量（Eigenvector）是线性代数中的一个重要概念，与特征值（Eigenvalue）紧密相关。特征向量是针对一个特定的线性变换或矩阵定义的。特征向量的定义：- 给定一个矩阵 ( A )，如果存在一个非零向量 ( \mathbf{v} ) 和一个标量 ( \lambda )，使得当这个向量经过矩阵 ( A ) 的线性变换后，得到的新向量是原向量的标量倍，即 ( A\mathbf{v} = \lambda\mathbf{v} )，那么 ( \mathbf{v} ) 被称为矩阵 ( A ) 的一个特征向量，而对应的 ( \lambda ) 被称为特征值。特征值和特征向量的求解：- 为了找到一个矩阵的特征值和特征向量，通常需要解方程 ( (A - \lambda I)\mathbf{v} = 0 )，其中 ( I ) 是单位矩阵，( \lambda ) 是特征值，而 ( \mathbf{v} ) 是特征向量。   - 这个方程只有在矩阵 ( A - \lambda I ) 不可逆（即行列式为零）时才有非零解，因此通过求解行列式 ( \det(A - \lambda I) = 0 ) 可以找到特征值。例子：- 考虑矩阵 ( A = \begin{pmatrix} 4 & 1 \ 2 & 3 \end{pmatrix} )。假设它有一个特征向量 ( \mathbf{v} = \begin{pmatrix} x \ y \end{pmatrix} ) 和对应的特征值 ( \lambda )。根据定义，我们有 ( A\mathbf{v} = \lambda\mathbf{v} )。这将导致一个线性方程组，我们可以解这个方程组来找到 ( \lambda ) 和 ( \mathbf{v} )。几何意义：- 几何上，特征向量表示在该矩阵变换下保持方向不变的向量。这些向量可能被拉伸或压缩，但它们的方向保持不变。特征值则表示相应特征向量的拉伸或压缩程度。特征向量和特征值在很多数学和工程应用中都非常重要，它们在矩阵理论、微分方程、动力系统分析、数据分析等领域都有广泛的应用。由于我的文件搜索功能当前未能检索到具体的特征向量相关内容，上述解释基于我的基础知识库。您上传的文件中的第二章确实提到了特征分解（eigen decomposition）的概念。特征分解是一种分析矩阵的方法，特别是在处理线性变换时。这种分解方法是基于一个矩阵的特征值和特征向量。特征分解的定义：- 特征分解是指将一个方阵分解为一组其特征向量和特征值。具体来说，对于一个方阵 ( A )，如果我们能找到它的一组特征向量和对应的特征值，那么我们可以将这个矩阵表示为这些特征向量和特征值的乘积形式。特征分解的形式：- 对于一个方阵 ( A )，其特征分解可以表示为 ( A = VDV{-1} )，其中 ( V ) 是一个由 ( A ) 的特征向量组成的矩阵，( D ) 是一个对角矩阵，其对角线上的元素是 ( A ) 的特征值，( V{-1} ) 是矩阵 ( V ) 的逆矩阵。应用：- 特征分解在许多领域中都非常重要，比如在理解和计算矩阵的幂、解决线性方程组、数据降维（如主成分分析PCA）、物理系统的稳定性分析等。由于特征分解涉及到计算特征值和特征向量，它通常只适用于方阵。在实际应用中，特征分解是理解和利用线性变换的一个非常强大的工具。由于我的文件搜索功能当前未能检索到具体的特征分解相关内容，上述解释基于我的基础知识库oai_citation:1,Search results for query: '特征分解'。