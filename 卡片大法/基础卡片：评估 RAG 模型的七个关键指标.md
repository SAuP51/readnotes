

基础卡片：评估 RAG 模型的七个关键指标2024-05-01原文：。评论：。参考：ITstudy => 001 大语言模型 => 13Article-LLM => 20240501七个帮助最小化RAG模型风险的指标原文：好的，现在我们来讨论评估你的 RAG 模型的七个关键指标。1、Rouge 分数。它也用来衡量召回率和完整性。当我们得到模型生成的回答后，我们会把它和一组人类生成的期望回答进行对比。接下来，我们要对比计算机生成的文字中的具体词语，我们不只对比一个词，而是会看一系列的词，看我们生成的回答与期望回答的完整性如何。这个分数会在 0 和 1 之间变化。2、BLEU 分数。不知道有没有人注意到这些都是法语单词。如果你知道这些评估方法的起源，我们非常欢迎你留言评论。BLEU 分数主要衡量的是精确度。因此，我们再次审视计算机生成的回应与我们期望的标准相比的情况。我们关注的是整个文本中各个词汇的精确度。在这种情况下，长回应可能会由于受到惩罚而影响其精确性和准确性，因为长回应相对于原文可能会被过分惩罚。因此，这是在使用 Bleu 评分方法时您可能需要考虑的一个因素。3、Metor 分数。它能给我们提供精确度和召回率的平均值，这是从第一点和第二点得出的。这是一种比较全面的评估模型性能的方式。4、PII（个人身份信息）。这就是所有能够识别你身份的信息，像电话号码、电子邮件、名字这样的信息。这些都是你可能不希望模型生成的，它们可能会让你从个人和消费者的角度承担巨大的责任。因此，了解模型的输出和输入都非常重要。5、HAP 分数（仇恨、滥用和粗言秽语）。如果模型输出有关仇恨、滥用或者粗言秽语的内容，那就不妙了。所以，你需要随时监控模型，确保这种信息不会出现。我们肯定不希望这种情况发生。6、上下文相关性。这个指标非常重要。比如说，我们提出一个关于纽约州的问题。这就是我对纽约州的描述。我们想要明确知道纽约州在哪里，它的首府是什么。所以，我们向检索增强型生成模型提出了两个问题，并希望得到一句话包含两个答案。如果我们的上下文相关性较差，我们可能会给出一个正确但和问题完全无关的答案。比如，纽约是一个帝国州，或者被称为帝国州。虽然这是一个事实，但并没有回答我们原来的问题，纽约在哪儿，首府是什么？这就是一个衡量上下文相关性的例子。7、错觉。我们要确保模型不给出错误的答案，然后让我们误以为是正确的。回到我们的纽约例子，为了得到低的错觉分数和高的相关性分数。我们需要回答这两个问题。纽约位于美国东海岸，它北邻新泽西州，西邻康涅狄格州。首府是奥尔巴尼。所以，答案没有错觉，而且和上下文非常相关。现在我们已经介绍了七个 RAG 评估指标。当然，还有许多其他的指标，我很希望在评论中听到你们用来监控 RAG 的一些最喜欢的指标。一定要使用这些指标来降低模型在实际应用中的风险。