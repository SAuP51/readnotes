

基础卡片：GPT-3 的目的是要大，其体现在三加一上2023-12-17解释：参考：ITStduy => 001大语言模型 => Article => 20231216刘嘉-通用人工智能范式转换与人类未来原文：现在进一步的研究发现，人工神经网络它越复杂，也就是它的层数越多、参数越多，也就是它越深，它越来越像人类的大脑，当它的体量增加之后，它就和我们人类大脑的这种运作方式就特别的相近。正是因为在这种理念的推动之下，在 2020 年 5 月份，终于涌现了一个当时我们把它称为暴力美学的巅峰，GPT-3 的出世。我们前面谈到 ChatGPT，我们谈到 GPT-3，大家说 GPT 到底是啥？G 就是代表深层次，P 就是代表 Pre-training，就是它是不需要监督的学习，第三个，它是基于一个架构，这就是 Google 发明的 Transformer，这么一个架构来做的一个东西。它是被称为暴力美学的巅峰，其实它的目的就是要大，它有哪大呢？有三加一。它的大体现在，第一个，它是大算力，当时为了能够让这个模型运作起来，微软专门为他们设计了一台超级 AI 计算机，这台计算机的算力放在当时，能够进世界超级计算机排名第五名，就是为了来训 GPT-3。第二个，它的架构极其复杂，它总共有 1750 亿个参数，比它的上一代足足多了两个量级，多了 100 倍的参数量，变成一个非常庞大的体系。第三个，它的数据量也是非常的大，英语维基百科大约有 600 万篇文章，我们在座的各位不吃不喝一辈子都读不完，而这只占它的训练材料多少呢？6/1000，也就是说在它里面就是沧海一粟，非常小的一部分，所以它真的是一个大数据量。这是三，大模型、大算力、大数据，还要加个一是什么呢？就是大的费用。当时 GPT-3 训练一次的费用是将近 500 万美元，在他们发表的那篇 GPT-3 的文章里面，他说我们突然发现我们程序里面有个小 Bug，有个小的漏洞，说后来我们想了想，还是别去修这个漏洞了，为什么？再修一次要重新再训一次，又得再花 500 万美元，我们觉得就这样吧。