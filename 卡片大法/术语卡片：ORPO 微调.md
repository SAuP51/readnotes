

术语卡片：ORPO 微调2024-04-04定义：xx例子：xx参考：001大语言模型 => 13Article-LLM => 20240616ORPO-Fine-Tuning-Streamlining-Large-Language-Model-Adaptation原文：PxxxxLarge Language Models (LLMs) have become a game-changer, capable of generating human-quality text, translating languages, and writing different kinds of creative content. However, their generic nature often requires fine-tuning for specific tasks. Traditionally, this involved a two-step process: supervised fine-tuning (SFT) and preference alignment. ORPO (Odds Ratio Policy Optimization) emerges as a groundbreaking technique, merging these steps into a single, powerful approach. This article delves into the intricate details of ORPO fine-tuning, providing a comprehensive understanding with illustrative examples.大型语言模型（LLMs）已经成为一种革命性的技术，能够生成高质量的人类文本、翻译语言和撰写各种创意内容。然而，它们的通用性通常需要为特定任务进行微调。传统上，这涉及两个步骤：监督微调（SFT）和偏好对齐。ORPO（Odds Ratio Policy Optimization）作为一种突破性的技术，将这两个步骤合并为一个强大的方法。本文深入探讨 ORPO 微调的复杂细节，并通过示例提供全面理解。Preference Pairs: In addition to data, ORPO leverages a crucial element called preference pairs. These pairs represent concrete examples of preferred and dispreferred outputs for a given input. They act as a guiding light, informing the LLM about the desired style and content of the generated text.Example: Consider the product description scenario again. A preference pair might consist of a product image (input) paired with a well-written, informative description highlighting key features and benefits (preferred output) and a generic, promotional description lacking details (dispreferred output).偏好对（preference pairs)：除了数据，ORPO 还利用一个关键元素，称为偏好对。这些对表示给定输入的首选和非首选输出的具体示例。它们充当指导作用，告知 LLM 所生成文本的期望风格和内容。示例：以产品描述为例，一个偏好对可能由一个产品图像（输入）配对一个写得好的、信息丰富的描述，突出关键特性和优点（首选输出）以及一个缺乏详细信息的通用促销描述（非首选输出）组成。Odds Ratio Calculation: This is where the magic happens. ORPO calculates the odds ratio between the probabilities of generating the preferred and dispreferred outputs for a given input. The odds ratio essentially quantifies the model's preference for the desired output.Imagine the odds ratio as a score. A high odds ratio indicates the model is more likely to generate the preferred output, while a low score suggests a preference for the dispreferred one.几率比计算（Odds Ratio Calculation）：这是关键步骤。ORPO 计算给定输入生成首选和非首选输出的概率之间的几率比。几率比量化了模型对期望输出的偏好程度。Example: Fine-Tuning a Chatbot with ORPOLet's consider fine-tuning an LLM for a customer service chatbot.Task-Specific Data: We provide the LLM with a collection of customer queries paired with corresponding human-written responses from customer service representatives.Preference Pairs: We create preference pairs where each pair consists of a customer query (input) paired with a helpful, informative response from a customer service agent (preferred output) and a generic, unhelpful response lacking resolution (dispreferred output).Example:Input (Customer Query): "My internet connection keeps dropping. Can you help?"Preferred Output (Customer Service Response): "Hi there, I apologize for the inconvenience. To troubleshoot the issue, let's try restarting your modem and router. Here are the steps…"Dispreferred Output: "Thanks for contacting us. We are aware of some internet connectivity issues in your area. Our technicians are working on resolving the problem." (This response lacks specific troubleshooting steps and doesn't offer immediate assistance).ORPO Training: The LLM is trained on the customer service dialogue data and preference pairs. During training, the odds ratio between generating helpful and unhelpful responses for each query is calculated. The model is then updated to favor generating informative responses with higher odds of resolving customer issues.By incorporating human preferences through preference pairs, ORPO ensures the chatbot learns to generate clear, solution-oriented responses, enhancing the customer service experience.示例：使用 ORPO 微调聊天机器人以微调客户服务聊天机器人为例。1、任务特定数据：我们为大语言模型提供一系列客户查询，并配有人类客服代表撰写的相应回复。2、偏好对：我们创建偏好对，每对包括一个客户查询（输入)、一个有帮助且信息丰富的客户服务回复（首选输出）以及一个缺乏解决问题的通用回复（不首选输出)。示例：输入（客户查询)："我的互联网连接总是掉线。你能帮忙吗？"首选输出（客户服务回复)："您好，很抱歉给您带来不便。为了排除故障，让我们尝试重新启动您的调制解调器和路由器。以下是步骤…"不推荐的输出:「感谢您联系我们。我们知道您所在地区存在一些互联网连接问题。我们的技术人员正在努力解决这个问题。」（这种回应缺乏具体的故障排除步骤，也没有提供即时的帮助）。ORPO 训练：大语言模型（LLM）在客户服务对话数据和偏好对比对上进行了训练。在训练期间，计算每个查询生成有帮助和无帮助响应的赔率比。然后更新模型以倾向于生成更有可能解决客户问题的信息性响应。通过偏好对引入人类偏好，ORPO 确保聊天机器人学会生成清晰、以解决方案为导向的响应，提升客户服务体验。