

术语卡片：检索增强式生成（RAG）2023-12-09解释：参考：ITStduy => 001大语言模型 => Article => 20231209检索增强生成RAG-从理论到LangChain实践原文：Since the realization that you can supercharge large language models (LLMs) with your proprietary data, there has been some discussion on how to most effectively bridge the gap between the LLM's general knowledge and your proprietary data. There has been a lot of debate around whether fine-tuning or Retrieval-Augmented Generation (RAG) is more suited for this (spoiler alert: it's both).This article first focuses on the concept of RAG and first covers its theory. Then, it goes on to showcase how you can implement a simple RAG pipeline using LangChain for orchestration, OpenAI language models, and a Weaviate vector database.Retrieval-Augmented Generation (RAG) is the concept to provide LLMs with additional information from an external knowledge source. This allows them to generate more accurate and contextual answers while reducing hallucinations.检索增强生成（RAG）的工作流程：从用户查询开始，通过向量数据库检索，到填充提示，最终形成回答的全过程。自从发现可以利用自有数据来增强大型语言模型（LLM）的能力以来，如何将 LLM 的通用知识与个人数据有效结合一直是热门话题。关于使用微调（fine-tuning）还是检索增强生成（RAG）来实现这一目标的讨论持续不断（剧透：两种方法都很关键）。这篇文章首先探讨 RAG 的理念，并对其理论基础进行阐释。接着，文章展示了如何结合 LangChain 进行操作管理、OpenAI 语言模型和 Weaviate 向量数据库，来实现一个简易的 RAG 流程。什么是检索增强生成？检索增强生成（RAG）是一个概念，它旨在为大型语言模型（LLM）提供额外的、来自外部知识源的信息。这样，LLM 在生成更精确、更贴合上下文的答案的同时，也能有效减少产生误导性信息的可能。