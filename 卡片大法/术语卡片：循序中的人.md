

术语卡片：循序中的人2024-06-05解释：参考：Ethan Mollick.(2024).2024029Co-Intelligence_Living-and-Working-with-AI.Penguin Publishing Group => 0301 Four Rules for Co-Intelligence原文：For now, AI works best with human help, and you want to be that helpful human. As AI gets more capable and requires less human help—you still want to be that human. So the second principle is to learn to be the human in the loop.The concept of "human in the loop" has its roots in the early days of computing and automation. It refers to the importance of incorporating human judgment and expertise in the operation of complex systems (the automated "loop"). Today, the term describes how AIs are trained in ways that incorporate human judgment. In the future, we may need to work harder to stay in the loop of AI decision-making.As AI improves, it will be tempting to delegate everything to it, relying on its efficiency and speed to get the job done. But AI can have some unexpected weaknesses. For one thing, they don't actually "know" anything. Because they are simply predicting the next word in a sequence, they can't tell what is true and what is not. It can help to think of the AI as trying to optimize many functions when it answers you, one of the most important of which is "make you happy" by providing an answer you will like. That goal often is more important than another goal, "be accurate." If you are insistent enough in asking for an answer about something it doesn't know, it will make up something, because " make you happy" beats "be accurate." LLMs' tendency to "hallucinate" or "confabulate" by generating incorrect answers is well known. Because LLMs are text prediction machines, they are very good at guessing at plausible, and often subtly incorrect, answers that feel very satisfying. Hallucination is therefore a serious problem, and there is considerable debate over whether it is completely solvable with current approaches to AI engineering. While newer, larger LLMs hallucinate much less than older models, they still will happily make up plausible but wrong citations and facts. Even if you spot the error, AIs are also good at justifying a wrong answer that they have already committed to, which can serve to convince you that the wrong answer was right all along!现在，AI 在人类的帮助下表现最好，而你希望成为那个帮助它的人。即使将来 AI 变得更加智能，需要的人工干预减少，你仍然要努力成为那个有用的人。因此，第二个原则是学会成为「循环中的人」。「循环中的人」这一概念源自计算和自动化的早期，强调在复杂系统中加入人类判断和专业知识的重要性。今天，这个术语描述了 AI 如何通过结合人类判断进行训练。将来，我们可能需要更加努力才能继续参与 AI 的决策过程。随着 AI 技术的进步，人们可能会倾向于将所有任务都交给 AI，依赖其高效和快速的特点来完成工作。但 AI 也有一些意想不到的弱点。首先，AI 实际上并不「知道」任何事情。因为它们只是在预测序列中的下一个词，所以无法判断什么是真实的，什么是不真实的。可以把 AI 想象成在回答时试图优化许多功能，其中一个最重要的功能是通过提供你喜欢的答案来「让你高兴」。这个目标通常比「准确」更重要。如果你坚持要求 AI 回答它不知道的问题，它可能会编造一些东西，因为「让你高兴」比「准确」更重要。大语言模型（LLM）倾向于通过生成错误答案来「幻觉」或「虚构」这一点已经广为人知。由于 LLM 是文本预测机器，它们非常擅长猜测看似合理但实际上错误的答案，这些答案往往令人满意。因此，幻觉是一个严重的问题，目前对于现有的 AI 工程方法是否能完全解决这一问题还存在很大争议。尽管更新、更大的 LLM 比旧模型在幻觉方面有所减少，但它们仍然会编造出看似合理但错误的引用和事实。即使你发现了错误，AI 也擅长为它们已经给出的错误答案辩护，这可能会让你相信那个错误的答案一直是对的。