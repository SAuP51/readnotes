

基础卡片：以文档为中心的处理方式更适合长上下文窗口的大语言模型2024-05-01原文：。评论：。参考：ITstudy => 001 大语言模型 => 13Article-LLM => 20240501RAG在长上下文大语言模型中的应用探讨原文：在这种情境下，管理安全性变得不那么容易。因此，可能存在一种较为理想的帕累托最优状态在中间位置。我还在 Twitter 上分享了这个想法。我认为，有些提出的观点是挺有道理的。我觉得，在文档级别进行包含处理是相当明智的选择。文档本身就是独立完整的内容块。那么，以文档为中心的处理方式怎么样呢？不进行分块，直接操作完整文档的内容。想象一下，如果我们采用这种以文档为中心的处理方式，我们还是需要解决如何将具体问题定位到正确的文档这一问题，这一点并没有改变。因此，我们通常会考虑各种查询分析方法，比如重新编写问题以优化信息检索，或者如何将问题正确地定向到适当的数据库，无论是关系型数据库、图数据库还是向量数据库，以及如何构建查询语句。例如，将文本转换为 SQL 语句，转换为用于图的 Cypher 语句，或者转换为向量存储的元数据过滤条件，这些方法在使用长文本大语言模型的情境下依然适用。你可能不会完全放弃你的 SQL 数据库去使用大语言模型。你还会继续使用 SQL 查询和图查询。虽然你可能对提取的数据更加宽容，但大部分结构化数据还是会被以传统形式存储。同理，对于非结构化数据，如文档，我们之前也提过，最好还是将文档独立存储，优先考虑直接检索整个文档，而不是过分关注如块大小这样的细节。我们已经尝试了一些优化文档检索的方法。其中一个我要特别提到的是所谓的「多维表征索引」技术。还有一篇很棒的论文《Dense X Retriever 或 Proposition Indexing》详细讨论了这一点。简而言之，这个方法包括取出原始文档，制作一个摘要，并对这个摘要进行索引。然后在检索过程中，你提出问题并将问题转换为查询关键词，只需使用一个高层次的摘要即可准确地找到所需的文档。接着，将完整文档提交给大语言模型（LLM，Large Language Model）以生成最终的内容。这是一个巧妙的方法，在这种情况下你无需处理整篇文档的嵌入问题。你可以利用精确的描述性摘要提示来创建文档摘要。这里你面临的任务简单明了：就是找到正确的文档。这比寻找正确的文档片段要简单得多。这种方法很有效。还有一些其他的变体，例如我接下来要介绍的「父文档检索器」，它允许你按需检索较小的文档片段，但仍会返回完整的文档。核心思想是，在生成过程中保持文档的完整性，而在检索时则使用文档摘要或片段等表述形式。这可以视为一个非常有趣的方法。第二种方法称为「Raptor」，这是一篇源自斯坦福的新论文，解决了如何在需要时跨多个文档整合信息的问题。通过这种方式，文档被编码并按主题聚类，每个类别的文档都会被简要总结，并且这一过程会递归进行，直到只生成一个针对所有文档的高级摘要。这种层次化的文档总结框架被全部编入索引，并在检索时使用。这样，如果你的问题涉及多个文档的信息，你就很可能找到一个包含答案的摘要。这是一个有效的方法来整合多文档中的信息。论文中还提到，尽管案例中的文档被分为不同的小片段，但我的视频和笔记本演示显示，这种方法同样适用于完整文档。要实现这一点，你需要考虑使用长文本嵌入模型，因为这涉及到整个文档的嵌入工作，这是追踪研究中一个非常有趣的方向。Hazy Research 在其博客上发布了一篇关于使用 Monarch Mixer 进行此类工作的精彩文章。所以这实际上是一种新的架构，它能处理更长的上下文。在 Together AI 上，他们提供了一个包含 32,000 个 Token 的嵌入模型，非常值得一试。我认为这是一个极具趣味的新趋势。长上下文嵌入与这种理念非常契合。例如，你可以将整个文档通过长上下文嵌入模型进行嵌入，然后高效地创建文档摘要树。这也是在长上下文大语言模型体系下处理全文档的一个有效策略。另外值得一提的是，我认为未来可能会从单次执行型 rag 向其他模式转变。在现有的 rag 系统中，我们通常将文档分块并嵌入后存入索引库，通过检索和生成内容，但实际上我们完全可以在生成或检索过程中添加推理步骤来纠错。有一篇称为 "Self-rag" 的论文详细介绍了这一点，我们已经用 Langraph 成功实现了这一机制，效果非常好。这个方法的核心是先评估文档与问题的相关性，如果不相关，则重写问题并重试。我们还会评估幻觉和答案的相关性。这种方式将 rag 从单次模式转变为一个循环流程，在这个流程中，我们会在多个阶段进行评估。这在长上下文大语言模型体系中也同样适用。事实上，你应该充分利用例如性能越来越好的大语言模型来进行这些评估。像 Langraph 这样的框架可以帮助你建立这样的流程，使得 rag 系统更加高效和自省。对于延迟的问题，我完全理解大家的担忧，确实，性能、准确性与延迟之间需要做出一定的权衡。在选择模型时，你可以考虑使用如 Grok 或 GPT-35 Turbo 这样的快速模型，这些模型在处理这些评估任务时表现出色。