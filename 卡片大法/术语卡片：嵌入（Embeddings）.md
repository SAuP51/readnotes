

术语卡片：嵌入（Embeddings）2023-11-23定义：解释：例子：参考：ITStduy => 001大语言模型 => Article => 20231122What-Is-ChatGPT-Doing-and-Why-Does-It-Work原文：Neural nets—at least as they're currently set up—are fundamentally based on numbers. So if we're going to to use them to work on something like text we'll need a way to represent our text with numbers. And certainly we could start (essentially as ChatGPT does) by just assigning a number to every word in the dictionary. But there's an important idea—that's for example central to ChatGPT—that goes beyond that. And it's the idea of "embeddings". One can think of an embedding as a way to try to represent the "essence" of something by an array of numbers—with the property that "nearby things" are represented by nearby numbers.And so, for example, we can think of a word embedding as trying to lay out words in a kind of "meaning space" in which words that are somehow "nearby in meaning" appear nearby in the embedding. The actual embeddings that are used—say in ChatGPT—tend to involve large lists of numbers. But if we project down to 2D, we can show examples of how words are laid out by the embedding:And, yes, what we see does remarkably well in capturing typical everyday impressions. But how can we construct such an embedding? Roughly the idea is to look at large amounts of text (here 5 billion words from the web) and then see "how similar" the "environments" are in which different words appear. So, for example, "alligator" and "crocodile" will often appear almost interchangeably in otherwise similar sentences, and that means they'll be placed nearby in the embedding. But "turnip" and "eagle" won't tend to appear in otherwise similar sentences, so they'll be placed far apart in the embedding.But how does one actually implement something like this using neural nets? Let's start by talking about embeddings not for words, but for images. We want to find some way to characterize images by lists of numbers in such a way that "images we consider similar" are assigned similar lists of numbers.神经网络 —— 至少在目前的设置中 —— 基本上是基于数字的。因此，如果我们要使用它们来处理像文本这样的东西，我们需要一种用数字表示文本的方法。当然，我们可以开始（基本上就像 ChatGPT 所做的）为字典中的每个单词分配一个数字。但有一个重要的概念 —— 例如对于 ChatGPT 至关重要 —— 超越了这一点。这就是「嵌入」（embeddings）的概念。可以将嵌入视为一种尝试通过数字数组来表示某物的「本质」的方式 —— 具有这样的特性：「邻近的事物」由邻近的数字表示。例如，我们可以将单词嵌入视为试图在某种「意义空间」中布局单词，其中在意义上「相近」的单词在嵌入中也相近。实际上在 ChatGPT 中使用的嵌入往往涉及大量的数字。但如果我们投影到 2D，我们可以展示单词在嵌入中的布局示例：的确，我们看到的确实很好地捕捉了典型的日常印象。但我们如何构建这样的嵌入呢？大致的想法是查看大量的文本（这里是来自网络的 50 亿个单词），然后看看不同单词出现的「环境」有多「相似」。例如，「鳄鱼」和「鳄鱼」通常会在其他相似的句子中几乎可互换地出现，这意味着它们将在嵌入中被放置在附近。但「萝卜」和「鹰」不会倾向于出现在其他相似的句子中，因此它们将在嵌入中被放置得很远。但我们如何实际使用神经网络来实现这样的事情呢？让我们从讨论图像的嵌入而不是单词的嵌入开始。我们想要找到一种方式来用数字列表来描述图像，以便「我们认为相似的图像」被分配给相似的数字列表。