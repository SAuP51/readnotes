

新知卡片：使用较大的语言模型和较低的量化精度，而不是使用较小的语言模型和较高的量化精度2023-12-21解释：参考：selfstudy => 003Paper => 2023046A-Survey-of-Large-Language-Models原文：Important Findings from Existing Work. Recently, a very comprehensive evaluation [421] has been conducted about the impact of multiple factors (e.g., model size and sensitivity) on the post-training quantization methods. Another study [422] examines the scaling law of k -bit quantization in inference performance. In addition to the overall performance, the study [423] specifically focuses on the potential impact of quantification on emergent capabilities, as well as the levels of performance that can be achieved across various levels of bit precision. Also, prior work (e.g., LLM.int8() [424], GPTQ [417], QLoRA [419], and GLM [93]) has also extensively examined the performance of quantization methods in various settings. Next, we summarize several important findings from these studies, which will be useful for those who may not want to delve into the technical details of quantization methods.• INT8 weight quantization can often yield very good results on LLMs, while the performance of lower precision weight quantization depends on specific methods [414, 416, 417, 421]. In most cases, INT8 weight quantization can be effectively applied to reduce the memory footprint without performance degradation. While for INT4 (or INT3) weight quantization, existing methods rely on specific strategies to reduce the performance degradation, e.g., layerwise method [415, 417], activation-aware scaling [416] and low-rank adapter tuning [419]. Interestingly, LLMs seem to be less sensitive to low-bit weight quantization than small-sized language models [421]. In practice, with the same memory cost, it is suggested to use a larger language model with a lower quantization precision rather than a smaller language model with a higher quantization precision. For example, a 4-bit 60GB LLM is demonstrated to have better performance than a 8-bit 30GB LLM [422]. Moreover, focusing on emergent capabilities, the study [423] finds that in-context learning, step-by-step reasoning, and instruction following all seem to be seldom affected with 4-bit weight quantization. This result suggests that INT4 quantization exhibits a favorable trade-off in terms of both total bits and performance of emergent abilities.从现有工作中汲取的重要发现。近期，已进行了一项关于训练后量化方法受多种因素（如模型大小和灵敏度）影响的全面评估 [421]。另一研究 [422] 探讨了 k 位量化在推理性能中的比例规律。除总体性能外，研究 [423] 还特别关注量化对模型突现能力的潜在影响，以及不同位精度水平下可达到的性能水平。此外，先前的研究（例如 LLM.int8()[424]、GPTQ [417]、QLoRA [419] 和 GLM [93]）也广泛检验了不同场景下量化方法的性能。接下来，我们总结这些研究的几个关键发现，这对于那些不愿深入了解量化方法技术细节的人来说非常有用。1、在大型语言模型（LLMs）上，INT8 权重量化通常能产生优秀的结果，而较低精度权重量化的性能则依赖于特定的方法[414, 416, 417, 421]。大多数情况下，INT8 权重量化能有效减少内存占用，同时保持性能。而对于 INT4（或 INT3）权重量化，现有方法依靠特定策略来减少性能下降，例如分层方法[415, 417]、激活感知缩放[416]和低秩适配器调整[419]。有趣的是，与小型语言模型相比，LLMs 对低位权重量化的敏感度似乎较低[421]。实践中，建议在相同的内存成本下使用较大的语言模型和较低的量化精度，而不是使用较小的语言模型和较高的量化精度。例如，一项研究证明了 4 位 60GB 的 LLM 性能优于 8 位 30GB 的 LLM [422]。此外，关注突现能力的研究[423]发现，在 4 位权重量化的情况下，上下文学习、逐步推理和指令遵循似乎很少受到影响。这表明，INT4 量化在总位数和突现能力性能方面表现出良好的平衡。