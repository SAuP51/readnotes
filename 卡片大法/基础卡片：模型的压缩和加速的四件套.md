

基础卡片：模型的压缩和加速的四件套2023-11-22内容：参考：ITStduy => 001大语言模型 => Video => 2023020卢菁大模型教程 => 0401卢菁-模型微调实战和经验分享原文：在完成模型训练之后，如何进行模型的压缩和加速操作。在深度学习领域中，整体上有一套被认为是加速的四件套，包括剪枝（pruning）、低秩分解（slimming）、蒸馏（distillation）和量化（quantization）。其中，剪枝主要是通过移除一些权重较小的参数来实现模型的简化，而低秩分解则是通过数学手段减少矩阵的规模。至于蒸馏，这是一个让小模型去学习大模型的过程。这三种方式在传统的自然语言处理（NLP）领域较为常见，但在大规模模型中更多采用的其实是模型量化这一技术。通过这些技术的运用，我们能够使模型变得更为精简而高效，以适应不同的应用场景和性能要求。在高性能计算以及人工智能不断进步的今天，这些技术的实践和研究，无疑是优化计算资源，推动科技发展的关键步骤。关于模型量化，其实这是一个非常简单的概念。例如，在以往，我构建的不定点树（应为浮点数结构），可能具有很大的数值范围，由于其占用的显存和比特率相对较高。接着，我会采取一种方式，强制使用一个 int8 类型的数据来表示它，即使用一个低精度的数据格式来表征一个高精度的数据。这个过程就是量化，即将一个浮点数（记作 R）转换成一个整数（记作 Q）。而反量化，正如其名，就是将整数再恢复为原先的浮点数。在这一转换过程中，大家可能注意到了一个四舍五入的操作，因此这一步实际上会导致所谓的量化误差。比如，一个数值原本是 1.25，经过量化再反量化后，返回的数据可能变成了 1.20，即精度损失了一些。虽然对于推理来说，这样的精度损失并不特别重要，但对于训练过程而言，影响则较大。