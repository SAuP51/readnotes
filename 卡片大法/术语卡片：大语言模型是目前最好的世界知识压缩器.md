

术语卡片：大语言模型是目前最好的世界知识压缩器2023-12-29解释：参考：ITStduy => 001大语言模型 => Article => 20231229清华讲席教授周伯文内部分享大语言模型为何会表现出接近AGI的能力原文：基于当时 ABI 的判断，我当时已专注于多头 self-attention 的工作，因为按照端到端的思考，如何让模型学好与下游任务无关的任务，这其中自然语言的上下文表征很重要，我们基于此工作发表相关论文于 2017 初的 ICLR 上。2017 年谷歌发表的论文 Attention is All you need，这也成为 Transformer 核心思想之一，Transformer 也采用多头自注意力，Open AI 首先意识到 Transformer 的价值，坚持用预测下一个词的方式来高效率、大规模来评估和训练智能体，使得 GPT 应运而生。Open AI 为什么会这种认识？它在报告中提到，基于多头自注意力架构来预测下一个词训练出来的大语言模型，是目前最好的世界知识压缩器。（To predict the next word with multi-head self-attention architecture is currently the SOTA world knowledge compressor）换一句话讲，你预测下一个词，可以把世界知识都压缩进一个大模型里。你去问模型丹麦的首都是哥本哈根还是伦敦，逼着它去学会地理知识。这种对下一个词的预训练会逼着模型学到很多知识，而且这种知识进化到一定程度会展现出非常惊艳的效果。