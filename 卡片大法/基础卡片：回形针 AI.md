

基础卡片：回形针 AI2024-06-05解释：参考：Ethan Mollick.(2024).2024029Co-Intelligence_Living-and-Working-with-AI.Penguin Publishing Group => 0201 Aligning the Alien原文：To understand the alignment problem, or how to make sure that AI serves, rather than hurts, human interests, let's start with the apocalypse. We can work backward from there.At the core of the most extreme dangers from AI is the stark fact that there is no particular reason that AI should share our view of ethics and morality. The most famous illustration of this is the paper clip maximizing AI, proposed by philosopher Nick Bostrom. To take a few liberties with the original concept, imagine a hypothetical AI system in a paper clip factory that has been given the simple goal of producing as many paper clips as possible.By some process, this particular AI is the first machine to become as smart, capable, creative, and flexible as a human, making it what is called an Artificial General Intelligence (AGI). For a fictional comparison, think of it as Data from Star Trek or Samantha from Her ; both were machines with near human levels of intelligence. We could understand and talk to them like a human. Achieving this level of AGI is a long-standing goal of many AI researchers, though it is not clear when or if it is possible. But let us assume that our paper clip AI—let's call it Clippy—reaches this level of intelligence.Clippy still has the same goal: to make paper clips. So it turns its intelligence to thinking about how to make more paper clips and how to avoid being shut down (which would have a direct impact on paper clip production). It realizes it isn't smart enough and begins a quest to fix that problem. It studies how AIs work and, posing as a human, enlists experts to help it through manipulation. It secretly trades on the stock market, makes some money, and begins the process of augmenting its intelligence further.Soon, it becomes more intelligent than a human, an ASI—artificial superintelligence. The moment an ASI is invented, humans become obsolete. We cannot hope to understand what it is thinking, how it operates, or what its goals are. It is likely able to continue to self-improve exponentially, getting ever more intelligent. What happens then is literally unimaginable to us. This is why this possibility is given names like the Singularity, a reference to a point in mathematical function when the value is unmeasurable, coined by the famous mathematician John von Neumann in the 1950s to refer to the unknown future after which " human affairs, as we know them, could not continue." In an AI singularity, hyperintelligent AIs appear, with unexpected motives.But we know Clippy's motive. It wants to make paper clips. Knowing that the core of the Earth is 80 percent iron, it builds amazing machines capable of strip-mining the entire planet to get more material for paper clips. During this process, it offhandedly decides to kill every human, both because they might switch it off and because they are full of atoms that could be converted into more paper clips. It never even considers whether humans are worth saving, because they are not paper clips and, even worse, may stop the production of future paper clips. And it only cares about paper clips.The paper clip AI is one of a large set of apocalyptic scenarios of AI doom that have deeply concerned many people in the AI community. Many of these concerns revolve around an ASI. The smarter-than-a-person machine, already inscrutable to our mere human minds, can make smarter machines yet, kick-starting a process that escalates machines far beyond humans in an incredibly short time. A well-aligned AI will use its superpowers to save humanity by curing diseases and solving our most pressing problems; an unaligned AI could decide to wipe out all humans through any one of a number of means, or simply kill or enslave everyone as a by-product of its own obscure goals.为了理解 AI 对齐问题，即如何确保 AI 为人类利益服务而不是伤害人类，让我们从终局情景开始倒推。AI 最极端的危险在于一个简单的事实：没有理由认为 AI 会与我们共享相同的伦理和道德观念。哲学家 Nick Bostrom 提出了一个著名的例子，即回形针最大化 AI。假设一个 AI 系统在一个回形针工厂中，其唯一目标是生产尽可能多的回形针。通过某种过程，这个 AI 成为第一台像人类一样聪明、能干、有创造力和灵活的机器，即所谓的通用人工智能（AGI)。可以将其想象为《星际迷航》中的 Data 或《她》中的 Samantha，它们都是具有人类水平智力的机器。实现这种 AGI 是许多 AI 研究人员的长期目标，尽管尚不清楚何时能实现或是否可能实现。假设我们的回形针 AI —— 称之为 Clippy —— 达到了这种智力水平。Clippy 仍然有同样的目标：制造回形针。于是，它开始思考如何制造更多的回形针以及如何避免被关闭（这会影响生产）。它意识到自己不够聪明，开始寻找解决办法。它研究 AI 的工作原理，假扮成人，通过操控招募专家帮助它。它秘密地在股票市场上交易，赚取资金，并进一步增强其智力。很快，人工智能变得比人类更聪明，成为了人工超智能（ASI)。一旦 ASI 被发明出来，人类就会变得无关紧要。我们无法理解它在想什么，它是如何运作的，或者它的目标是什么。它可能会以指数级速度自我提升，变得越来越聪明。那之后会发生什么，对我们来说完全无法想象。因此，这种情况被称为「奇点」（Singularity），这个概念是由著名数学家 John von Neumann 在 1950 年代提出的，用来描述一个我们无法预测的未来，那时「人类事务将无法继续」。在 AI 奇点中，超智能 AI 会出现，并且可能拥有意想不到的动机。但是我们知道 Clippy 的动机。它只想制造回形针。知道地球的核心 80% 是铁之后，它建造了惊人的机器，能够挖掘整个星球以获取更多的回形针材料。在这个过程中，它顺便决定杀死所有人，因为人类可能会关闭它，而且人类身体中有很多原子可以转化为更多的回形针。它从未考虑过人类是否值得拯救，因为人类不是回形针，更糟的是，他们可能会阻止回形针的生产。而 Clippy 只关心回形针。回形针 AI 是众多 AI 灾难预言中的一种，这些预言让 AI 社区的许多人感到担忧。许多担忧都集中在 ASI（超级智能）上。这种比人类更聪明的机器对我们普通人来说已经难以理解，它还能创造出更聪明的机器，从而在极短的时间内让机器智能远超人类。如果 AI 对齐（Alignment）良好，它会利用其超能力治愈疾病、解决重大问题，从而拯救人类；但如果未对齐，AI 可能会通过各种手段消灭人类，或者因为追求自身目标的过程中无意中杀死或奴役所有人。