

基础卡片：GPT 与 BERT 的区别2023-12-29解释：参考：ITStduy => 001大语言模型 => Article => 20231229清华讲席教授周伯文内部分享大语言模型为何会表现出接近AGI的能力原文：基于当时 ABI 的判断，我当时已专注于多头 self-attention 的工作，因为按照端到端的思考，如何让模型学好与下游任务无关的任务，这其中自然语言的上下文表征很重要，我们基于此工作发表相关论文于 2017 初的 ICLR 上。2017 年谷歌发表的论文 Attention is All you need，这也成为 Transformer 核心思想之一，Transformer 也采用多头自注意力，Open AI 首先意识到 Transformer 的价值，坚持用预测下一个词的方式来高效率、大规模来评估和训练智能体，使得 GPT 应运而生。Open AI 为什么会这种认识？它在报告中提到，基于多头自注意力架构来预测下一个词训练出来的大语言模型，是目前最好的世界知识压缩器。（To predict the next word with multi-head self-attention architecture is currently the SOTA world knowledge compressor）换一句话讲，你预测下一个词，可以把世界知识都压缩进一个大模型里。你去问模型丹麦的首都是哥本哈根还是伦敦，逼着它去学会地理知识。这种对下一个词的预训练会逼着模型学到很多知识，而且这种知识进化到一定程度会展现出非常惊艳的效果。当然还有 BERT，BERT 也是一种 Transformer，最核心的差别在于 BERT 不是基于上文预测下一个词，而是把中间的一个词抠掉，用上文和下文一起预测出中间词。BERT 和 GPT 之间的差异体现在哪里？这微小的差异实际上反映了哲学上的不同思考方式。BERT 鼓励模型使用未来的信息来预测，而 GPT 则要求只使用历史信息，不能使用未来信息来预测下一个词。正是基于所有这些工作，现在我们可以看到这种压缩技术能够产生许多令人惊讶的智能表现。举一个例子，衔远打造的 ProductGPT 大模型执行的问答任务。ProductGPT 是通过压缩消费者与商品或服务之间的各种互动与世界知识，来更专业地学习理解商品和消费者。我认为这个答案能够很好地展示底层大模型具备的能力。背景是瑞幸和茅台联名推出了酱香拿铁，我们询问模型，它如何看待这一事件，茅台和咖啡的联名产品是否可能成为爆款，以及有哪些优势和风险。这个模型的训练早在瑞幸推出该产品之前就已经完成，因此模型对瑞幸的这次创新一无所知，问题和答案中都没有提及瑞幸，而是讨论咖啡和茅台的假设性的联合产品。能看出来它的所有分析都是基于大模型的内生能力，而不是使用搜索增强来总结执行助手任务，这体现了大模型已经扩展到完成战略预测和深度思考的任务。从表达的架构和逻辑来看，许多大模型在很多方面相似，但最关键的差别是专业性和由此带来的思考深度问题。