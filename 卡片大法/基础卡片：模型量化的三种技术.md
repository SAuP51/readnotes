

基础卡片：模型量化的三种技术2023-11-22内容：参考：ITStudy => 001大语言模型 => Video => 2023020卢菁大模型教程 => 0401卢菁-模型微调实战和经验分享原文：关于模型量化，其实这是一个非常简单的概念。例如，在以往，我构建的不定点树（应为浮点数结构），可能具有很大的数值范围，由于其占用的显存和比特率相对较高。接着，我会采取一种方式，强制使用一个 int8 类型的数据来表示它，即使用一个低精度的数据格式来表征一个高精度的数据。这个过程就是量化，即将一个浮点数（记作 R）转换成一个整数（记作 Q）。而反量化，正如其名，就是将整数再恢复为原先的浮点数。在这一转换过程中，大家可能注意到了一个四舍五入的操作，因此这一步实际上会导致所谓的量化误差。比如，一个数值原本是 1.25，经过量化再反量化后，返回的数据可能变成了 1.20，即精度损失了一些。虽然对于推理来说，这样的精度损失并不特别重要，但对于训练过程而言，影响则较大。针对量化技术，我们实际上有三种不同的方法。第一种，即 Post-Training Dynamic Quantization（简称 PDTQ），这种方法是直接对一个已经训练好的模型进行量化。这种方法的优势在于它非常简单，且运算量小。但其弊端在于，它只能量化模型参数本身，不能量化激活函数输出的结果。这是因为在模型未接收输入之前，我们无法知晓输入是什么，因此也就没办法对激活函数输出的数值进行有效且合理的量化，只能对模型权重进行量化。第二种方法，会用到一些测试数据进行量化。其过程中，首先用 FP32 格式（全精度浮点数）跑一些测试数据，其好处就在于可以将激活函数输出的结果也量化。因此，从理论上讲，第二种方法的量化效果会更好，因为它能量化更多的内容。第三种方式，则是 Quantization-Aware Training（QAT）。这种方法是在训练模型的时候就直接进行量化，将量化的过程直接嵌入到模型学习中。实际上，这种方法是效果最佳的，但其成本也最为昂贵。目前，我们正在应用第一种方法对模型进行量化。虽然我们也在探索第二种和第三种方法，但到目前为止还没有得到非常满意的结果。因此，我们目前依然采用第一种方法。这样一来，我们就可以提高 GPU 显存的利用率，并且整数运算的速度相比浮点数要快很多，各方面的推理速度也相应得到了提升。