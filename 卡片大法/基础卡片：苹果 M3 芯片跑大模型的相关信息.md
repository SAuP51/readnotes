

基础卡片：苹果 M3 芯片跑大模型的相关信息2023-12-13解释：参考：ITStduy => 001大语言模型 => Article => 20231101从大模型角度看苹果M3系列芯片原文：对于大模型的部署，算力设备的下面几个指标至关重要：显存；带宽；算力；功耗；框架；价格；咱们分别来看，首先是显存。对于大模型的部署来说，显存大真的太重要了，不然直接就是塞不下。苹果 M 系列芯片都采用了统一内存架构，当然 M3 也不会例外，M3 芯片支持的内存容量最高达 128 GB。这有多夸张呢，举个例子，拿今年 (2023 年 6 月 6 日) 发布的 M2 UItra 来说，它的最大内存甚至可以飙到足足 192 GB (这里因为双芯片拼接，AMD 也有这技术)，在苹果芯片的统一内存架构下，这 192 GB 内存就是 192 GB 显存，这意味着八张 4090、两张 H100 才能装得下的 AI 大模型，在 M2 Ultra 这一张芯片上就能够跑起来。那要是后面的 M3 Ultra 呢 (按现在 M3 最大 128 GB 来估算，M3 Ultra 甚至能够轻松突破 200+ GB)，想想是非常可怕。而这一切都是得益于苹果芯片的「统一内存架构」，简单点说就是内存能当显存用。拿 LLaMA-65B 来说，它有 650 亿参数，显存需求是 130 GB，这还真就能直接装得下。在大模型越来越流行的今天，苹果的这套架构就非常具有想象空间了。再说带宽，在苹果的统一内存架构中，CPU、GPU 和统一内存是直接通过硅介质层连在一起的，所以数据传输带宽非常之高，拿 M2 Ultra 来说，数据读写带宽达到了 800 GB/s。这就已经能够跟正儿八经的独立显卡的显存速度掰掰手腕了，拿 4090 来说，它的显存带宽速度是 1008 GB/s，这是在一个数量级上的。但也需要明确的是，比如 A100 的显存带宽就达到了 1935 GB/s、H100 的显存带宽甚至达到了 2TB/s，这种专业计算卡的带宽都是要比 M2 Ultra 快不少的。但要是拿 NvLink 的带宽 600 GB/s 来说，M2 Ultra 还是胜出的。对于很多 AI 同学还是拿 PCIE 进行数据传输的来说，真的是神仙打架...再说算力，算力这一块在苹果芯片上是比较迷的指标，而英伟达的显卡通常都会标的十分清晰明了，比如 H100 的 FP32 算力是 51 TFLOPS、Int8 算力是 1513 TOPS，比如 A100 的 FP32 算力是 19.5 TFLOPS、Int8 算力是 624 TOPS。这可能跟苹果和英伟达的产品矩阵不同有关，英伟达的主要产品就是显卡，而算力是显卡的必要参数；而苹果的产品不在于算力卡，所以强调的性能大多是结合产品表现的，比如在 Mac 上就是跑分。再说功耗，这个指标是苹果 M 芯片比较有优势的地方，由于本身的定位就是个人笔记本电脑产品，所以功耗设计不可能很高。最重要的，M3 芯片采用了目前最为先进的 3nm 工艺生产的，这意味着在同样的芯片面积上能够堆叠更加多的晶体管，能效比的提升会十分明显。再说框架/软件生态的支持，现在应该可以直接说最流行的深度学习框架就是 PyTorch 了吧，咱们进 PyTorch 的官网，其实 PyTorch 很早就有对 Mac M 芯片的支持。所以对于采用 PyTorch 进行训练、微调大模型的场景，迁移成本会很小。