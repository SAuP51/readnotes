

基础卡片：指令微调的三大效果2023-12-19解释：参考：selfstudy => 003Paper => 2023046A-Survey-of-Large-Language-Models原文：5.1.3 The Effect of Instruction Tuning In this part, we discuss the effect of instruction tuning on LLMs in three major aspects.Performance Improvement. Despite being tuned on a moderate number of instances, instruction tuning has become an important way to improve or unlock the abilities of LLMs [69]. Recent studies have experimented with language models in multiple scales (ranging from 77M to 540B), showing that the models of different scales can all benefit from instruction tuning [69, 345], yielding improved performance as the parameter scale increases [94]. Further, smaller models with instruction tuning can even perform better than larger models without fine-tuning [28, 69]. Besides the model scale, instruction tuning demonstrates consistent improvements in various model architectures, pre-training objectives, and model adaptation methods [69]. In practice, instruction tuning offers a general approach to enhancing the abilities of existing language models [69] (including small-sized PLMs). Also, it is much less costly than pretraining, since the amount of instruction data required by LLMs is significantly smaller than pre-training data.TABLE 8: Basic statistics of the required number of GPUs, tuning time, batch size (denoted as BS) per device (full tuning and LoRA tuning), and inference rate (the number of generated tokes per second). Our experiments are conducted based on two Linux servers having 8 A800-80G SXM4 GPUs with 6 NVSwitch and 8 3090-24G GPUs, respectively. The major difference between A800 and A100 lies in the NVLink interconnect speed. Thus, our estimations about training and inference efficiency would be slightly improved for A100, while the rest memory consumption would remain the same. For full tuning experiments, we use data parallel training, ZeRO Stage 3, BF16, and gradient checkpointing. Additionally, the LoRA tuning can be executed on one 80G GPU utilizing INT8 quantization with the rank setting set to 16. All the experiments are conducted with Alpaca-52K dataset by training LLaMA models three epochs. The max sequence length for both training settings is set to 512. The inference experiments are performed with the batch size set to 1.Task Generalization. Instruction tuning encourages the model to understand natural language instructions for task completion. It endows LLMs with the ability (often considered as an emergent ability) to follow human instructions [31] to perform specific tasks without demonstrations, even on unseen tasks [69]. A large number of studies have confirmed the effectiveness of instruction tuning to achieve superior performance on both seen and unseen tasks [95, 345]. Also, instruction tuning has been shown to be useful in alleviating several weaknesses of LLMs (e.g., repetitive generation or complementing the input without accomplishing a certain task) [66, 69], leading to a superior capacity to solve real-world tasks for LLMs. Furthermore, LLMs trained with instruction tuning can generalize to related tasks across languages. For example, BLOOMZ-P3 [94] is fine-tuned based on BLOOM [78] using English-only task collection P3 [167]. Interestingly, BLOOMZ-P3 can achieve a more than 50% improvement in multilingual sentence completion tasks compared to BLOOM, which shows that instruction tuning can help LLMs acquire general task skills from English-only datasets and transfer such skills into other languages [94]. In addition, it has been found that using English-only instructions can produce satisfactory results on multilingual tasks [94], which helps reduce the effort of instruction engineering for a specific language.Domain Specialization. Existing LLMs have showcased superior capabilities in traditional NLP tasks (e.g., generation and reasoning) and daily questions. However, they may still lack domain knowledge to accomplish specific tasks, such as medicine, law, and finance (See Section 8 for a detailed discussion of LLMs in different applications). Instruction tuning is an effective approach to adapting existing general LLMs to be domain-specific experts. For instance, researchers propose to fine-tune Flan-PaLM [69] using medical datasets to create Med-PaLM [356], a medical knowledge assistant that achieves performance levels comparable to those of expert clinicians. Furthermore, a recent study [357] fine-tunes FLAN-T5 to support e-commerce recommender systems with natural language instructions, showing strong performance in a variety of recommendation tasks. There are also several open-sourced medical models instructiontuned based on LLaMA [57], such as BenTsao [358]. Also, researchers explore instruction tuning on law [359], finance [360], and arithmetic computation [361].在这一部分中，我们讨论了对大型语言模型（LLMs）进行指令调整在三个主要方面的效果。1、性能提升。尽管是在适度数量的实例上进行调整，指令调整已成为一种重要的方式，用于提高或解锁 LLMs 的能力 [69]。近期的研究对不同规模（从 77M 到 540B）的语言模型进行了实验，显示不同规模的模型都能从指令调整中受益 [69, 345]，随着参数规模的增加，性能得到提升 [94]。此外，较小的模型经过指令调整后，甚至可以比没有进行微调的较大模型表现得更好 [28, 69]。除了模型规模外，指令调整在不同的模型架构、预训练目标和模型适应方法上都表现出了一致的改进 [69]。在实践中，指令调整提供了一种通用的方法来增强现有语言模型的能力 [69]（包括小型预训练语言模型 PLMs）。而且，与预训练相比，它的成本要低得多，因为 LLMs 所需的指令数据量远小于预训练数据量。表 8：所需 GPU 数量、调整时间、每设备批量大小（标记为 BS，包括完整调整和 LoRA 调整）以及推理速率（每秒生成的 token 数量）的基本统计数据。我们的实验是基于两台 Linux 服务器进行的，分别配备了 8 个 A800-80G SXM4 GPU 和 8 个 3090-24G GPU，它们的主要区别在于 NVLink 互连速度。因此，对于 A100，我们对训练和推理效率的估计会略有提高，而剩余的内存消耗则保持不变。对于完整调整实验，我们采用了数据并行训练、ZeRO 第 3 阶段、BF16 和梯度检查点技术。此外，LoRA 调整可以在单个 80G GPU 上执行，使用 INT8 量化，排名设置为 16。所有实验都是在 Alpaca-52K 数据集上进行的，通过训练 LLaMA 模型三个周期来完成。两种训练设置的最大序列长度均设置为 512。推理实验是以批量大小设置为 1 进行的。2、任务泛化。指令调整鼓励模型理解自然语言指令以完成任务。它赋予大型语言模型（LLMs）能力（通常被认为是新出现的能力），使其能够根据人类指令 [31] 执行特定任务，即使是未见过的任务 [69]。大量研究证实，指令调整在完成已见和未见任务上都能实现卓越的性能 [95, 345]。同时，指令调整已显示出有助于缓解 LLMs 的若干弱点（例如，重复生成或补充输入而不完成特定任务） [66, 69]，使 LLMs 具备解决现实世界任务的更强大能力。此外，经过指令调整训练的 LLMs 可以泛化到跨语言的相关任务上。例如，BLOOMZ-P3 [94] 是基于 BLOOM [78] 并使用仅英语的任务集 P3 [167] 进行微调的，与 BLOOM 相比，在多语言句子完成任务上可以实现 50% 以上的改进，这表明指令调整可以帮助 LLMs 从仅英语数据集中获取通用任务技能，并将这些技能转移到其他语言 [94]。此外，研究发现，使用仅英语指令可以在多语言任务上取得令人满意的结果 [94]，有助于减少为特定语言进行指令工程的工作量。3、领域专业化。现有 LLMs 在传统自然语言处理任务（例如生成和推理）以及日常问题上展示出卓越能力。然而，它们可能在完成特定领域任务（如医学、法律和金融）方面仍缺乏必要的知识（详见第 8 节 LLMs 在不同应用中的详细讨论）。指令调整是一种有效的方法，可以将现有的通用 LLMs 适配成特定领域的专家。例如，研究者提出使用医疗数据集对 Flan-PaLM [69] 进行微调，创建了医疗知识助理 Med-PaLM [356]，其性能与专业临床医生相当。此外，最近的一项研究 [357] 对 FLAN-T5 进行了微调，以支持带有自然语言指令的电子商务推荐系统，在各种推荐任务中表现出色。还有一些基于 LLaMA [57] 进行指令调整的开源医疗模型，例如 BenTsao [358]。研究人员还在法律 [359]、金融 [360] 和算术计算 [361] 等领域探索了指令调整。