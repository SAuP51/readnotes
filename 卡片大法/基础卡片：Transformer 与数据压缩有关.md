

基础卡片：Transformer 与数据压缩有关2023-11-27解释：参考：ITStduy => 001大语言模型 => Article => 20231127OpenAI大神首席科学家Ilya最新11月长篇访谈原文：我们一直在寻找。在某些时候，我们意识到，嘿，如果你训练一个大型神经网络，一个非常非常大的 Transformer 来越来越好地预测文本，就会发生一些非常令人惊讶的事情。这个认识也是一点点慢慢到来的。我们正在探索生成模型。我们正在探索有关下一个单词预测的想法。这些想法也与压缩有关。我们正在探索它们。Transformer 出来了。我们真的很兴奋。我们当时想，这是最伟大的事情。我们现在要做 Transformer。它显然比之前的任何东西都要优越。我们开始做 Transformer。我们做了 GPT-1。GPT-1 开始显示出非常有趣的生命迹象。这导致我们开发了 GPT-2。然后最终是 GPT-3。GPT-3 的出现确实令许多人大为震惊，这项技术的吸引力无疑是巨大的。目前，每个人都在尝试一个特定的公式，即在越来越多的数据上训练一个越来越大的 Transformer。对我个人而言，最大的觉醒时刻是从 GPT-2 过渡到 GPT-3，这一步骤中的功能和能力的提升是巨大的。然后，OpenAI 发布了一些非常有趣的文章，围绕着不同的知识领域、专业领域、思想链或模型，研究了一些可以突然以紧急形式完成的其他事情。