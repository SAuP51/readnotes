

基础卡片：GPT-3 的1750 亿参数是怎么算出来的2023-11-27解释：参考：ITStduy => 001大语言模型 => Article => 20231121Understanding-AI补充说明：需要结合原文中的图片一起看，就两层神经元：隐藏层和输出层。自下往上看，有12288 个输入值（向量维度）输入到每个神经元，一共49152个神经元，每个神经元又有12288个输出值。所以每层的参数：12288x49152+49152×12288。原文：After the attention heads transfer information between word vectors, there’s a feed-forward network3 that “thinks about” each word vector and tries to predict the next word. No information is exchanged between words at this stage: the feed-forward layer analyzes each word in isolation. However, the feed-forward layer does have access to any information that was previously copied by an attention head. Here’s the structure of the feed-forward layer in the largest version of GPT-3:The green and purple circles are neurons: mathematical functions that compute a weighted sum of their inputs.4What makes the feed-forward layer powerful is its huge number of connections. We’ve drawn this network with three neurons in the output layer and six neurons in the hidden layer, but the feed-forward layers of GPT-3 are much larger: 12,288 neurons in the output layer (corresponding to the model’s 12,288-dimensional word vectors) and 49,152 neurons in the hidden layer.So in the largest version of GPT-3, there are 49,152 neurons in the hidden layer with 12,288 inputs (and hence 12,288 weight parameters) for each neuron. And there are 12,288 output neurons with 49,152 input values (and hence 49,152 weight parameters) for each neuron. This means that each feed-forward layer has 49,152 * 12,288 + 12,288 * 49,152 = 1.2 billion weight parameters. And there are 96 feed-forward layers, for a total of 1.2 billion * 96 = 116 billion parameters! This accounts for almost two-thirds of GPT-3’s overall total of 175 billion parameters.在注意力头将信息在词向量之间传递之后，有一个前馈网络 3 将「思考」每个词向量并尝试预测下一个词。在这个阶段，词与词之间不交换信息：前馈层独立分析每个词。然而，前馈层确实可以访问之前由注意力头复制的任何信息。以下是 GPT-3 最大版本中前馈层的结构：绿色和紫色的圆圈是神经元：计算其输入的加权和的数学函数。4前馈层之所以强大，是因为它巨大的连接数。我们画的这个网络在输出层有三个神经元，在隐藏层有六个神经元，但 GPT-3 的前馈层要大得多：输出层有 12,288 个神经元（对应于模型的 12,288 维词向量）和隐藏层有 49,152 个神经元。因此，在 GPT-3 最大的版本中，隐藏层有 49,152 个神经元，每个神经元有 12,288 个输入（因此有 12,288 个权重参数）。还有 12,288 个输出神经元，每个神经元有 49,152 个输入值（因此有 49,152 个权重参数）。这意味着每个前馈层有 49,152 * 12,288 + 12,288 * 49,152 = 12 亿个权重参数。而且有 96 个前馈层，总共有 12 亿 * 96 = 1160 亿个参数！这几乎占了 GPT-3 总共 1750 亿参数的三分之二。