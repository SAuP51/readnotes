

术语卡片：大语言模型的Lora2024-05-12解释：参考：selfstudy => 003Paper => 2024004LoRA-Low-Rank-Adaption-of-Lanrge-Language-Models原文：We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the change in weights during model adaptation also has a low "intrinsic rank" , leading to our proposed Low-Rank Adaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers' change during adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3 175B as an example, we show that a very low rank (i.e., r in Figure 1 can be one or two) sufﬁces even when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient.LoRA possesses several key advantages.A pre-trained model can be shared and used to build many small LoRA modules for different tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the matrices A and B in Figure 1, reducing the storage requirement and task-switching overhead signiﬁcantly.LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters. Instead, we only optimize the injected, much smaller low-rank matrices.Our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by construction.LoRA is orthogonal to many prior methods and can be combined with many of them, such as preﬁx-tuning. We provide an example in Appendix E.我们从 Li 等人（2018a）和 Aghajanyan 等人（2020）的研究中得到启发，这些研究发现，尽管模型参数众多，但它们实际上存在于一个较低的维度空间中。基于这一发现，我们提出了一个假设：在模型调整过程中，权重的变化也遵循一个较低的「内在秩」规律。这一假设催生了我们的低秩适应（LoRA）方法。LoRA 方法的核心在于，它允许我们在不改变预训练模型权重的情况下，通过调整密集层变化的小型秩分解矩阵来训练神经网络的某些部分，这一过程在图 1 中有详细展示。以 GPT-3 175B 为例，我们证明了即使模型的全秩（d）高达 12,288，我们仍然可以使用非常低的秩（r，例如一或二）来实现高效的学习，这使得 LoRA 在存储和计算资源的使用上都极为高效。LoRA 方法的几个关键优势包括：预训练的模型可以被广泛共享，并用于为不同任务创建多个小型的 LoRA 模块。通过冻结共享模型，并简单地替换图 1 中的矩阵 A 和 B，我们能够快速地在不同任务之间切换，这大大减少了存储需求和任务切换所需的时间。LoRA 方法使得训练过程更加高效，并且通过减少对硬件的要求，使得使用自适应优化器的门槛降低了三倍。这是因为我们不需要为大部分参数计算梯度或维护优化器状态，而是专注于优化那些更小、更关键的低秩矩阵。我们的设计是基于简单的线性原理，这意味着在模型部署时，我们可以将可训练的矩阵与冻结的权重合并，从而在推理时不会引入任何延迟，这与完全微调的模型相比，性能是一致的。LoRA 方法与许多现有的技术是兼容的，并且可以与它们结合使用，例如前缀调优。我们在附录 E 中提供了一个结合使用的示例。Conclusion and Future WorkFine-tuning enormous language models is prohibitively expensive in terms of the hardware required and the storage/switching cost for hosting independent instances for different tasks. We propose LoRA, an efﬁcient adaptation strategy that neither introduces inference latency nor reduces input sequence length while retaining high model quality. Importantly, it allows for quick task-switching when deployed as a service by sharing the vast majority of the model parameters. While we focused on Transformer language models, the proposed principles are generally applicable to any neural networks with dense layers.There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning or LoRA is far from clear – how are features learned during pre-training transformed to do well on downstream tasks? We believe that LoRA makes it more tractable to answer this than full ﬁnetuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are there more principled ways to do it? 4) Finally, the rank-deﬁciency of ∆W suggests that W could be rank-deﬁcient as well, which can also be a source of inspiration for future works.在结论与未来工作的部分我们强调了微调大型语言模型所面临的挑战，包括高昂的硬件需求和为不同任务维护独立模型实例的存储与切换成本。为此，我们提出了 LoRA，这是一种高效的模型适应方法，它不会增加推理延迟，也不会缩短输入序列长度，同时保证了模型的质量。LoRA 的另一个重要优势是，在作为服务部署时，它允许快速切换不同的任务，同时共享大部分模型参数，从而提高了灵活性和效率。尽管我们的研究主要集中在 Transformer 语言模型上，但我们提出的 LoRA 原则同样适用于任何包含密集层的神经网络，具有广泛的适用性。未来的研究有许多可能的方向。1）LoRA 技术可以与其他的优化技术相结合，这种结合可能会带来额外的性能提升，就像不同角度的光线汇聚在一起，照亮了问题的不同方面。2）尽管微调或 LoRA 技术背后的工作原理尚未完全明了，但我们知道，预训练阶段学习到的知识是如何被巧妙地转化，以适应各种特定任务的。LoRA 技术提供了一种更为简洁的方式来探索这一过程，就像用一把精巧的钥匙打开了知识转化的大门。3）目前，我们选择应用 LoRA 的权重矩阵时，主要依赖于经验法则。我们想知道，是否存在更为科学的方法来指导这一选择，就像寻找一把更精确的尺子来衡量知识的深度。4）最后，∆W 矩阵的秩不足现象提示我们，原始的 W 矩阵也可能存在秩不足的情况。这一发现可能成为未来研究的新起点，就像在知识的海洋中发现了一片新的岛屿，等待我们去探索。