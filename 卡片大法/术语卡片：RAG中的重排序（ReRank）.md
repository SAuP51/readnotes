

术语卡片：RAG中的重排序（ReRank）2024-05-13解释：参考：selfstudy => 003Paper => 2023077Retrieval-Augmented-Generation-for-Large-Language-Models-A-Survey原文：After retrieving valuable context from the database, merging it with the query for input into LLM poses challenges. Presenting all relevant documents to the LLM at once may exceed the context window limit. Concatenating numerous documents to form a lengthy retrieval prompt is ineffective, introducing noise and hindering the LLM's focus on crucial information. Additional processing of the retrieved content is necessary to address these issues.• ReRank: Re-ranking to relocate the most relevant information to the edges of the prompt is a straightforward idea. This concept has been implemented in frameworks such as LlamaIndex, LangChain, and HayStack [ Blagojevi, 2023 ] . For instance, Diversity Ranker prioritizes reordering based on document diversity, while LostInTheMiddleRanker alternates placing the best document at the beginning and end of the context window. Simultaneously, addressing the challenge of interpreting vector-based simulated searches for semantic similarity, approaches like cohereAI rerank [ Cohere, 2023 ] , bgererank 5 , or LongLLMLingua [ Jiang et al., 2023a ] recalculate the semantic similarity between relevant text and query.• Prompt Compression Research indicates that noise in retrieved documents adversely affects RAG performance. In post-processing, the emphasis lies in compressing irrelevant context, highlighting pivotal paragraphs, and reducing the overall context length. Approaches such as Selective Context [ Litman et al., 2020 ] and LLMLingua [ Anderson et al., 2022 ] utilize small language models to calculate prompt mutual information or perplexity, estimating element importance. However, these methods may lose key information in RAG or long-context scenarios. Recomp [ Xu et al., 2023a ] addresses this by training compressors at different granularities. Long Context [ Xu et al., 2023b ] , in dealing with extensive contexts, decomposes and compresses, while "Walking in the Memory Maze" [ Chen et al., 2023a ] designs a hierarchical summarization tree to enhance LLM's key information perception.检索后处理流程在从数据库中检索出有价值的上下文后，将其与查询内容合并输入到大语言模型（LLM）会遇到挑战。一次性向大语言模型展示所有相关文档可能会超出其处理的上下文窗口限制。将多个文档拼接成一个冗长的检索提示不仅效率低，还会引入噪声，影响大语言模型聚焦关键信息。因此，需要对检索到的内容进行额外处理，以解决这些问题。1、ReRank（重新排序）：重新排序，将最相关的信息置于提示的前后边缘，是一个简单直接的方法。这一思路已在如 LlamaIndex、LangChain 和 HayStack 等框架中得到应用 [Blagojevi, 2023]。例如，Diversity Ranker 会根据文档的多样性进行重新排序，而 LostInTheMiddleRanker 则会交替地将最佳文档放在上下文窗口的开始和结束位置。同时，为了应对基于向量的语义相似度模拟搜索的挑战，方法如 cohereAI rerank [Cohere, 2023]、bgererank5 或 LongLLMLingua [Jiang et al., 2023a]，会重新计算相关文本与查询之间的语义相似度。2、Prompt 压缩：研究显示，检索文档中的噪音会对 RAG 性能产生不利影响。在处理的后期阶段，我们主要关注于压缩无关紧要的上下文，凸显关键段落，并缩短整体的上下文长度。例如 Selective Context [Litman et al., 2020] 和 LLMLingua [Anderson et al., 2022] 等方法，它们运用小型语言模型来计算提示之间的互信息或困惑度，以此估算各个元素的重要性。不过，在 RAG 或者长篇上下文的情境中，这些方法可能会遗失关键信息。Recomp [Xu et al., 2023a] 通过训练不同精细程度的压缩器来应对这一问题。在处理长篇上下文[Xu et al., 2023b]时，这种方法通过分解和压缩来处理大量的上下文内容，而「在记忆迷宫中漫步」[Chen et al., 2023a]则设计了一个分层次的总结树来增强大语言模型（LLM）对关键信息的感知能力。