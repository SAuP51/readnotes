

基础卡片：大模型的思维链提示词何时生效2023-12-24解释：参考：selfstudy => 003Paper => 2023046A-Survey-of-Large-Language-Models原文：When CoT Prompting Works For LLMs? Since CoT reasoning is an emergent ability [31], it only has a positive effect on sufficiently large models (typically containing 10B or more parameters [33]) but not on small models. Moreover, 57since CoT prompting augments the standard prompting with intermediate reasoning steps, it is mainly effective for the tasks that require step-by-step reasoning [33], e.g., arithmetic reasoning, commonsense reasoning, and symbolic reasoning. Whereas, for other tasks that do not rely on complex reasoning, CoT prompting might lead to worse performance than standard prompting [438], e.g., MNLIm/mm, SST-2, and QQP from GLUE [260]. Interestingly, it seems that the performance gain brought by CoT prompting could be significant only when standard prompting yields poor results [33].01 CoT 提示何时对 LLMs 有效？由于 CoT 推理是一种涌现能力[31]，它仅在足够大的模型上有效（通常包含 10B 或更多参数[33]），而对小型模型则不适用。此外，由于 CoT 提示在标准提示的基础上增加了中间推理步骤，它主要对需要逐步推理的任务有效[33]，例如算术推理、常识推理和符号推理。然而，对于不依赖复杂推理的其他任务，CoT 提示可能导致性能不如标准提示[438]，例如 GLUE [260]中的 MNLIm/mm、SST-2 和 QQP。有趣的是，当标准提示的结果不佳时，CoT 提示带来的性能提升似乎才会显著[33]。