

基础卡片：大模型推理过程占用显存分析2023-12-26解释：参考：ITStudy => 001大语言模型 => Article => 20231218分析transformer模型的参数量计算量中间激活KVcache原文：在神经网络的推理阶段，没有优化器状态和梯度，也不需要保存中间激活。少了梯度、优化器状态、中间激活，模型推理阶段占用的显存要远小于训练阶段。模型推理阶段，占用显存的大头主要是模型参数，如果使用 float16 来进行推理，推理阶段模型参数占用的显存大概是 2Φ bytes 。如果使用 KV cache 来加速推理过程，KV cache 也需要占用显存，KV cache 占用的显存下文会详细介绍。此外，输入数据也需要放到 GPU 上，还有一些中间结果（推理过程中的中间结果用完会尽快释放掉），不过这部分占用的显存是很小的，可以忽略。在推断阶段，transformer 模型加速推断的一个常用策略就是使用 KV cache。一个典型的大模型生成式推断包含了两个阶段：1、预填充阶段：输入一个 prompt 序列，为每个 transformer 层生成 key cache 和 value cache（KV cache）。2、解码阶段：使用并更新 KV cache，一个接一个地生成词，当前生成的词依赖于之前已经生成的词。假设输入序列的长度为 s，输出序列的长度为 n，以 float16 来保存 KV cache，那么 KV cache 的峰值显存占用大小为 b(s+n)h * l * 2 * 2 = 4blh(s+n) 。这里第一个 2 表示 K/V cache，第二个 2 表示 float16 占 2 个 bytes。以 GPT3 为例，对比 KV cache 与模型参数占用显存的大小。GPT3 模型占用显存大小为 350GB。假设批次大小 b=64，输入序列长度 s=512，输出序列长度 n=32，则 KV cache 占用显存为 4blh(s+n)=164,282,499,072bytes≈164GB，大约是模型参数显存的 0.5 倍。