# 
> 2018000模板




Sixteen


			Biology, the Criminal Justice System, and (Oh, Why Not?) Free Will*



DON’T FORGET TO CHECK THEIR TEAR DUCTS


			Some years back a foundation sent a letter to various people, soliciting Big Ideas for a funding initiative of theirs. The letter said something along the lines of “Send us a provocative idea, something you’d never propose to another foundation because they’d label you crazy.”

			That sounded fun. So I sent them a proposal titled “Should the Criminal Justice System Be Abolished?” I argued that the answer was yes, that neuroscience shows the system makes no sense and they should fund an initiative to accomplish that.

			“Ha-ha,” they said. “Well, we asked for it. That certainly caught our attention. That’s a great idea to focus on interactions between neuroscience and the law. Let’s do a conference.”

			So I went to a conference with some neuroscientists and some legal types—law professors, judges, and criminologists. We learned one another’s terminology, for example seeing how we neuroscientists and the legal people use “possible,” “probable,” and “certainty” differently. We discovered that most of the neuroscientists, including me, knew nothing about the workings of the legal world, and that most of the legal folks had avoided science since being traumatized by ninth-grade biology. Despite the two-culture problem, all sorts of collaborations got started there, which eventually grew into a network of people studying “neurolaw.”

			Fun, stimulating, interdisciplinary hybrid vigor. And frustrating to me, because I kind of meant the title of the proposal that I had written. The current criminal justice system needs to be abolished and replaced with something that, while having some broad features in common with the current system,* would have utterly different underpinnings. Which I’m going to try to convince you of. And that’s just the first part of this chapter.

			—

			You can’t be less controversial than stating that the criminal justice system needs reform and that this should involve more science and less pseudoscience in the courtroom. If nothing else, consider this: according to the Innocence Project, nearly 350 people, a mind-boggling 20 of them on death row, imprisoned an average of fourteen years, have been exonerated by DNA fingerprinting.1

			Despite that, I’m going to mostly ignore criminal justice reform by science. Here are some hot-button topics in that realm that I’m going to bypass entirely:


What to do about the power and ubiquity of automatic, implicit biases (leading to, for example, juries meting out harsher decisions to African American defendants with darker skin). Should Implicit Association Tests be used in jury selection to eliminate people with strong, pertinent biases?

				Whether neuroimaging information regarding a defendant’s brain should be admissible in a courtroom.2 This has grown less contentious as neuroimaging has transitioned from revolutionary to a standard approach in science’s tool kit. But there remains the issue of whether juries should be shown actual neuroimages—the worry is that nonexperts are readily overly impressed with exciting, color-enhanced Pictures of the Brain (it’s turning out to be less of an issue than feared).

				Whether neuroimaging data regarding someone’s veracity should have a place in the courtroom (or in the workplace regarding security clearances). Basically, I know of no expert who thinks the technique is sufficiently accurate. Nonetheless, there are entrepreneurs selling the approach (including, I kid you not, a company called No Lie MRI). This issue extends to lower-tech but equally unreliable versions of is-that-brain-lying? This includes electroencephalograms (EEGs), which are admissible in Indian courtrooms.3

				What should be the IQ cutoff for someone to be smart enough to be executed? The standard is an IQ of 70 or higher, and debate concerns whether it should be an average of 70 across multiple IQ tests, or if achieving that magic number even once qualifies you for being executed. This issue pertains to about 20 percent of people on death row.4

				What to do with the fact that scientific findings can generate new types of cognitive biases in jurors. For example, the belief that schizophrenia is a biological disorder makes jurors less likely to convict schizophrenics for their actions but more likely to view them as more incurably dangerous.5

				The legal system distinguishes between thoughts and actions; what to do as neuroscience increasingly reveals the former. Are we approaching precrime detection, predicting who will commit a crime? In the words of one expert, “We’re going to have to make a decision about the skull as a privacy domain.”6

				And of course there’s that problem of judges judging more harshly when their stomachs are gurgling.*7



			—

			All of these are important issues, and I think reforms are needed at the intersection of progressive politics, civil liberties, and tough standards about new science. In other words, a standard liberal agenda. Most of the time I’m a clichéd card-carrying liberal; I even know the theme songs to many of NPR’s programs. Nonetheless, this chapter won’t take anything resembling a liberal approach to reforming criminal justice. The reason why is summarized in the following example of a classically liberal approach to a legal issue.

			It’s the middle of the 1500s. Perhaps because of lax societal standards and people being morally deprived and/or depraved, Europe is overrun with witches. It’s a huge problem—people fear going out at night; polls show that peasants-in-the-street list “witches” as more of a threat than “the plague” or “the Ottomans”; would-be despots gain supporters by vowing to be tough on witches.

			Fortunately, there are three legal standards for deciding if someone is guilty of witchcraft:8


The flotation test. Since witches reject the sacrament of baptism, water will reject their body. Take the accused, bound, and toss them into some water. If they float, they’re a witch. If they sink, they’re innocent. Quickly now, retrieve innocent person.

				The devil’s-spot test. The devil enters someone’s body to infect them with witch-ness, and that point of entry is left insensitive to pain. Systematically do something painful to every spot on the accused’s body. If some spot is much less sensitive to pain than the rest, you’ve found a devil’s spot and identified a witch.

				The tear test. Tell the accused the story of the crucifixion of Our Lord. Anyone not moved to tears is a witch.



			These well-established criteria allow authorities fighting this witch wave to identify and suitably punish thousands of witches.

			In 1563 a Dutch physician named Johann Weyer published a book, De Praestigiis Daemonum, advocating reform of the witch justice system. He, of course, acknowledged the malign existence of witches, the need to punish them sternly, and the general appropriateness of witch-fighting techniques like those three tests.

			However, Weyer aired an important caveat pertinent to older female witches. Sometimes, he noted, elderly people, especially women, have had atrophy of their lachrymal glands, making it impossible to cry tears. Uh-oh—this raises the specter of false convictions of people as witches. The concerned, empathic Weyer counseled, “Make sure you’re not torching some poor elderly person simply because her tear ducts don’t work anymore.”

			Now that’s a liberal reform of the witch justice system, imposing some sound thinking in one tiny corner of an irrational edifice. Much like what scientifically based reform of our current system does, which is why something more extreme is needed.*





THREE PERSPECTIVES


			Let’s get down to cases. There are three ways of viewing the place of biology in making sense of our behaviors, criminal or otherwise:


We have complete free will in our behavior.

				We have none.

				Somewhere in between.



			If people are forced to carefully follow the logical extensions of their views, probably less than a thousandth of a percent would support the first proposition. Suppose someone convulsing with a grand mal epileptic seizure, flinging their arms around, strikes someone. If you truly believe we freely control our behavior, you must convict them of assault.

			Virtually everyone considers that absurd. Yet that legal outcome would have occurred half a millennium ago in much of Europe.9 That seems ludicrous because in the last few centuries the West has crossed a line and left it so far behind that a world on the other side is unimaginable. We embrace a concept that defines our progress—“It’s not him. It’s his disease.” In other words, at times biology can overwhelm anything resembling free will. This woman didn’t bump into you maliciously; she’s blind. This soldier standing in formation didn’t pass out because he doesn’t have what it takes; he’s diabetic and needs his insulin. This woman isn’t heartless because she didn’t help the elderly person who had fallen; she’s paralyzed from a spinal cord injury. Similar shifts in the perception of criminal responsibility have occurred in other realms. For example, from two to seven centuries ago, prosecution of animals, objects, and corpses thought to have intentionally harmed someone was commonplace. Some of these trials had a weirdly modern tint to them—in a 1457 trial of a pig and her piglets for eating a child, the pig was convicted and executed, whereas the piglets were found to be too young to have been responsible for their acts. Whether the judge cited the maturational state of their frontal cortices is unknown.

			Thus hardly anyone believes that we have complete conscious control over our behavior, that biology never constrains us. We’ll ignore this stance forever after.





DRAWING LINES IN THE SAND


			Nearly everyone believes in the third proposition, that we are somewhere between complete and no free will, that this notion of free will is compatible with the deterministic laws of the universe as embodied in biology. Only a subset of versions of this view fit the fairly narrow philosophical stance called “compatibilism.” Instead this broader view is that we have something resembling a spirit, a soul, an essence that embodies our free will, from which emanates behavioral intent; and that this spirit coexists with biology that can sometimes constrain it. It’s a kind of libertarian dualism (“libertarian” in the philosophical rather than political sense), what Greene calls “mitigated free will.” It’s encapsulated in the idea that well-intentioned spirit, while willing, can be thwarted by flesh that is sufficiently weak.

			—

			Let’s start with the definitive legal framing of mitigated free will.

			In 1842 a Scotsman named Daniel M’Naghten tried to assassinate British prime minister Robert Peel.10 He mistook Peel’s private secretary, Edward Drummond, for the prime minister and shot him at close range, killing him. At his arraignment M’Naghten stated, “The Tories in my native city have compelled me to do this. They follow and persecute me wherever I go, and have entirely destroyed my peace of mind. They followed me to France, into Scotland . . . wherever I go. I cannot get no rest from them night or day. I cannot sleep at night. . . . I believe they have driven me into a consumption. I am sure I shall never be the man I formerly was. . . . They wish to murder me. It can be proved by evidence. . . . I was driven to desperation by persecution.”

			In today’s terminology M’Naghten had some form of paranoid psychosis. It may not have been schizophrenia—his delusional symptoms started many years later than the typical age of onset of the disease. Regardless of the diagnosis, M’Naghten had abandoned his business and spent the previous two years wandering Europe, hearing voices, convinced that he was being spied upon and persecuted by powerful people, with Peel his most diabolical tormentor. In the words of a doctor who testified as to his insanity, “The delusion was so strong that nothing but a physical impediment could have prevented him from committing the act [i.e., murder].” M’Naghten was so clearly impaired that the prosecution withdrew criminal charges, agreeing with the defense that he was insane. The jury agreed, and M’Naghten spent the rest of his life in insane asylums, reasonably well treated by the standards of the time.

			There was bellowing protest after the jury’s decision, ranging from the man in the street to Queen Victoria—M’Naghten had gotten away with murder. The presiding judge was grilled by Parliament and stood by the decision. The equivalent of the Supreme Court was tasked by Parliament with assessing the case and supported him. And out of the decision came the formalization of what is now the common criterion for finding someone innocent by reason of insanity, namely the “M’Naghten rule”: if, at the time of the crime, the person is so “laboring under such a defect of reason from disease of the mind,” that he cannot distinguish right from wrong.*

			The M’Naghten rule was at the core of John Hinckley Jr. being found not guilty for reasons of insanity in his attempted assassination of Reagan in 1981, being hospitalized rather than jailed. There was considerable “He’s getting away with it” outrage afterward; a number of states banned the M’Naghten criterion, and Congress essentially banned it for federal cases with the 1984 Insanity Defense Reform Act.* Nonetheless, the reasoning behind M’Naghten has generally withstood the test of time.

			This is the essence of a stance of mitigated free will—people need to be held responsible for their actions, but being floridly psychotic can be a mitigating circumstance. It is the idea that there can be “diminished” responsibility for our actions, that something can be semivoluntary.

			Here’s how I’ve always pictured mitigated free will:

			There’s the brain—neurons, synapses, neurotransmitters, receptors, brain-specific transcription factors, epigenetic effects, gene transpositions during neurogenesis. Aspects of brain function can be influenced by someone’s prenatal environment, genes, and hormones, whether their parents were authoritative or their culture egalitarian, whether they witnessed violence in childhood, when they had breakfast. It’s the whole shebang, all of this book.

			And then, separate from that, in a concrete bunker tucked away in the brain, sits a little man (or woman, or agendered individual), a homunculus at a control panel. The homunculus is made of a mixture of nanochips, old vacuum tubes, crinkly ancient parchment, stalactites of your mother’s admonishing voice, streaks of brimstone, rivets made out of gumption. In other words, not squishy biological brain yuck.

			And the homunculus sits there controlling behavior. There are some things outside its purview—seizures blow the homunculus’s fuses, requiring it to reboot the system and check for damaged files. Same with alcohol, Alzheimer’s disease, a severed spinal cord, hypoglycemic shock.

			There are domains where the homunculus and that brain biology stuff have worked out a détente—for example, biology is usually automatically regulating your respiration, unless you must take a deep breath before singing an aria, in which case the homunculus briefly overrides the automatic pilot.

			But other than that, the homunculus makes decisions. Sure, it takes careful note of all the inputs and information from the brain, checks your hormone levels, skims the neurobiology journals, takes it all under advisement, and then, after reflecting and deliberating, decides what you do. A homunculus in your brain, but not of it, operating independently of the material rules of the universe that constitute modern science.

			That’s what mitigated free will is about. I see incredibly smart people recoil from this and attempt to argue against the extremity of this picture rather than accept its basic validity: “You’re setting up a straw homunculus, suggesting that I think that other than the likes of seizures or brain injuries, we are making all our decisions freely. No, no, my free will is much softer and lurks around the edges of biology, like when I freely decide which socks to wear.” But the frequency or significance with which free will exerts itself doesn’t matter. Even if 99.99 percent of your actions are biologically determined (in the broadest sense of this book), and it is only once a decade that you claim to have chosen out of “free will” to floss your teeth from left to right instead of the reverse, you’ve tacitly invoked a homunculus operating outside the rules of science.

			This is how most people accommodate the supposed coexistence of free will and biological influences on behavior.* For them, nearly all discussions come down to figuring what our putative homunculus should and shouldn’t be expected to be capable of. To get a feel for that, let’s look at some of these debates.





Age, Maturity of Groups, Maturity of Individuals


			In 2005’s Roper v. Simmons decision, the Supreme Court ruled that you can’t execute someone for a crime committed before the age of eighteen. The appropriate reasoning was straight out of chapters 6 and 7: the brain, especially the frontal cortex, is not yet at adult levels of emotional regulation and impulse control. In other words, adolescents, with their adolescent brains, aren’t as culpable as adults. The reasoning was the same as why the pig was executed but not her piglets.

			In the years since, there have been related rulings. In 2010’s Graham v. Florida and 2012’s Miller v. Alabama, the Court emphasized that juvenile offenders have the highest potential for reform (because of their still-developing brains) and thus banned life sentences without parole for them.

			These decisions have prompted a number of debates:


Just because adolescents are, on the average, less neurobiologically and behaviorally mature than adults doesn’t rule out the possibility of some individual adolescents being as mature, thus being appropriately held to adult standards of culpability. Related to that is the obvious absurdity of implying that something neurobiologically magical happens on the morning of someone’s eighteenth birthday, endowing them with adult levels of self-control. The usual response to these points is that, yes, these are true, but the law often relies on group-level attributes with arbitrary age boundaries (e.g., the age at which someone can vote, drink, or drive). There is this willingness because you can’t test every teenager each year, month, hour, to determine whether they are mature enough yet to, say, vote. But it’s worth doing so when it comes to a teenage murderer.

				In another contrarian view the issue isn’t whether a seventeen-year-old is as mature as an adult but whether they are mature enough. Sandra Day O’Connor, in dissenting from the Roper decision, wrote, “The fact that juveniles are generally less culpable for their misconduct than adults does not necessarily mean that a 17-year-old murderer cannot be sufficiently culpable to merit the death penalty” (her emphasis). Another dissenter, the late Antonin Scalia, wrote that it is “absurd to think that one must be mature enough to drive carefully, to drink responsibly, or to vote intelligently, in order to be mature enough to understand that murdering another human being is profoundly wrong.”11



			Amid these differing opinions everyone, including O’Connor and Scalia, agrees that there exist age-related boundaries on free will—everyone’s homunculus was once too young to have its adult powers.12 Maybe it wasn’t tall enough yet to reach all the control dials; maybe it was distracted from its job by fretting about that gross pimple on its forehead. And that needs to be considered during legal judgments. Just as with piglets and pigs, it’s just an issue of when a homunculus is old enough.





The Nature and Magnitude of Brain Damage


			Essentially everyone working with a model of mitigated free will accepts that if there is enough brain damage, responsibility for a criminal act goes out the window. Even Stephen Morse of the University of Pennsylvania, a strident critic of neuroscience in the courtroom (much more later), concedes, “Suppose we could show that the higher deliberative centers in the brain seem to be disabled in these cases. If these are people who cannot control episodes of gross irrationality, we’ve learned something that might be relevant to the legal ascription of responsibility.”13 In this view, mitigating biological factors are relevant if the capacity for reasoning has been grossly impaired.

			Thus, if someone had their entire frontal cortex destroyed, you probably shouldn’t hold them responsible for their actions, because their rationality is grossly impaired when deciding their own courses of action.14 But the issue then becomes where a line is drawn on a continuum—what if 99 percent of the frontal cortex is destroyed? What if 98 percent? This is of great practical importance, since a large percentage of death row inmates have a history of damage to the frontal cortex, particularly of the most disabling type, namely early in life.

			In other words, amid differing opinions about where a line should be drawn, believers in mitigated free will agree that massive amounts of brain damage overwhelm a homunculus, while it should be expected to handle at least some damage.





Responsibility at the Level of the Brain and at the Social Level


			The renowned neuroscientist Michael Gazzaniga, one of the leading lights and elders of the field, has taken an extremely odd path in writing, “Free will is an illusion, but you’re still responsible for your actions.” This is expounded at length in a challenging book of his, Who’s in Charge? Free Will and the Science of the Brain. Gazzaniga fully accepts the entirely material nature of the brain but nonetheless sees room for responsibility. “Responsibility exists at a different level of organization: the social level, not in our determined brains.” I think either he is actually saying, “Free will is an illusion, but for practical reasons, we are still going to hold you responsible for your actions,” or he is hypothesizing some manner of homunculus that exists only at a social level. In response to the latter idea, the pages of this book show how our social world is ultimately as much a product of our determined, materialist brains as are our simple motor movements.*15





The Time Course of Decision Making


			Another well-established fault line in a stance of mitigated free will is that our capacity for free will moves to the forefront with decisions that are slow and deliberative, whereas biological factors may push free will aside in split-second-decision situations. In other words, the homunculus is not always sitting right at the helm in the bunker; instead it occasionally wanders off to grab a snack, and if something exciting suddenly arises, those neurons may fire off commands to muscles and produce a behavior before the homunculus can rush back and hit that big red button on the control panel.

			Issues of getting to the red button in time intersect with issues of the adolescent brain. A number of critics of Roper v. Simmons, starting with O’Connor in her dissenting opinion, noted a seeming contradiction. The American Psychological Association (APA) had filed an amicus curiae brief in the case, emphasizing that adolescents (i.e., their brains) are so immature that they can’t be held to adult criminal standards with sentencing. Turns out that the same APA had filed a brief some years earlier in a different case, emphasizing that adolescents are sufficiently mature that they should be able to choose whether to have an abortion, even without parental consent.

			Well, that’s a bit awkward, and it sure makes the APA and its ilk appear to be flip-flopping along ideological grounds, O’Connor charged. Laurence Steinberg, whose research on adolescent brain development was covered heavily in chapter 7 (and whose work was influential in the Roper v. Simmons decision), offers a logical resolution.16 Deciding whether to have an abortion involves logical reasoning about moral, social, and interpersonal issues, stretching out over days to weeks. In contrast, deciding whether to, say, shoot someone can involve issues of impulse control over the course of seconds. The frontal immaturity of the adolescent brain is more pertinent to split-second issues of impulse control than to slow, deliberative reasoning processes. Or in a mitigated-free-will framework, rapid-fire, impulsive behaviors can occur while the homunculus has gone to the bathroom.





Causation and Compulsion


			Some proponents of mitigated free will distinguish between the concepts of “causation” and “compulsion.”17 In a way that feels a bit nebulous, the former involves every behavior having been caused by something, of course, but the latter reflects only a subset of behaviors being really, really caused by something, something that compromises rational, deliberative processes. In this view some behaviors are more deterministically biological than others.

			This has been relevant to schizophrenic delusions. Suppose someone suffering from schizophrenia has auditory hallucinations, including a voice telling him to commit a crime; he does so.

			Some courts have viewed this as not mitigating. If your friend suggests that you mug someone, the law expects you to resist, even if it’s an imaginary friend in your head.

			But others see distinctions depending on qualities of the auditory hallucinations. In that view, if a schizophrenic individual commits a crime because a voice in his head demanded it, yes, his act was caused by that voice, but that doesn’t excuse the crime. In contrast, consider a schizophrenic individual committing a crime because thundering choruses of taunting, threatening, cajoling voices in his head, complete with baying hellhounds and choirs of trombones playing loud atonal music, command him every waking moment to do the crime. When he succumbs and does so, it is deemed more excusable, because those voices constituted a compulsion to act.*

			Thus in this view even a sensible homunculus can lose it and agree to virtually anything, just to get the hellhounds and trombones to stop.





Starting a Behavior Versus Halting It


			It is virtually ordained that any discussion of volition and biology eventually considers the “Libet experiment.”18 In the 1980s neuroscientist Benjamin Libet of UCSF reported something fascinating. A subject is hooked up to an EEG machine, which monitors patterns of electrical excitation in the brain. She sits quietly, looking at a clock. She has been instructed to flick her wrist whenever she feels like it and to note the time, down to the second, when she decided to do so.

			Libet would identify something in the EEG data called a “readiness potential”—a signal from the motor cortex and supplementary premotor areas that a movement would soon be initiated. And consistently, readiness potentials appeared about half a second before the reported time of conscious intent to move. Interpretation: your brain “decided” to move before you were even aware of it. Thus, how can you claim to have chosen when to move, evidence of free will, if the cascade of neural signaling culminating in movement started before you consciously chose? Free will is an illusion.

			Naturally, this finding generated speculation, controversy, replications, elaborations, refutations, and nuances that are beyond me. One criticism concerned a necessary limitation of the approach. In this view, there’s free will, you freely decide when to move your wrist, and that readiness potential is a consequence of your decision. What’s the five-hundred-millisecond delay, in that case? That’s the lag time between the instant when the decision to move first occurs and when (a) attention is then focused on the clock and (b) the position of the second hand is interpreted. In other words, the supposed half-second lag is an artifact of the experimental design, not a real thing. Other criticisms concerned the ambiguity of feeling that you intend to move. Other criticisms are more arcane than I can follow.

			A very different interpretation of the finding was offered, interestingly, by Libet. Yes, maybe your brain prepares to initiate a behavior before there is conscious awareness of the decision, meaning that your belief that you consciously chose to move is wrong. But in that lag time is the potential to consciously choose to veto that action. In the pithy words of V. S. Ramachandran (of mirror neuron speculation in chapter 14), we may not have free will, but we have “free won’t.”19

			Predictably, this intriguing counterinterpretation has fueled more discussions, experiments, and counter-counterinterpretations. For us, surveying different disputes concerning mitigated free will, this entire debate is about the nature of a homunculus’s control panel. How many of its buttons and switches and dials that go up to eleven are involved in initiating a behavior versus halting it?

			—

			Thus a view of mitigated free will makes room for both biological causation of behavior and free will, and all the discussions merely concern where lines in the sand are drawn and how inviolate they are. This prepares us to consider what I think is the most important line-drawing debate.





“You Must Be So Smart” Versus “You Must Have Worked So Hard”


			Stanford psychologist Carol Dweck has done groundbreaking work on the psychology of motivation. In the late 1990s she reported something important. Kids do a task, take a test, something, where they do it well. You then praise them in one of two ways—“What a great score; you must be so smart” or “What a great score, you must have worked so hard.” When you praise kids for working hard, they tend to work harder the next time, show more resilience, enjoy the process more, and become more likely to value the accomplishment for its own sake (rather than for the grade). Praise kids for being smart, and precisely the opposite occurs. When it becomes all about being smart, effort begins to seem suspect, beneath you—after all, if you’re really so smart, you shouldn’t have to work hard; you glide, you don’t sweat and grunt.20

			Beautiful work that has achieved cult status among many thoughtful parents of gifted kids, who want to understand when their child’s smarts shouldn’t come into the picture.

			Why do “You’re so smart” and “You work so hard” have such different effects? Because they fall on either side of one of the deepest lines drawn by believers in mitigated free will. It is the belief that one assigns aptitude and impulse to biology and effort and resisting impulse to free will.

			It’s cool to see natural ability in action. The great all-around athlete who has never seen pole-vaulting before, watches it once, tries it once, and soars like a pro. Or the singer whose voice has always had a natural timbre that evokes emotions you never knew existed. Or that student in your class who obviously just gets it, two seconds into your explaining something really abstruse.

			That’s impressive. But then there’s inspiring. When I was a kid, I repeatedly read a book about Wilma Rudolph. She was the fastest female runner in the world in 1960, an Olympian who became a civil rights pioneer. Definitely impressive. But consider that she was born prematurely, underweight, one of twenty-two kids in a poor Tennessee family, and—get this—at age four got polio, resulting in a leg brace and a twisted foot. Polio, she was crippled by polio. And she defied every expert’s expectations, worked and worked and worked through the pain, and became the fastest there was. That’s inspiring.

			In many domains we can sort of grasp the materialist building blocks of natural ability. Someone has the optimal ratio of slow-twitch to fast-twitch muscle fibers, producing a natural pole-vaulter. Or has vocal cords with the perfect degree of velvety peach fuzz (I’m winging it here) to produce an extraordinary voice. Or the ideal combination of neurotransmitters, receptors, transcription factors, and so on, producing a brain that rapidly intuits abstractions. And we can also perceive the building blocks in someone who is merely okay, or lousy, at any of these.

			But Rudolph-esque accomplishments seem different. You’re exhausted, demoralized, and it hurts like hell but you push on; you want to take an evening off, see a movie with a friend, but resume studying; there’s that temptation, no one’s looking, everyone else does it, but you know it’s wrong. It seems so hard, so improbable to think of those same neurotransmitters, receptors, or transcription factors when considering feats of willpower. There seems a much easier answer—you’re seeing the Calvinist work ethic of a homunculus sprinkled with the right kind of fairy dust.

			Here’s a great example of this dualism. Recall Jerry Sandusky, the Penn State football coach who was a horrific serial child molester. After his conviction came an opinion piece on CNN. Writing under the provocative heading of “Do pedophiles deserve sympathy?” James Cantor of the University of Toronto reviewed the neurobiology of pedophilia. For example, it runs in families in ways suggesting genes play a role. Pedophiles have atypically high rates of brain injuries during childhood. There’s evidence of endocrine abnormalities during fetal life. Does this raise the possibility that a neurobiological die is cast, that some people are destined to be this way? Precisely. Cantor concludes, “One cannot choose to not be a pedophile.”21

			Brave and correct. And then Cantor does a stunning mitigated-free-will long jump. Does any of that biology lessen the condemnation and punishment that Sandusky deserved? No. “One cannot choose to not be a pedophile, but one can choose to not be a child molester.”

			This establishes a dichotomy of what things are supposedly made of:

			 				 					 					 				 				 					 						 							Biological stuff

						 						 							Homuncular grit



					 						 							Destructive sexual urges

						 						 							Resisting acting upon them



					 						 							Delusionally hearing voices

						 						 							Resisting their destructive commands



					 						 							Proclivity toward alcoholism

						 						 							Not drinking



					 						 							Having epileptic seizures

						 						 							Not driving if you didn’t take your meds



					 						 							Not all that bright

						 						 							Getting going when the going gets tough



					 						 							Not the loveliest of faces

						 						 							Resisting getting that huge, hideous nose ring



				 			 			Here are just a few of the things we’ve seen in this book that can influence the column on the right: blood glucose levels; the socioeconomic status of your family of birth; a concussive head injury; sleep quality and quantity; prenatal environment; stress and glucocorticoid levels; whether you’re in pain; if you have Parkinson’s disease and which medication you’ve been prescribed; perinatal hypoxia; your dopamine D4 receptor gene variant; if you have had a stroke in your frontal cortex; if you suffered childhood abuse; how much of a cognitive load you’ve borne in the last few minutes; your MAO-A gene variant; if you’re infected with a particular parasite; if you have the gene for Huntington’s disease; lead levels in your tap water when you were a kid; if you live in an individualist or a collectivist culture; if you’re a heterosexual male and there’s an attractive woman around; if you’ve been smelling the sweat of someone who is frightened. On and on. Of all the stances of mitigated free will, the one that assigns aptitude to biology and effort to free will, or impulse to biology and resisting it to free will, is the most permeating and destructive. “You must have worked so hard” is as much a property of the physical universe and the biology that emerged from it as is “You must be so smart.” And yes, being a child molester is as much a product of biology as is being a pedophile. To think otherwise is little more than folk psychology.





BUT DOES ANYTHING USEFUL ACTUALLY COME OF THIS?


			As I noted, the most formidable skeptic of the relevance of neuroscience to the legal system is Stephen Morse, who has written extensively and effectively about the subject.22 He is the definitive advocate of free will being compatible with a deterministic world. He’s fine with M’Naghten and recognizes that there can be sufficient brain damage to compromise the notion of responsibility—“Various causes can produce genuine excusing conditions, such as lack of rational or control capacity.” But beyond those rare instances, he believes, neuroscience offers little that should challenge the notion of responsibility. As he has quipped, “Brains don’t kill people. People kill people.”

			Morse epitomizes the skepticism about bringing neuroscience into the courtroom. For one thing, he viscerally cringes at how much of a fad “neurolaw” and “neurocriminology” have become. A wonderfully sardonic writer,* he has announced the discovery of the disorder “brain overclaim syndrome,” whose sufferers have gotten carried away with the importance of neuroscience because they’ve been “infected and inflamed by stunning advances in our understanding of the brain,” causing them to “make moral and legal claims that the new neuroscience does not entail and cannot sustain.”

			One absolutely valid criticism of his is a narrow, practical one. This is the worry, aired earlier, that juries will give undue weight to neuroimaging data just because of how impressive the images are. Apropos of that, Morse has called neuroscience “determinism du jour, grabbing the attention previously given to psychological or genetic determinism. . . . The only thing different about neuroscience is that we have prettier pictures and it appears more scientific.”

			Another valid criticism concerns findings in neuroscience usually merely being descriptive (e.g., “Brain region A projects to brain region Q”) or correlative (e.g., “Elevated levels of neurotransmitter X and of behavior Z tend to go together”). Data such as those don’t disprove free will. In the words of philosopher Hilary Bok, “The claim that a person chose her action does not conflict with the claim that some neural processes or states caused it; it simply redescribes it.”23

			This is a point I’ve made throughout the book, namely that description and correlation are nice, but actual causal data are the gold standard (e.g., “When you raise the levels of neurotransmitter X, behavior Z happens more often”). That is the source of some of our most powerful demonstrations of the material bases of our more complex behaviors—for example, transcranial magnetic stimulation techniques that transiently activate or inactivate a part of the cortex can change someone’s moral decision making, decisions about punishment, or levels of generosity and empathy. That’s causality.

			It is when we get to the issue of causality that Morse distinguishes between causation and compulsion. He writes, “Causation is neither an excuse per se nor the equivalent of compulsion, which is an excusing condition.” Morse describes himself as a “thoroughgoing materialist” and states, “We live in a causal universe, which includes human action.” But try as I might, I cannot see any way of making this distinction that does not tacitly require a homunculus that is outside the causal universe, a homunculus that can be overwhelmed by “compulsion” but that can and should handle “causation.” In the words of philosopher Shaun Nichols, “It seems like something has to give, either our commitment to free will or our commitment to the idea that every event is completely caused by the preceding events.”24

			Despite these criticisms of his criticisms, my stance has a major problem, one that causes Morse to conclude that the contributions of neuroscience to the legal system “are modest at best and neuroscience poses no genuine, radical challenges to concepts of personhood, responsibility, and competence.”25 The problem can be summarized in a hypothetical exchange:


Prosecutor: So, professor, you’ve told us about the extensive damage that the defendant sustained to his frontal cortex when he was a child. Has every person who has sustained such damage become a multiple murderer, like the defendant?

			Neuroscientist testifying for the defense: No.

			Prosecutor: Has every such person at least engaged in some sort of serious criminal behavior?

			Neuroscientist: No.

			Prosecutor: Can brain science explain why the same amount of damage produced murderous behavior in the defendant?

			Neuroscientist: No.

			The problem is that, even amid all these biological insights that allow us to be snitty about those silly homunculi, we still can’t predict much about behavior. Perhaps at the statistical level of groups, but not when it comes to individuals.





Explaining Lots and Predicting Little


			If a person’s leg is fractured, how predictable is it that they will have trouble walking? I think it would be safe to predict something close to 100 percent. If they have serious inflammatory lung disease, how predictable is it that their breathing will be labored at times and that they will tire easily? Again, around 100 percent. Same for the effects of significant blockage of blood flow to the legs or extensive cirrhosis of the liver.

			Let’s switch to the brain and neurological dysfunction. What if someone has had a brain injury, and the neurons around the resulting scar tissue rewire so that they stimulate both themselves and one another—how predictable is it that the person will have a seizure? How about if they have congenital weaknesses in the walls of the blood vessels throughout the brain—how likely is a cerebral aneurysm at some point? How about if they have a mutation in the gene that causes Huntington’s disease—how likely are they to have a neuromuscular disorder by age sixty? Really high in all cases; probably approaching 100 percent.

			Let’s incorporate behavior. If someone has extensive frontocortical damage, how predictable is it that you’d note something odd about them, behaviorally, after a five-minute conversation? Something like 75 percent.

			Now let’s consider a broader range of behaviors. How predictable is it that this person with the frontal damage will do something horrifically violent at some point? Or that someone who was abused repeatedly as a child will become an abusive adult? That a soldier who went through a battle that killed his buddies will develop PTSD? That a person with the “montane vole” polygamous version of the vasopressin receptor gene promoter will have numerous failed marriages? That a person with a particular array of glutamate receptor subtypes throughout their cortex and hippocampus will have an IQ above 140? That someone raised with extensive childhood adversity and loss will have a major depressive disorder? All under 50 percent, often way under.

			So how do a fractured leg inevitably impairing locomotion and the noninevitabilities of the previous paragraph differ? Do the latter somehow involve “less” biology? Is the point that the brain contains a nonbiological homunculus but that leg bones do not?

			Hopefully, after this many pages, the start of an answer is apparent. It’s not that there’s “less” biology in those circumstances related to social behavior. It’s that it’s qualitatively different biology.

			When a bone shatters, there’s a relatively straight line of steps leading to inflammation and pain that will impair the person’s gait (should he try to walk an hour later). That straight line of biology won’t be altered by conventional variation in his genome, his prenatal hormone exposure, the culture he was raised in, or when he ate lunch. But as we’ve seen, all of those variables can influence social behaviors that shape our best and worst moments.

			The biology of the behaviors that interest us is, in all cases, multifactorial—that is the thesis of this book.

			Let’s see what “multifactorial” means in a practical sense. Consider someone with frequent depression who is visiting a friend today, pouring her heart out about her problems. How much could you have predicted the global depression and today’s behavior by knowing about her biology?

			Suppose “knowing about her biology” consisted only of knowing what version of the serotonin transporter gene she has. How much predictive power does that give you? As we saw in chapter 8, not much—say, 10 percent. What if “knowing about her biology” consists of knowing the status of that gene plus knowing if one of her parents died when she was a child? More, maybe 25 percent. How knowing her serotonin transporter gene status + childhood adversity status + whether she is living alone in poverty? Maybe up to 40 percent. Add knowledge of the average level of glucocorticoids in her bloodstream today. Maybe a bit more. Toss in knowing if she’s living in an individualist or a collectivist culture. Some more predictability.* Know if she is menstruating (which typically exacerbates symptoms in seriously depressed women, making it more likely that they’ll be socially withdrawn rather than reaching out to someone). Some more predictability. Maybe even above the 50 percent mark by now. Add enough factors, many of which, possibly most of which, have not yet been discovered, and eventually your multifactorial biological knowledge will give you the same predictive power as in the fractured-bone scenario. Not different amounts of biological causation; different types of causation.

			The artificial intelligence pioneer Marvin Minsky once defined free will as “internal forces I do not understand.”26 People intuitively believe in free will, not just because we have this terrible human need for agency but also because most people know next to nothing about those internal forces. And even the neuroscientist on the witness stand can’t accurately predict which individual with extensive frontal damage will become the serial murderer, because science as a whole still knows about only a handful of those internal forces. Shattered bone → inflammation → constricted movement is easy. Neurotransmitters + hormones + childhood + ____ + ____ + isn’t.*27

			Another factor comes into play. When I go to the Web of Science, a search engine for scanning databases of papers published in science and medical journals. Under search terms I put in “oxytocin” and “trust”—just to pick an example of the umpteen links between biology and social behavior that we’ve covered. And up comes the news that 193 papers have been published on the subject. Consider the following figure, showing that most of those papers have been published in the last few years.

			Same with the next figure, a search for “oxytocin” and “social behavior” or, after that, “transcranial magnetic stimulation” and “decision-making,” and then “brain” and “aggression.”





			 				 				Visit bit.ly/2nyi5Ip for a larger version of these graphs.



			 			And just to give a sense of some more of these:

			 				 				Visit bit.ly/2neYFVP for a larger version of this table.



			Our behaviors are constantly shaped by an array of subterranean forces. What these figures and the table show is that most of these forces involve biology that, not that long ago, we didn’t know existed.

			So what do we do with Minsky’s definition of free will needing to be amended to “internal forces I do not understand yet”?





HOW THEY WILL VIEW US


			If you still think there is mitigated free will, there are three possible routes to take at this juncture.

			To appreciate the first, let’s briefly consider epilepsy. Scientists understand a lot about the neurological bases of seizures and how they involve firing with abnormally high frequency and synchrony. But not that long ago, say, a century ago, epilepsy was viewed as a type of mental illness. And before that it was thought by many to be a communicable infectious disease. And at other times and places, it was thought to be caused by menstruation, or excessive sex, or excessive masturbation. But in 1487 two German scholars uncovered a cause of epilepsy that really seemed to hit the nail on the head.

			The two Dominican friars, Heinrich Kramer and Jakob Sprenger, published Malleus Maleficarum (Latin for “Hammer of the Witches”), the definitive treatise about why someone becomes a witch, how to identify them, and what to do with them. What was one of the surest ways to identify a witch? If they are seized by Satan, if they convulse from the malign power of the devil within them.

			Their guideline was the Gospel according to Mark, 9:14–29. A man brings his son to Jesus, saying there is something wrong with him and asking Jesus to cure him—a spirit comes and seizes him, making him mute, and then that spirit throws him to the ground, where he foams at the mouth and grinds his teeth and becomes rigid. The man presents his son, who is promptly seized by that spirit and falls to the ground, convulsing and foaming. Jesus perceives that the boy is infested with an unclean spirit and commands that vile spirit to come out and be gone. The seizing ceases.

			Thus seizures were a sign of demonic possession, a certain marker of a witch. Malleus Maleficarum arrived in time to take advantage of mass production through the recently invented printing press. In the words of historian Jeffrey Russell, “The swift propagation of the witch hysteria by the press was the first evidence that Gutenberg had not liberated man from original sin.” The book was widely read and went through more than thirty editions over the subsequent century. Estimates are that from 100,000 to a million people were persecuted, tortured, or killed as witches in the aftermath.*28

			I don’t think much of Kramer and Sprenger. My assumption is that they were sadistic monsters, but that could reflect my being influenced too much by the likes of The Name of the Rose or The Da Vinci Code. Maybe they were opportunists who reasoned that the book would make their careers. Maybe they were utterly sincere.

			Instead I imagine a scenario of an evening during the late fifteenth century. A church inquisitor comes home from work weary, burdened. His wife coaxes him to talk—“It was a usual day of condemning witches, but this one case bothered me. Everyone testified about this woman who falls and gnashes and convulses—a witch, without a doubt. I don’t feel sorry for her—no one told her to open wide for Satan. But she had these two beautiful kids—you should have seen them, just so confused as to why their mother was being taken away. Distraught husband also. So that part was hard, seeing them suffer. But it is what it is—we burned her, of course.” Burnings and killings and centuries were to pass until we in the West would have learned enough to say, “It’s not her; it’s her disease.”*

			We’re only a first few baby steps into understanding any of this, so few that it leaves huge, unexplained gaps that perfectly smart people fill in with a homunculus. Nevertheless, even the staunchest believers in free will must admit that it is hemmed into tighter spaces than in the past. It’s less than two centuries since science first taught us that the frontal cortex has something to do with appropriate behavior. Less than seventy years since we learned that schizophrenia is a biochemical disorder. Perhaps fifty years since we learned that reading problems of a type that we now call dyslexia aren’t due to laziness but instead involve microscopic cortical malformations. Twenty-five since we learned that epigenetics alters behavior. The influential philosopher Daniel Dennett has written about the free will that is “worth wanting.” If there really is free will, it’s getting consigned to domains too mundane to be worth the effort to want—do I want briefs or boxer shorts today?29

			Recall those charts and table showing the recentness of these scientific discoveries. If you believe that starting tonight, at midnight, something will happen and science will stop, that there will be no new publications, findings, or knowledge relevant to this book, that we now know everything there is, then it is clear what one’s stance should be—there are some rare domains where extremes of biological dysfunction cause involuntary changes in behavior, and we’re not great at predicting who undergoes such changes. In other words, the homunculus is alive and well.

			But if you believe that there will be the accrual of any more knowledge, you’ve just committed to either the view that any evidence for free will ultimately will be eliminated or the view that, at the very least, the homunculus will be jammed into ever tinier places. And with either of those views, you’ve also agreed that something else is virtually guaranteed: that people in the future will look back at us as we do at purveyors of leeches and bloodletting and trepanation, as we look back at the fifteenth-century experts who spent their days condemning witches, that those people in the future will consider us and think, “My God, the things they didn’t know then. The harm that they did.”

			Archaeologists do something impressive, reflecting disciplinary humility. When archaeologists excavate a site, they recognize that future archaeologists will be horrified at their primitive techniques, at the destructiveness of their excavating. Thus they often leave most of a site untouched to await their more skillful disciplinary descendants. For example, astonishingly, more than forty years after excavations began, less than 1 percent of the famed Qin dynasty terra-cotta army in China has been uncovered.

			Those adjudicating trials don’t have the luxury of adjourning for a century until we really understand the biology of behavior. But at the very least the system needs the humility of archaeology, a sense that, above all else, we shouldn’t act irrevocably.

			But what do we actually do in the meantime? Simple (which is easy for me to say, looking at the legal world from the soothing distance of my laboratory): probably just three things. One is easy, one is very challenging to implement, and the third is nearly impossible.

			First the easy one. If you reject free will and the discussion turns to the legal system, the crazy-making, inane challenge that always surfaces is that you’d do nothing about criminals, that they’d be free to walk the streets, wreaking havoc. Let’s trash this one instantly—no rational person who rejects free will actually believes this, would argue that we should do nothing because, after all, the person has frontal damage, or because, after all, evolution has selected for the damaging trait to traditionally be adaptive, or because, after all . . . People must be protected from individuals who are dangerous. The latter can no more be allowed to walk the streets than you can allow a car whose brakes are faulty to be driven. Rehabilitate such people if you can, send them to the Island of Misfit Toys forever if you can’t and they are destined to remain dangerous. Josh Greene and Jonathan Cohen of Princeton wrote an extremely clearheaded piece on this, “For the Law, Neuroscience Changes Nothing and Everything.” Where neuroscience and the rest of biology change nothing is in the continued need to protect the endangered from the dangerous.30

			Now for the nearly impossible issue, the one that “changes everything”—the issue of punishment. Maybe, just maybe, a criminal must suffer punishment at junctures in a behaviorist framework, as part of rehabilitation, part of making recidivism unlikely by fostering expanded frontal capacity. It is implicit in the very process of denying a dangerous individual their freedom by removing them from society. But precluding free will precludes punishment being an end in and of itself, punishment being imagined to “balance” the scales of justice.

			It is the punisher’s mind-set where everything must be changed. The difficulty of this is explored in the superb book The Punisher’s Brain: The Evolution of Judge and Jury (2014) by Morris Hoffman, a practicing judge and legal scholar.31 He reviews the reasons for punishment: As we see from game theory studies, because punishment fosters cooperation. Because it is in the fabric of the evolution of sociality. And most important, because it can feel good to punish, to be part of a righteous and self-righteous crowd at a public hanging, knowing that justice is being served.

			This is a deep, atavistic pleasure. Put people in brain scanners, give them scenarios of norm violations. Decision making about culpability for the violation correlates with activity in the cognitive dlPFC. But decision making about appropriate punishment activates the emotional vmPFC, along with the amygdala and insula; the more activation, the more punishment.32 The decision to punish, the passionate motivation to do so, is a frothy limbic state. As are the consequences of punishing—when subjects punish someone for making a lousy offer in an economic game, there’s activation of dopaminergic reward systems. Punishment that feels just feels good.

			It makes sense that we’ve evolved such that it is limbic froth that is at the center of punishing, and that a pleasurable dopaminergic surge rewards doing so. Punishment is effortful and costly, ranging from forgoing a reward when rejecting a lowball offer in the Ultimatum Game to our tax dollars paying for the dental plan of the prison guard who operates the lethal injection machine. That rush of self-righteous pleasure is what drives us to shoulder the costs. This was shown in one neuroimaging study of economic game play. Subjects alternated between being able to punish lousy offers at no cost and having to spend points they had earned to do so. And the more dopaminergic activation during no-cost punishment, the more someone would pay to punish in the other condition.33

			Thus the nearly impossible task is to overcome that. Sure, as I said, punishment would still be used in an instrumental fashion, to acutely shape behavior. But there is simply no place for the idea that punishment is a virtue. Our dopaminergic pathways will have to find their stimulation elsewhere. I sure don’t know how best to achieve that mind-set. But crucially, I sure do know we can do it—because we have before: Once people with epilepsy were virtuously punished for their intimacy with Lucifer. Now we mandate that if their seizures aren’t under control, they can’t drive. And the key point is that no one views such a driving ban as virtuous, pleasurable punishment, believing that a person with treatment-resistant seizures “deserves” to be banned from driving. Crowds of goitrous yahoos don’t excitedly mass to watch the epileptic’s driver’s license be publicly burned. We’ve successfully banished the notion of punishment in that realm. It may take centuries, but we can do the same in all our current arenas of punishment.

			Which brings us to the huge practical challenge. The traditional rationales behind imprisonment are to protect the public, to rehabilitate, to punish, and finally to use the threat of punishment to deter others. That last one is the practical challenge, because such threats of punishment can indeed deter. How can that be done? The broadest type of solution is incompatible with an open society—making the public believe that imprisonment involves horrific punishments when, in reality, it doesn’t. Perhaps the loss of freedom that occurs when a dangerous person is removed from society must be deterrence enough. Perhaps some conventional punishment will still be needed if it is sufficiently deterring. But what must be abolished are the views that punishment can be deserved and that punishing can be virtuous.

			None of this will be easy. When contemplating the challenge to do so, it is important to remember that some, many, maybe even most of the people who were prosecuting epileptics in the fifteenth century were no different from us—sincere, cautious, and ethical, concerned about the serious problems threatening their society, hoping to bequeath their children a safer world. Just operating with an unrecognizably different mind-set. The psychological distance from them to us is vast, separated by the yawning chasm that was the discovery of “It’s not her, it’s her disease.” Having crossed that divide, the distance we now need to go is far shorter—it merely consists of taking that same insight and being willing to see its valid extension in whatever directions science takes us.

			The hope is that when it comes to dealing with humans whose behaviors are among our worst and most damaging, words like “evil” and “soul” will be as irrelevant as when considering a car with faulty brakes, that they will be as rarely spoken in a courtroom as in an auto repair shop. And crucially, the analogy holds in a key way, extending to instances of dangerous people without anything obviously wrong with their frontal cortex, genes, and so on. When a car is being dysfunctional and dangerous and we take it to a mechanic, this is not a dualistic situation where (a) if the mechanic discovers some broken widget causing the problem, we have a mechanistic explanation, but (b) if the mechanic can’t find anything wrong, we’re dealing with an evil car; sure, the mechanic can speculate on the source of the problem—maybe it’s the blueprint from which the car was built, maybe it was the building process, maybe the environment contains some unknown pollutant that somehow impairs function, maybe someday we’ll have sufficiently powerful techniques in the auto shop to spot some key molecule in the engine that is out of whack—but in the meantime we’ll consider this car to be evil. Car free will also equals “internal forces we do not understand yet.”*34

			Many who are viscerally opposed to this view charge that it is dehumanizing to frame damaged humans as broken machines. But as a final, crucial point, doing that is a hell of a lot more humane than demonizing and sermonizing them as sinners.





POSTSCRIPT: NOW FOR THE HARD PART


			Well, so much for the criminal justice system. Now on to the really difficult part, which is what to do when someone compliments your zygomatic arches.

			If we deny free will when it comes to the worst of our behaviors, the same must also apply to the best. To our talents, displays of willpower and focus, moments of bursting creativity, decency, and compassion. Logically it should seem as ludicrous to take credit for those traits as to respond to a compliment on the beauty of your cheekbones by thanking the person for implicitly having praised your free will, instead of explaining how mechanical forces acted upon the zygomatic arches of your skull.

			It will be so difficult to act that way. I am willing to admit that I have acted egregiously in this regard. My wife and I have brunch with a friend, who serves fruit salad. We proclaim, “Wow, the pineapple is delicious.” “They’re out of season,” our host smugly responds, “but I lucked out and found a decent one.” My wife and I express awestruck worship—“You really know how to pick fruit. You are a better person than we are.” We are praising the host for this supposed display of free will, for the choice made at the fork in life’s road that is pineapple choosing. But we’re wrong. In reality, genes had something to do with the olfactory receptors our host has that help detect ripeness. Maybe our host comes from a people whose deep and ancient cultural values include learning how to feel up a pineapple to tell if it’s good. The sheer luck of the socioeconomic trajectory of our host’s life has provided the resources to prowl an overpriced organic market playing Peruvian folk Muzak. Yet we praise our host.

			I can’t really imagine how to live your life as if there is no free will. It may never be possible to view ourselves as the sum of our biology. Perhaps we’ll have to settle for making sure our homuncular myths are benign, and save the heavy lifting of truly thinking rationally for where it matters—when we judge others harshly.




