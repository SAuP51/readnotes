# 
> 2018000模板



Seventeen


			War and Peace





Let’s review some facts. The amygdala typically activates when seeing a face of another race. If you’re poor, by the time you’re five, your frontal cortical development probably lags behind average. Oxytocin makes us crappy to strangers. Empathy doesn’t particularly translate into compassionate acts, nor does refined moral development translate into doing the harder, right thing. There are gene variants that, in particular settings, make you prone toward antisocial acts. And bonobos aren’t perfectly peaceful—they wouldn’t be masters of reconciliation if they didn’t have conflicts to reconcile.

			All this makes one mighty pessimistic. Yet the rationale for this book is that, nonetheless, there’s ground for optimism.

			Thus this final chapter’s goals are (a) to evidence that things have improved, that many of our worst behaviors are in retreat, our best ones ascendant; (b) to examine ways to improve this further; (c) to derive emotional support for this venture, to see that our best behaviors can occur in the most unlikely circumstances; (d) and finally, to see if I can actually get away with calling this chapter “War and Peace.”





SOMEWHAT BETTER ANGELS


			When it comes to our best and worst behaviors, the world is astonishingly different from that of the not-so-distant past. At the dawn of the nineteenth century, slavery occurred worldwide, including in the colonies of a Europe basking in the Enlightenment. Child labor was universal and would soon reach its exploitative golden age with the Industrial Revolution. And there wasn’t a country that punished mistreatment of animals. Now every nation has outlawed slavery, and most attempt to enforce that; most have child labor laws, rates of child labor have declined, and it increasingly consists of children working alongside their parents in their homes; most countries regulate the treatment of animals in some manner.

			The world is also safer. Fifteenth-century Europe averaged 41 homicides per 100,000 people per year. Currently only El Salvador, Venezuela, and Honduras, at 62, 64, and 85, respectively, are worse; the world averages 6.9, Europe averages 1.4, and there are Iceland, Japan, and Singapore at 0.3.

			Here are things that are rarer in recent centuries: Forced marriages, child brides, genital mutilation, wife beating, polygamy, widow burning. Persecution of homosexuals, epileptics, albinos. Beating of schoolchildren, beating of beasts of burden. Rule of a land by an occupying army, by a colonial overlord, by an unelected dictator. Illiteracy, death in infancy, death in childbirth, death from preventable disease. Capital punishment.

			Here are things invented in the last century: Bans on the use of certain types of weapons. The World Court and the concept of crimes against humanity. The UN and the dispatching of multinational peacekeeping forces. International agreements to hinder trafficking of blood diamonds, elephant tusks, rhino horns, leopard skins, and humans. Agencies that collect money to aid disaster victims anywhere on the planet, that facilitate intercontinental adoption of orphans, that battle global pandemics and send medical personnel to any place of conflict.

			Yes, I know, I’m an utter naïf if I think laws are universally enforced. For example, in 1981 Mauritania became the last country to ban slavery; nevertheless, today roughly 20 percent of its people are slaves, and the government has prosecuted a total of one slave owner.1 I recognize that little has changed in many places; I have spent decades in Africa living around people who believe that epileptics are possessed and that the organs of murdered albinos have healing powers, where beating of wives, children, and animals is the norm, five-year-olds herd cattle and haul firewood, pubescent girls are clitoridectomized and given to old men as third wives. Nonetheless, worldwide, things have improved.

			The definitive account of this is Pinker’s monumental The Better Angels of Our Nature: Why Violence Has Declined.2 It’s a scholarly work that’s gut-wrenchingly effective in documenting just how bad things once were. Pinker graphically describes the appalling historical inhumanity of humans. Roughly half a million people died in the Roman Colosseum to supply audiences of tens of thousands the pleasure of watching captives raped, dismembered, tortured, eaten by animals. Throughout the Middle Ages, armies swept across Eurasia, destroying villages, killing every man, consigning every woman and child to slavery. Aristocracy accounted for a disproportionate share of violence, savaging peasantry with impunity. Religious and governmental authorities, ranging from Europeans to Persians, Chinese, Hindus, Polynesians, Aztecs, Africans, and Native Americans, invented means of torture. For a bored sixteenth-century Parisian, entertainment might consist of a cat burning, execution of a “criminal” animal, or bearbaiting, where a bear, chained to a post, would be torn apart by dogs. It is a sickeningly different world; Pinker quotes the writer L. P. Hartley: “The past is a foreign country: they do things differently there.”

			Better Angels has provoked three controversies:





Why Were People So Awful Then?


			For Pinker the answer is clear. Because people had always been so awful. This is chapter 9’s debate—when was war invented, was ancestral hunter-gatherer life about Hobbes or Rousseau? As we saw, Pinker is in the camp holding that organized human violence predates civilization, stretching back to our last common ancestor with chimps. And as reviewed, most experts convincingly disagree, suggesting that data have been cherry-picked, hunter-horticulturalists mislabeled as hunter-gatherers, and newfangled sedentary hunter-gatherers inappropriately grouped with traditional nomadic ones.





Why Have People Gotten Less Awful?


			Pinker’s answer reflects two factors. He draws on the sociologist Norbert Elias, whose notion of the “civilizing process” centered on the fact that violence declines when states monopolize force. That is coupled with spread of commerce and trade, fostering realpolitik self-restraint—recognizing that it’s better to have this other person alive and trading with you. Their well-being begins to matter, prompting what Pinker calls an “escalator of reasoning”—an enlarged capacity for empathy and Us-ness. This underlies the “rights revolution”—civil rights, women’s rights, children’s rights, gay rights, animal rights. This view is a triumph of cognition. Pinker yokes this to the “Flynn effect,” the well-documented increase in average IQ over the last century; he invokes a moral Flynn effect, as increasing intelligence and respect for reasoning fuel better Theory of Mind and perspective taking and an increased ability to appreciate the long-term advantages of peace. In the words of one reviewer, Pinker is “not too fainthearted to call his own culture civilized.”3

			Predictably, this has drawn fire from all sides. The Left charges that this giddy overvaluing of the dead-white-male Enlightenment fuels Western neoimperialism.4 My personal political instincts run in this direction. Nonetheless, one must admit that the countries with minimal violence, extensive social safety nets, few child brides, numerous female legislators, and sacrosanct civil liberties are usually direct cultural descendants of the Enlightenment.

			Meanwhile, the Right claims that Pinker ignores religion, pretending that decency was invented in the Enlightenment.5 He is eloquently unapologetic about this—for him much of what has gone right reflects people’s “shifting from valuing souls to valuing lives.” For others the criticism is that this escalator of reasoning fetishizes cognition over affect—after all, sociopaths have great Theory of Mind, a (damage-induced) purely rational mind makes abhorrent moral judgments, and a sense of justice is fueled by the amygdala and insula, not the dlPFC. Obviously, this many pages into the book, I feel that the interaction of reasoning and feeling is key.





Have People Really Gotten Less Awful?


			This has been very contentious. Pinker offers the sound bite “We may be living in the most peaceful era in our species’ existence.” The fact most driving this optimism is that, except for the Balkan wars, Europe has been at peace since 1945, the longest stretch in history. For Pinker, this “Long Peace” represents the West coming to its senses after the ruin of World War II, seeing how the advantages of being a common market outweigh those of being a perpetually warring continent, plus some expanding empathy thrown in on the side.

			Critics characterize this as Eurocentrism. Western countries may kumbaya one another, but they’ve sure made war elsewhere—France in Indochina and Algeria, Britain in Malaya and Kenya, Portugal in Angola and Mozambique, the USSR in Afghanistan, the United States in Vietnam, Korea, and Latin America. Moreover, parts of the developing world have been continuously at war for decades—consider the eastern Congo. Most important, such wars have been made bloodier because the West invented the idea of having client states fight proxy wars for them. After all, the late twentieth century saw the United States and USSR arm the warring Somalia and Ethiopia, only to switch to arming the other side within a few years. The Long Peace has been for Westerners.

			The claim of violence declining steadily over the last millennium also must accommodate the entire bloody twentieth century. World War II killed 55 million people, more than any conflict in history. Throw in World War I, Stalin, Mao, and the Russian and Chinese civil wars, and you’re up to 130 million.

			Pinker does something sensible that reflects his being a scientist. He corrects for total population size. Thus, while the eighth century’s An Lushan Rebellion and civil war in Tang dynasty China killed “only” 36 million, that represented one sixth of the world’s population—the equivalent of 429 million in the midtwentieth century. When deaths are expressed as a percentage of total population, World War II is the only twentieth-century event cracking the top ten, behind An Lushan, the Mongol conquests, the Mideast slave trade, the fall of the Ming dynasty, the fall of Rome, the deaths caused by Tamerlane, the annihilation of Native Americans by Europeans, and the Atlantic slave trade.

			Critics have questioned this—“Hey, stop using fudge factors to somehow make World War II’s 55 million dead less than the fall of Rome’s 8 million.” After all, 9/11’s murders would not have evoked only half as much terror if America had 600 million instead of 300 million citizens. But Pinker’s analysis is appropriate, and analyzing rates of events is how you discover that today’s London is much safer than was Dickens’s or that some hunter-gatherer groups have homicide rates that match Detroit’s.

			But Pinker failed to take things one logical step further—also correcting for differing durations of events. Thus he compares the half dozen years of World War II with, for example, twelve centuries of the Mideast slave trade and four centuries of Native American genocide. When corrected for duration as well as total world population, the top ten now include World War II (number one), World War I (number three), the Russian Civil War (number eight), Mao (number ten), and an event that didn’t even make Pinker’s original list, the Rwandan genocide (number seven), where 700,000 people were killed in a hundred days.*

			This suggests both good and bad news. Compared with the past, we are extraordinarily different in terms of whom we extend rights to and feel empathy for and what global ills we counter. And things are better in terms of fewer people acting violently and societies attempting to contain them. But the bad news is that the reach of the violent few is ever greater. They don’t just rage about events on another continent—they travel there and wreak havoc. The charismatically violent inspire thousands in chat rooms instead of a mob in their village. Like-minded lone wolves more readily meet and metastasize. And the chaos once let loose with a cudgel or machete occurs now with an automatic weapon or bomb, with far more horrific consequences. Things have improved. But that doesn’t mean they’re good.

			Thus we now consider insights provided by this book that might help.





SOME TRADITIONAL ROUTES


			First there’s the strategy for reducing violence that stretches back tens of thousands of years—moving. If two individuals in a hunter-gatherer band are having tensions, one frequently shifts to a neighboring band, sometimes voluntarily, sometimes not. Similarly, interband tensions are reduced when one shifts to a different location, an advantage of nomadicism. A recent study of the hunter-gathering Hadza of Tanzania showed an additional benefit to this fluidity straight out of chapter 10. Specifically, it facilitates highly cooperative individuals associating with one another.6

			Then there are the beneficial effects of trade, as emphasized by anthropologists, as well as Pinker. From trading at a village market to signing international trade agreements, it is often true that where goods do not pass frontiers, armies will. It’s a version of Thomas Friedman’s somewhat tongue-in-cheek Golden Arches Theory of peace—countries with McDonald’s don’t fight one another. While there are exceptions (e.g., the U.S. invasion of Panama, the Israeli invasion of Lebanon), Friedman’s broad point holds—countries that are sufficiently stable that they are integrated into global markets with the likes of McDonald’s and prosperous enough that their people keep those establishments in business likely conclude that the trade advantages of peace outweigh the imagined spoils of war.**7

			This isn’t surefire—for example, despite being major trading partners, Germany and the UK fought World War I—and there’s no shortage of people willing to go to war, even at the cost of disrupted trade and scarce commodities. Moreover, “trade” is double-edged. It’s certifiably groovy when occurring between indigenous rain forest hunters; it’s certifiably vile if you’re protesting the WTO. But as long as countries can wage war on distant nations, long-distance trade that makes them interdependent is a good deterrent.

			Cultural diffusion in general (which includes trade) can also facilitate peace. This can have a modern tint—across 189 countries, digital access predicts increased civil liberties and media freedom. Moreover, the more civil liberties in a neighboring country, the stronger this effect, as ideas flow with goods.8





Religion


			Well, I’d love to skip this section, but I can’t. That’s because religion is arguably our most defining cultural invention, an incredibly powerful catalyst for both our best and worst behaviors.

			When introducing the pituitary in chapter 4, I didn’t feel obliged to first disclose my feelings about the gland. But the equivalent feels appropriate here. Thus: I was raised highly observant and Orthodox, felt intensely religious. But then, around age thirteen, the whole edifice collapsed; ever since, I’ve been incapable of any religiosity or spirituality and more readily focus on religion’s destructive than its beneficial aspects. But I like being around religious people and am moved by them—while baffled by how they can believe that stuff. And I fervently wish that I could. The end.

			As emphasized in chapter 9, we’ve created a staggering variety of religions. In considering solely religions with worldwide reaches, there are some important commonalities:


They all involve facets of religiosity that are intensely personal, solitary, and individualized, as well as facets that are about community; as we’ll see, these are very different realms when it comes to fostering our best and worst behaviors.

				All involve personal and communal ritualized behaviors that comfort in times of anxiety; however, many of those anxieties were created by the religion itself. 					The anxiety-reducing effects of belief are logical, given that psychological stress is about lack of control, predictability, outlets, and social support. Depending on the religion, belief brings an explanation for why things happen, a conviction that there is a purpose, and the sense of a creator who is interested in us, who is benevolent, who responds to human entreaties, who preferentially responds to entreaties from people like you. No wonder religiosity has health benefits (independent of the community support that it brings and the decreased rates of substance abuse).

					Recall the role of the anterior cingulate cortex (ACC) in sounding an alarm when there is a discrepancy between how you thought things worked and how they actually do. After controlling for personality and cognitive abilities, more religious people show less ACC activation when getting news of a negative discrepancy. Other studies show the anxiety-reducing effects of repetitive religious rituals.9



				Finally, all the world religions distinguish between Us and Them, though they differ as to what is required to be an Us and whether the pertinent attributes are immutable.



			Enough is known about the neurobiology of religiosity that there’s even a journal called Religion, Brain and Behavior. Reciting a familiar prayer activates mesolimbic dopaminergic systems. Improvising one activates regions associated with Theory of Mind, as you try to understand a deity’s perspective (“God wants me to be humble in addition to grateful; better make sure I mention that”). Moreover, more activation of this Theory of Mind network correlates with a more personified image of a deity. Believing that someone is faith healing deactivates the (cognitive) dlPFC, suspending disbelief. And performing a familiar ritual activates cortical regions associated with habit and reflexive evaluation.10

			So are religious people nicer than nonreligious ones? It depends on whether they’re interacting with in- or out-group members. Okay, are religious people nicer to in-group members? Numerous studies say yes—more volunteering (with or without a religious context), charitable giving, and spontaneous prosociality, more generosity, trust, honesty, and forgiveness in economic games. However, numerous studies show no differences.11

			Why the discrepancy? For starters, it matters whether data are self-reported—religious people tend to inflate reports of their prosociality more than do nonreligious people. Another factor is whether the prosociality is public—conspicuous display is particularly important to those religious people who strongly need social approval. As more context dependency, in one study religious people were more charitable than nonreligious ones—but only on their Sabbath.12

			Another important issue: what kind of religion? As introduced in chapter 9, Ara Norenzayan, Azim Shariff, and Joseph Henrich of the University of British Columbia have identified links between features of various religions and aspects of prosociality.13 As we saw, small-band cultures (such as hunter-gatherers) rarely invent moralizing deities. It is not until cultures are large enough that people regularly interact anonymously with strangers that it becomes commonplace to invent a judgmental god—the Judeo-Christian/Muslim deity.

			In such cultures overt and subliminal religious cues boost prosociality. In one study religious subjects unscrambled sentences that did or didn’t contain religious terms (e.g., spirit, divine, sacred); doing the former prompted generosity afterward. This is reminiscent of chapter 3’s finding that merely seeing a pair of eyes posted on a wall makes people more prosocial. And showing that this is about being monitored, unscrambling sentences with secular terms such as “jury,” “police,” or “contract” had the same effect.14

			Thus reminders of a judgmental god(s) boosts prosociality. It also matters what that deity does about transgressions. Within and among cultures, the more punitive the god, the more generosity to an anonymous coreligionist. Do punitive gods make for more punitive people (at least in an economic game)? In one study, no—save your cash, God’s got it covered. In another, yes—a punitive god would want me to be punitive as well. The UBC group has shown something ironic. Priming people to think of God as punitive decreases cheating; thinking of God as forgiving increases it. The researchers then studied subjects from sixty-seven countries, considering the prevalence in each of belief in the existence of a heaven and hell. The greater the skew toward belief in hell, rather than heaven, the lower the national crime rate. When it comes to Eternity, sticks apparently work better than carrots.

			And what about religion facilitating the worst in us, with respect to Thems? Well, one piece of evidence for this is, uh, like, human history. Every major religion has historical blood on its hands—Buddhist monks led the persecution of Rohingya Muslims in Burma, and a Quaker in the White House oversaw the carpet bombing of North Vietnam for Christmas.*15 This ranges from religious wars, which are, to cite a quote generally attributed to Napoleon, “people killing each other over who has the better imaginary friend,” to secular ones where, nevertheless, omniscient support is requested and proclaimed. Religion is a particularly tenacious catalyst of violence. Catholics and Protestants have been killing each other in Europe for nearly 500 years, Shiites and Sunnis for 1,300. Violent disagreements about differing economic or governmental models never last as long—this would be like people still killing each other today over, say, Eastern Roman Emperor Heraclius’s 610 decision to switch the official language from Latin to Greek. As shown in a study of six hundred terrorist groups spanning forty years, religiously based terrorism persists the longest and is least likely to subside due to fighters joining the political process.

			Religious primes foster out-group hostility. In a “field study” where people were surveyed in different locations in a cosmopolitan European city, merely walking past a church made Christians express more conservative, negative attitudes toward non-Christians. Another study examined the priming effects of a violent god. Subjects read a Bible passage in which a woman is murdered by a mob from another tribe. Her husband consults with his tribesmen and forms an army that takes revenge by attacking the other tribe (in biblical fashion, destroying their cities and killing every human and animal). Half the subjects were told this story. In the story told to the other half, while contemplating revenge, the army asks for advice from God, who sanctions them to majorly chasten the other tribe.16

			 				 				Visit bit.ly/2mNNLLf for a larger version of this graph.



			Participants then played a competitive game in which each round’s loser was blasted with a loud noise at a volume chosen by the other player. Reading the scene where God sanctions their desire for violence increased the volume with which opponents were chastened.

			No surprise: the effect was bigger in males than in females. Big surprise: subjects were either devout Mormons at Brigham Young University or students of typically liberal religions at a Dutch university, and the effect was equally strong in both groups. Biggest surprise: even among subjects who did not subscribe to the Bible (a surprisingly high 1 percent of the Brigham Young students and 73 percent of the Dutch), godly sanction increased their aggressiveness (though to a lesser extent). Thus, divine sanction of violence can increase aggression even in people whose religiosity probably doesn’t include a vengeful god, as well as among those who don’t even believe there’s divine anything.

			Of course, this is not a uniform effect of religion; Norenzayan distinguishes between private and communal religiosity in surveying support for suicide bombers among Palestinians.17 In a refutation of “Islam = terrorism” idiocy, people’s personal religiosity (as assessed by how often they prayed) didn’t predict support for terrorism. However, frequently attending services at a mosque did. The author then polled Indian Hindus, Russian Orthodox adherents, Israeli Jews, Indonesian Muslims, British Protestants, and Mexican Catholics as to whether they’d die for their religion and whether people of other religions caused the world’s troubles. In all cases frequent attendance of religious services, but not frequent prayer, predicted those views. It’s not religiosity that stokes intergroup hostility; it’s being surrounded by coreligionists who affirm parochial identity, commitment, and shared loves and hatreds. This is hugely important.

			—

			What should one make of these various findings? Religiosity isn’t going anywhere.* Given that, it seems that boosting in-group sociality is best done with a moralizing, punitive god. The standard, wearisome critique of atheism is that lack of a god(s) produces nihilistic amorality; the standard response is that it’s pretty unimpressive if you are kind only because you fear damnation. Unimpressive or not, it appears to be useful. The big challenge is when communal aspects of religiosity fuel out-group hostility. It’s useless to call for religions to broaden the extent of their Us-ness. Religions are quirky as to who is an Us, ranging from “only those who look, act, talk, and pray like people in our sect” to “all of life.” It will be discouragingly tough to shift religions from the former to the latter.





Contact


			As introduced in chapter 11, many have speculated that inter-group tensions are reduced by contact—when people get to know one another, everyone gets along. But despite that salutary possibility, intergroup contact readily elevates hostilities.18

			As seen in chapter 9, intergroup contact worsens things when the two groups are treated unequally or are unequal in number; where the smaller group is surrounded; where intergroup boundaries are ambiguous; when the groups vie to display symbols of their sacred values (e.g., Northern Irish Protestants marching with Orangemen flags through Catholic neighborhoods). Elbows rubbed raw.

			Obviously, the opposite is needed to minimize threat and anxiety—groups encountering each other in equal numbers and treatment, in a neutral setting free of agitprop and where there is institutional oversight of the venture. Most important, interactions work best when there is a shared goal, especially when it is successful. This revisits chapter 11—a shared goal reprioritizes Us/Them dichotomies, bringing this novel combined Us to the forefront.

			Under those conditions, sustained intergroup contact generally decreases prejudices, often to a large extent and in a generalized, persistent manner. This was the conclusion of a 2006 meta-analysis of some five hundred studies comprising over 250,000 subjects from thirty-eight countries; beneficial effects were roughly equal for group differences in race, religion, ethnicity, or sexual orientation. As examples, a 1957 study concerning desegregation of the Merchant Marines showed that the more trips white seamen took with African Americans, the more positive their racial attitudes. Same for white cops as a function of time spent with African American partners.19

			A more recent meta-analysis provides additional insights: (a) The beneficial effects typically involve both more knowledge about and more empathy for the Thems. (b) The workplace is a particularly effective place for contact to do its salutary thing. Decreased prejudice about the Thems at work often generalizes to Thems at large, and even sometimes to other types of Thems. (c) Contact between a traditionally dominant group and a subordinate minority usually decreases prejudice more in the former; the latter have higher thresholds. (d) Novel routes of interacting—such as sustained online relationships—can work a bit as well.20

			All good news. Contact theory has prompted an experimental approach where people, most typically adolescents or young adults, from groups in conflict are brought together for anything from one-hour discussions to summer camps. They’ve most frequently involved Palestinians and Israelis, Northern Irish Catholics and Protestants, or opposing groups from the Balkans, Rwanda, or Sri Lanka, with the idea that participants will return home and spread their attitudinal shifts. This notion of germination prompted the name of one such program, Seeds of Peace.

			Group pictures show Muslims and Jews, Catholics and Protestants, Tutsis and Hutu, Croats and Bosnians arm in arm; this is better than puppies. Do the programs work? Depends on what counts as “working.” According to one expert, Stephen Worchel of the University of Hawaii, effects are generally positive—less fear and more positive views of Thems, more of a perception of Thems as heterogeneous, more recognition of faults of the Us, and more of a perception of oneself as an atypical Us.

			This is the immediate aftermath. Disappointingly, these effects are usually transient. Individuals from across lines rarely stay in touch; in one survey of Palestinian and Israeli teenagers, 91 percent were not. Persistent reductions in prejudice usually involve exceptionalism—“Yes, most Thems are awful, but I hung out with a Them once who was okay.” When there is major transformation, the peace-mongering convert loses street cred back home when they broadcast this. For example, no prominent peace activist has emerged from the thousands of participants in the Middle Eastern Seeds of Peace.*

			Here’s a way to think about contact: instead of hating a Them for what his ancestors did, you await the day that you’re irritated with him for, say, eating the last s’more, or setting the office thermostat too low, or never returning to its proper place in the barn that plowshare that used to be a sword. Now, that’s progress. The core of that thought is Susan Fiske’s demonstration that automatic other-race-face amygdala responses can be undone when subjects think of that face as belonging to a person, not a Them. The ability to individuate even monolithic and deindividuated monsters can be remarkable.

			A moving example of this is told by Pumla Gobodo-Madikizela in her book A Human Being Died That Night: A South African Story of Forgiveness (Cape Town: David Philip, 2003). Gobodo-Madikizela, raised in a black township of apartheid South Africa, managed to forge an educational path all the way to a PhD in clinical psychology. As a free South Africa dawned, she worked on the Truth and Reconciliation Commission, where she had a task to give anyone pause. It concerned Eugene de Kock, the man with the most literal apartheid-era blood on his hands. De Kock had commanded the elite counterinsurgency unit of the South African Police and personally overseen kidnappings, torture, and murders of black activists. He had been tried, convicted, and given a life sentence. Gobodo-Madikizela was to interview him about his death squad; clinical psychologist that she is, over the course of over forty hours talking with him, her main focus became to understand this man.

			He was a predictably multifaceted, contradictory, real human, rather than an archetype. He was remorseful in some ways, unrepentant in others; indifferent to some of his appalling brutality while proud of his patchwork of principles about whom he wouldn’t kill; he pointed fingers at his bosses (who mostly escaped justice by depicting him as a rogue vigilante rather than the civil servant of apartheid that he was) while emphasizing his command of his killers. He shattered her by tentatively asking if he had killed any of her loved ones (he had not).

			And Gobodo-Madikizela found herself deeply troubled by her growing empathy for de Kock.

			A defining moment came one day when de Kock was recounting something that made him markedly distressed. Gobodo-Madikizela reflexively reached out and—a taboo act—touched his finger between the jail bars. The next morning her arm felt leaden, as if paralyzed by the touch. She struggled with whether her granting him this contact was a sign of her power or his (with him somehow manipulating her into the act). When she next saw him, he compounded her storm of feelings by thanking her and confessing that it was his trigger hand that she had touched. No, this was not the start of an unlikely friendship, as violins play in the background. But the automaticity, the empathy implicit in her reaching out to him, shows that somehow, remarkably, the tenuous elements of Us-ness she now shared with de Kock had dominated at that moment.





Burning and Unburning Bridges


			A phenomenon in many settings of conflict is burning cultural bridges as a way to forge a new, powerful Us category. Consider the Mau Mau rebellion in Kenya in the 1950s. The brunt of British colonialism in Kenya had focused on one tribe, the Kikuyu, who had the bad luck of living on precisely the rich farm land that the colonials appropriated; Kikuyu suffering finally boiled over into the Mau Mau insurrection.*

			The agricultural Kikuyu were not particularly bellicose (unlike, say, the nearby pastoralist Maasai, who had been terrorizing the Kikuyu forever), and inculcating new Mau Mau fighters required powerful symbolic effort. Oath making had great cultural significance to Kikuyus, and Mau Mau oath making notoriously involved horrendous violations of Kikuyu norms and taboos, acts guaranteeing shunning at home. The message was clear: “You have burned a bridge; your only Us is us.”

			This strategy is often used in a horrifying realm of modern violence, namely rebel groups transforming kidnapped children into soldiers.21 Sometimes this involves new recruits having to burn symbolic cultural bridges. But also, perhaps reflecting recognition of kids’ limited abstract cognition, something more concrete is employed—the forced killing of family members by such children. We are your family now.

			When child soldiers are liberated, their chances of growing into healthy, functioning adults soars if a relative is found who will accept them. If a bridge is unburned.22



			—

			As I write, there’s news of the rescue of a few of the two-hundred-plus Nigerian schoolgirls kidnapped in 2014 by the terrorist group Boko Haram. What these girls experienced is unimaginable—terror, pain, forced labor, endless rapes, pregnancies, AIDS. And as these few are returned home, many are shunned—for their AIDS, for the belief that they’ve been brainwashed into being sleeper terrorists, for the rape-born children they carry. This does not auger well for their being anything other than broken forever.

			Chapter 11 emphasized pseudospeciation, when Thems are made to seem so different that they hardly count as human. Chapter 15 considered the skill of demagogues at this, framing hated Thems as insects, rodents, bacteria, malignancies, and feces. That provides a clear punch line: be wary of rabble-rousers who frame Thems as things to step on, spray with toxins, or flush down toilets. Simple.

			But pseudospeciating propaganda can be subtler. In the fall of 1990 Iraq invaded Kuwait, and in the run-up to the Gulf War, Americans were sickened by a story that emerged. On October 10, 1990, a fifteen-year-old refugee from Kuwait appeared before a congressional Human Rights Caucus.23

			The girl—she would give only her first name, Nayirah—had volunteered in a hospital in Kuwait City. She tearfully testified that Iraqi soldiers had stolen incubators to ship home as plunder, leaving over three hundred premature infants to die.

			Our collective breath was taken away—“These people leave babies to die on the cold floor; they are hardly human.” The testimony was seen on the news by approximately 45 million Americans, was cited by seven senators when justifying their support of war (a resolution that passed by five votes), and was cited more than ten times by George H. W. Bush in arguing for U.S. military involvement. And we went to war with a 92 percent approval rating of the president’s decision. In the words of Representative John Porter (R-Illinois), who chaired the committee, after Nayirah’s testimony, “we have never heard, in all this time, in all circumstances, a record of inhumanity, and brutality, and sadism, as the ones that [Nayirah had] given us today.”

			Much later it emerged that the incubator story was a pseudospeciating lie. The refugee was no refugee. She was Nayirah al-Sabah, the fifteen-year-old daughter of the Kuwaiti ambassador to the United States. The incubator story was fabricated by the public relations firm Hill + Knowlton, hired by the Kuwaiti government with the help of Porter and cochair Representative Tom Lantos (D-California). Research by the firm indicated that people would be particularly responsive to stories about atrocities against babies (ya think?), so the incubator tale was concocted, the witness coached. The story was disavowed by human rights groups (Amnesty International, Human Rights Watch) and the media, and the testimony was withdrawn from the Congressional Record—long after the war.

			Be careful when our enemies are made to remind us of maggots and cancer and shit. But also beware when it is our empathic intuitions, rather than our hateful ones, that are manipulated by those who use us for their own goals.





Cooperation


			As explored in chapter 10, understanding the evolution of cooperation poses two challenges.

			The first is the fundamental problem of how cooperation ever starts; the dispiriting logic of the Prisoner’s Dilemma shows that whoever takes the first cooperative step becomes one step behind.

			As we saw, one plausible solution concerns founder populations—when a subset of a population becomes isolated and its average degree of relatedness rises, fueling cooperation through kin selection.24 Should that founder population rejoin the general population, their cooperative tendencies will outcompete everyone else, thus propagating cooperation. Another solution involves green-beard effects, that poor man’s version of kin selection, where a genetic trait generates a conspicuous marker and a cooperative bent toward bearers of that marker. In that setting the green beard–less will be outcompeted unless they also evolve cooperation. As we saw, green-beard effects occur in various species.

			This raises the second challenge, namely understanding why humans are so extraordinarily cooperative with nonrelatives. We hold elevator doors open for strangers, take turns at four-way stop signs, get off buses in an orderly manner. We build cultures involving millions of people sharing conventions. This requires more than founder effects and green beards; in the years since Hamilton and Axelrod made “tit for tat” trendy, tons of work has explored human-specific mechanisms for fostering cooperation. There are many.

			Open-ended play. Two individuals play the Prisoner’s Dilemma, knowing that after a single round, they’ll never meet again. Rationality decrees that you defect; there’ll never be a chance to catch up if you fall behind in that first round. What about two rounds? Well, the second round requires noncooperation for the same reasons the single-round game does. In other words, it never makes sense to cooperate in the final round. Thus, round 2 behavior determined, the game defaults to a single-round game—where the rational strategy is to defect. Three rounds? The same. In other words, playing for a known number of rounds biases against cooperation, and the more rational the players, the more they foresee this. It’s open-ended play that fosters cooperation—an unknown number of rounds, producing the shadow of the future, where retribution is possible and the advantages of sustained mutual cooperation accumulate with increasing numbers of interactions.25

			Multiple games. Two individuals play two games against each other simultaneously (alternating rounds between the two) where one game has a much lower threshold for establishing cooperation than the other. Once cooperation is established in that less cutthroat game, there is psychological spillover of cooperation into the other. This is why managers of tense, competitive offices bring in soothing outsiders to lead trust games, hoping that the low-threshold demands for trust there will spill over into work life.

			Open-book play. This is where the other player can see if you’ve been a jerk to people in the past. Reputation is a powerful facilitator of cooperation. That’s what a moralizing god is about—the book whose play is eternally open. As we saw in chapter 9, everyone from hunter-gatherers to urbanites gossip, doing so to open reputation books wider.26

			Open-book play mediates a uniquely sophisticated type of human cooperation, namely “indirect reciprocity.” Person A helps person B, who helps C, who helps D. . . . The reciprocity between two individuals in a closed interaction is like barter. But indirect, pay-it-forward reciprocity is like money, where the common currency is reputation.27





Punishment


			Other animals don’t have reputations or ponder whether their interactions are open-ended. However, punishment to promote cooperation occurs in numerous species—this is shown when a male baboon who is being an aggressive brute to a female is chased out of the troop for a while by the victim and her relatives. Punishment can strongly facilitate cooperation, but its implementation is potentially double-edged in humans.

			All cultures show some degree of willingness to pay a cost to punish norm violators, and high degrees of willingness correlate with high levels of prosociality. One study examined rural Ethiopians who subsisted on selling charcoal made from wood from local forests—a classic tragedy of the commons scenario: no one is likely to spontaneously limit logging to keep the forest healthy. The study showed that villages with high average levels of willingness to administer costly punishment in an economic game were the ones with the most patrols to prevent overcutting of trees and the healthiest forests. And as seen in chapter 9, cultures with gods who punish norm violations are atypically prosocial.28

			A complication in costly punishment is the cost—the danger that the costs of monitoring for and punishing violations may outweigh the benefits of the cooperation induced. A solution is to reduce surveillance after long stretches of cooperation—in other words, to trust. For example, probably very few Amish purchase costly retinal-scanner home security systems.29

			Another complication concerns who does the punishing. In other species it is usually the victim, the second party. By definition, punishment in two-person games in humans (e.g., the Ultimatum Game) is always by the second party. In that setting the punisher forgoes the measly share offered, (a) in the hopes of deriving visceral satisfaction from depriving the first party of their larger share (and, as seen in the last chapter, that is a major motivator of punishment, fueled by the amygdala and insula); (b) in an effort to shape the first party into making fairer offers to the second party in the future; or (c) as an altruistic act, hoping to shape the first party into being more decent to whomever they play next. This is complex for second parties, balancing costs and benefits, heart and mind, birds in the hand and in the bush. It might also result in the first party being offended by the rejection and becoming even less cooperative thereafter—an outcome in some game scenarios.30

			Humans uniquely and very effectively boost cooperation through third-party punishment meted out by objective outsiders. However, such punishing can be costly to the third party, meaning that there’s the evolutionary challenge not just of jump-starting cooperation but also of jump-starting altruistic third-party punishment.31

			The answer, as repeatedly derived by humans, is to add layers. Develop secondary punishment, punishing someone who fails to do third-party punishment—the world of honor codes, where you’re punished if you don’t report a violation. An alternative is to reward third-party punishers—humans make livings as cops and judges. Moreover, recent theoretical and empirical work shows that being a conspicuous third-party punisher makes people trust you. But who monitors third-party punishers? Here is where you get people to share and lower the cost by taking sociality to the max—costs are shouldered by everyone, and free riders are punished (e.g., we pay taxes and punish tax evaders). When the moving parts are balanced, you generate extraordinary levels of cooperation.32

			The moving parts were examined in a fascinating 2010 Science paper. The authors studied 113,000 online participants, who each purchased an item (a souvenir photo) under one of the following conditions:33


Could buy for a set price. (This was the control condition.)

				Could pay whatever they wanted; sales soared but people tended to pay tiny amounts, putting the “store” in the red.

				Were charged the original price, knowing that the company gave X percent of earnings to charity; sales increased, but less than X percent, and the store lost money.

				Could pay whatever they wanted, with half of that going to a charity. This boosted both sales and the price voluntarily paid, yielding profits for the store and a large charitable contribution.



			In other words, while evidence of corporate social responsibility (scenario C) boosts sales a bit, it’s far more effective when the individual and the business share social responsibility and the individual determines the amount of money donated.





Choosing Your Partner


			As we’ve seen, cooperators outcompete more numerous noncooperators to the extent that the former can find one another. This is the logic behind green beards facilitating finding a kindred soul (if not kin). Thus, when that element is introduced into a game (with the ability to refuse to play with someone), cooperation soars, and more cheaply than by punishing defectors.34

			—

			These findings reveal numerous theoretical routes for fostering cooperation, and with real-life equivalents; moreover, we’ve learned a lot about which work best when. This is how we’ve evolved to collectively raise barns for neighbors, plant and harvest the whole village’s rice crop, or coordinate marching-band members to form a picture of their school’s mascot.

			And, oh yeah, to reiterate an idea aired previously, “cooperation” is a value-free term. Sometimes it takes a village to ransack a neighboring village.





Reconciliation, and Things That Are Not Synonymous with It


			“So I’d caught a colobus monkey and was eating, getting to the good part, when this guy comes by, starts really begging for some. This got on my nerves and I snarled at him. Instead of taking a hint, he lunges, grabs the monkey’s arm, starts yanking—so I bit his shoulder. He cleared out fast and sat at the other end of the clearing, his back to me.

			“Once I calmed down, I thought a bit. To be honest, I probably should have shared some food with him. And while he definitely crossed a line when he grabbed, I probably should have nipped him instead of a real bite. So I’m feeling kind of bad. And besides, we work well together on patrols—it’s probably good if we sort things out.

			“So I take the monkey, sit near him. We’re all awkward—he’s not looking at me, I pretend there’s a nettle between my toes. But eventually I give him some of the meat, he grooms me a bit. The whole thing was stupid, we should have done that in the first place.”

			If you’re a chimp, reconciliation is easy once your heart rate returns to normal. Sometimes for us too—touch a friend’s shoulder, give a self-effacing grimace, say, “Hey, look, just now I was being a—” and they cut you off, saying, “No, no, it was me. I shouldn’t . . .” and things are okay.

			Easy. How about when everyone’s trying to patch things up after your people have slaughtered three quarters of theirs, or after they came as colonials, stole your land, and forced you to live in slum “homelands” for decades? Trickier.

			We’re the only species that institutionalizes reconciliation and that grapples with “truth,” “apology,” “forgiveness,” “reparations,” “amnesty,” and “forgetting.”

			The apogee of institutionalized complexity is the truth and reconciliation commission (TRC). The first came in the 1980s, and they’ve been depressingly useful ever since, occurring, for example, in Bolivia, Canada, Australia, Nepal, Rwanda, and Poland. Some TRCs have been in stable countries (Canada and Australia) facing up to their long history of abuse of indigenous peoples. Most, however, have come after a nation emerged from a bloody, divisive transition—a dictator overthrown, a civil war settled, a genocide halted. The popular perception is that their purpose is for perpetrators of abuse to confess, express remorse, and beg for forgiveness from victims, who then grant it, resulting in tearful embraces between the two.

			But instead TRCs are typically exercises in pragmatism, where perpetrators basically say, “This is what I did, and I vow to never harm your people again,” and the victims basically say, “Okay, we vow to not seek extrajudicial retribution.” An often towering achievement, if less heartwarming.

			Probably the best-studied TRC was South Africa’s after the defeat of apartheid. It came with enormous moral legitimacy, being overseen by Desmond Tutu, and gained further legitimacy by, though overwhelmingly focusing on the acts of whites, also examining atrocities by African liberation fighters. Hearings were public and included victims getting to tell their stories. More than six thousand of the perpetrators testified and applied for amnesty; this was granted to 13 percent.

			What happened to the tearful forgiveness scenarios? What about perpetrators at least showing remorse for their actions? It was not required, and few did. The goal was not to transform those individuals; it was to increase the odds that the shattered nation would function. In follow-up studies by the South African Centre for the Study of Violence and Reconciliation, victim participants commonly felt “that the TRC had been more successful at the national than the local level.” Many were outraged that there were no apologies, no reparations, that many perpetrators remained in their jobs. Interestingly, echoing chapter 15, many were equally angry about symbolic changes that had not occurred—not only is this killer still a cop, but there’s still a holiday/monument/street name celebrating apartheid. A wide majority of black (but not white) South Africans saw the TRC as fair and successful, and it accompanied South Africa’s miraculously transitioning to freedom, rather than descending into civil war. Thus TRCs show the differences between reconciliation and the likes of remorse and forgiveness.*35

			As every parent knows, a transparently insincere apology accomplishes little and can even worsen things. But deep remorse is different. The New Yorker recounts the story of Lu Lobello, an American Iraq War veteran who accidentally killed three members of a family, collateral damage during a firefight; haunted by it, he spent nine years tracking down the survivors to apologize. Or consider Hazel Bryan Massery, the snarling white teenager at the center of the iconic 1957 civil-rights-movement photograph of Elizabeth Eckford attempting to integrate Little Rock Central High School. A few years later Massery contacted Eckford to apologize.36

			Do apologies “work”? It depends. One issue is what the person is apologizing for, ranging from the concrete (“I’m sorry I broke your toy”) to the global and essentialist (“I’m sorry I’ve viewed your people as not fully human”). Another is what the apologizer aims to do about their remorse. And there’s the makeup of the recipient of the apology. Studies show that (a) victims who are oriented toward the workings of a collective system respond most to apologies that emphasize failure of that system (“I’m sorry, we police are supposed to protect, not break laws”); (b) victims most oriented to relationships respond most to apologies that are empathic (“I’m sorry for the pain that I caused you, for taking your son”); and (c) victims who are most autonomous and independent respond most to apologies accompanied by offers of compensation. There is also the issue of who is apologizing. What does it mean that in 1993 Bill Clinton apologized to Japanese Americans for their World War II internment? While the apology was laudable, and accompanied by reparative money, could Clinton speak for FDR?37

			The issue of reparations is immensely complicated. At one extreme, reparations can be the ultimate proof of sincerity. This is at the heart of the slavery reparations movement—so much of America’s growth into economic privilege was built on slavery, and so many of the subsequent benefits of the successful economy have been systematically denied to African Americans, that there should be reparations to the descendants of slaves. At the other extreme, reparations meant to purchase forgiveness offend—this was the reasoning behind the newly born state of Israel’s refusal of reparations from Germany, unless it was accompanied by adequate remorse.

			At the end of these steps might arise one of the strangest things humans do—we forgive.38 For starters, forgiving is not forgetting. If nothing else, that’s neurobiologically unlikely. A rat learns to associate a bell with a shock and freezes when it hears it. When the next day the bell repeatedly sounds without being accompanied by a shock, causing the freezing behavior to “extinguish,” the memory trace of that learning does not evaporate. Instead it is overlaid with newer learning—“Today the bell is not bad news.” As proof, suppose that the day after that, the bell again signals shock. If the initial learning of “bell = shock” had been erased, it would take as long this day to learn the association as it did the first. Instead there is rapid reacquisition: “bell = shock again.” Forgiving someone doesn’t mean you’ve forgotten what he did.

			There is a subset of victims who claim to have forgiven the perpetrator, to have relinquished their anger and desire for punishment. I include the word “claim” not to imply skepticism but to indicate that forgiveness is a self-reported state that can be claimed but not proven.

			Forgiveness can occur as a religious imperative. In the June 2015 Charleston church massacre, white supremacist Dylann Roof killed nine parishioners at the Emanuel African Methodist Episcopal Church. Two days later, at Roof’s arraignment, stunningly, family members of the dead were there to forgive him and pray for his soul.39

			Forgiveness can take extraordinary cognitive reappraisal. Consider the case of Jennifer Thompson-Cannino and Ronald Cotton.40 In 1984 Thompson-Cannino was raped by a stranger. In a police lineup she identified Cotton with great certainty; despite claiming innocence, he was convicted and sentenced to life in prison. In the years after, friends tentatively wondered if she could now put the nightmare behind her. “Like hell I’m able to” would be her response. She was consumed with her hatred for Cotton, with her desire to harm him. And then, more than ten years into his prison sentence, DNA evidence exonerated Cotton. Another man had done it; he was incarcerated in Cotton’s prison for other rapes and bragged about getting away with this one. Thompson-Cannino had identified the wrong man and convinced a jury. Issues of hatred or forgiveness were now on the other foot.

			When they finally met, after Cotton’s release and pardon, Thompson-Cannino said, “If I spent every minute of every hour of every day for the rest of my life telling you that I’m sorry, can you ever forgive me?” And Cotton said, “Jennifer, I forgave you years ago.” His ability to do so involved profound reappraisal: “Forgiving Jennifer for picking me out of that lineup as her rapist took less time than people think. I knew she was a victim and was hurting real bad. . . . We were the victims of the same injustice by the same man, and this gave us a common ground to stand on.” A complete reappraisal that made them Us in their victimhood. The two now lecture together about the need for judicial reform.

			Ultimately, forgiveness is usually about one thing—“This is for me, not for you.” Hatred is exhausting; forgiveness, or even just indifference, is freeing. To quote Booker T. Washington, “I shall allow no man to belittle my soul by making me hate him.” Belittle and distort and consume. Forgiveness seems to be at least somewhat good for your health—victims who show spontaneous forgiveness, or who have gone through forgiveness therapy (as opposed to “anger validation therapy”) show improvements in general health, cardiovascular function, and symptoms of depression, anxiety, and PTSD. Chapter 14 explored how compassion readily, perhaps inevitably, contains elements of self-interest. The compassionate granting of forgiveness epitomizes this.41

			We’ve now focused on forgiveness, apology, reparation, reconciliation, and the extent to which TRCs were about reconciliation rather than forgiveness. What about the “truth” part? It facilitates the healing process enormously. In the TRCs, perpetrators spilling truth—detailed, exhaustive, unflinching, and public—was the highest priority for victims. It’s the need to know what happened; it’s getting the villain to say the words; it’s to show the world, “Look what they did to us.”





Recognizing Our Irrationalities


			Despite the claims of some economists, we are not rational optimization machines. We are more generous in games than logic predicts; we decide if someone is guilty based on reasoning but then decide their punishment based on emotion; roughly half of us make different decisions about sacrificing one to save five, depending if it involves pushing a person versus pulling a lever; we effortlessly resist cheating in circumstances where no one would know; we make strong moral decisions without being able to explain why. Thus it’s a good idea to recognize the systematic features to our irrationality.

			Sometimes we aim to eliminate these irrationalities. Perhaps the most fundamental one is the common visceral resistance to a simple fact—you don’t make treaties with friends; it’s to be expected that you passionately hate those whose hands you are about to shake, and that can’t be an impediment to doing so. Another domain concerns discrepancies between our conscious opinions and what our implicit biases lead us to do. As we saw, Us/Them edges can be softened when implicit biases are made explicit. Doing so need not eliminate that bias—after all, you can’t readily reason yourself out of a belief that you weren’t originally reasoned into. Instead, revealing implicit biases indicates where to focus your monitoring to lessen their impact. This notion can be applied to all the realms of our behaviors being shaped by something implicit, subliminal, interoceptive, unconscious, subterranean—and where we then post-hoc rationalize our stance. For example, every judge should learn that judicial decisions are sensitive to how long it’s been since they ate.

			Another example to watch out for is the human potential for irrational optimism. For example, while people might accurately assess the risk of a behavior, they tend toward distortive optimism when assessing risk to themselves—“Nah, that couldn’t happen to me.” Irrational optimism can be great; it’s why only about 15 percent instead of 99 percent of humans get clinically depressed. But as emphasized by the Nobel Prize–winning psychologist Daniel Kahneman, irrational optimism in warfare is disastrous. This can range from the theologically optimistic conviction that God is on your side to the tendency of military strategists to overestimate their side’s capabilities and underestimate those of the opposition—“piece of cake, full steam ahead” becomes the logical conclusion.42

			A final domain of irrationality that must be recognized concerns chapter 15’s “sacred values,” where purely symbolic acts can count for more than hard-nosed material concessions. Rationality may be key to establishing peace, but the irrational importance of sacred values is key to establishing lasting peace.





Our Incompetence at and Aversion to Killing


			Video cameras are sufficiently ubiquitous these days to make “privacy” a threatened phenomenon. One consequence of such ubiquity is that scientists can be voyeuristic in new ways. Which has produced an interesting finding.

			It concerns riots in soccer stadiums—“football hooliganism,” battles between ethnic or nationalist groups, partisans of each team, or often right-wing skinheads going at it. Footage of such events shows that few people actually fight. Most are on the sidelines watching or running around like agitated, headless chickens. Of those who fight, most throw an ineffectual punch or two before discovering that punching makes your hand hurt. The actual fighters are a tiny subset. As stated by one researcher, “humans are bad at [close-range, hand-to-hand] violence, even if civilization makes us a bit better at it.”43

			Even more interesting is the evidence of our strong inhibitions against doing grievous harm to someone up close.

			The definitive exploration of this is the 1995 book On Killing: The Psychological Cost of Learning to Kill in War and Society, by David Grossman, a professor of military science and retired U.S. Army colonel.44

			He frames the book around something noted after the Battle of Gettysburg. Of the almost 27,000 single-load muskets recovered from the field, almost 24,000 of them were loaded and unfired; 12,000 were loaded multiple times, 6,000 loaded three to ten times. Lots of soldiers were standing there thinking, “I’m going to shoot soon, yes I am, hmm, maybe I should reload my rifle first.” These weapons were recovered from the thick of the battlefield, from men whose lives were at risk while they were reloading. In Gettysburg most deaths were caused by artillery, not the infantry on the ground. In the heat of crazed battle, most men would load, tend to the wounded, shout orders, run away, or wander in a daze.

			Similarly, in World War II only 15 to 20 percent of riflemen ever fired their guns. The rest? Running messages, helping people load ammunition, tending to buddies—but not aiming a rifle at someone nearby and pulling a trigger.

			Psychologists of warfare emphasize how, in the heat of battle, people don’t shoot another human out of hatred or obedience, or even from knowing that this enemy is trying to kill them. Instead it’s the pseudokinship of bands of brothers—to protect your buddies, to not let the guys next to you down. But outside those motivations, humans show a strong natural aversion to killing at close range. The most resistance is against hand-to-hand combat with a knife or bayonet. Next comes short-range firing with a pistol, then long-range firing, all the way to the easiest, which is bombs and artillery.

			The resistance can be psychologically modified. It’s easier when you aren’t targeting an identified individual—throwing a grenade into a group rather than shooting at one person. Killing as an individual is harder than in a group—while only that small subset of World War II riflemen fired their weapons, nearly all weapons operated by a team (e.g., machine guns) were fired. Responsibility is diluted, much as when a firing squad would know that one of them had received a blank, allowing every shooter to know that they might not have actually killed someone.

			Grossman’s premise is supported by something new and startling. Since it morphed from “battle fatigue” or “shell shock” into a formal psychiatric illness, combat PTSD has been framed as a result of the sheer terror of being under attack, of someone trying to kill you and those around you. As we’ve seen, it is an illness where fear conditioning is overgeneralized and pathological, an amygdala grown large, hyperreactive, and convinced that you are never safe. But consider drone pilots—soldiers who sit in control rooms in the United States, directing drones on the other side of the planet. They are not in danger. Yet their rates of PTSD are just as high as those of soldiers actually “in” war.

			Why? Drone pilots do something horrifying and fascinating, a type of close-range, intimate killing like nothing in history, using imaging technology of extraordinary quality. A target is identified, and a drone might be positioned invisibly high in the sky over the person’s house for weeks, the drone operators always watching, waiting, say, for a gathering of targets in the house. You watch the target coming and going, eating dinner, taking a nap on his deck, playing with his kids. And then comes the command to fire, to release your Hellfire missile at supersonic speed.

			Here’s one drone pilot, describing his first “kill”—three Afghanis targeted from his air force base in Nevada. The missile has hit, and he watches through an infrared camera, which transmits heat signatures:


The smoke clears, and there’s pieces of the two guys around the crater. And there’s this guy over here, and he’s missing his right leg above his knee. He’s holding it, and he’s rolling around, and the blood is squirting out of his leg, and it’s hitting the ground, and it’s hot. His blood is hot. But when it hits the ground, it starts to cool off; the pool cools fast. It took him a long time to die. I just watched him. I watched him become the same color as the ground he was lying on.45

			But there would be more. Pilots wait to see who retrieves the bodies, who comes to the funeral, ready to perhaps release another strike. Or in other circumstances the pilot might watch as an American convoy approaches a roadside IED booby trap, unable to warn them, or watch insurgents execute a shrieking civilian begging for mercy.

			The pilot above was twenty-one when he made that first kill; he would eventually accumulate 1,626 drone-mediated kills.* No personal danger, an omnipresent eye in the sky. He could finish his shift and get a doughnut on the way home. Yet he and many of his fellow drone pilots succumb to devastating PTSD.

			After reading Grossman, the explanation is simple. The deepest trauma is not the fear of being killed. It’s doing the close-up, individuated killing, watching someone for weeks and then turning him the color of the ground. Grossman cites that during World War II there were low rates of psychiatric breakdowns among sailors and medics—people who were just as endangered as infantrymen but killed either impersonally or not at all.

			Militaries train soldiers to override their inhibitions against killing, and Grossman notes that the training has become more effective—trainees no longer fire at bull’s-eyes; instead it’s rapid-fire situations of mobile virtual-reality figures coming at you, where shooting becomes reflexive. In the Korean War, 55 percent of American riflemen fired their weapons; in the Vietnam War, over 90 percent. And this was before the rise of violent, desensitizing video games.

			Maybe there will soon be completely different types of wars. Perhaps drones themselves will decide when to fire. Maybe wars will consist of autonomous weapons fighting each other, or each side racing to win with the most effective cyberattack on the other’s computers. But as long as we still see the faces of those we kill, this seemingly natural inhibition will be vital.





THE POSSIBILITIES


			It’s remarkable the things humans can spend their lives studying. You can be a coniologist or a caliologist, studying dust or birds’ nests, respectively. There are batologists and brontologists, pondering brambles and thunder, and vexillologists and zygologists, with their dazzling knowledge of flags and of methods for fastening things. On and on—odontology and odonatology, phenology and phonology, parapsychology and parasitology. A rhinologist and a nosologist fall in love and have a child who becomes a rhinological nosologist, studying the classification of nose diseases.

			The preceding pages suggest the possibility of “peaceology,” the scientific study of the effects of trade, demographics, religion, intergroup contact, reconciliation, and so on, on the ability of humans to live in peace. An intellectual venture with great potential to help the world.

			But with each new example of us at our worst, from the pinpricks of petty meanness to massive carnage, this intellectual venture can feel like rolling a boulder uphill. And thus, to falsely separate cognition and affect, we conclude these many pages by fueling the emotional rather than intellectual certainty that there is hope, that things can change, that we can be changed, that we personally can cause change.





Rousseau with a Tail


			For more than thirty years I spent my summers studying savanna baboons in the Serengeti ecosystem in East Africa. I love baboons, but I must admit that they’re often violent and abusive, so that the weak suffer at the canines of the strong. Okay, some detachment—they’re a highly sexually dimorphic tournament species with extensive escalated aggression and a strong propensity toward frustration displacement—i.e., they can be intensely shitty to one another.

			—

			In the mid-1980s the baboon troop adjacent to my study group hit the jackpot. Their territory included a tourist lodge; as at tourist places anywhere in the wilds, it had always been a challenge keeping wildlife from feeding on food garbage. Hidden in a grove of trees far from the lodge was a deep garbage pit, surrounded by a fence. But baboons climb fences, fences get knocked down, gates are left open—and that neighboring troop had taken to foraging daily in the dump. Like another widely dispersed primate, humans, baboons eat almost anything—fruit, plants, tubers, insects, eggs, prey they’ve killed, dead things they’ve scavenged.



					The remainders of one of my males, the morning after being attacked by a coalition of rivals



			This transformed the “Garbage Dump” troop. Baboons normally descend from their sleeping trees at dawn and walk ten miles a day foraging. Garbage Dumpers slept in trees above the dump, waddled down at eight o’clock to meet the garbage tractor from the lodge, spend ten minutes in frenzied competition for discarded roast beef and drumsticks and plum pudding, and then waddle out for a nap. I’d even darted Garbage Dump animals and studied them with colleagues—they put on weight, thickened with subcutaneous fat, had elevated circulating levels of insulin and triglycerides, had the start of metabolic syndrome.46



					Breakfast time, as garbage is dumped from a cart



			Somehow baboons in “my” troop got word of the feasting over the hill, and soon half a dozen would head over each morning to join. It wasn’t random who did this, who would try to compete for food against fifty or sixty Thems. The ones who tried were male, big, and aggressive. And morning is when baboons do much of their socializing—sitting in contact, grooming, playing—so going for garbage meant forgoing the socializing. The males who went each morning were the most aggressive, least affiliative members of the troop.

			Not long after, there was a tuberculosis outbreak among the Garbage Dump baboons. In humans tuberculosis is a chronic disease, slowly consuming you with “consumption.” In nonhuman primates, TB is wildfire, spreading rapidly, killing within weeks. Kenyan wildlife vet colleagues and I identified the cause of the outbreak—the meat inspector at the lodge was being bribed to approve tubercular cows for slaughter; animals were killed, unsightly lesionish organs discarded and then consumed by baboons. Most of the Garbage Dump troop died, as did all my males who raided the dump.47

			This was kinda upsetting to me; I habituated a new troop at the other end of the park and wouldn’t go anywhere near the remnants of my troop for half a dozen years. Finally, my soon-to-be wife visiting Kenya for the first time, I worked up the nerve to return to the troop, to show her the baboons of my youth.

			They were unlike any baboon troop documented, exactly like what you’d expect if you eliminated half the adult males, producing a 2:1 female-to-male ratio instead of the typical 1:1, and if the males remaining were particularly unaggressive and affiliative.48

			They stayed close together, sat in contact, and groomed more than average. Levels of aggression were lower, and in an informative way. Males still had a dominance hierarchy; number three would still fight with numbers four and two, defending his status and seeking a promotion. But there was minimal displacement aggression onto innocent bystanders—when number three lost a fight, he’d rarely terrorize number ten or a female. Stress hormone levels were low; the neurochemistry of anxiety and benzodiazepines worked differently in these individuals.



			Here’s a measure of it, a picture that, if you’re a baboon-ologist, is more surprising than one showing baboons inventing the wheel—two adult males grooming. That hardly ever happens. Except in this troop.

			And now the most important part. Female baboons remain in their birth troop, whereas males get itchy around puberty and leave, trying their luck anywhere from the troop next door to one thirty miles away. By the time I returned to this troop, most of the males who had avoided the TB had died; the troop was filled with males who had transferred in after the TB. In other words, adolescent males had grown up in typical baboon troops and then joined this one and adopted the style of low aggression and high affiliation. The troop’s social culture was being transmitted.

			How? Adolescents who joined the troop were no less aggressive or displacing than those joining other troops—there wasn’t self-selection. There was no evidence of social instruction occurring. Instead the most likely explanation involved resident females. These were probably the least stressed female baboons on earth, not being subject to typical male displacement aggression. In this more relaxed state, they were more willing to risk affiliative overtures to new individuals—in a typical baboon troop it is more than two months before females first groom or sexually solicit new transfer males; in this troop it was a matter of days to weeks. Coupled with the lack of displacement aggression from resident males, this caused the new-transfer males to gradually change, assimilating into the troop culture in about six months. Thus, when treated in a less aggressive, more affiliative manner, adolescent baboons start doing the same.

			In 1965 a rising star of primatology, Irven DeVore of Harvard, published the first overview of the subject.49 Discussing his own specialty, savanna baboons, he wrote that they “have acquired an aggressive temperament as a defense against predators, and aggressiveness cannot be turned on and off like a faucet. It is an integral part of the monkeys’ personalities, so deeply rooted that it makes them potential aggressors in every situation.” Thus savanna baboons became, literally, textbook examples of an aggressive, stratified, male-dominated primate. Yet as we see here, this picture is not universal or inevitable.

			Humans have formed both small nomadic bands and megastates and have demonstrated a flexibility whereby uprooted descendants of the former function in the latter. Human mating patterns are atypically flexible, and our societies feature monogamy, polygyny, or polyandry. We have fashioned religions where certain types of violence earn you paradise and others where the same violence consigns you to hell. Basically, if baboons unexpectedly show this much social plasticity, so can we. Anyone who says that our worst behaviors are inevitable knows too little about primates, including us.





One Person


			Somewhere between neurons, hormones, and genes on one hand and culture, ecological influences, and evolution on the other, sits the individual. And with more than seven billion of us, it’s easy to feel that no single individual can make much of a difference.

			But we know that’s not true. There’s the obligatory list of those who changed everything—Mandela, Gandhi, MLK, Rosa Parks, Lincoln, Aung San Suu Kyi. Yes, they often had scads of advisers. But they were the catalysts, the ones who paid with their freedom or their lives. And there are whistle-blowers who took great risks to trigger change—Daniel Ellsberg, Karen Silkwood, W. Mark Felt (Watergate’s Deep Throat), Samuel Provance (the U.S. soldier who revealed the abuses at Abu Ghraib Prison), Edward Snowden.*

			But there are also lesser-known people, acting alone or in small numbers, with extraordinary impact. Take Mohamed Bouazizi, a twenty-six-year-old fruit seller in Tunisia, then in its twenty-third year of corrupt and repressive rule by a dictator. At the market the police hassled Bouazizi about an imaginary permit, expecting a bribe. He refused, not out of principle—he’d often bribed—but because he lacked the money. He was kicked and spat upon, his fruit cart overturned. His complaint at the government office was ignored. And within an hour of being preyed upon by the police, on December 10, 2010, Bouazizi stood in front of that office, doused himself with gasoline, shouted, “How do you expect me to make a living?” and set himself on fire.

			Bouazizi’s immolation and death triggered protests in Tunisia against the leader, Zine El Abidine Ben Ali, against his ruling party, against the police. The protests grew, and within a month the government and Ben Ali were overthrown. Bouazizi’s act led to protests in Egypt, toppling Hosni Mubarak’s thirty-year dictatorship. Likewise in Yemen, ending Ali Abdullah Saleh’s thirty-four-year rule. And in Libya, leading to the overthrow and killing of Muammar Gaddafi after forty-three years in power. And in Syria, where protests morphed into civil war. And in Jordan, Oman, and Kuwait, leading to the resignations of their prime ministers. And in Algeria, Iraq, Bahrain, Morocco, and Saudi Arabia, producing semblances of governmental reform. The Arab Spring. Bouazizi wasn’t thinking about political reform in the Muslim world when he lit the match; instead there was rage with nowhere to go but inward. Make what you will of the Arab Spring’s brief hopefulness, followed by new strongmen, violence, refugees, and the catastrophe of Syria and ISIS. And perhaps history makes the self-immolator as much as the self-immolator makes history—regional discontent had long been brewing. Regardless, Bouazizi’s singular act catalyzed millions in twenty countries to decide that they could cause change.



					Antigovernment protestors displaying a picture of Bouazizi



			There have been other singular acts. In the mid-1980s a commemoration was being held at the Pearl Harbor Memorial on the anniversary of the attack. A group of survivors who had gathered were approached by an elderly man. This was his third trip to the memorial, trying to work up his nerve. He approached the survivors and, in halting English, apologized.50

			The man, Zenji Abe, was a fighter pilot in the 1937 Japanese invasion of China and throughout the Pacific during World War II—including helping to lead the attack on Pearl Harbor.

			Little in his earlier life predicted Abe’s apologizing as an old man. His inculcation into war started early, when he joined a military academy as a seventh grader. His experience of war was detached—he never killed an American soldier at close quarters. The Pearl Harbor attack felt like a training exercise. His sense of responsibility could readily have been blunted, as the bomb he dropped hadn’t detonated. And his country had been defeated.

			Some things favored Abe’s gesture. He had been captured and spent a year as a POW, treated decently by Americans. And he felt shame about the attack—pilots had been told that war against America had been declared that morning, that American defenses would be ready. He soon learned that instead it was a sneak attack.

			Some larger factors favored his gesture as well. Japanese/American relations had transformed. And Americans were not traditional enemies. The racial, cultural, and geographic distance might have facilitated pseudospeciation of Americans, but it was nouveau pseudospeciation, contrasting with centuries of hatred of a nearby enemy—Abe never went to China to apologize for the Rape of Nanking. As we know, Thems come in different categories.

			So these likelihoods and unlikelihoods converged, and Abe stood there, along with nine other pilots who had flown that day, apologizing. Some survivors refused the overture. Most accepted it. Abe and other pilots made subsequent trips to Pearl Harbor and had multiday meetings with American survivors; reconciliative handshakes were broadcast on the Today show for the fiftieth anniversary. Survivors generally considered the pilots to have just been following orders and found their current actions brave and admirable. Abe became close with one survivor, Richard Fiske, a docent at the memorial. Fiske had been on one of the ships during the attack, lost many friends among the 2,390 Americans killed, fought at Iwo Jima, described himself as so hating the Japanese that he developed a bleeding ulcer. For reasons he never fully understood, Fiske was the first to accept Abe’s gesture. Other Japanese and Americans became close as well, visiting the homes and, eventually, the grave sites of their ex-enemies.



					Left: Zenji Abe, December 6th, 1941; right: Abe and Richard Fiske, December 6th, 1991



			The process was rich in symbols, starting with an apology that, as we’ve seen, changes nothing and everything. Abe gave Fiske money so that, for the rest of his life, Fiske placed flowers monthly at the memorial. Fiske, a bugler, took to playing not only taps at the memorial, but the Japanese equivalent as well. Some semblance of Us-ness had emerged that included everyone there that infamous day.

			Perhaps most important, Abe’s singular act isn’t singular. There are now travel agencies specializing in serving American Vietnam War vets returning to Vietnam for reconciliation ceremonies with ex–Viet Cong. Veterans have spearheaded organizations such as Friends of Danang, doing service projects in Vietnam, building schools, clinics, and literal bridges.51

			This picture segues into another extraordinary act. Arguably the single most shocking event of the Vietnam War, an atrocity that finally shook America’s self-perception as a force of good, was the My Lai Massacre.

			On March 16, 1968, a company of American soldiers, under the command of Lieutenant William Calley Jr., attacked the unarmed civilians of the village of My Lai.52 The company had been in Vietnam all of three months and had had no direct enemy contact. They had, however, suffered twenty-eight deaths or injuries due to booby traps and mines, reducing the company’s number to around one hundred. The common interpretation, one that we readily recognize by now, is that they had a fierce, vengeful desire to connect faces to this faceless enemy. The official rationale was that the village harbored Viet Cong fighters and civilian sympathizers; there is minimal evidence to support this. Some of the participants reported being instructed to kill only Viet Cong fighters; others that they should kill everyone, burn houses, kill livestock, and destroy wells.

			Regardless of these conflicting reports, the rest, as they say, is agonizing history. Between 350 and 500 unarmed civilians, including infants and elderly people, were killed. Bodies were mutilated and dumped down wells, huts and fields set ablaze, numerous women gang-raped before being killed. Calley was described to have personally shot children under their mothers who had died sheltering them. The Americans encountered no enemy fire, found no military-aged men. It was destruction of biblical proportions, or Roman proportions, or Crusader, or Viking, or . . . This destruction was photographed. The horror is worsened because My Lai was not a solitary atrocity, and the government labored to conceal events and slapped Calley on the wrist, sentencing him to three years of house arrest.

			There was by no means universal participation by Americans (ultimately twenty-six soldiers were criminally charged, with Calley the only one convicted; “just following orders” was the order of the day).*53 Individual thresholds varied. One soldier killed a mother and child and then refused to do more. Another helped herd civilians together but refused to fire. Some refused orders outright, even in the face of threats of court-martial or being shot. One, PFC Michael Bernhardt, refused and threatened to report events to superiors; officers subsequently placed him on more dangerous patrols, perhaps hoping he’d be killed.



					Iconic photos of the nightmare. Left: civilians seconds before being killed; the woman in the back holding her child had just been raped. Right: dead villagers



			And three men halted the killings. Predictably, they were outsiders. The catalyst was Warrant Officer Hugh Thompson Jr., age twenty-five, who was flying a helicopter, along with two crew members, Glenn Andreotta and Lawrence Colburn. Perhaps pertinent to what occurred was the fact that Thompson descended from Native American survivors of the Trail of Tears death march; his religious parents raised him, in the 1950s in rural Georgia, to oppose segregation. Colburn and Andreotta were observant Catholics.

			Thompson and his crew had flown over the village, intending to aid the infantry fighting Viet Cong. Instead of evidence of a battle, they saw masses of dead civilians. Thompson initially thought that the village was under attack, with Americans protecting villagers, but couldn’t figure out where the attack was coming from. He landed the copter amid the chaos and saw one soldier, Sergeant David Mitchell, firing into a mass of injured, wailing civilians in a ditch and another, Captain Ernest Medina, shoot a woman point-blank; Thompson realized who was doing the attacking. He confronted Calley, who was higher ranking than him and told him to mind his damn business.

			Thompson saw a group of women, children, and elderly men huddling by a bunker with American soldiers approaching them, preparing to attack. Discussing what happened next, more than twenty years later, he described his feelings about those soldiers: “It’s—they were the enemy at that time, I guess. They were damn sure the enemy to the people on the ground.” He did something of dizzying strength and bravery, something that proves every word in this book about how Us/Them categorizations can change in an instant. Hugh Thompson landed his helicopter between the villagers and the soldiers, trained his machine guns on his fellow Americans, and ordered his crew to mow them down if they attempted to further harm the villagers.*,*



					Left: Glenn Andreotta; Right, right to left: Hugh Thompson, Lawrence Colburn, and Do Hoa, who they rescued from the ditch as a child, My Lai village, 1998



			Thus we have one person impulsively changing history in twenty countries, another who overcame decades of hatred to catalyze reconciliation, others who overcame every reflex of their training to do the right thing. Time for one last singular person, one who inspires me enormously.

			The person was the Anglican cleric John Newton, born in 1725.54 Well, that doesn’t sound too exciting. He’s best known for composing the hymn “Amazing Grace.” Oh, cool; that, along with Leonard Cohen’s “Hallelujah,” always move me. Newton also was an abolitionist, a mentor to William Wilberforce in his parliamentary battle to outlaw slavery in the British Empire. Okay, getting better. Now get this—as a young man, Newton had captained a slave ship. Bingo, that’s the setup—a man overseeing and profiting from slavery, a flash of religious and moral insight, dramatic recategorization of Us and Them, dramatic expansion of his humanity, dramatic commitment to make amends for the savagery he had done. You can practically see chapter 5’s neural plasticity on fire in Newton’s brain.

			Nothing resembling this occurred.

			Newton, the son of a ship captain, goes to sea with his father at age eleven. At eighteen he is pressed into service in the navy, tries to desert, and is flogged. Newton manages to escape and works on a West African slave ship. Get ready for him to see the similarity between the captivity of these people and his own experience, to have a revelation.

			No such thing occurs.

			He works on the slave ship and is apparently so detested by everyone that they dump him in what is now Sierra Leone with a slaver who gives him to his wife as a slave. He’s rescued; the ship he is on, returning to England, is caught in a horrific storm and starts to sink. Newton calls out to God, the ship doesn’t sink, and he has a spiritual conversion to evangelical Christianity. He signs up to work on another slave ship. Get ready now—he’s found God, has just been a slave himself, and is poised to suddenly recognize the horror that was the slave trade.

			Nope.

			He professes some sympathy for slaves, grows deeper into his evangelical conversion. He eventually becomes captain of a slave ship and works another six years before stopping. At last he’s seen his actions for what they are.

			Not that either.

			It’s because his health was declining from those tough voyages. He works as a tax collector, studies theology, applies to become an Anglican priest. And he invests his money in slave-trading ventures. In the parlance of my native Brooklyn, from when it was not yet trendy, can you believe this fuggin’ guy?

			He becomes a popular preacher, known for his sermons and pastoral concern; he composes hymns, speaks out for the poor and downtrodden. Presumably, somewhere along the way he stops investing in slavery; maybe because of his conscience, maybe because better investments come along. Still, not a word about slavery. Finally he publishes a pamphlet denouncing it, thirty-four years after stopping being a slaver. That’s a lot of time spent as a blind wretch. Newton’s is a rare voice among abolitionists, someone who has witnessed those horrors, let alone inflicted them. He becomes the major abolitionist voice in England and lives to see England ban the slave trade in 1807.

			There’s no way I could ever be Thompson, Andreotta, or Colburn. I’m not brave; I run away to solitary African field sites instead of confronting difficult things. Maybe, at best, I would have been one of the soldiers standing in confusion, compelled by the inhibitions that Grossman discusses into repeatedly checking my rifle to make sure it was loaded, rather than firing it. I see little indication that as an old man I will achieve the grace and moral stature of a Zenji Abe or a Richard Fiske. Bouazizi’s act is incomprehensible to me.

			But Newton, Newton is different; Newton is familiar. He takes convenient comfort from the Bible’s embrace of slavery, spends decades resisting the possibility of his personal morality moving past its conventions. He shows great empathy but applies it selectively. He expands his circle of who counts as an Us, but only so far. We saw how the person who emerges from the crowd to run into the burning building typically acts before thinking, displaying an ingrained automaticity of doing the harder, better thing. There’s no automaticity with Newton. We can practically see his dlPFC laboring with all that rationalizing—“There’s nothing I can do,” “It’s too big for one person to challenge,” “Better to be concerned about the needy who are close to home,” “I can use the profits from the investments for good works,” “Those people really are so fundamentally different,” “I’m tired.” Yes, journeys begin with a single step, but with Newton it’s ten steps forward, nine self-serving ones back. Thompson’s moment of moral perfection feels as unattainable to me as aspiring to be a gazelle or a waterfall or an incandescent sunset. But there’s hope for us, with our foibles and inconsistencies and frailties, as we watch Newton slowly lurch his way toward being a moral titan.





					1788 illustration created by abolitionists of the number of slaves (487) a British ship could legally hold during a trans-Atlantic voyage. In actuality, ships transported far more people than that.





Finally—the Potential for Collective Power


			There is an anecdote from the Peninsular War of 1807–14, told by Major General George Bell, then an ensign: There was a bridge separating the opposing British and French, with a sentry posted by each side to sound an alarm should the enemy rush across the bridge.55 A British officer was making rounds and found the British sentry there in an unlikely situation—carrying British and French muskets, one on each shoulder, seemingly guarding the bridge for the two opposing armies, with no French sentry in sight. His explanation? His French counterpart had snuck off to buy some liquor for them to share and, naturally, he was watching the other guy’s gun.

			Fraternizing between enemy soldiers is remarkably frequent in war. It’s most common when they’re the same race and major religion and when they are enlisted men rather than officers. It’s also more common when individual enemies, rather than groups, encounter each other, when it’s the same person day after day (e.g., guarding the bridge opposite you), when someone could have shot you but didn’t. Fraternizing rarely involves discussions about life, death, and geopolitics; instead it’s things like bartering food (since the other side’s rations can’t be as bad as yours), cigarettes, or alcohol or complaining about the miserable weather, the miserable officers.56

			In the Spanish Civil War, Republican and Fascist troops regularly met at night to drink, barter, and exchange newspapers, everyone on the lookout for officers. In the Crimean War there was regular bartering across enemy lines of Russian vodka for French baguettes. One British soldier in the Peninsular War described how in the evenings, British and French troops played cards around campfires. And in the American Civil War, Yankee and Rebel soldiers would fraternize, barter, trade newspapers, and, with piercing poignancy, hold joint baptismal services the evening before a battle that would clearly be a bloodbath.

			Thus enemy soldiers have frequently found common ground. A little over a hundred years ago, two such events occurred on a stunning scale.

			It must be admitted that some good came of World War I—thanks to the subsequent collapse of three empires, people in the Baltic, the Balkans, and Eastern Europe gained independence. But from anyone else’s perspective, it was a pointless slaughter of fifteen million people. The war to end all wars, leading to the ruinous peace to end all peace, turned out to be just another of centuries of examples of Europe devouring its young with meaningless conflict. But amid the quagmire of World War I came two examples of hope that, for want of a better word, seem almost miraculous.



					German and British soldiers posing together



			First is the Christmas Truce of 1914, when officers up and down the trenches tentatively shouted, “No shoot,” in another language and met opposing officers in no-man’s-land. The truce began as an agreement to halt hostilities during Christmas dinner and for retrieval of the dead.

			Things spread from there. As extensively documented, soldiers on both sides loaned each other shovels for digging graves. And then helped out. And then held joint burial services. Which led to exchanges of food, drink, and tobacco. Eventually, unarmed soldiers swarmed into no-man’s-land, prayed and caroled together, shared dinner, exchanged gifts. Enemy combatants took group portraits; buttons and helmets were exchanged as souvenirs; plans were made to meet when the war was over. Most famously, soccer matches were held with improvised balls, with scores rarely kept.57

			One historian records a chilling anecdote concerning a German soldier writing home about the truce, mentioning that not everyone participated—there was one soldier who condemned the others as traitors, an obscure corporal named . . . Hitler. But for most of the five hundred miles of trenches, the truce held through Christmas, and often even New Year’s. It took officers’ threats of court-martial to get everyone back to fighting, soldiers wishing their counterparts a safe war. Stunning, moving, heartbreaking. And with only sporadic exceptions, it never happened again, as even brief Christmas truces to retrieve the dead led to court-martials.

			Why did the 1914 truce work? The unique static nature of trench warfare meant that soldiers faced each other day after day. This prompted often-friendly taunting across the lines in the period preceding Christmas, establishing a vague sense of connection. Moreover, the repeated interactions produced a “shadow of the future”—betray the truce, and expect no-holds-barred revenge.

			The success was also aided by everyone sharing the same Judeo-Christian tradition and Western European culture; many knew the others’ language, had visited the others’ country. They were of the same race, and pejoratively calling the enemy “Fritz” completely differs from the pseudospeciation of the Vietnam War’s “gooks,” “slants,” and “dinks.”

			Additional factors explain why the truce mostly involved British and German troops. While the French fought passionately on their own soil, Brits had no particular animosity toward Germans and typically perceived themselves as fighting to save les derrières of the French, their frequent historical enemy. Ironically, during the truce British soldiers would tell Germans that they both should be fighting the French. Meanwhile, by chance, most of the German soldiers were Saxons, who expressed a cousinly affinity for British Anglo-Saxons, suggesting that they should both be fighting the Prussians, the resented dominating group in Germany.

			And perhaps most important, the truce was aided by top-down approval. Officers typically negotiated; figures such as the pope called for a truce; it was a holiday that stood for peace and good will toward all men.

			Thus we have the Christmas Truce. Remarkably, something even more miraculous occurred during the war. In what has been termed the Live and Let Live phenomenon, soldiers in the trenches repeatedly evolved stable truces without exchanging a word, without a shared religious holiday, without the sanction of officers and leaders.

			How did this occur? As documented by the historian Tony Ashworth in Trench Warfare: 1914–1918, it would begin passively. Troops on both sides ate around the same time, and guns would go silent then—who wants to interrupt dinner in order to kill someone or be killed? The same would occur during awful weather, when everyone’s priority became flooded trenches or avoiding freezing to death.58

			Mutual restraint also emerged in circumstances shadowed by the future. Wagon trains delivering food were easy artillery targets but were left unharmed, to prevent reciprocal shelling. Similarly, latrines were spared.

			These truces emerged when soldiers chose not to do something. But truces were also established by overt action. How? Have your best sniper put a bullet into the wall of an abandoned house near enemy lines. Then have him do it again and again, repeatedly hitting the same spot. What are you communicating? “Look how good our guy is. He could have aimed at you instead but chose not to. What are you going to do about it?” And the other side would reciprocate with their best sniper. An agreement to shoot over each other’s heads had been established.

			The key was ritualization—shooting repeatedly at the same inconsequential target, renewing the commitment to peace daily at the same time.

			Live and Let Live truces could withstand perturbations. Soldiers signaled the other side that they had to shoot for real for a while—officers were coming. The system survived violations. If some gung-ho rookie lobbed a shell into the others’ trenches, the most common convention was two shells back, often aimed at important targets. And then the peace would resume. (Ashworth describes such a violation, where Germans unexpectedly fired a shell into British trenches. Soon a German shouted, “We are very sorry about that; we hope no one was hurt. It is not our fault, it is that damned Prussian artillery.” And back flew two British shells.)

			Live and Let Live truces emerged repeatedly. And repeatedly brass in the rear would intervene, rotating troops, threatening court-martials, ordering savage raids requiring hand-to-hand combat that would shatter any sense of shared interests between enemies.

			We see the evolution—initial low-cost overtures with immediate benefits, such as not shooting during dinner, transitioning through gradations of increasingly elaborate restraint and signaling. And we recognize the modified Tit for Tat in dealing with truce violations, with its propensity toward cooperation, punishment for violations, mechanisms for forgiveness, and clear rules.

			So, hooray, just like social bacteria, we can evolve cooperation. But one thing that a cooperative bacterium lacks is a psyche. Ashworth thoughtfully explored the psychology of how Live and Let Live participants began to view the enemy.

			He described a sequence of steps. First, once any mutual restraint emerged, the enemy had established that they were rational, with incentives to hold fire. This prompted a sense of responsibility in dealing with them; this was initially purely self-serving—don’t violate an agreement because they’ll violate back. With time, the responsibility developed a moral tinge, tapping into most people’s resistance to betraying someone who deals reliably with them. The specific motivations for truces generated insights—“Hey, they don’t want dinner disturbed any more than we do; they don’t want to fight in this rain either; they also deal with officers who screw up everything.” There’d be a creeping sense of camaraderie.

			This produced something striking. The war machines in combatant countries spewed the usual pseudospeciating propaganda. But in studying soldiers’ diaries and letters, Ashworth observed minimal hostility toward the enemy expressed by trench soldiers; the further from the front, the more hostility. In the words of one frontline soldier, quoted by Ashworth, “At home one abuses the enemy, and draws insulting caricatures. How tired I am of grotesque Kaisers. Out here, one can respect a brave, skillful, and resourceful enemy. They have people they love at home, they too have to endure mud, rain and steel.”

			Us and Them would be in flux. If someone is shooting at you or your band of brothers, they are certainly a Them. But otherwise Them was more likely to be the rats and lice, the mold in the food, the cold. As well as any comfortable officer at headquarters who would be—in the words of another trench soldier—“[an] abstract tactician who from far away disposes of us.”



					American and German propaganda posters



			These truces could not persist; the final phases of the war obliterated them, as the British High Command adopted a nightmarish strategy of war by attrition.

			In thinking of the Christmas Truce and the Live and Let Live system, I always have the same fantasy, a very different one from the fantasy that began this book. What would have happened if there had been two additional inventions during World War I? The first is modern mass communications—texting, Twitter, Facebook. The second is a mind-set that emerged only among World War I’s shattered survivors—the cynicism of modernity. Men up and down hundreds of miles of trenches repeatedly reinvented Live and Let Live, unaware that they were not alone. Imagine texts bouncing along and across the trenches, a million soldiers at death’s door saying, “This is bullshit. None of us here want to fight anymore, and we’ve figured out a way to stop.” They could have ended it, tossed down their guns, could have ignored or ridiculed or killed any objecting officer spouting obscenities about God and country, could have gone home to kiss their loved ones and then face the real enemy, the bloated aristocracy who would sacrifice them for their own power.

			—

			It is easy to have this fantasy about the Great War, a distant museum piece festooned with twirly mustaches and silly plumed officers’ helmets. It behooves us to step back from the grainy black-and-white photos and to consider a hugely difficult thought experiment. Our contemporary adversaries kidnap girls and sell them into slavery, commit atrocities and, instead of concealing them, display the evidence online. When I read the news of the things they’ve done, I hate them passionately. It’s impossible to imagine kicking back, having a group sing-along of “I Saw Mommy Kissing Santa Claus” and exchanging Christmas tchotchkes with Al-Qaeda grunts.

			Yet time does interesting things. The hatred between Americans and Japanese during World War II was boundless. American recruiting posters advertised “Jap Hunting Licenses”; one veteran of the Pacific theater described a common event, writing in the Atlantic in 1946: [American soldiers] “boiled the flesh off enemy skulls to make table ornaments for sweethearts, or carved their bones into letter openers.”59 And there’s the bestial treatment of American POWs by the Japanese. If Richard Fiske had wound up a POW, Zenji Abe might have helped march him to death; if the former had killed the latter in battle, he might have made a souvenir of his skull. And instead, more than fifty years later, one would write a letter of condolence to the other’s grandchildren upon the death of Grandpa.

			—

			A key point of the previous chapter was that those in the future will look back on us and be appalled at what we did amid our scientific ignorance. A key challenge in this chapter is to recognize how likely we are to eventually look back at our current hatreds and find them mysterious.

			Daniel Dennett has pondered a scenario of someone undergoing surgery without anesthesia but with absolute knowledge that afterward they’d receive a drug that would erase all memory of the event. Would pain be less painful if you knew that it would be forgotten? Would the same happen to hatred, if you knew that with time it would fade and the similarities between Us and Them would outweigh the differences? And that a hundred years ago, in a place that was hell on earth, those with the most temptation to hate often didn’t even need the passage of time for that to happen?





			The philosopher George Santayana provided us with an aphorism so wise that it has suffered the fate of becoming a cliché—“Those who cannot remember the past are condemned to repeat it.” In the context of this final chapter, we must turn Santayana on his head—those who do not remember the extraordinary truces of the World War I trenches, or who do not learn of Thompson, Colburn, and Andreotta, or of the reconciliative distances traveled by Abe and Fiske, Mandela and Viljoen, Hussein and Rabin, or of the stumbling, familiar moral frailties that Newton vanquished, or who do not recognize that science can teach us how to make events like these more likely—those who do not remember these are condemned to be less likely to repeat these reasons to hope.




