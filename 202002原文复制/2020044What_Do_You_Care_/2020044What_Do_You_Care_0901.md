So he caught on. They were reporters from the「MacNeil/Lehrer Newshour.」

They were very nice, and we talked about this and that for the show Monday night. Somewhere along the line I told them I was going to have my own press conference on Tuesday, and I was going to give out my report—even though it was going to appear as an appendix three months later. They said my report sounded interesting, and they’d like to see it. By this time we’re all very friendly, so I gave them a copy.

They dropped me off at my cousin’s house, where I was staying. I told Frances about the show, and how I gave the reporters a copy of my report. Frances puts her hands to her head, horrified.

I said,「Yes, that was a dumb mistake, wasn’t it! I’d better call ’em up and tell ’em not to use it.」

I could tell by the way Frances shook her head that it wasn’t gonna be so easy!

I call one of them up:「I’m sorry, but I made a mistake: I shouldn’t have given my report to you, so I’d prefer you didn’t use it.」

「We’re in the news business, Dr. Feynman. The goal of the news business is to get news, and your report is newsworthy. It would be completely against our instincts and practice not to use it.」

「I know, but I’m naive about these things. I simply made a mistake. It’s not fair to the other reporters who will be at the press conference on Tuesday. After all, would you like it if you came to a press conference and the guy had mistakenly given his report to somebody else? I think you can understand that.」

「I’ll talk to my colleague and call you back.」

Two hours later, they call back—they’re both on the line—and they try to explain to me why they should use it:「In the news business, it’s customary that whenever we get a document from somebody (he way we did from you, it means we can use it.」

「I appreciate that there are conventions in the news business, but I don’t know anything about these things, so as a courtesy to me, please don’t use it.」

It went back and forth a little more like that. Then another「We’ll call you back.」and another long delay. I could tell from the long delays that they were having a lot of trouble with this problem.

I was in a very good fettle, for some reason. I had already lost, and I knew what I needed, so I could focus easily. I had no difficulty admitting complete idiocy—which is usually the case when I deal with the world—and I didn’t think there was any law of nature which said I had to give in. I just kept going, and didn’t waver at all.

It went late into the night: one o’clock, two o’clock, we’re still working on it.「Dr. Feynman, it’s very unprofessional to give someone a story and then retract it. This is not the way people behave in Washington.」

「It’s obvious I don’t know anything about Washington. But this is the way I behave—like a fool. I’m sorry, but it was simply an error, so as a courtesy, please don’t use it.」

Then, somewhere along the line, one of them says,「If we go ahead and use your report, does that mean you won’t go on the show?」

「You said it; I didn’t.」

「We’ll call you back.」

Another delay.

Actually, I hadn’t decided whether I’d refuse to go on the show, because I kept thinking it was possible I could undo my mistake. When I thought about it, I didn’t think I could legitimately play that card. But when one of them made the mistake of proposing the possibility, I said,「You said it; I didn’t」—very cold—as if to say,「I’m not threatening you, but you can figure it out for yourself, honey!」

They called me back, and said they wouldn’t use my report.

When I went on the show, I never got the impression that any of the questions were based on my report. Mr. Lehrer did ask me whether there had been any problems between me and Mr. Rogers, but I weaseled: I said there had been no problems.

After the show was over, the two reporters told me they thought the show went fine without my report. We left good friends.

I flew back to California that night, and had my press conference on Tuesday at Caltech. A large number of reporters came. A few asked questions about my report, but most of them were interested in the rumor that I had threatened to take my name off the commission report. I found myself telling them over and over that I had no problem with Mr. Rogers.

Afterthoughts

NOW that I’ve had more time to think about it, I still like Mr. Rogers, and I still feel that everything’s okay. It’s my judgment that he’s a fine man. Over the course of the commission I got to appreciate his talents and his abilities, and I have great respect for him. Mr. Rogers has a very good, smooth way about him, so I reserve in my head the possibility—not as a suspicion, but as an unknown—that I like him because he knew how to make me like him. I prefer to assume he’s a genuinely fine fellow, and that he is the way he appears. But I was in Washington long enough to know that I can’t tell.

I’m not exactly sure what Mr. Rogers thinks of me. He gives me the impression that, in spite of my being such a pain in the ass to him in the beginning, he likes me very much. I may be wrong, but if he feels the way I feel toward him, it’s good.

Mr. Rogers, being a lawyer, had a difficult job to run a commission investigating what was essentially a technical question. With Dr. Keel’s help, I think the technical part of it was handled well. But it struck me that there were several fishinesses associated with the big cheeses at NASA.

Every time we talked to higher level managers, they kept saying they didn’t know anything about the problems below them. We’re getting this kind of thing again in the Iran-Contra hearings, but at that time, this kind of situation was new to me: either the guys at the top didn’t know, in which case they should have known, or they did know, in which case they’re lying to us.

When we learned that Mr. Mulloy had put pressure on Thiokol to launch, we heard time after time that the next level up at NASA knew nothing about it. You’d think Mr. Mulloy would have notified a higher-up during this big discussion, saying something like,「There’s a question as to whether we should fly tomorrow morning, and there’s been some objection by the Thiokol engineers, but we’ve decided to fly anyway—what do you think?」But instead, Mulloy said something like,「All the questions have been resolved.」There seemed to be some reason why guys at the lower level didn’t bring problems up to the next level.

I invented a theory which I have discussed with a considerable number of people, and many people have explained to me why it’s wrong. But I don’t remember their explanations, so I cannot resist telling you what I think led to this lack of communication in NASA.

When NASA was trying to go to the moon, there was a great deal of enthusiasm: it was a goal everyone was anxious to achieve. They didn’t know if they could do it, but they were all working together.

I have this idea because I worked at Los Alamos, and I experienced the tension and the pressure of everybody working together to make the atomic bomb. When somebody’s having a problem—say, with the detonator—everybody knows that it’s a big problem, they’re thinking of ways to beat it, they’re making suggestions, and when they hear about the solution they’re excited, because that means their work is now useful: if the detonator didn’t work, the bomb wouldn’t work.

I figured the same thing had gone on at NASA in the early days: if the space suit didn’t work, they couldn’t go to the moon. So everybody’s interested in everybody else’s problems.

But then, when the moon project was over, NASA had all these people together: there’s a big organization in Houston and a big organization in Huntsville, not to mention at Kennedy, in Florida. You don’t want to fire people and send them out in the street when you’re done with a big project, so the problem is, what to do?

You have to convince Congress that there exists a project that only NASA can do. In order to do so, it is necessary—at least it was apparently necessary in this case—to exaggerate: to exaggerate how economical the shuttle would be, to exaggerate how often it could fly, to exaggerate how safe it would be, to exaggerate the big scientific facts that would be discovered.「The shuttle can make so-and-so many flights and it’ll cost such-and-such; we went to the moon, so we can do it!」

Meanwhile, I would guess, the engineers at the bottom are saying,「No, no! We can’t make that many flights. If we had to make that many flights, it would mean such-and-such!」And,「No, we can’t do it for that amount of money, because that would mean we’d have to do thus-and-so!」

Well, the guys who are trying to get Congress to okay their projects don’t want to hear such talk. It’s better if they don’t hear, so they can be more「honest」—they don’t want to be in the position of lying to Congress! So pretty soon the attitudes begin to change: information from the bottom which is disagreeable—」We’re having a problem with the seals; we should fix it before we fly again」—is suppressed by big cheeses and middle managers who say,「If you tell me about the seals problems, we’ll have to ground the shuttle and fix it.」Or,「No, no, keep on flying, because otherwise, it’ll look bad,」or「Don’t tell me; I don’t want to hear about it.」

Maybe they don’t say explicitly「Don’t tell me,」but they discourage communication, which amounts to the same thing. It’s not a question of what has been written down, or who should tell what to whom; it’s a question of whether, when you do tell somebody about some problem, they’re delighted to hear about it and they say「Tell me more」and「Have you tried such-and-such?」or they say「Well, see what you can do about it」—which is a completely different atmosphere. If you try once or twice to communicate and get pushed back, pretty soon you decide,「To hell with it.」

So that’s my theory: because of the exaggeration at the top being inconsistent with the reality at the bottom, communication got slowed up and ultimately jammed. That’s how it’s possible that the higher-ups didn’t know.

The other possibility is that the higher-ups did know, and they just said they didn’t know.

I looked up a former director of NASA—I don’t remember his name now—who is the head of some company in California. I thought I’d go and talk to him when I was on one of my breaks at home, and say,「They all say they haven’t heard. Does that make any sense? How does someone go about investigating them?」

He never returned my calls. Perhaps he didn’t want to talk to the commissioner investigating higher-ups; maybe he had had enough of NASA, and didn’t want to get involved. And because I was busy with so many other things, I didn’t push it.

There were all kinds of questions we didn’t investigate. One was this mystery of Mr. Beggs, the former director of NASA who was removed from his job pending an investigation that had nothing to do with the shuttle; he was replaced by Graham shortly before the accident. Nevertheless, it turned out that, every day, Beggs came to his old office. People came in to see him, although he never talked to Graham. What was he doing? Was there some activity still being directed by Beggs?

From time to time I would try to get Mr. Rogers interested in investigating such fishinesses. I said,「We have lawyers on the commission, we have company managers, we have very fine people with a large range of experiences. We have people who know how to get an answer out of a guy when he doesn’t want to say something. I don’t know how to do that. If a guy tells me the probability of failure is 1 in 10 5, I know he’s full of crap—but I don’t know what’s natural in a bureaucratic system. We oughta get some of the big shots together and ask them questions: just like we asked the second-level managers like Mr. Mulloy, we should ask the first level.」

He would say,「Yes, well, I think so.」

Mr. Rogers told me later that he wrote a letter to each of the big shots, but they replied that they didn’t have anything they wanted to say to us.

There was also the question of pressure from the White House.

It was the President’s idea to put a teacher in space, as a symbol of the nation’s commitment to education. He had proposed the idea a year before, in his State of the Union address. Now, one year later, the State of the Union speech was coming up again. It would be perfect to have the teacher in space, talking to the President and the Congress. All the circumstantial evidence was very strong.

I talked to a number of people about it, and heard various opinions, but I finally concluded that there was no pressure from the White House.

First of all, the man who pressured Thiokol to change its position, Mr. Mulloy, was a second-level manager. Ahead of time, nobody could predict what might get in the way of a launch. If you imagine Mulloy was told「Make sure the shuttle flies tomorrow, because the President wants it,」you’d have to imagine that everybody else at his level had to be told—and there are a lot of people at his level. To tell that many people would make it sure to leak out. So that way of putting on pressure was very unlikely.

By the time the commission was over, I understood much better the character of operations in Washington and in NASA. I learned, by seeing how they worked, that the people in a big system like NASA know what has to be done—without being told.

There was already a big pressure to keep the shuttle flying. NASA had a flight schedule they were trying to meet, just to show the capabilities of NASA—never mind whether the president was going to give a speech that night or not. So I don’t believe there was any direct activity or any special effort from the White House. There was no need to do it, so I don’t believe it was done.

I could give you an analog of that. You know those signs that appear in the back windows of automobiles—those little yellow diamonds that say BABY ON BOARD, and things like that? You don’t have to tell me there’s a baby on board; I’m gonna drive carefully anyway! What am I supposed to do when I see there’s a baby on board: act differently? As if I’m suddenly gonna drive more carefully and not hit the car because there’s a baby on board, when all I’m trying to do is not hit it anyway!

So NASA was trying to get the shuttle up anyway: you don’t have to say there’s a baby on board, or there’s a teacher on board, or it’s important to get this one up for the President.

Now that I’ve talked to some people about my experiences on the commission, I think I understand a few things that I didn’t understand so well earlier. One of them has to do with what I said to Dr. Keel that upset him so much. Recently I was talking to a man who spent a lot of time in Washington, and I asked him a particular question which, if he didn’t take it right, could be considered a grave insult. I would like to explain the question, because it seems to me to be a real possibility of what I said to Dr. Keel.

The only way to have real success in science, the field I’m familiar with, is to describe the evidence very carefully without regard to the way you feel it should be. If you have a theory, you must try to explain what’s good and what’s bad about it equally. In science, you learn a kind of standard integrity and honesty.

In other fields, such as business, it’s different. For example, almost every advertisement you see is obviously designed, in some way or another, to fool the customer: the print that they don’t want you to read is small; the statements are written in an obscure way. It is obvious to anybody that the product is not being presented in a scientific and balanced way. Therefore, in the selling business, there’s a lack of integrity.

My father had the spirit and integrity of a scientist, but he was a salesman. I remember asking him the question「How can a man of integrity be a salesman?」

He said to me,「Frankly, many salesmen in the business are not straightforward—they think it’s a better way to sell. But I’ve tried being straightforward, and I find it has its advantages. In fact, I wouldn’t do it any other way. If the customer thinks at all, he’ll realize he has had some bad experience with another salesman, but hasn’t had that kind of experience with you. So in the end, several customers will stay with you for a long time and appreciate it.」

My father was not a big, successful, famous salesman; he was the sales manager for a medium-sized uniform company. He was successful, but not enormously so.

When I see a congressman giving his opinion on something, I always wonder if it represents his real opinion or if it represents an opinion that he’s designed in order to be elected. It seems to be a central problem for politicians. So I often wonder: what is the relation of integrity to working in the government?

Now, Dr. Keel started out by telling me that he had a degree in physics. I always assume that everybody in physics has integrity—perhaps I’m naive about that—so I must have asked him a question I often think about:「How can a man of integrity get along in Washington?」

It’s very easy to read that question another way:「Since you’re getting along in Washington, you can’t be a man of integrity!」

Another thing I understand better now has to do with where the idea came from that cold affects the O-rings. It was General Kutyna who called me up and said,「I was working on my carburetor, and I was thinking: what is the effect of cold on the O-rings?」

Well, it turns out that one of NASA’s own astronauts told him there was information, somewhere in the works of NASA, that the O-rings had no resilience whatever at low temperatures—and NASA wasn’t saying anything about it.

But General Kutyna had the career of that astronaut to worry about, so the real question the General was thinking about while he was working on his carburetor was,「How can I get this information out without jeopardizing my astronaut friend?」His solution was to get the professor excited about it, and his plan worked perfectly.

*Richard’s younger sister, Joan, has a Ph.D. in physics, in spite of this preconception that only boys are destined to be scientists.

*Note for foreign readers: the quota system was a discriminatory practice of limiting the number of places in a university available to students of Jewish background.

*Feynman was suffering from abdominal cancer. He had surgery in 1978 and 1981. After he returned from Japan, he had more surgery, in October 1986 and October 1987.

*Hideki Yukawa. Eminent Japanese physicist; Nobel Prize, 1949.

*Four years later Richard and Gweneth met the king of Sweden—at the Nobel Prize ceremony.

†The Feynmans’ dog.

*Gweneth was expecting Carl at the time.

†Kiwi.

‡Carl. This letter was written in 1963.

*About 200 square feet.

* Daughter Michelle was about eleven when this letter was written, in 1980 or 1981.

*The「New Zealand lectures,」delivered in 1979, are written up in QED: The Strange Theory of Light and Matter (Princeton University Press, 1985).

†These letters were contributed by Freeman Dyson. They are the first and last letters he wrote that mention Richard Feynman. Other letters are referred to in Dyson’s book Disturbing the Universe.

*A family friend.

*As it turned out, Feynman was not to be disappointed: Carl works at the Thinking Machines Company, and daughter Michelle is studying to become a commercial photographer.

†This letter was contributed by Henry Bethe.

*The National Aeronautics and Space Administration

*NASA’s Jet Propulsion Laboratory, located in Pasadena; it is administered by Caltech.

*Note for foreign readers: a flight that leaves the West Coast around 11 P.M. and arrives on the East Coast around 7 A.M., five hours and three time zones later.

*Note for foreign readers: Sally Ride was the first American woman in space.

*Later in our investigation we discovered that it was this leak check which was a likely cause of the dangerous bubbles in the zinc chromate putty that I had heard about at JPL.

*The tang is the male part of the joint; the clevis is the female part (see Figure 13).

*The thing Feynman was going to break up was the baloney (the「bull——」) about how good everything was at NASA.

*The Office of Management and Budget.

*The reference is to Feynman’s method of slicing string beans, recounted in Surely You’re Joking, Mr. Feynman!

*Note for foreign readers: the Warren Report was issued in 1964 by the Warren Commission, headed by retired Supreme Court Chief Justice Earl Warren, which investigated the assassination of President John K. Kennedy.

*Feynman’s way of saying,「whatever it was.」

*Later, Mr. Lovingood sent me that report. It said things like「The probability of mission success is necessarily very close to 1.0」—does that mean it is close to 1.0, or it ought lo be close to 1.0?—and「Historically, this high degree of mission success has given rise to a difference in philosophy between unmanned and manned space flight programs; i.e., numerical probability versus engineering judgment.」As far as I can tell,「engineering judgment」means they’re just going to make up numbers! The probability of an engine-blade failure was given as a universal constant, as if all the blades were exactly the same, under the same conditions. The whole paper was quantifying everything. Just about every nut and bolt was in there:「The chance that a HPHTP pipe will burst is 10-7.」You can’t estimate things like that; a probability of 1 in 10,000,000 is almost impossible to estimate. It was clear that the numbers for each part of the engine were chosen so that when you add everything together you gel 1 in 100.000.

*I had heard about this from Bill Graham. He said that when he was first on the job as head of NASA, he was looking through some reports and noticed a little bullet:「*4,000 cycle vibration is within our data base.」He thought that was a funny-looking phrase, so he began asking questions. When he got all the way through, he discovered it was a rather serious matter: some of the engines would vibrate so much that they couldn’t be used. He used it as an example of how difficult it is to get information unless you go down and check on it yourself.

*Note for foreign readers: Federal Aviation Administration.

*This refers to「Safecracker Meets Safecracker,」another story told in Surely You’re Joking, Mr. Feynman!

Appendix F: Personal Observations on the Reliability of the Shuttle

Introduction

It appears that there are enormous differences of opinion as to the probability of a failure with loss of vehicle and of human life.* The estimates range from roughly 1 in 100 to 1 in 100,000. The higher figures come from working engineers, and the very low figures come from management. What are the causes and consequences of this lack of agreement? Since 1 part in 100,000 would imply that one could launch a shuttle each day for 300 years expecting to lose only one, we could properly ask,「What is the cause of management’s fantastic faith in the machinery?」

We have also found that certification criteria used in flight readiness reviews often develop a gradually decreasing strictness. The argument that the same risk was flown before without failure is often accepted as an argument for the safety of accepting it again. Because of this, obvious weaknesses are accepted again and again—sometimes without a sufficiently serious attempt to remedy them, sometimes without a flight delay because of their continued presence.

There are several sources of information: there are published criteria for certification, including a history of modifications in the form of waivers and deviations; in addition, the records of the flight readiness reviews for each flight document the arguments used to accept the risks of the flight. Information was obtained from direct testimony and reports of the range safety officer, Louis J. Ullian, with respect to the history of success of solid fuel rockets. There was a further study by him (as chairman of the Launch Abort Safety Panel, LASP) in an attempt to determine the risks involved in possible accidents leading to radioactive contamination from attempting to fly a plutonium power supply (called a radioactive thermal generator, or RTG) on future planetary missions. The NASA study of the same question is also available. For the history of the space shuttle main engines, interviews with management and engineers at Marshall, and informal interviews with engineers at Rocketdyne, were made. An independent (Caltech) mechanical engineer who consulted for NASA about engines was also interviewed informally. A visit to Johnson was made to gather information on the reliability of the avionics (computers, sensors, and effectors). Finally, there is the report「A Review of Certification Practices Potentially Applicable to Man-rated Reusable Rocket Engines,」prepared at the Jet Propulsion Laboratory by N. Moore et al. in February 1986 for NASA Headquarters, Office of Space Flight. It deals with the methods used by the FAA and the military to certify their gas turbine and rocket engines. These authors were also interviewed informally.

Solid Rocket Boosters (SRB)

An estimate of the reliability of solid-fuel rocket boosters (SRBs) was made by the range safety officer by studying the experience of all previous rocket flights. Out of a total of nearly 2900 flights, 121 failed (1 in 25). This includes, however, what may be called「early errors」—rockets flown for the first few times in which design errors are discovered and fixed. A more reasonable figure for the mature rockets might be 1 in 50. With special care in selecting parts and in inspection, a figure below 1 in 100 might be achieved, but 1 in 1000 is probably not attainable with today’s technology. (Since there are two rockets on the shuttle, these rocket failure rates must be doubled to get shuttle failure rates due to SRB failure.)

NASA officials argue that the figure is much lower. They point out that「since the shuttle is a manned vehicle, the probability of mission success is necessarily very close to 1.0.」It is not very clear what this phrase means. Does it mean it is close to 1 or that it ought to be close to I ? They go on to explain,「Historically, this extremely high degree of mission success has given rise to a difference in philosophy between manned space flight programs and unmanned programs; i.e., numerical probability usage versus engineering judgment.」(These quotations are from「Space Shuttle Data for Planetary Mission RTG Safety Analysis,」pages 3-1 and 3-2, February 15, 1985, NASA, JSC.) It is true that if the probability of failure was as low as 1 in 100,000 it would take an inordinate number of tests to determine it: you would get nothing but a string of perfect flights with no precise figure—other than that the probability is likely less than the number of such flights in the string so far. But if the real probability is not so small, flights would show troubles, near failures, and possibly actual failures with a reasonable number of trials, and standard statistical methods could give a reasonable estimate. In fact, previous NASA experience had shown, on occasion, just such difficulties, near accidents, and even accidents, all giving warning that the probability of flight failure was not so very small.

Another inconsistency in the argument not to determine reliability through historical experience (as the range safety officer did) is NASA’s appeal to history:「Historically, this high degree of mission success…」Finally, if we are to replace standard numerical probability usage with engineering judgment, why do we find such an enormous disparity between the management estimate and the judgment of the engineers? It would appear that, for whatever purpose—be it for internal or external consumption—the management of NASA exaggerates the reliability of its product to the point of fantasy.

The history of the certification and flight readiness reviews will not be repeated here (see other parts of the commission report), but the phenomenon of accepting seals that had shown erosion and blowby in previous flights is very clear. The Challenger flight is an excellent example: there are several references to previous flights; the acceptance and success of these flights are taken as evidence of safety. But erosion and blowby are not what the design expected. They are warnings that something is wrong. The equipment is not operating as expected, and therefore there is a danger that it can operate with even wider deviations in this unexpected and not thoroughly understood way. The fact that this danger did not lead to a catastrophe before is no guarantee that it will not the next time, unless it is completely understood. When playing Russian roulette, the fact that the first shot got off safely is of little comfort for the next. The origin and consequences of the erosion and blowby were not understood. Erosion and blowby did not occur equally on all flights or in all joints: sometimes there was more, sometimes less. Why not sometime, when whatever conditions determined it were right, wouldn’t there be still more, leading to catastrophe?

In spite of these variations from case to case, officials behaved as if they understood them, giving apparently logical arguments to each other—often citing the「success」of previous flights. For example, in determining if flight 51-L was safe to fly in the face of ring erosion in flight 51-C, it was noted that the erosion depth was only one-third of the radius. It had been noted in an experiment cutting the ring that cutting it as deep as one radius was necessary before the ring failed. Instead of being very concerned that variations of poorly understood conditions might reasonably create a deeper erosion this time, it was asserted there was「a safety factor of three.」

This is a strange use of the engineer’s term「safety factor.」If a bridge is built to withstand a certain load without the beams permanently deforming, cracking, or breaking, it may be designed for the materials used to actually stand up under three times the load. This「safety factor」is to allow for uncertain excesses of load, or unknown extra loads, or weaknesses in the material that might have unexpected flaws, et cetera. But if the expected load comes on to the new bridge and a crack appears in a beam, this is a failure of the design. There was no safety factor at all, even though the bridge did not actually collapse because the crack only went one-third of the way through the beam. The O-rings of the solid rocket boosters were not designed to erode. Erosion was a clue that something was wrong. Erosion was not something from which safety could be inferred.

There was no way, without full understanding, that one could have confidence that conditions the next time might not produce erosion three times more severe than the time before. Nevertheless, officials fooled themselves into thinking they had such understanding and confidence, in spite of the peculiar variations from case to case. A mathematical model was made to calculate erosion. This was a model based not on physical understanding but on empirical curve fitting. Specifically, it was supposed that a stream of hot gas impinged on the O-ring material, and the heat was determined at the point of stagnation (so far, with reasonable physical, thermodynamical laws). But to determine how much rubber eroded, it was assumed that the erosion varied as the .58 power of heat, the .58 being determined by a nearest fit. At any rate, adjusting some other numbers, it was determined that the model agreed with the erosion (to a depth of one-third the radius of the ring). There is nothing so wrong with this analysis as believing the answer! Uncertainties appear everywhere in the model. How strong the gas stream might be was unpredictable; it depended on holes formed in the putty. Blowby showed that the ring might fail, even though it was only partially eroded. The empirical formula was known to be uncertain, for the curve did not go directly through the very data points by which it was determined. There was a cloud of points, some twice above and some twice below the fitted curve, so erosions twice those predicted were reasonable from that cause alone. Similar uncertainties surrounded the other constants in the formula, et cetera, et cetera. When using a mathematical model, careful attention must be given to the uncertainties in the model.

Space Shuttle Main Engines (SSME)

During the flight of the 51-L the three space shuttle main engines all worked perfectly, even beginning to shut down in the last moments as the fuel supply began to fail. The question arises, however, as to whether—had the engines failed, and we were to investigate them in as much detail as we did the solid rocket booster—we would find a similar lack of attention to faults and deteriorating safety criteria. In other words, were the organization weaknesses that contributed to the accident confined to the solid rocket booster sector, or were they a more general characteristic of NASA? To that end the space shuttle main engines and the avionics were both investigated. No similar study of the orbiter or the external tank was made.

The engine is a much more complicated structure than the solid rocket booster, and a great deal more detailed engineering goes into it. Generally, the engineering seems to be of high quality, and apparently considerable attention is paid to deficiencies and faults found in engine operation.

The usual way that such engines are designed (for military or civilian aircraft) may be called the component system, or bottom-up design. First it is necessary to thoroughly understand the properties and limitations of the materials to be used (turbine blades, for example), and tests are begun in experimental rigs to determine those. With this knowledge, larger component parts (such as bearings) are designed and tested individually. As deficiencies and design errors are noted they are corrected and verified with further testing. Since one tests only parts at a time, these tests and modifications are not overly expensive. Finally one works up to the final design of the entire engine, to the necessary specifications. There is a good chance, by this time, that the engine will generally succeed, or that any failures are easily isolated and analyzed because the failure modes, limitations of materials, et cetera, are so well understood. There is a very good chance that the modifications to get around final difficulties in the engine are not very hard to make, for most of the serious problems have already been discovered and dealt with in the earlier, less expensive stages of the process.

The space shuttle main engine was handled in a different manner—top down, we might say. The engine was designed and put together all at once with relatively little detailed preliminary study of the materials and components. But now, when troubles are found in bearings, turbine blades, coolant pipes, et cetera, it is more expensive and difficult to discover the causes and make changes. For example, cracks have been found in the turbine blades of the high-pressure oxygen turbopump. Are they caused by flaws in the material, the effect of the oxygen atmosphere on the properties of the material, the thermal stresses of startup or shutdown, the vibration and stresses of steady running, or mainly at some resonance at certain speeds, or something else? How long can we run from crack initiation to crack failure, and how does this depend on power level? Using the completed engine as a test bed to resolve such questions is extremely expensive. One does not wish to lose entire engines in order to find out where and how failure occurs. Yet, an accurate knowledge of this information is essential to acquiring a confidence in the engine reliability in use. Without detailed understanding, confidence cannot be attained.

A further disadvantage of the top-down method is that if an understanding of a fault is obtained, a simple fix—such as a new shape for the turbine housing—may be impossible to implement without a redesign of the entire engine.

The space shuttle main engine is a very remarkable machine. It has a greater ratio of thrust to weight than any previous engine. It is built at the edge of—sometimes outside of—previous engineering experience. Therefore, as expected, many different kinds of flaws and difficulties have turned up. Because, unfortunately, it was built in a top-down manner, the flaws are difficult to find and to fix. The design aim of an engine lifetime of 55 mission equivalents (27,000 seconds of operation, either in missions of 500 seconds each or on a test stand) has not been obtained. The engine now requires very frequent maintenance and replacement of important parts such as turbopumps, bearings, sheet metal housings, et cetera. The high-pressure fuel turbopump had to be replaced every three or four mission equivalents (although this may have been fixed, now) and the high-pressure oxygen turbopump every five or six. This was, at most, 10 percent of the original design specifications. But our main concern here is the determination of reliability.

In a total of 250,000 seconds of operation, the main engines have failed seriously perhaps 16 times. Engineers pay close attention to these failings and try to remedy them as quickly as possible by test studies on special rigs experimentally designed for the flaw in question, by careful inspection of the engine for suggestive clues (like cracks), and by considerable study and analysis. In this way, in spite of the difficulties of top-down design, through hard work many of the problems have apparently been solved.

A list of some of the problems (and their status) follows:

Turbine blade cracks in high-pressure fuel turbopumps (HPFTP). (May have been solved.)

Turbine blade cracks in high-pressure oxygen fuel turbopumps (HPOTP). (Not solved.)

Augmented spark igniter (ASI) line rupture. (Probably solved.)

Purge check valve failure. (Probably solved.)

ASI chamber erosion. (Probably solved.)

HPFTP turbine sheet metal cracking. (Probably solved.)

HPFTP coolant liner failure. (Probably solved.)

Main combustion chamber outlet elbow failure. (Probably solved.)

Main combustion chamber inlet elbow weld offset. (Probably solved.)

HPOTP subsynchronous whirl. (Probably solved.)

Flight acceleration safety cutoff system (partial failure in a redundant system). (Probably solved.)

Bearing spalling. (Partially solved.)

A vibration at 4000 hertz making some engines inoperable. (Not solved.)

Many of these apparently solved problems were the early difficulties of a new design: 13 of them occurred in the first 125,000 seconds and only 3 in the second 125,000 seconds. Naturally, one can never be sure that all the bugs are out; for some, the fix may not have addressed the true cause. Thus it is not unreasonable to guess there may be at least one surprise in the next 250,000 seconds, a probability of 1/500 per engine per mission. On a mission there are three engines, but it is possible that some accidents would be self-contained and affect only one engine. (The shuttle can abort its mission with only two engines.) Therefore, let us say that the unknown surprises do not, in and of themselves, permit us to guess that the probability of mission failure due to the space shuttle main engines is less than 1/500. To this we must add the chance of failure from known, but as yet unsolved, problems. These we discuss below.

(Engineers at Rocketdyne, the manufacturer, estimate the total probability as 1/10,000. Engineers at Marshall estimate it as 1/300, while NASA management, to whom these engineers report, claims it is 1/10,000. An independent engineer consulting for NASA thought 1 or 2 per 100 a reasonable estimate.)

The history of the certification principles for these engines is confusing and difficult to explain. Initially the rule seems to have been that two sample engines must each have had twice the time operating without failure, as the operating time of the engine to be certified (rule of 2x). At least that is the FAA practice, and NASA seems to have adopted it originally, expecting the certified time to be 10 missions (hence 20 missions for each sample). Obviously, the best engines to use for comparison would be those of greatest total operating time (flight plus test), the so-called fleet leaders. But what if a third sample engine and several others fail in a short time? Surely we will not be safe because two were unusual in lasting longer. The short time might be more representative of the real possibilities, and in the spirit of the safety factor of 2, we should only operate at half the time of the short-lived samples.

The slow shift toward a decreasing safety factor can be seen in many examples. We take that of the HPFTP turbine blades. First of all the idea of testing an entire engine was abandoned. Each engine has had many important parts (such as the turbopumps themselves) replaced at frequent intervals, so the rule of 2x must be shifted from engines to components. Thus we accept an HPFTP for a given certification time if two samples have each run successfully for twice that time (and, of course, as a practical matter, no longer insisting that this time be as long as 10 missions). But what is「successfully」? The FAA calls a turbine blade crack a failure, in order to really provide a safety factor greater than 2 in practice. There is some time that an engine can run between the time a crack originally starts and the time it has grown large enough to fracture. (The FAA is contemplating new rules that take this extra safety time into account, but will accept them only if it is very carefully analyzed through known models within a known range of experience and with materials thoroughly tested. None of these conditions applies to the space shuttle main engines.)

Cracks were found in many second-stage HPFTP turbine blades. In one case three were found after 1900 seconds, while in another they were not found after 4200 seconds, although usually these longer runs showed cracks. To follow this story further we must realize that the stress depends a great deal on the power level. The Challenger flight, as well as previous flights, was at a level called 104 percent of rated power during most of the time the engines were operating. Judging from some material data, it is supposed that at 104 percent of rated power, the time to crack is about twice that at 109 percent, or full power level (FPL). Future flights were to be at 109 percent because of heavier payloads, and many tests were made at this level. Therefore, dividing time at 104 percent by 2, we obtain units called equivalent full power level (EFPL). (Obviously, some uncertainty is introduced by that, but it has not been studied.) The earliest cracks mentioned above occurred at 1375 seconds EFPL.

Now the certification rule becomes「limit all second-stage blades to a maximum of 1375 seconds EFPL.」If one objects that the safety factor of 2 is lost, it is pointed out that the one turbine ran for 3800 seconds EFPL without cracks, and half of this is 1900 so we are being more conservative. We have fooled ourselves in three ways. First, we have only one sample, and it is not the fleet leader: the other two samples of 3800 or more seconds EFPL had 17 cracked blades between them. (There are 59 blades in the engine.) Next, we have abandoned the 2x rule and substituted equal time (1375). And finally, the 1375 is where a crack was discovered. We can say that no crack had been found below 1375, but the last time we looked and saw no cracks was 1100 seconds EFPL. We do not know when the crack formed between these times. For example, cracks may have been formed at 1150 seconds EFPL. (Approximately two-thirds of the blade sets tested in excess of 1375 seconds EFPL had cracks. Some recent experiments have, indeed, shown cracks as early as 1150 seconds.) It was important to keep the number high, for the shuttle had to fly its engines very close to their limit by the time the flight was over.

Finally, it is claimed that the criteria have not been abandoned, and that the system is safe, by giving up the FAA convention that there should be no cracks, and by considering only a completely fractured blade a failure. With this definition no engine has yet failed. The idea is that since there is sufficient time for a crack to grow to fracture, we can ensure that all is safe by inspecting all blades for cracks. If cracks are found, replace the blades, and if none are found, we have enough time for a safe mission. Thus, it is claimed, the crack problem is no longer a flight safety problem, but merely a maintenance problem.

This may in fact be true. But how well do we know that cracks always grow slowly enough so that no fracture can occur in a mission? Three engines have run for long time periods with a few cracked blades (about 3000 seconds EFPL), with no blade actually breaking off.

A fix for this cracking may have been found. By changing the blade shape, shot-peening the surface, and covering it with insulation to exclude thermal shock, the new blades have not cracked so far.

A similar story appears in the history of certification of the HPOTP, but we shall not give the details here.

In summary, it is evident that the flight readiness reviews and certification rules show a deterioration in regard to some of the problems of the space shuttle main engines that is closely analogous to the deterioration seen in the rules for the solid rocket boosters.

Avionics

By「avionics」is meant the computer system on the orbiter as well as its input sensors and output actuators. At first we will restrict ourselves to the computers proper, and not be concerned with the reliability of the input information from the sensors of temperature, pressure, et cetera; nor with whether the computer output is faithfully followed by the actuators of rocket firings, mechanical controls, displays to astronauts, et cetera.

The computing system is very elaborate, having over 250,000 lines of code. Among many other things it is responsible for the automatic control of the shuttle’s entire ascent into orbit, and for the descent until the shuttle is well into the atmosphere (below Mach 1), once one button is pushed deciding the landing site desired. It would be possible to make the entire landing automatic. (The landing gear lowering signal is expressly left out of computer control, and must be provided by die pilot, ostensibly for safety reasons.) During orbital flight the computing system is used in the control of payloads, in the display of information to the astronauts, and in the exchange of information with the ground. It is evident that the safety of flight requires guaranteed accuracy of this elaborate system of computer hardware and software.

In brief, hardware reliability is ensured by having four essentially independent identical computer systems. Where possible, each sensor also has multiple copies—usually four—and each copy feeds all four of the computer lines. If the inputs from the sensors disagree, either a certain average or a majority selection is used as the effective input, depending on the circumstances. Since each computer sees all copies of the sensors, the inputs are the same, and because the algorithms used by each of the four computers are the same, the results in each computer should be identical at each step. From time to time they are compared, but because they might operate at slightly different speeds, a system of stopping and waiting at specified times is instituted before each comparison is made. If one of the computers disagrees or is too late in having its answer ready, the three which do agree are assumed to be correct and the errant computer is taken completely out of the system. If, now, another computer fails, as judged by the agreement of the other two, it is taken out of the system, and the rest of the flight is canceled: descent to the landing site is instituted, controlled by the two remaining computers. It is seen that this is a redundant system since the failure of only one computer does not affect the mission. Finally, as an extra feature of safety, there is a fifth independent computer, whose memory is loaded with only the programs for ascent and descent, and which is capable of controlling the descent if there is a failure of more than two of the computers of the main line of four.

There is not enough room in the memory of the mainline computers for all the programs of ascent, descent, and payload programs in flight, so the memory is loaded by the astronauts about four times from tapes.

Because of the enormous effort required to replace the software for such an elaborate system and to check out a new system, no change in the hardware has been made since the shuttle transportation system began about fifteen years ago. The actual hardware is obsolete—for example, the memories are of the old ferrite-core type. It is becoming more difficult to find manufacturers to supply such old-fashioned computers that are reliable and of high enough quality. Modern computers are much more reliable, and they run much faster. This simplifies circuits and allows more to be done. Today’s computers would not require so much loading from tapes, for their memories are much larger.

The software is checked very carefully in a bottom-up fashion. First, each new line of code is checked; then sections of code (modules) with special functions are verified. The scope is increased step by step until the new changes are incorporated into a complete system and checked. This complete output is considered the final product, newly released. But working completely independently is a verification group that takes an adversary attitude to the software development group and tests the software as if it were a customer of the delivered product. There is additional verification in using the new programs in simulators, et cetera. An error during this stage of verification testing is considered very serious, and its origin is studied very carefully to avoid such mistakes in the future. Such inexperienced errors have been found only about six times in all the programming and program changing (for new or altered payloads) that has been done. The principle followed is: all this verification is not an aspect of program safety; it is a test of that safety in a noncatastrophic verification. Flight safety is to be judged solely on how well the programs do in the verified tests. A failure here generates considerable concern.

To summarize, then, the computer software checking system is of highest quality. There appears to be no process of gradually fooling oneself while degrading standards, the process so characteristic of the solid rocket booster and space shuttle main engine safety systems. To be sure, there have been recent suggestions by management to curtail such elaborate and expensive tests as being unnecessary at this late date in shuttle history. Such suggestions must be resisted, for they do not appreciate the mutual subtle influences and sources of error generated by even small program changes in one part of a program on another. There are perpetual requests for program changes as new pay-loads and new demands and modifications are suggested by the users. Changes are expensive because they require extensive testing. The proper way to save money is to curtail the number of requested changes, not the quality of testing for each.

One might add that the elaborate system could be very much improved by modern hardware and programming techniques. Any outside competition would have all the advantages of starting over. Whether modern hardware is a good idea for NASA should be carefully considered now.

Finally, returning to the sensors and actuators of the avionics system, we find that the attitude toward system failure and reliability is not nearly as good as for the computer system. For example, a difficulty was found with certain temperature sensors sometimes failing. Yet eighteen months later the same sensors were still being used, still sometimes failing, until a launch had to be scrubbed because two of them failed at the same time. Even on a succeeding flight this unreliable sensor was used again. And reaction control systems, the rocket jets used for reorienting and control in flight, still are somewhat unreliable. There is considerable redundancy, but also a long history of failures, none of which has yet been extensive enough to seriously affect a flight. The action of the jets is checked by sensors: if a jet fails to fire, the computers choose another jet to fire. But they are not designed to fail, and the problem should be solved.

Conclusions

If a reasonable launch schedule is to be maintained, engineering often cannot be done fast enough to keep up with the expectations of the originally conservative certification criteria designed to guarantee a very safe vehicle. In such situations, safety criteria are altered subtly—and with often apparently logical arguments—so that flights can still be certified in time. The shuttle therefore flies in a relatively unsafe condition, with a chance of failure on the order of a percent. (It is difficult to be more accurate.)

Official management, on the other hand, claims to believe the probability of failure is a thousand times less. One reason for this may be an attempt to assure the government of NASA’s perfection and success in order to ensure the supply of funds. The other may be that they sincerely believe it to be true, demonstrating an almost incredible lack of communication between the managers and their working engineers.

In any event, this has had very unfortunate consequences, the most serious of which is to encourage ordinary citizens to fly in such a dangerous machine—as if it had attained the safety of an ordinary airliner. The astronauts, like test pilots, should know their risks, and we honor them for their courage. Who can doubt that McAuliffe* was equally a person of great courage, who was closer to an awareness of the true risks than NASA management would have us believe?

Let us make recommendations to ensure that NASA

officials deal in a world of reality, understanding technological weaknesses and imperfections well enough to be actively trying to eliminate them. They must live in a world of reality in comparing the costs and utility of the shuttle to other methods of entering space. And they must be realistic in making contracts and in estimating the costs and difficulties of each project. Only realistic flight schedules should be proposed—schedules that have a reasonable chance of being met. If in this way the government would not support NASA, then so be it. NASA owes it to the citizens from whom it asks support to be frank, honest, and informative, so that these citizens can make the wisest decisions for the use of their limited resources.

For a successful technology, reality must take precedence over public relations, for Nature cannot be fooled.

*Leighton’s note: The version printed as Appendix F in the commission report does not appear to have been edited, so I took it upon myself to smooth it out a little bit.

*Note for foreign readers: Christa McAuliffe, a schoolteacher, was to have been the first ordinary citizen in space—a symbol of the nation’s commitment to education, and of the shuttle’s safety.

EPILOGUE

Preface

WHEN I was younger, I thought science would make good things for everybody. It was obviously useful; it was good. During the war I worked on the atomic bomb. This result of science was obviously a very serious matter: it represented the destruction of people.

After the war I was very worried about the bomb. I didn’t know what the future was going to look like, and I certainly wasn’t anywhere near sure that we would last until now. Therefore one question was—is there some evil involved in science?

Put another way—what is the value of the science I had dedicated myself to—the thing I loved—when I saw what terrible things it could do? It was a question I had to answer.

「The Value of Science」is a kind of report, if you will, on many of the thoughts that came to me when I tried to answer that question.

Richard Feynman

The Value of Science*

FROM time to time people suggest to me that scientists ought to give more consideration to social problems—especially that they should be more responsible in considering the impact of science on society. It seems to be generally believed that if the scientists would only look at these very difficult social problems and not spend so much time fooling with less vital scientific ones, great success would come of it.

It seems to me that we do think about these problems from time to time, but we don’t put a full-time effort into them—the reasons being that we know we don’t have any magic formula for solving social problems, that social problems are very much harder than scientific ones, and that we usually don’t get anywhere when we do think about them.

I believe that a scientist looking at nonscientific problems is just as dumb as the next guy—and when he talks about a nonscientific matter, he sounds as naive as anyone untrained in the matter. Since the question of the value of science is not a scientific subject, this talk is dedicated to proving my point—by example.

