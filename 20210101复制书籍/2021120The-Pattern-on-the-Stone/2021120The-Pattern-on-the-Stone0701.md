## 0701. Speed: Parallel Computers

B esides differing in the amount of memory, one type of universal computer can differ from another in the speed with which its operations are carried out. The speed of a computer is generally governed by the amount of time it takes to move data in and out of memory.

The sort of computers we have been talking about so far are sequential computers — they operate on one word of data at a time. The reason that conventional computers operate this way is largely historical. In the late 1940s and early 1950s, when computers were being developed, the switching elements (relays and vacuum tubes) were expensive but relatively fast. The memory elements (mercury delay lines, magnetic drums) were relatively cheap and slow. They were especially suitable for producing sequential streams of data. Computers were designed to keep the expensive processor switches as usefully busy as possible, while not placing too much demand on the speed of the memory. These early computers were room-size, with the expensive processor on one side and the slow memory on the other. A trickle of data flowed between.

As computer technology improved, software grew increasingly complex and expensive and it became increasingly difficult to train programmers, so in order to preserve the investment in software and training, the basic structure of computers remained unchanged. There was little motivation to rethink the two-part design, because technology was progressing at such a rapid rate that it was easy to build faster and cheaper machines just by re-creating the same kind of computer in a new technology.

The speed of computers tended to double every two years. Vacuum tubes were replaced by transistors and eventually by integrated circuits. Delay-line memories were replaced by magnetic-core memories, and then also by integrated circuits. The room-size machines shrank to a silicon chip the size of a thumbnail. Through all this changing technology, the simple processor-connected-to-memory design stayed the same. If you look at a modern single-chip computer under a microscope, you can still see the vestiges of the room full of vacuum tubes: one area of the chip is devoted to processing, another to memory. This is true in spite of the fact that the processor and memory parts of the computer are now made by the same methods, often on the same piece of silicon. The patterns of activity are still optimized for the earlier, two-part design. The section of the silicon chip that implements processing is kept very busy; the part that implements memory still trickles out data one word at a time.

The flow of data between processor and memory is the bottleneck of a sequential computer. The root problem is that the memory is designed to access a single location on each instruction cycle. As long as we stick with this fundamental design, the only way to increase the speed of the computer is to reduce the cycle time. For many years, the cycle time of computers could be reduced by increasing the speed of the switches: faster switches led to faster computers. Now this is no longer such an effective strategy. The speed of today's large computers is limited primarily by the time required to propagate information along their wires, and that in turn is limited by the finite speed of light. Light travels at about a foot per nanosecond (a billionth of a second). The cycle time of the fastest computers today is about a nanosecond, and it is no coincidence that their processors are less than a foot across. We are approaching the limits of how much we can speed up the computer without changing the basic design.

0701 速度：并行计算机

除了存储空间不同之外，不同类型的通用计算机的运算速度也有所不同。计算机的速度通常取决于将数据写入和移出存储器所花费的时间。

目前为止，我们所讨论的计算机都是顺序计算机，即它们每次只能处理一个计算机字长的数据。传统计算机之所以以这种方式运行，主要是历史原因使然。20 世纪 40 年代末和 50 年代初期，当时计算机刚被发明，开关元件（继电器和真空管）的价格非常昂贵，但速度相对较快；存储元件（汞延迟线和磁鼓）的价格低廉，但运行速度较慢。它们的组合特别适合处理连续的数据流。设计者让计算机昂贵的处理器始终处于忙碌状态，但对存储速度没有提出过多要求。这些早期的计算机有一间房那么大，其中一边是昂贵的处理器，另一边是运行缓慢的存储器，有少量数据在两者之间流动。

随着计算机技术的进步，软件变得越来越复杂和昂贵，培训程序员的难度也变得越来越大。因此，为了节省在软件和培训方面的投入，计算机的基本架构基本没变。重新思考这种双边设计结构的动力不够强大，因为技术的进步非常之快，通过新技术便能轻易地制造出运行速度更快、价格更便宜的计算机。

计算机的运行速度通常每两年就会翻一倍。晶体管取代了真空管，最后又被集成电路替代。磁芯存储器取代了延迟线存储器，最终也被集成电路替代。房间般大小的机器最终被缩小为只有指甲盖大小的硅芯片。在这一系列的工艺变革过程中，处理器与存储器相连的设计模式一直保持不变。如果你在显微镜下观察现代的单片机，依然可以看到「满屋」真空管的痕迹：芯片的一部分区域用于处理，另一部分区域用于存储。尽管现在计算机的处理器和存储器在制造工艺上是相同的，且两者都位于同一块硅芯片上，但其工作模式只是原设计的优化而已，仍为早期的两部分，彼此隔开。然而，用作处理功能的那部分芯片一直非常繁忙，而用于存储功能的那部分芯片仍然每次只能输出一个计算机字长的数据。

处理器和存储器之间的数据流速度是顺序计算机的瓶颈所在，其根本问题在于，存储器在每个指令周期内只能访问单个地址。只要我们维持这种基本的设计架构，那么提高计算机运行速度的唯一方法就是，缩短每一个指令周期的时间。多年以来，我们通过提高开关的速度来缩短计算机的指令周期，即更快的开关能使计算机的运行速度更快。然而，这种策略不再有效。如今，大型计算机的运算速度主要受限于信息在线路中的传播用时，而这一点又受到光速的限制。光以每纳秒（十亿分之一秒）一英尺（1 英尺 = 0.304 8 米）的速度传播。如今，速度最快的计算机的指令周期约为一纳秒，所以这些计算机的处理器尺寸小于一英尺并非巧合。在不修改基础设计架构的前提下，我们已经接近计算机的速度极限了。

### 7.1 Parallelism

To work any faster, today's computers need to do more than one operation at once. We can accomplish this by breaking up the computer memory into lots of little memories and giving each its own processor. Such a machine is called a parallel computer. Parallel computers are practical because of the low cost and small size of microprocessors. We can build a parallel computer by hooking together dozens, hundreds, or even thousands of these smaller processors. The fastest computers in the world are massively parallel computers, which use thousands or even tens of thousands of processors.

As I have earlier described, computers are constructed in a hierarchy of building blocks, with each level of the hierarchy serving as the step to the level above. Parallel computers are the obvious next level in this scheme, with the computers themselves as building blocks. Such a construction can be called either a parallel computer or a computer network. The distinction between the two is somewhat arbitrary and has more to do with how the system is used than how it works. Usually a parallel computer is in one location, whereas a network is spread out geographically, but there are exceptions in both these rules. Generally, if a group of connected computers is used in a coordinated fashion, we call it a parallel computer. If the computers are used somewhat independently, we call the connected computers a computer network.

Putting a large number of computers together to achieve greater speed seems like an obvious step, but for many years the consensus among computer scientists was that it would work for only a few applications. I spent a large part of my early career arguing with people who believed it was impractical, or even impossible, to build and program general-purpose massively parallel computers. This widespread skepticism was based on two misapprehensions — one about how complex such a system would have to be, and the other about how the components of such a system would work together.

Scientists tended to overestimate the complexity of parallel computers, because they underestimated — or at least underappreciated — the rate of improvement in microelectronics-fabrication technology. It was not so much that they were ignorant of this trend as that the rate of technological change was unprecedented in history; it was thus extremely difficult for expectations and intuitions to keep abreast of the change. I remember giving a talk at a computer conference at the New York Hilton in the mid 1970s, in which I pointed out that current trends indicated that there would soon be more microprocessors than people in the United States. This was considered an outrageous extrapolation at the time. Although microprocessors had already been produced, the popular view of a computer was still that of a large set of refrigerator-size cabinets with blinking lights. In the question period at the end of the talk, one of my ill-disposed listeners asked, in a voice laden with sarcasm,「Just what do you think people are going to do with all these computers? It's not as if you needed a computer in every doorknob!」The audience burst out laughing, and I was at a loss for an answer, but as a matter of fact in that same hotel today, every doorknob contains a microprocessor that controls the lock.

Another reason that people were skeptical about parallel computers was more subtle, and also more valid. It was the perceived inefficiency of breaking a computation into many concurrent parts. This problem continues to limit the application of parallel computers today, but it is not nearly as serious a limitation as it was once thought to be. Part of the reason for the overestimation of the difficulty was a series of misleading experiences with early parallel machines. The first parallel digital computers were built in the 1960s by connecting two or three large sequential computers. In most cases, the multiple processing units shared a single memory, so that each of the processors could access the same data. These early parallel computers were usually programmed to give each processor a different task: for example, in a database application, one processor would retrieve the records, another processor would tabulate statistics, and the third would print out the results. The processors were used much as different operators on an assembly line, each doing one stage of the computation.

There were several inefficiencies inherent in this scheme, all of which seemed to grow along with the number of processors. One inefficiency was that the task had to be divided up into more or less independent stages. While it was often possible to divide tasks into two or three stages, it was difficult to see how to divide one task into ten or a hundred stages. As one detractor of parallel processing explained it to a newspaper reporter,「Well, two reporters might be able to write a newspaper article faster by having one of them gather the news while another writes the story, but a hundred reporters working on the article together would probably not be able to get it written at all.」Such arguments were pretty convincing.

Another inefficiency stemmed from the shared access to a single memory. A typical memory could retrieve only one word at a time from a given area of memory, and this limitation on the rate of access created an obvious bottleneck in the system, which limited its performance. If more processors were added to a system already limited by its fetch rate, the processors would all end up spending more time waiting for data, and the efficiency of the system would decrease.

Moreover, processors had to be careful not to create an inconsistency by altering data that another processor was looking at. For example, consider an airline-reservation system. If one processor is working on the problem of reserving seats, it looks to see if a seat is empty and then reserves the seat if it is. If two processors are booking seats for two different passengers simultaneously, there is a potential problem: they may both notice that the same seat is empty and decide to reserve it before either has a chance to mark it as taken. To avoid this kind of mishap, a processor had to go through an elaborate sequence that locked out other processors from accessing data while that processor was looking at a data word. This further aggravated the inefficiencies attendant on the competition for the system's memory, and in the worst case it would reduce the speed of a multiprocessor system to the speed of a single processor — and even to less than the speed of a single processor. As noted, these inefficiencies worsened as the number of processors increased.

The final source of inefficiency appeared to be even more fundamental: the difficulty of balancing the tasks assigned to the various processors. To return to the assembly-line analogy: we can see that the rate of computation will be governed by the speed of the slowest step. If there is just one slow operation, the rate of computation is set by that single operation. It is not unreasonable to expect that in this case, too, the efficiency of the system will decrease as we increase the number of processors.

The best formulation of these inefficiency problems is known as Amdahl's Law, after Gene Amdahl, the computer designer who came up with it in the 1960s. Amdahl's argument went something like this: There will always be a part of the computation which is inherently sequential — work that can be done by only one processor at a time. Even if only 10 percent of the computation is inherently sequential, no matter how much you speed up the remaining 90 percent, the computation as a whole will never speed up by more than a factor of 10. The processors working on the 90 percent that can be done in parallel will end up waiting for the single processor to finish the sequential 10 percent of the task. This argument suggests that a parallel computer with 1,000 processors will be extremely inefficient, since it will be only about ten times faster than a single processor. When I was trying to get funding to build my first parallel computer — a massively parallel computer with 64,000 processors — the first question I usually got at the end of one of my pitches was「Haven't you ever heard of Amdahl's Law?」

Of course, I had heard of Amdahl's Law, and to tell you the truth I saw nothing wrong with the reasoning behind it. Yet I knew for certain, even though I couldn't prove it, that Amdahl's Law did not apply to the problems I was trying to solve. The reason I was so confident is that the problems I was working on had already been solved on a massively parallel processor — the human brain. I was a student at the Artificial Intelligence Laboratory at MIT, and I wanted to build a machine that could think.

When I first visited the MIT Artificial Intelligence Lab as a freshman undergraduate in 1974, the field of AI (as it was coming to be known) was in a state of explosive growth. The first programs that could follow simple instructions written in plain English were being developed, and a computer that understood human speech seemed just around the corner. Computers were excelling at games like chess, which had been considered too complicated for them just a few years earlier. Artificial vision systems were recognizing simple objects, like line drawings and piles of blocks. Computers were even passing calculus exams and solving simple analogy problems taken from IQ tests. Could general-purpose machine intelligence be all that far off?

But by the time I joined the laboratory as a graduate student a few years later, the problems were looking more difficult. The simple demonstrations had turned out to be just that. Although lots of new principles and powerful methods had been invented, applying them to larger, more complicated problems didn't seem to work. At least part of the problem lay with the speed limitations of computers. AI researchers found it unfruitful to extend their experiments to cases involving more data, because the computers were already slow, and adding more data just made them slower. It was frustrating, for example, to try to get a machine to recognize a pile of objects when recognizing a single object required hours of computing time.

The computers were slow because they were sequential; they could do only one thing at a time. A computer must look at a picture pixel by pixel; by contrast, a brain perceives an entire picture instantly and can simultaneously match what it sees to every image it knows. For this reason, a human being is much faster than a computer at recognizing objects, even though the neurons in the human visual system are much slower than the transistors in the computer. This difference in design inspired me, as it did many others, to look for ways of designing massively parallel computers — computers that could perform millions of operations concurrently and exploit parallelism more like the brain does. Because I knew that the brain was able to get fast performance from slow components, I also knew that Amdahl's Law did not always apply.

I now understand the flaw in Amdahl's argument. It lies in the assumption that a fixed portion of the computation, even just 10 percent, must be sequential. This estimate sounds plausible, but it turns out not to be true of most large computations. The false intuition came from a misunderstanding about how parallel processors would be used. The crux of the issue is in how the work of a computation is divided among the processors: it might seem at first that the best way to divide a computation among several processors is to give each a different part of the program to execute. This works to an extent, but (as the aforementioned journalistic analogy suggests) it suffers from the same drawbacks as assigning a task to a team of people: much of the potential concurrency is lost in the problems associated with coordination. Programming a computer by breaking up the program is like having a large team of people paint a fence by assigning one person to the job of opening the paint, another to preparing the surface, another to applying the paint, and another to cleaning the brushes. This functional breakdown requires a high degree of coordination, and after a certain point adding more people doesn't help speed up the task.

A more efficient way to use a parallel computer is to have each processor perform similar work, but on a different section of the data. This so-called data parallel decomposition of the task is like getting a fence painted by assigning each worker a separate section of fence. Not all problems break up as easily as painting a fence, but where large compuations are concerned this method works surprisingly well. For instance, image-processing tasks can be composed in concurrent parts by assigning a little patch of the image to each processor. A search problem, like playing chess, can be decomposed by having each processor simultaneously search different lines of play. In these examples, the degree of speed-up achieved is almost proportional to the number of processors — so the more of them the better. A little additional time must be spent dividing the problem among the processors and gluing the answers together, but if the problem is large the computation can be performed very efficiently, even on a parallel machine with tens of thousands of processors.

The computations just described can fairly obviously be decomposed to run in parallel, but data parallel decomposition also works on more complicated tasks. There are surprisingly few large problems that cannot be handled efficiently by parallel processing. Even problems that most people think are inherently sequential can usually be solved efficiently on a parallel computer. An example is the chain-following problem. My children play a game called Treasure Hunt, which is based on the chain-following problem. I give them a piece of paper with a clue about where the next clue is hidden. That clue leads to the next clue, and so on, until they reach the treasure at the end. In the computational version of this game, the program is given, as input, the address of a location in memory containing the address of another location. That location contains the address of still another, and so on. Eventually, the address specifies a memory location containing a special word that indicates it is the end of the chain. The problem is to find the last location from the first.

Initially, the chain-following problem looks like the epitome of an inherently sequential computation, because there seems to be no way for the computer to find the last location in the chain without following the linked addresses through the entire chain. The computer has to look at the first location to find the address of the second, then look at the second to find the address of the third, and so on. It turns out, however, that the problem can be solved in parallel. A parallel computer with a million processors can find the last element in a chain of a million addresses in twenty steps.

The trick is to divide the problem in half at every step, a bit like the approach used in the sorting algorithms in chapter 5. Assume that each of the million memory locations has its own processor, which can send a message to any other processor. To find the end of the chain, every processor begins by sending its own address to the processor that follows it in the chain — that is, the processor whose address is stored in its memory location. Each processor then knows the address not only of the processor that comes after it but also of the one that precedes it. The processor uses this information to send the address of its successor to its predecessor. Now each processor knows the address of the processor that lies two ahead of it in the chain, so now the chain connecting the first processor to the last processor is half the length it was originally. This reduction step is then repeated, and each time it is repeated, the length of the chain is halved. After twenty iterations of the reduction step, the first processor in a million-memory chain knows the address of the last. Similar methods can be applied to many other tasks that seem inherently sequential.

As of this writing, parallel computers are still relatively new, and it is not yet understood what other types of problems can be decomposed to efficiently take advantage of many processors. A rule of thumb seems to be that parallelism works best on problems with large amounts of data, because if there are lots of data elements there is plenty of similar work to divide among processors.

One reason that most computations can be decomposed into concurrent subproblems is that most computations are models of the physical world. Computations based on physical models can operate in parallel because the physical world operates in parallel. Computer graphics images, for example, are often synthesized by an algorithm that simulates the physical process of light reflecting off the surfaces of objects. The picture is drawn from a mathematical description of the shapes of the objects by calculating how each ray of light would bounce from surface to surface while traveling from the source to the eye. The calculation of all the light rays can proceed concurrently, because the bouncing of light proceeds concurrently in the physical world.

A typical example of the kind of computation well suited to a parallel computer is a simulation of the atmosphere, used in predicting the weather. The three-dimensional array of numbers that represents the atmosphere is analogous to three-dimensional physical space. Each number specifies the physical parameter of a certain volume of air — for example, the pressure in a cube of atmosphere 1 kilometer on a side. Each of these cubes will be represented by a few numbers specifying average temperature, pressure, wind velocity, and humidity. To predict how the volume of air in one such cube will evolve, the computer calculates how air flows between neighboring volumes: for instance, if more air flows into a volume than out of it, then the pressure in that volume will increase. The computer also calculates the changes attributable to factors such as sunlight and evaporation. The atmospheric simulation is calculated in a series of steps, each of which corresponds to a small increment of time — say, half an hour — so that the flow of simulated air and water among the cells in the array is analogous to the flow of real air and water in the pattern of weather. The result is a kind of three-dimensional moving picture inside the computer — a picture that behaves according to physical laws.

Of course, the accuracy of this simulation will depend both on the resolution and the accuracy of the three-dimensional image, which accounts for the notorious inaccuracy of weather predictions over time. If the resolution of the model is increased and the initial conditions are measured more accurately, then the prediction will be better — although even a very high resolution will never be perfect over a long period of time, because the initial state of the atmosphere cannot be measured with exact precision. Like the game of roulette, weather systems are chaotic, so a small change in initial conditions will produce a significant change in the outcome. On a parallel computer, each processor can be assigned the responsibility for predicting the weather in a tiny area. When wind blows from one area to the next, then the processors modeling those areas must communicate. Processors modeling geographically separated areas can proceed almost independently, in parallel, because the weather in these areas is almost independent. The computation is local and concurrent, because the physics that governs the weather is also local and concurrent.

While the weather simulation is linked to physical law in an obvious manner, many other computations are linked more subtly to the physical world. For instance, calculating telephone bills is concurrent, because telephones (and telephone customers) operate independently in the physical world. The only problems we don't know how to solve efficiently on a parallel computer are those for which the growing dimension of the problem is analogous to the passage of time. An example is the problem of predicting the future positions of the planets. (Ironically, this is the very problem for which many of our mathematical tools of computation were originally invented.)

The paths of the planets are the consequence of well-defined rules of momentum and gravitational interactions between the nine planets and the Sun. (For simplicity's sake, we will ignore the effects of small bodies, such as moons and asteroids.) All the information necessary to solve the problem can be represented by nine coordinates, so there isn't much data. The computational difficulty of the problem comes from the fact that the calculation must be made, as far as we know, by calculating the successive positions of the planets at each of billions of tiny steps, each representing a short period of time. The only way we know how to calculate the positions of the planets a million years in the future is to calculate their position at each intermediate time between now and then. If there is a trick to solving this problem concurrently, such as the one used in the chain-following problem, I am not aware of it. On the other hand, as far as I know, no one has proved that this orbit problem is inherently sequential. It remains an open question.

Highly parallel computers are now fairly common. They are used mostly in very large numerical calculations (like the weather simulation) or in large database calculations, such as extracting marketing data from credit card transactions. Since parallel computers are built of the same parts as personal computers, they are likely to become less expensive and more common with time. One of the most interesting parallel computers today is the one that is emerging almost by accident from the networking of sequential machines. The worldwide network of computers called the Internet is still used primarily as a communications system for people. The computers act mostly as a medium — storing and delivering information (like electronic mail) that is meaningful only to humans. I am convinced that this will change. Already standards are beginning to emerge that allow these computers to exchange programs as well as data. The computers on the Internet, working together, have a potential computational capability that far surpasses any individual computer that has ever been constructed.

I believe that eventually the Internet will grow to include the computers embedded in telephone systems, automobiles, and simple home appliances. Such machines will read their inputs directly from the physical world rather than relying on humans as intermediaries. As the information available on the Internet becomes richer, and the types of interaction among the connected computers become more complex, I expect that the Internet will begin to exhibit emergent behavior going beyond any that has been explicitly programmed into the system. In fact the Internet is already beginning to show signs of emergent behavior, but so far most of it is pretty simple: plagues of computer viruses and unpredicted patterns of message routing. As computers on the network begin to exchange interacting programs instead of just electronic mail, I suspect that the Internet will start to behave less like a network and more like a parallel computer. I suspect that the emergent behavior of the Internet will get a good deal more interesting.

并行性

为了达到更快的处理速度，现在的计算机需要同时执行多个操作。为了实现这个目标，我们可以将计算机存储器分为多份，并为每一份提供独立的处理器。这种计算机被称为并行计算机。由于微处理器的成本低廉、尺寸较小，所以并行计算机是实际可行的。我们可以将数十、数百甚至数千个小尺寸的处理器连接起来，组装成一台并行计算机。世界上最快的计算机是大规模并行计算机，它们所用的处理器数目高达数千甚至数万个。

如前所述，计算机是在构件的层次结构上搭建起来的，每层结构都是上层结构的基础。在这种模式中，计算机本身就属于基础构件，而并行计算机是其上面的一个层次。这种结构可被称为并行计算机或者计算机网络，两者之间并无明确的差别，也许其差别更多地与系统的应用有关，而非单个计算机的工作过程。一般来说，并行计算机位于一个地点，而计算机网络则在地理上是广泛分布的。不过，这两条规则都存在例外。通常，如果一组互联的计算机以协同的方式合作，我们便会称之为并行计算机。如果这些计算机在某种程度上是独立工作的，那么我们便将这些互联的计算机称为计算机网络。

将大量计算机连接起来获得更快的运算速度，这看起来似乎是显而易见的选择。不过，计算机科学家的共识是，这种结构只适用于少数几种应用场景。有些人认为，制造通用的大规模并行计算机并对其进行编程是不切实际，甚至是不可能实现的。在我职业生涯的早期，我花费了大量时间来反驳这种观点。这种普遍存在的怀疑源于两种误解，一种是误解了这个系统的复杂程度，另一种是误解这个系统各个组成部分之间的协同工作原理。

科学家倾向于高估并行计算机的复杂性，因为他们低估了或者至少没有重视微电子制造技术的进步速度。与其说他们对这一趋势一无所知，不如说这类技术变革的速度前所未有地快，致使人们很难跟上脚步。20 世纪 70 年代中期，我参加了纽约希尔顿酒店举行的一次计算机会议，我在发言中指出，当前的趋势表明，不久以后，美国的微处理器数目将会超过美国的人口总数。当时大家认为这是一种武断。尽管微处理器已经被生产出来了，但大众对计算机的普遍印象依然停留在闪着指示灯的冰箱大小的「柜子」上。在我报告结束后的提问环节，一名不怀善意的听众用充满讽刺的语气问道：「你认为人们将如何处理这么多计算机呢？看来不会是为每个门柄都安装一台计算机吧！」听众不禁大笑起来，我一下子回答不上来，事实上，现在这家酒店的每个门柄上都装有一个控制门锁的微处理器。

人们怀疑并行计算机的另一个理由更为微妙，也更有道理。大家都知道，将计算分成许多并行的部分，会导致运行效率低下。现在这个问题依然制约着并行计算机的应用效率，但它并没有想象中的那么严重。高估这个问题的难度的一部分原因源自对早期并行计算机的一系列误解。20 世纪 60 年代，第一批并行计算机是通过将两三个顺序计算机相连而组成的。在大多数情况下，多个并行处理单元共享一个存储器，以便每个处理器访问相同的数据。通过编程，这些早期的并行计算机可以给每个处理器分配一个不同的任务。例如，在数据库应用程序中，第一个处理器用于检索数据，第二个用于汇总统计数据，第三个则用于打印结果。这些处理器相当于生产线上的不同工人，每个工人负责完成一个环节的计算任务。

这种方法存在固有的低效率问题，其严重程度会随着处理器数目的增加而增加。第一个低效率问题源于任务必须被分解为或多或少的独立阶段。任务被分解为两三个阶段不成问题，但若想将其分解为 10 个或者 100 个阶段，就非常困难了。正如并行计算的质疑者向一名报纸记者解释的那样：「如果让一名记者收集新闻素材，让另一位记者撰写新闻文章，那么这两位记者可能会很快写完一篇文章。但如果 100 名记者都在撰写这篇文章，可能就根本无法完成工作。」这种说法非常具有说服力。

第二个低效率问题源于存储器的共享访问模式。典型的存储器每次只能从给定区域内检索一个计算机字，这种受限的访问速度造成了明显的瓶颈，并限制了系统的性能。如果在读取速率已经受限的系统中增加更多处理器，那么处理器将在等待数据方面花费更多时间，系统效率将会更低。

此外，处理器必须格外小心地避免某些情况造成的不一致性，比如修改另一个处理器正在查看的数据。以航空公司的订票系统为例，如果一个处理器正在进行订座操作，它会查看某个座位是否为空，如果为空就会预订此座位。如果两个处理器同时为不同的乘客预订座位，就会带来一个问题：它们可能会同时发现有一个座位是空的，并且在对方还未占据之前都决定将这个座位标记为预留状态。为了避免出现这种问题，处理器必须采取一系列周密的操作，使某个处理器在查询数据时避免其他处理器修改该数据。这种对系统存储器的争夺会进一步降低效率，可能导致的最坏情况是，多处理器系统的速度将会降低至单处理器的速度，甚至更低。如上所述，这些低效率问题会随着处理器数量的增加变得更为严峻。

第三个低效率问题的根源似乎更为本质，即如何将任务均衡地分配给不同的处理器。回到生产线的例子中，我们可以从中发现，计算的运行速度取决于速度最慢的环节。如果只存在一个慢速环节，那么计算的运行速度就会由该环节决定。在这种情况下，系统的效率会随着处理器数量的增加而降低。

安达尔定律完美地解释了这些低效率问题，该定律由计算机设计师吉恩·安达尔（Gene Amdahl）于 20 世纪 60 年代提出，并以其名命名。安达尔的结论如下：总有一部分计算具有内在的顺序性，它们每次只能由单个处理器完成。即使只有 10% 的计算任务，它们实质上也具有内在的顺序性，无论如何加速剩余 90% 的并行计算任务，整体计算速度的提升比例永远不会超过 10 倍。当处理器完成那 90% 的并行计算任务后，会继续等待单个处理器来完成按顺序执行的这 10% 的计算任务。这个结论表明，具有 1 000 个处理器的并行计算机的效率极低，因为它只会比单个处理器的速度快 10 倍左右。当我试图申请基金来建造我的第一个并行计算机（一台拥有 64000 个处理器的大型并行计算机）时，收到的第一个问题通常是：「你有没有听说过安达尔定律？」

我当然听说过安达尔定律，而且我认为这个定律背后的推理过程没有问题。然而，我也确信，安达尔定律并不适用于我试图解决的问题，即便我无法证明这一点。我之所以如此确信是因为，我正在研究的问题已经通过一台大规模并行计算机得到了解决，这台计算机便是人类的大脑。当我还是麻省理工学院人工智能实验室的一名学生时，就想制造一台可以思考的机器。

1974 年，当我以本科新生的身份第一次访问麻省理工学院人工智能实验室时，人工智能领域正处于爆炸性发展的阶段。第一代用简单的英文执行书写指令的程序正在开发中，能理解人类语言的计算机即将诞生；计算机在国际象棋等游戏中表现出色，而在几年前，这些游戏对它们来说还过于复杂；人工视觉系统能识别出简单的物体，例如线条画和成堆的积木；计算机甚至通过了简单的微积分测试，并解决了智商测试中的一些简单问题。通用人工智能真的离我们遥遥无期吗？

几年后，当我以研究生的身份加入人工智能实验室时，问题看起来变得更加复杂了。一些简单的演示表明情况确实如此。尽管研究人员发明了许多崭新的原理和强大的工具，但当应用于更大规模、更复杂的问题时，它们并不奏效。其中有部分问题的解决受限于计算机的运行速度。人工智能的研究人员发现：将实验推广至涉及更多数据的场合时常常徒劳无获，因为计算机的运行速度已经够慢了，增加更多的数据只会拖慢它们的速度。例如，在计算机识别单个物体就需要数小时的情况下，再让计算机去识别一堆物体，结果无疑会令人感到沮丧。

计算机的运行速度之所以很慢是因为，它们是按顺序执行的，也就是说，它们每次只能做一件事情，比如计算机必须逐个像素地查看一幅图像。相比之下，人类大脑能瞬间感知整幅图像，并立即将看到的图像和已知的图像进行匹配。正是由于这个原因，人类在识别物体方面比计算机快得多，即便人类视觉系统中的神经元比计算机中的晶体管慢得多。这种设计上的差异激发了我以及其他许多人去寻找大规模并行计算机的设计方法，这类计算机可以同时执行数百万次运算，并且能够像大脑那样利用并行性。既然大脑能够从低速的部件中获得高速的性能，因此，我认为安达尔定律并非适用于所有情形。

现在，我知道了安达尔定律的缺陷，那就是，它假设计算任务中有固定比例的任务一定是按顺序执行的，即使只有 10%。这个假设看似合理，但事实上，大多数大规模计算并非如此。这种错误的直觉来源于对并行处理器使用方式的误解。问题的关键在于，如何在处理器之间分配计算任务。初看起来，最佳分配方式是让每个处理器分别执行程序中的不同部分。这种方式在一定程度上是有效的。然而，这就类似于向一组队伍分配任务时遇到的问题（正如前面提到的记者写稿的类比），具有如下缺点：大部分潜在的并行性都会消失于与协调相关的问题中。通过分解程序的方式对计算机进行编程，就类似于协调一大群人粉刷篱笆，即让第一个人打开油漆桶，让第二个人来处理篱笆表面，让第三个人来粉刷油漆，让第四个人来清洗刷子。这个分解过程需要高度的协调性，而且到一定程度之后，增加更多的人手并不能加快任务的完成速度。

另一种更有效地使用并行计算机的方式是，让每个处理器执行相似的任务，但使用不同部分的数据。这种所谓的数据并行分解方法类似于在粉刷篱笆的任务中为每个工人分配一块独立的篱笆。虽然并非所有的问题都像粉刷篱笆一样容易分解，但这种方法在大型计算任务中的应用效果非常好。例如，通过给每个处理器分配一小块图像，图像处理任务就可以以并行的方式被完成；在国际象棋这样的搜索问题中，通过让每个处理器同时搜索不同的走法，便可以实现任务的并行分解。在这些例子中，速度的提升几乎与处理器的数量成正比，即处理器数量越多，效果越好。当然，给处理器分配任务以及收集处理器的答案会花费额外的时间。如果问题的规模很大，计算任务的完成情况会更加高效，即便有成千上万个处理器并行执行任务。

很明显，上述这些计算任务可以在分解后并行执行。对于更复杂的任务，数据并行分解的方法同样行之有效。令人感到惊讶的是，无法以并行计算的方式处理的大型问题寥寥无几。大部分人认为的具有顺序性的计算问题也能通过并行计算机得到有效解决。其中一个例子是追踪链问题。我的小孩曾玩过的寻宝游戏就是一个基于追踪链的问题。我给他们一张纸条，纸条上的线索与下一条线索的隐藏地点相关，而且那条线索又指向了下一条线索，以此类推，直到他们最后找到宝藏。在这个游戏的计算机版本中，对于给定的程序，其输入是存储器中的一个位置的地址，这个位置存储了另一个位置的地址，而后面这个位置存储的依然是下一个位置的地址，以此类推。最后，包含特殊的计算机字的地址会指定某个存储位置，这个特殊的计算机字会指示该地址就是地址链条的末端。这个问题就是要从第一个位置出发找到最后一个位置。

初看起来，追踪链问题是一个典型的具有顺序性的计算问题，因为如果计算机不沿着整个链条追踪相连的地址，就无法找到链条的最后一个位置。为了找到第二个位置的地址，计算机必须找到第一个位置，为了找到第三个位置的地址，计算机必须找到第二个位置，以此类推。然而事实证明，这个问题可以通过并行的方法解决。具有 100 万个处理器的并行计算机可以在 20 个步骤之内找到一条包含 100 万个地址的链条的最后一个位置。

上述过程的窍门在于，每一步都将问题的规模缩小一半，这和第 5 章介绍的排序算法有些类似。假设 100 万个存储位置都有自己的处理器，并且可以给任何其他处理器发送信息。为了找到链条的末端，每个处理器首先会将自己的地址发送给链条中紧跟它的处理器，而紧跟它的后一个处理器的地址存储于前一个处理器的内存位置中。这样，每个处理器不仅知道了它后面的处理器的地址，还知道了前面的处理器的地址。然后，处理器利用此信息将它后面的处理器的地址发送给它前面的处理器。此时，每个处理器都知道了沿此链条后继下一个处理器的地址。因此，此时连接第一个和最后一个处理器的链条长度与原来的相比缩短了一半。接着重复该简化步骤，每重复一次，链条长度就会减半。经过 20 步的简化过程之后，在包含 100 万个存储地址的链条中，第一个处理器便知道了最后一个处理器的地址。类似的方法也可以应用于完成许多其他看似具有顺序性的计算任务。

在撰写本书时，并行计算机仍然是一种相对较新的技术。目前，我们尚不清楚何种类型的任务能被分解且有效利用多处理器的优势。不过，这里有一条经验：数据量大的问题最适合用并行技术来解决，因为当数据量很大时，处理器之间就能分配到许多相似的计算任务。

大多数计算任务能被分解成并行处理的子问题，原因之一是，大多数计算都基于物理世界的模型。这类计算可以通过并行的方式运行，因为物理世界的运行方式也是并行的。例如，计算机表示的图像通常通过算法合成，该算法模拟了光线从物理表面反射的过程。我们可以计算出每条光线从光源传输到眼睛，以及从一个表面反射到另一个表面的过程，由此从物体形状的数学描述中获得图像。所有关于光线的计算都可以同时进行，因为光在真实物理世界中是同时完成反射的。

适合并行计算的典型例子还有天气预报所需的大气模拟。代表大气的三维数字矩阵类似于三维物理空间。每个数字代表了一定体积的大气的某个物理参数，比如 1 立方千米单位容量内的大气压强。每个立方体都可以由几个代表平均温度、压力、风速以及湿度等物理量的数字表示。为了预测这些立方体中的大气将如何变化，计算机需要计算相邻空间内空气的流动过程。例如，如果某个空间内的空气流入量大于空气流出量，那么这个空间内的空气压力就会上升。计算机还会计算日照和水汽蒸发等因素带来的变化量。大气的模拟过程由一系列计算而来，每个步骤都对应着一段时间，比如半小时，因此在矩阵的单元之间模拟出的空气和水的流动情况类似于真实天气中空气和水的流动情况。计算机的最终模拟结果便是一种三维的移动图像，一种按物理规律变化的图像。

当然，模拟精度取决于三维图像的分辨率和准确度，这些因素是导致天气预报随着时间的推移而变得不准确的罪魁祸首。如果模型分辨率越高，初始条件测量得越精确，则预测结果就会越准确，但即使分辨率再高，从长远来看，天气预报也永远不可能达到百分之百的准确度，因为天气的初始状态不可能被精准无误地测量出来。和轮盘机游戏一样，天气系统是一种混沌系统，初始条件的微小扰动就能使结果产生巨大变化。在并行计算机中，每个处理器都可以负责预测小块区域的天气。当风从一块区域吹向另一块区域时，对这些区域进行建模的处理器之间必须进行通信。那些对地理上分离的区域进行建模的处理器可以独立、并行地运行，因为这些区域的天气之间几乎没有关系。模拟计算既可以是局部的，也可以是并行的，因为控制天气的物理定律也具有这两种特性。

天气模拟和物理定律之间显然存在联系。许多其他计算任务和物理世界之间的联系更为微妙。例如，电话费用的结算方法是并行的，因为电话和所对应的客户在物理世界中是独立运行的。只有一类问题我们不知道如何在并行计算机中有效地解决，即那些规模随着时间的推移而不断变大的问题，比如预测行星的轨迹。具有讽刺意味的是，最初正是为了解决这个问题，许多数学计算工具才被发明出来。

行星的轨道是九大行星和太阳之间动量和引力相互作用法则的结果。为了简单起见，我们将忽略诸如月球和小行星等小型天体的影响。我们可以用 9 个坐标来表示解决这个问题所需的全部信息，因此数据量并不大。这个问题的计算难度在于如下事实：进行运算时需要计算出行星的连续轨迹。而这个过程由数十亿个小阶段组成，每个阶段都代表了一段很短的时间。我们知道的唯一能计算出行星在未来 100 万年后的位置的方法是，计算出它们从现在起每隔一个时间段后的位置。一方面，我不知道这个问题是否存在并行的解法，正如解决追踪链问题时使用的方法；另一方面，据我所知，没有人能证明，轨道预测问题在本质上是顺序性问题。因此，这是一个悬而未决的问题。

当今，高度并行的计算机已经相当普遍，主要应用于大型数值的计算（例如天气模拟）或者大型数据库的计算，例如从信用卡交易记录中提取市场营销数据。由于并行计算机和个人计算机的组成零件是相同的，因此随着时间的推移，它们会变得更加便宜和常见。最有趣的一种并行计算机碰巧是从顺序计算机网络中出现的。这个被称为互联网的全球计算机网络主要被用作通信系统。这些计算机主要起到了媒介的作用，即存储和发送那些仅对人们有意义的信息，比如电子邮件。我相信今后的情况会出现变化，因为现在已经开始出现允许计算机像交换数据那样交换程序的标准了。互联网上的计算机一起合作产生的潜在计算能力，远远超过了历史上的任何一台计算机。

我相信，互联网发展到最后，一定会将电话、汽车以及家用电器都嵌入计算机内。这些计算机将直接从物理世界中读取和输入信息，不再依赖人类作为中间人。随着互联网上的信息变得越来越丰富，以及相连的计算机之间的交互形式变得越来越复杂，我预测，互联网将会开始呈现出一些涌现行为（emergent behavior），这些行为会超出程序规定的系统行为范围。事实上，互联网已经开始显示出这种迹象。不过，到目前为止，大部分内容都十分简单，比如计算机病毒的蔓延、无法预测的信息路由模式等。随着网络中的计算机可以交换程序，而不仅是收发电子邮件，互联网的行为将会变得不像网络，而更像并行计算机。我还坚信，互联网中的涌现行为会变得更加有趣。