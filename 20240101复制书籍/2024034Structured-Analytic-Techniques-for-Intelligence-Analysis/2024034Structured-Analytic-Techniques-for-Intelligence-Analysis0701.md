## 0701. Diagnostic Techniques

A nalysis conducted by the intelligence, law enforcement, and business communities will never achieve the accuracy and predictability of a true science because the information with which analysts must work is typically incomplete, ambiguous, and potentially deceptive. The analytic process can, however, benefit from the lessons of science and adapt some of the elements of scientific reasoning.

The scientific process involves observing, categorizing, formulating hypotheses, and then testing those hypotheses. Generating and testing hypotheses is a core function of structured analysis. A possible explanation of the past or a judgment about the future is a hypothesis that needs to be tested by collecting and presenting evidence. This chapter focuses on several key techniques that support the Diagnostic Reasoning process, including challenging key assumptions about what the information reveals, developing Chronologies and Timelines, generating alternative hypotheses, and testing the validity of hypotheses and the quality of argumentation. Practice in using three of the techniques—Key Assumptions Check, Multiple Hypothesis Generation, and Analysis of Competing Hypotheses—will help analysts become proficient in the first three of the Five Habits of the Master Thinker (see chapter 3 ): challenging assumptions, generating alternative explanations, and identifying inconsistent evidence.

The generation and testing of hypotheses is a skill, and its subtleties do not come naturally. It is a form of reasoning that people can learn to use for dealing with high-stakes situations. What does come naturally is drawing on our existing body of knowledge and experience (mental model) to make an intuitive judgment. 1 In most circumstances in our daily lives, this is an efficient approach that works most of the time. For intelligence analysis, however, it is not adequate, because intelligence issues are generally so complex, and the risk and cost of error are too great. Also, the situations are often novel, so the intuitive judgment shaped by past knowledge and experience may well be wrong.

Good analysis of a complex issue must start with a set of alternative hypotheses. Another practice that the experienced analyst borrows from the scientist's toolkit involves the testing of alternative hypotheses. The truth of a hypothesis can never be proven beyond doubt by citing only evidence that is consistent with the hypothesis, because the same evidence may be and often is consistent with more than one hypothesis. Science often proceeds by refuting or disconfirming hypotheses. A hypothesis that cannot be refuted should be taken just as seriously as a hypothesis that seems to have a lot of evidence in favor of it. A single item of evidence that is shown to be inconsistent with a hypothesis can be grounds for rejecting that hypothesis. The most tenable hypothesis is often the one with the least evidence against it.

Analysts often test hypotheses by using a form of reasoning known as abduction, which differs from the two better known forms of reasoning, deduction and induction. Abductive reasoning starts with a set of facts. One then develops hypotheses that, if true, would provide the best explanation for these facts. The most tenable hypothesis is the one that best explains the facts. Because of the uncertainties inherent in intelligence analysis, conclusive proof or refutation of hypotheses is the exception rather than the rule.

Use of Diagnostic Techniques can provide a strong antidote to several cognitive biases. It can reduce the influence of Confirmation Bias by exposing analysts to new ideas and multiple permutations, and mitigate the impact of Evidence Acceptance Bias, which is accepting data as true because it helps create a more coherent story. Diagnostic Techniques also protect analysts against falling into the intuitive traps of Relying on First Impressions, Ignoring Inconsistent Evidence, and Projecting Past Experiences.

The first part of this chapter describes techniques for challenging key assumptions, establishing analytic baselines, and identifying the relationships among the key variables, drivers, or players that may influence the outcome of a situation. These and similar techniques allow analysts to imagine new and alternative explanations for their subject matter.

The second section describes three techniques for generating hypotheses. Other chapters include additional techniques for generating hypotheses, but which also have a variety of other purposes. These include Cluster Brainstorming, Nominal Group Technique, and Venn Analysis ( chapter 6 ); the Delphi Method and Classic Quadrant Crunching™ ( chapter 8 ); various forms of Foresight analysis ( chapter 9 ); and Critical Path Analysis and Decision Trees ( chapter 10 ).

This chapter concludes with a discussion of five techniques for testing hypotheses, detecting deception, and evaluating the strength of an argument. These techniques spur the analyst to become more sensitive to the quality of the data and the strength of the logic and to look for information that not only confirms but can disconfirm the hypothesis. One of these, Analysis of Competing Hypotheses (ACH), was developed by Richards J. Heuer Jr. specifically for use in intelligence analysis.

Overview of Techniques

Key Assumptions Check is one of the most important and frequently used techniques. Analytic judgment is always based on a combination of evidence and assumptions—or preconceptions—that influence how the evidence is interpreted. The Key Assumptions Check is a systematic effort to make explicit and question the assumptions (i.e., mental model) that guide an analyst's thinking.

Chronologies and Timelines are used to organize data on events or actions. They are used whenever it is important to understand the timing and sequence of relevant events or to identify key events and gaps.

Cross-Impact Matrix is a technique that can be used after any form of brainstorming that identifies a list of variables relevant to an analytic project. The results of the brainstorming session are put into a matrix, which is used to guide a group discussion that systematically examines how each variable influences all other variables to which it is related in a particular problem context. The group discussion is often a valuable learning experience that provides a foundation for further collaboration. Results of cross-impact discussions should be retained for future reference as a cross-check after the analysis is completed.

Multiple Hypothesis Generation can be accomplished in many ways. This book describes three techniques—Simple Hypotheses, Quadrant Hypothesis Generation, and the Multiple Hypothesis Generation. Simple Hypotheses is the easiest to use but not always the best selection. Quadrant Hypothesis Generation is used to identify a set of hypotheses when the outcome is likely to be determined by just two driving forces. Multiple Hypothesis Generation is used to identify a large set of possible hypotheses. The latter two techniques are particularly useful in identifying sets of M utually E xclusive and C omprehensively E xhaustive (MECE) hypotheses.

Diagnostic Reasoning applies hypothesis testing to the evaluation of significant new information. Such information is evaluated in the context of all plausible explanations of that information, not just in the context of the analyst's well-established mental model. The use of Diagnostic Reasoning reduces the risk of surprise, as it ensures that an analyst will have considered some alternative conclusions. Diagnostic Reasoning differs from the ACH technique in that it evaluates a single item of evidence; ACH deals with an entire issue involving multiple pieces of evidence and a more complex analytic process.

Analysis of Competing Hypotheses is the application of Karl Popper's philosophy of science to the field of intelligence analysis. 2 Popper was one of the most influential philosophers of science of the twentieth century. He is known for, among other things, his position that scientific reasoning should start with multiple hypotheses and proceed by rejecting or eliminating hypotheses, tentatively accepting only those hypotheses that cannot be refuted. This process forces an analyst to recognize the full uncertainty inherent in most analytic situations. ACH helps the analyst sort and manage relevant information to identify paths for reducing that uncertainty.

The Inconsistencies Finder™ is a simplified version of ACH that helps analysts evaluate the relative credibility of a set of hypotheses based on the amount of identified disconfirming information. It provides a quick framework for identifying inconsistent data and discovering the hypotheses that are most likely to be correct.

Deception Detection employs a set of checklists analysts can use to determine when to anticipate deception, how to determine if one is being deceived, and what to do to avoid being deceived. It is also useful for detecting the presence of Digital Disinformation or "Fake News." The possibility of deception by a foreign intelligence service, economic competitor, or other adversary organization is a distinctive type of hypothesis that can be included in any ACH analysis. Information identified through the Deception Detection technique can then be entered as relevant information in an ACH matrix.

Argument Mapping is a method that can be used to put a single hypothesis to a rigorous logical test. The structured visual representation of the arguments and evidence makes it easier to evaluate any analytic judgment. Argument Mapping is a logical follow-on to an ACH analysis. It is a detailed presentation of the arguments for and against a single hypothesis; ACH is a more general analysis of multiple hypotheses. The successful application of Argument Mapping to the hypothesis favored by the ACH analysis would increase confidence in the results of both analyses.

7.1 Key Assumptions Check

Analytic judgment is always based on a combination of evidence and assumptions, or preconceptions, which influences how the evidence is interpreted. 3 The Key Assumptions Check is a systematic effort to make explicit and question the assumptions (the mental model) that guide an analyst's interpretation of evidence and reasoning about a problem. Such assumptions are usually necessary and unavoidable as a means to fill gaps in the incomplete, ambiguous, and sometimes deceptive information with which the analyst must work. They are driven by the analyst's education, training, and experience, plus the organizational context in which the analyst works.

An organization really begins to learn when its most cherished assumptions are challenged by counterassumptions. Assumptions underpinning existing policies and procedures should therefore be unearthed, and alternative policies and procedures put forward based upon counterassumptions.

—Ian I. Mitroff and Richard O. Mason, Creating a Dialectical Social Science: Concepts, Methods, and Models (1981)

The Key Assumptions Check is one of the most common techniques used by intelligence analysts because they typically need to make assumptions to fill information gaps. In the intelligence world, these assumptions are often about another country's intentions or capabilities, the way governmental processes usually work in that country, the relative strength of political forces, the trustworthiness or accuracy of key sources, the validity of previous analyses on the same subject, or the presence or absence of relevant changes in the context in which the activity is occurring. Assumptions are often difficult to identify because many sociocultural beliefs are held unconsciously or so firmly that they are assumed to be truth and not subject to challenge.

When to Use It

Any explanation of current events or estimate of future developments requires the interpretation of evidence. If the available evidence is incomplete or ambiguous, this interpretation is influenced by assumptions about how things normally work in the country or company of interest. These assumptions should be made explicit early in the analytic process.

If a Key Assumptions Check is not done at the outset of a project, it can still prove extremely valuable if done during the coordination process or before conclusions are presented or delivered. When a Key Assumptions Check is done early in the process, it is often desirable to review the assumptions again later—for example, just before or just after drafting the report. The task is to determine whether the assumptions still hold true or should be modified.

When tracking the same topic or issue over time, analysts should consider reassessing their key assumptions on a periodic basis, especially following a major new development or surprising event. If, on reflection, one or more key assumptions no longer appear to be well-founded, analysts should inform key policymakers or corporate decision makers working that target or issue that a foundational construct no longer applies or is at least doubtful.

Value Added

Preparing a written list of one's working assumptions at the beginning of any project helps the analyst do the following:

Identify the specific assumptions that underpin the basic analytic line. Achieve a better understanding of the fundamental dynamics at play. Gain a better perspective and stimulate new thinking about the issue. Discover hidden relationships and links among key factors. Identify what developments would call a key assumption into question. Avoid surprises should new information render old assumptions invalid.

A Key Assumptions Check helps analysts mitigate the impact of heuristics that, when misapplied, can impede analytic thinking, including the tendency to accept a given value of an assumption or something unknown as a proper starting point for generating an assessment (Anchoring Effect), reaching an analytic judgment before sufficient information is collected and proper analysis performed (Premature Closure), and judging the frequency of an event by the ease with which instances come to mind (Availability Heuristic). It also safeguards an analyst against several classic mental mistakes, including the tendency to overdraw conclusions when presented with only a small amount of data (Overinterpreting Small Samples), assume the same dynamic is in play when something appears to be in accord with past experiences (Projecting Past Experiences), and failing to factor something into the analysis because the analyst lacks an appropriate category or "bin" for that item of information (Lacking Sufficient Bins).

Conducting a Key Assumptions Check gives analysts a better understanding of the suppositions underlying their key judgments or conclusions. Doing so helps analysts establish how confident they should be in making their assessment and disseminating their key findings.

The Method

The process of conducting a Key Assumptions Check is relatively straightforward in concept but often challenging to put into practice. One challenge is that participating analysts must be open to the possibility that they could be wrong. It helps to involve several well-regarded analysts who are generally familiar with the topic but have no prior commitment to any set of assumptions about the issue in the process. Engaging a facilitator is also highly recommended. Keep in mind that many "key assumptions" turn out to be "key uncertainties." Randolph Pherson's extensive experience as a facilitator of analytic projects indicates that approximately one in every four key assumptions collapses on careful examination.

The following are steps in conducting a Key Assumptions Check:

Gather a small group of individuals who are working the issue along with a few "outsiders." The primary analytic unit already is working from an established mental model, so the "outsiders" are needed to bring a different perspective. Ideally, the facilitator should notify participants about the topic beforehand and ask them to bring to the meeting a list of assumptions they make about the topic. If they do not do this beforehand, start the meeting with a silent brainstorming session by asking each participant to write down several assumptions on an index card. Collect the cards and list the assumptions on a whiteboard or easel for all to see. Elicit additional assumptions. Work from the prevailing analytic line to identify additional arguments that support it. Use various devices to help prod participants' thinking:

Ask the standard journalistic questions. Who: Are we assuming that we know who all the key players are? What: Are we assuming that we know the goals of the key players? How: Are we assuming that we know how they are going to act? When: Are we assuming that conditions have not changed since our last report or that they will not change in the foreseeable future? Where: Are we assuming that we know where the real action is going to be? Why: Are we assuming that we understand the motives of the key players? Use of phrases such as "will always," "will never," or "would have to be" suggests that an idea is not being challenged. Perhaps it should be. Use of phrases such as "based on" or "generally the case" suggests the presence of a challengeable assumption. When the flow of assumptions starts to slow down, ask, "What else seems so obvious that one would not normally think about challenging it?" If no one can identify more assumptions, then there is an assumption that they do not exist, which itself is an assumption subject to challenge. After identifying a full set of assumptions, critically examine each assumption and ask,

Why am I confident that this assumption is correct? In what circumstances might this assumption be untrue? Could it have been true in the past but not any longer? How much confidence do I have that this assumption is valid? If it turns out to be invalid, how much impact would this have on the analysis? Place each assumption in one of three categories:

Basically solid (S) Correct with some caveats (C) Unsupported or questionable—the "key uncertainties" (U) Refine the list. Delete assumptions that do not hold up to scrutiny and add new ones that emerge from the discussion. If an assumption generates a lot of discussion, consider breaking it into two assumptions or rephrasing it to make the statement more explicit. Above all, emphasize those assumptions that would, if wrong, lead to changing the analytic conclusions. Consider whether key uncertainties should be converted into intelligence collection requirements or research topics.

When concluding the analysis, remember that the probability of your analytic conclusion being accurate cannot be greater than the weakest link in your chain of reasoning. Review your assumptions, assess the quality of evidence and reliability of sources, and consider the overall difficulty and complexity of the issue. Then make a rough estimate of the probability that your analytic conclusion will turn out to be wrong. Use this number to calculate the rough probability of your conclusion turning out to be accurate. For example, a three in four chance (75 percent) of being right equates to a one in four chance (25 percent) of being wrong. This focus on how and why we might be wrong is needed to offset the natural human tendency toward reluctance to admit we might be wrong.

Figure 7.1 shows apparently flawed assumptions made in the Wen Ho Lee espionage case during the 1990s and what further investigation showed about these assumptions. A Key Assumptions Check could have identified weaknesses in the case against Lee much earlier.

Relationship to Other Techniques

The Key Assumptions Check is frequently paired with other techniques because assumptions play an important role in all structured analytic efforts. It is important to get them right. For example, when an assumption is critical to an analysis, and questions remain about the validity of that assumption, it may be desirable to follow the Key Assumptions Check with a What If? Analysis. Imagine a future (or a present) in which the assumption is wrong. What could have happened to make it wrong, how could that have happened, and what are the consequences?

Figure 7.1 Key Assumptions Check: The Case of Wen Ho Lee Source: Pherson Associates, LLC, 2019.

There is a particularly noteworthy interaction between Key Assumptions Check and ACH. Key assumptions need to be included as "evidence" in an ACH matrix to ensure that the matrix is an accurate reflection of the analyst's thinking. Assumptions often emerge during a discussion of relevant information while filling out an ACH matrix. This happens when an analyst assesses the consistency or inconsistency of an item of evidence with a hypothesis and concludes that the designation is dependent upon something else—usually an assumption. Classic Quadrant Crunching™ ( chapter 8 ) and Simple Scenarios, the Cone of Plausibility, and Reversing Assumptions ( chapter 9 ) all use assumptions and their opposites to generate multiple explanations or outcomes.

Origins of This Technique

Although assumptions have been a topic of analytic concern for a long time, the idea of developing a specific analytic technique to focus on assumptions did not occur until the late 1990s. The discussion of Key Assumptions Check in this book is from Randolph H. Pherson, Handbook of Analytic Tools and Techniques , 5th ed. (Tysons, VA: Pherson Associates, LLC, 2019).

7.2 Chronologies and Timelines

A Chronology is a list that places events or actions in the order in which they occurred, usually in narrative or bulleted format. A Timeline is a graphic depiction of those events put in context of the time of the events and the time between events. Both are used to identify trends or relationships among the events or actions and, in the case of a Timeline, among the events and actions as well as other developments in the context of the overarching intelligence problem.

When to Use It

Chronologies and Timelines aid in organizing events or actions. The techniques are useful whenever analysts need to understand the timing and sequence of relevant events. They can also reveal significant events or important gaps in the available information. The events may or may not have a cause-and-effect relationship.

Chronologies and Timelines are usually developed at the onset of an analytic task to ascertain the context of the activity under review. They can be used in postmortems to break down the stream of reporting, find the causes for analytic failures, and highlight significant events after an intelligence or business surprise. Chronologies and Timelines are also useful for organizing information in a format that can be readily understood in a briefing or when presenting evidence to a jury.

Value Added

Chronologies and Timelines help analysts identify patterns and correlations among events. Analysts can use them to relate seemingly disconnected events to the big picture; to highlight or identify significant changes; or to assist in the discovery of trends, developing issues, or anomalies. They can serve as a catchall for raw data when the meaning of the data is not yet clear. Multiple-level Timelines allow analysts to track concurrent events that may affect one another.

The activities on a Timeline can lead an analyst to hypothesize the existence of previously unknown events. In other words, the series of known events may make sense only if other previously unknown events had occurred. The analyst can then look for other indicators of those missing events.

Chronologies and Timelines are useful tools analysts can use to counter the impact of cognitive biases and heuristics, including accepting data as true without assessing its credibility because it helps "make the case" (Evidence Acceptance Bias), seeing patterns in random events as systematic and part of a coherent story (Desire for Coherence and Uncertainty Reduction), and providing quick and easy answers to difficult problems (Mental Shotgun). It can also mitigate the impact of several intuitive traps, including giving too much weight to first impressions or initial data that attracts our attention at the time (Relying on First Impressions), not paying sufficient attention to the impact of the absence of information (Ignoring the Absence of Information), and discarding or ignoring information that is inconsistent with what one would expect to see (Ignoring Inconsistent Evidence).

The Method

Chronologies and Timelines are effective yet simple ways to order incoming information when processing daily message traffic. A Microsoft Word document or an Excel spreadsheet can log the results of research and marshal evidence. Tools such as the Excel drawing function or Analysts' Notebook can be helpful in drawing the Timeline. Follow these steps:

When researching the problem, ensure that the relevant information is listed with the date or order in which it occurred. It is important to properly reference the data to help uncover potential patterns or links. Be sure to distinguish between the date the event occurred and the date the report was received. Review the Chronology or Timeline by asking the following questions:

What are the temporal distances between key events? If "lengthy," what caused the delay? Are there missing pieces of data that may fill those gaps that should be collected? Was information overlooked that may have had an impact on or be related to the events? Conversely, if events seem to have happened more rapidly than expected or if some of the events do not appear to be related, is it possible that the analyst has information related to multiple event Timelines? Does the Timeline have all the critical events that are necessary for the outcome to occur? When did the information become known to the analyst or a key player? Are there information or intelligence gaps? Are there any points along the Timeline when the target is particularly vulnerable to the collection of intelligence or information or countermeasures? What events outside this Timeline could have influenced the activities? If preparing a Timeline, synopsize the data along a horizontal or vertical line. Use the space on both sides of the line to highlight important analytic points. For example, place facts above the line and points of analysis or commentary below the line. Alternatively, contrast the activities of different groups, organizations, or streams of information by placement above or below the line. If multiple actors are involved, you can use multiple lines, showing how and where they converge. For example, multiple lines could be used to show (1) the target's activities, (2) open source reporting about the events, (3) supplemental classified or proprietary information, and (4) analytic observations or commentary. Look for relationships and patterns in the data connecting persons, places, organizations, and other activities. Identify gaps or unexplained time periods, and consider the implications of the absence of evidence. Prepare a summary chart detailing key events and key analytic points in an annotated Timeline.

Potential Pitfalls

In using Timelines, analysts may assume, incorrectly, that events following earlier events were caused by the earlier events. Also, the value of this technique may be reduced if the analyst lacks imagination in identifying contextual events that relate to the information in the Chronology or Timeline.

Description Figure 7.2 Timeline Estimate of Missile Launch Date Source: Pherson Associates, LLC, 2019. Note: A TEL is a transporter, erector, and launcher for missiles.

Example

A team of analysts working on strategic missile forces knows what steps are necessary to prepare for and launch a nuclear missile. (See Figure 7.2 .) The analysts have been monitoring a country they believe is close to testing a new variant of its medium-range surface-to-surface ballistic missile. They have seen the initial steps of a test launch in mid-February and decide to initiate a concentrated watch of the primary and secondary test launch facilities. Observed and expected activities are placed into a Timeline to gauge the potential dates of a test launch. The analysts can thus estimate when a possible missile launch may occur and make decision makers aware of indicators of possible activity.

Origins of This Technique

Chronologies and Timelines are well-established techniques used in many fields. The information here is from M. Jones, "Sorting, Chronologies, and Timelines," The Thinker's Toolkit (New York: Three Rivers Press, 1998), chapter 6 ; and from Pherson Associates training materials.

7.3 Cross-Impact Matrix

The Cross-Impact Matrix helps analysts deal with complex problems when "everything is related to everything else." By using this technique, analysts and decision makers can systematically examine how each factor in a context influences all other factors to which it appears related.

When to Use It

The Cross-Impact Matrix is useful early in a project when a group is still in a learning mode trying to sort out a complex situation. Whenever a brainstorming session or other meeting is held to identify all the variables, drivers, or players that may influence the outcome of a situation, the next logical step is to use a Cross-Impact Matrix to examine the interrelationships among each of these variables. A group discussion of how each pair of variables interacts can be an enlightening learning experience and a good basis on which to build ongoing collaboration. How far one goes in completing the matrix and producing a description of the effects associated with each variable may vary depending upon the nature and significance of the project. At times, just the discussion is adequate.

Analysis of cross-impacts is useful when the following occurs:

A situation is in flux, and analysts need to understand all the factors that might influence the outcome. This requires understanding how all the factors relate to one another, and how they might influence one another. A situation is stable, and analysts need to identify and monitor all the factors that could upset that stability. This, too, requires understanding how the various factors might interact to influence one another. A significant event has just occurred, and analysts need to understand the implications of the event. What other significant forces are influenced by the event, and what are the implications of this influence?

Value Added

When analysts are estimating or forecasting future events, they consider the dominant forces and potential future events that might influence an outcome. They then weigh the relative influence or likelihood of these forces or events, often considering them individually without regard to potentially important interactions. The Cross-Impact Matrix provides a context for the discussion of these interactions. This discussion often reveals that variables or issues once believed to be simple and independent are interrelated. The sharing of information during a discussion of each potential cross-impact can provide an invaluable learning experience. For this reason alone, the Cross-Impact Matrix is a useful tool that can be applied at some point in almost any study that seeks to explain current events or forecast future outcomes.

The Cross-Impact Matrix provides a structure for managing the complexity that makes most analysis so difficult. It requires that analysts clearly articulate all assumptions about the relationships among variables. Doing so helps analysts defend or critique their conclusions by tracing the analytical argument back through a path of underlying premises.

Use of the Cross-Impact Matrix is particularly effective in helping analysts avoid being influenced by heuristics such as stopping the search for a cause when a seemingly satisfactory answer is found (Premature Closure), selecting the first answer that appears "good enough" (Satisficing), and seeing patterns in random events as systematic and part of a coherent world (Desire for Coherence and Uncertainty Reduction). It can also provide a powerful antidote to several intuitive pitfalls, including overinterpreting conclusions from a small sample of data (Overinterpreting Small Samples), giving too much weight to first impressions or initial data that appears important at the time (Relying on First Impressions), and continuing to hold to a judgment when confronted with additional or contradictory evidence (Rejecting Evidence).

The Method

Assemble a group of analysts knowledgeable on various aspects of the subject. The group brainstorms a list of variables or events that would likely have some effect on the issue being studied. The project coordinator then creates a matrix and puts the list of variables or events down the left side of the matrix and the same variables or events across the top.

The group then fills out the matrix, considering, and then recording, the relationship between each variable or event and every other variable or event. For example, does the presence of Variable 1 increase or diminish the influence of Variables 2, 3, 4, and so on? Or does the occurrence of Event 1 increase or decrease the likelihood of Events 2, 3, 4, and so forth? If one variable does affect the other, the positive or negative magnitude of this effect can be recorded in the matrix by entering a large or small + or a large or small – in the appropriate cell (or by making no marking at all if there is no significant effect). The terminology used to describe the relationship between each pair of variables or events is based on whether it is "enhancing," "inhibiting," or "unrelated."

The matrix shown in Figure 7.3 has six variables, with thirty possible interactions. Note that the relationship between each pair of variables is assessed twice, as the relationship may not be symmetric. That is, the influence of Variable 1 on Variable 2 may not be the same as the impact of Variable 2 on Variable 1. It is not unusual for a Cross-Impact Matrix to have substantially more than thirty possible interactions, in which case careful consideration of each interaction can be time-consuming.

Description Figure 7.3 Cross-Impact Matrix

Analysts should use the Cross-Impact technique to focus on significant interactions between variables or events that may have been overlooked, or combinations of variables that might reinforce one another. Combinations of variables that reinforce one another can lead to surprisingly rapid changes in a predictable direction. On the other hand, recognizing that there is a relationship among variables and the direction of each relationship may be sufficient for some problems.

The depth of discussion and the method used for recording the results are discretionary. Each depends upon how much you are learning from the discussion, which will vary from one application of this matrix to another. If the group discussion of the likelihood of these variables or events and their relationships to one another is a productive learning experience, keep it going. If key relationships are identified that are likely to influence the analytic judgment, fill in all cells in the matrix and take good notes. If the group does not seem to be learning much, cut the discussion short.

As a collaborative effort, team members can conduct their discussion—and periodically review—their key findings online. As time permits, analysts can enter new information or edit previously entered information about the interaction between each pair of variables. This record will serve as a point of reference or a memory jogger throughout the project.

Relationship to Other Techniques

Matrices as a generic technique with many types of applications are discussed in chapter 5 . The use of a Cross-Impact Matrix as described here frequently follows some form of brainstorming at the start of an analytic project. Elicit the assistance of other knowledgeable analysts in exploring all the relationships among the relevant factors identified in the brainstorming session. Analysts can build on the discussion of the Cross-Impact Matrix by developing a visual Mind Map or Concept Map of all the relationships.

See also the discussion of the Complexity Manager technique in chapter 10 . An integral part of the Complexity Manager technique is a form of Cross-Impact Analysis that takes the analysis a step further toward an informed conclusion.

Origins of This Technique

The Cross-Impact Matrix technique was developed in the 1960s as one element of a quantitative futures analysis methodology called Cross-Impact Analysis. Richards J. Heuer Jr. became familiar with it when the CIA was testing the Cross-Impact Analysis methodology. He started using it as an intelligence analysis technique, as described here, more than forty years ago. For simple instructions for using the Cross-Impact Matrix and printable templates, go to http://discoveryoursolutions.com/toolkit/cross_impact_matrix.html .

7.4 Multiple Hypothesis Generation

In broad terms, a hypothesis is a potential explanation or conclusion that is to be tested by collecting and analyzing evidence. It is a declarative statement that has not been established as true—an "educated guess" based on observation to be supported or refuted by more observation or through experimentation.

A good hypothesis should satisfy the following criteria represented by the mnemonic STOP:

S tatement, not a question. T estable and falsifiable. O bservation—and knowledge-based. P redicts anticipated results clearly.

Hypothesis Generation should be an integral part of any rigorous analytic process because it helps the analyst think broadly and creatively about a range of possibilities and avoid being surprised when common wisdom turns out to be wrong. The goal is to develop a list of hypotheses that can be scrutinized and tested over time against existing relevant information as well as new data that may become available in the future. Analysts should strive to make the hypotheses mutually exclusive and the list as comprehensively exhaustive as possible—thereby satisfying the imperative that hypotheses should be M utually E xclusive and C omprehensively E xhaustive (MECE).

There are many techniques used to generate hypotheses, including techniques discussed elsewhere in this book, such as Venn Analysis, Cluster Brainstorming, several forms of Foresight analysis, Classic Quadrant Crunching™, Starbursting, Delphi Method, and Decision Trees. This section discusses techniques developed specifically for hypothesis generation and then presents the method for three different techniques: Simple Hypotheses, Quadrant Hypothesis Generation, and the Multiple Hypotheses Generator ® .

When to Use It

Gaining confidence in a hypothesis is not a function solely of accumulating evidence in its favor but in showing that situations that could establish its falsity do not, in fact, happen. Analysts should develop multiple hypotheses at the start of a project when the following occurs:

the importance of the subject matter requires a systematic analysis of all alternatives, many factors or variables are involved in the analysis, a high level of uncertainty exists about the outcome, analysts or decision makers hold competing views.

Simple Hypotheses is often used to broaden the spectrum of plausible hypotheses. It utilizes Cluster Brainstorming to create potential hypotheses based on affinity groups. Quadrant Hypothesis Generation works best when the problem has two key drivers. In these circumstances, a 2-×-2 matrix is adequate for creating hypotheses that reflect the situations posited in each quadrant. The Multiple Hypotheses Generator ® is particularly helpful when there is a reigning lead hypothesis.

Value Added

Multiple Hypothesis Generation provides a structured way to generate a comprehensive set of mutually exclusive hypotheses. This can increase confidence that an important hypothesis has not been overlooked. It can also help reduce cognitive biases, such as seeking only the information that is consistent with the lead hypothesis (Confirmation Bias), accepting a given value of something or a lead hypothesis as a proper—or the only—starting point for conducting an analysis (Anchoring Effect), stopping the search for a cause when a seemingly satisfactory answer is found before sufficient information and proper analysis can be performed (Premature Closure), and seeing patterns in random events as systematic and part of a coherent story (Desire for Coherence and Uncertainty Reduction). When the techniques are used properly, choosing a lead hypothesis becomes much less critical than making sure that analysts have considered all possible explanations.

The techniques are particularly useful in helping intelligence analysts overcome some classic intuitive traps, such as the following:

Assuming a Single Solution. Most analysts quickly develop an initial lead hypothesis to explain the topic at hand and continue to test the initial hypothesis as new information appears. A good analyst will simultaneously consider a few alternatives. This helps ensure that no diagnostic information is ignored because it does not support the lead hypothesis. For example, when individuals move large amounts of money from China to other countries, financial analysts are likely to suspect that ill-begotten monies are being laundered. On some occasions, however, the funds could have been obtained through legitimate means; this alternative hypothesis should be in play until it can be disproven by the facts of the case. Overinterpreting Small Samples. Analysts frequently are pressed to "make a call" when there is insufficient data to support the assessment. In such cases, a preferred strategy is to offer several possible alternatives. The U.S. Intelligence Community—and the world as a whole—would have been better served if the National Intelligence Council in its Iraq WMD National Intelligence Estimate had proffered three (and not just the first two) hypotheses that (1) Iraq had a substantial WMD program and the intelligence community had not yet found the evidence, (2) Iraq had a more modest program but could readily accelerate production in areas that had fallen fallow, or (3) Iraq had no WMD program and reporting to the contrary was deception. Projecting Past Experiences. When under pressure, analysts can select a hypothesis primarily because it avoids a previous error or replicates a past success. A prime example of this was the desire not to repeat the mistake of underestimating Saddam Hussein's WMD capabilities in the run-up to the second U.S. war with Iraq. Relying on First Impressions. When pressed for time, analysts can also fall into the trap of giving too much weight to first impressions or initial data that attracts their attention at the time. Analysts are especially susceptible to this bias if they have recently visited a country or have viewed a particularly vivid or disturbing video.

7.4.1 The Method: Simple Hypotheses

To use the Simple Hypotheses method, define the problem and determine how the hypotheses will be used at the beginning of the project. Hypotheses can be applied several ways: (1) in an Analysis of Competing Hypotheses, (2) in some other hypothesis-testing project, (3) as a basis for developing scenarios, or (4) as a means to draw attention to particularly positive or worrisome outcomes that might arise. Figure 7.4.1 illustrates the process.

Gather together a diverse group to review the available information and explanations for the issue, activity, or behavior that you want to evaluate. In forming this diverse group, consider including different types of expertise for different aspects of the problem, cultural expertise about the geographic area involved, different perspectives from various stakeholders, and different styles of thinking (left brain/right brain, male/female). Then do the following:

Ask each member of the group to write down on an index card up to three alternative explanations or hypotheses. Prompt creative thinking by using the following:

Applying theory. Drawing from the study of many examples of the same phenomenon.

Description Figure 7.4.1 Simple Hypotheses Source: Globalytica, LLC, 2019. Comparison with historical analogies. Comparing current events to historical precedents. Situational logic. Representing all the known facts and an understanding of the underlying forces at work at the given time and place. Collect the cards and display the results on a whiteboard. Consolidate the list to avoid any duplication. Employ additional individual and group brainstorming techniques, such as Cluster Brainstorming, to identify key forces and factors. Aggregate the hypotheses into affinity groups and label each group. Use problem restatement and consideration of the opposite to develop new ideas. Update the list of alternative hypotheses. If the hypotheses will be used in Analysis of Competing Hypotheses, strive to keep them mutually exclusive—that is, if one hypothesis is true, all others must be false. Have the group clarify each hypothesis by asking the journalist's classic list of questions: Who, What, How, When, Where, and Why? Select the most promising hypotheses for further exploration.

7.4.2 The Method: Quadrant Hypothesis Generation

Use the four-quadrant technique to identify a basic set of hypotheses when two key driving forces are likely to determine the outcome of an issue. The technique identifies four potential scenarios that represent the extreme conditions for each of the two major drivers. It spans the logical possibilities inherent in the relationship and interaction of the two driving forces, thereby generating options that analysts otherwise may overlook.

Quadrant Hypothesis Generation is easier and quicker to use than the Multiple Hypotheses Generator ® , but it is limited to cases in which the outcome of a situation will be determined by two major driving forces—and it depends on the correct identification of these forces. The technique is less effective when more than two major drivers are present or when analysts differ over which forces constitute the two major drivers.

The steps for using Quadrant Hypothesis Generation follow:

Identify two main drivers by using techniques such as Cluster Brainstorming or by surveying subject-matter experts. A discussion to identify the two main drivers can be a useful exercise. Construct a 2-×-2 matrix using the two drivers. Think of each driver as a continuum from one extreme to the other. Write the extremes of each of the drivers at the end of the vertical and horizontal axes. Fill in each quadrant with the details of what the end state would be if shaped by the two extremes of the drivers. Develop signposts or indicators that show whether events are moving toward one of the hypotheses. Use the signposts or indicators of change to develop intelligence collection strategies or research priorities to determine the direction in which events are moving.

Description Figure 7.4.2 Quadrant Hypothesis Generation: Four Hypotheses on the Future of Iraq

Figure 7.4.2 shows an example of a Quadrant Hypothesis Generation chart. In this case, analysts have been tasked with developing a paper to project possible futures for Iraq, focusing on the potential end state of the government. The analysts have identified and agreed upon the two key drivers in the future of the government: the level of centralization of the federal government and the degree of religious control of that government. They develop their quadrant chart and lay out the four logical hypotheses based on their decisions.

The four hypotheses derived from the quadrant chart can be stated as follows:

The final state of the Iraq government will be a centralized state and a secularized society. The final state of the Iraq government will be a centralized state and a religious society. The final state of the Iraq government will be a decentralized state and a secularized society. The final state of the Iraq government will be a decentralized state and a religious society.

7.4.3 The Method: Multiple Hypotheses Generator ®

The Multiple Hypotheses Generator ® is a technique for developing multiple alternatives for explaining an issue, activity, or behavior. Analysts often can brainstorm a useful set of hypotheses without such a tool, but the Multiple Hypotheses Generator ® may give greater confidence than other techniques that analysts have not overlooked a critical alternative or outlier. Analysts should employ the Multiple Hypotheses Generator ® to ensure that they have considered a broad array of potential hypotheses. In some cases, they may have considerable data and want to ensure that they have generated a set of plausible explanations consistent with all the data at hand. Alternatively, they may have been presented with a hypothesis that seems to explain the phenomenon at hand and been asked to assess its validity. The technique helps analysts rank alternative hypotheses from the most to least credible, focusing on those at the top of the list as those deemed most worthy of attention.

To use this method:

Gather a diverse group to define the issue, activity, or behavior under study. Often, it is useful to ask questions in the following ways:

What variations could be developed to challenge the lead hypothesis that . . .? What are the possible permutations that would flip the assumptions contained in the lead hypothesis that . . .? Identify the Who, What, and Why for the lead hypothesis. Then generate plausible alternatives for each relevant key component. Review the lists of alternatives for each of the key components; strive to keep the alternatives on each list mutually exclusive. Generate a list of all possible permutations, as shown in Figure 7.4.3 .

Figure 7.4.3 Multiple Hypotheses Generator ® : Generating Permutations Source: Globalytica, LLC, 2019. Discard any permutation that simply makes no sense. Evaluate the credibility of the remaining permutations by challenging the key assumptions of each component. Some of these assumptions may be testable themselves. Assign a "credibility score" to each permutation using a 1-to-5-point scale where 1 is low credibility and 5 is high credibility. Re-sort the remaining permutations, listing them from most credible to least credible. Restate the permutations as hypotheses, ensuring that each meets the criteria of a good hypothesis. Select from the top of the list those hypotheses most deserving of attention.

Potential Pitfalls

The value of this technique is limited to the ability of analysts to generate a robust set of alternative explanations. If group dynamics are flawed, the outcomes will be flawed. Whether the correct hypothesis will emerge from this process and analysts identify it as such cannot be guaranteed, but the prospect of the correct hypothesis being included in the set of hypotheses under consideration is greatly increased.

Relationship to Other Techniques

The product of any Foresight analysis process can be thought of as a set of alternative hypotheses. Quadrant Hypothesis Generation is a specific application of the generic method called Morphological Analysis, described in chapter 9 . Alternative Futures Analysis uses a similar quadrant chart approach to define four potential outcomes, and Multiple Scenarios Generation uses the approach to define multiple sets of four outcomes. Both of these techniques are also described in chapter 9 .

Origins of This Technique

The generation and testing of hypotheses is a key element of scientific reasoning. The Simple Hypotheses approach and Quadrant Hypothesis Generation are described in the Handbook of Analytic Tools and Techniques, 5th ed. (Tysons, VA: Pherson Associates, LLC, 2019) and Pherson Associates training materials. The description of the Multiple Hypotheses Generator ® can be found in the fourth edition of the Handbook of Analytic Tools and Techniques (Reston, VA: Pherson Associates, LLC, 2015).

7.5 Diagnostic Reasoning

Diagnostic Reasoning is the application of hypothesis testing to a new development, a single new item of information or intelligence, or the reliability of a source. It differs from Analysis of Competing Hypotheses (ACH) in that it is used to evaluate a single item of relevant information or a single source; ACH deals with an entire range of hypotheses and multiple items of relevant information.

When to Use It

Analysts should use Diagnostic Reasoning if they find themselves making a snap intuitive judgment while assessing the meaning of a new development, the significance of a new report, or the reliability of a stream of reporting from a new source. Often, much of the information used to support one's lead hypothesis turns out to be consistent with alternative hypotheses as well. In such cases, the new information should not—and cannot—be used as evidence to support the prevailing view or lead hypothesis.

The technique also helps reduce the chances of being caught by surprise. It ensures that the analyst or decision maker will have given at least some consideration to alternative explanations. The technique is especially important to use when an analyst—or decision maker—is looking for evidence to confirm an existing mental model or policy position. It helps the analyst assess whether the same information is consistent with other reasonable conclusions or with alternative hypotheses.

Value Added

The value of Diagnostic Reasoning is that it helps analysts balance their natural tendency to interpret new information as consistent with their existing understanding of what is happening—that is, the analyst's mental model. The technique prompts analysts to ask themselves whether this same information is consistent with other reasonable conclusions or alternative hypotheses. It is a common experience to discover that much of the information supporting belief in the most likely conclusion is of limited value because that same information is consistent with alternative conclusions. One needs to evaluate new information in the context of all possible explanations of that information, not just in the context of a well-established mental model.

The Diagnostic Reasoning technique helps the analyst identify the information that is essential to support a hypothesis and avoid the mistake of focusing attention on one vivid scenario or explanation while ignoring other possibilities or alternative hypotheses (Vividness Bias). When evaluating evidence, analysts tend to assimilate new information into what they currently perceive. Diagnostic Reasoning protects them from the traps of seeking only the information that is consistent with the lead hypothesis (Confirmation Bias) and selecting the first answer that appears "good enough" (Satisficing).

Experience can handicap experts because they often hold tightly to timeworn models—and a fresh perspective can be helpful. Diagnostic Reasoning helps analysts avoid the intuitive trap of assuming the same dynamic is in play when something seems to accord with an analyst's past experiences (Projecting Past Experiences). It also helps analysts counter the pitfall of continuing to hold to an analytic judgment when confronted with a mounting list of evidence that contradicts the initial conclusion (Rejecting Evidence) and dismissing information at first glance without considering all possible alternatives (Ignoring Inconsistent Evidence).

The Method

Diagnostic Reasoning is a process that focuses on trying to refute alternative judgments rather than confirming what you already believe to be true. Here are the steps to follow:

When you receive a potentially significant item of information, make a mental note of what it seems to mean (i.e., an explanation of why something happened or what it portends for the future). Make a quick, intuitive judgment based on your current mental model. Define the focal question. For example, Diagnostic Reasoning brainstorming sessions often begin with questions like

Are there alternative explanations for the lead hypothesis (defined as . . .) that would also be consistent with the new information, new development, or new source of reporting? Is there a reason other than the lead hypothesis that . . .? Brainstorm, either alone or in a small group, the alternative judgments that another analyst with a different perspective might reasonably deem to have a chance of being accurate. Make a list of these alternatives. For each alternative, ask the following question: If this alternative were true or accurate, how likely is it that I would have seen, but possibly ignored, information that was consistent with this alternative explanation? Make a tentative judgment based on consideration of these alternatives. If the new information is equally likely with each of the alternatives, the information has no diagnostic value and can be ignored. If the information is clearly inconsistent with one or more alternatives, those alternatives might be ruled out. Following this mode of thinking for each of the alternatives, decide which alternatives need further attention and which can be dropped from consideration or put aside until new information surfaces. Proceed by seeking additional evidence to refute the remaining alternatives rather than to confirm them.

Potential Pitfalls

When new information is received, analysts need to validate that the new information is accurate and not deceptive or intentionally misleading. It is also possible that none of the key information turns out to be diagnostic, or that all relevant information will not come to light.

Relationship to Other Techniques

Diagnostic Reasoning is an integral part of two other techniques: Analysis of Competing Hypotheses and Indicators Validation and Evaluation ( chapter 9 ). It is presented here as a separate technique to show that its use is not limited to those two techniques. It is a fundamental form of critical reasoning that should be widely used in intelligence analysis.

Origins of This Technique

Diagnostic Reasoning has been the principal method for medical problem solving for many years. For information on the role of Diagnostic Reasoning in the medical world, see the following publications: Albert S. Elstein, "Thinking about Diagnostic Thinking: A Thirty-Year Perspective," Advances in Health Science Education, published online by Springer Science+Business Media, August 11, 2009; and Pat Croskerry, "A Universal Model of Diagnostic Reasoning," Academic Medicine 84, no. 8 (August 2009).

7.6 Analysis of Competing Hypotheses

ACH is an analytic process that identifies a complete set of alternative hypotheses, systematically evaluates data that are consistent or inconsistent with each hypothesis, and proceeds by rejecting hypotheses rather than trying to confirm what appears to be the most likely hypothesis. The process of rejecting rather than confirming hypotheses applies to intelligence analysis the scientific principles advocated by Karl Popper, one of the most influential philosophers of science of the twentieth century. 4

ACH starts with the identification of a set of mutually exclusive alternative explanations or outcomes called hypotheses. The analyst assesses the consistency or inconsistency of each item of relevant information with each hypothesis, and then selects the hypothesis that best fits the relevant information. The scientific principle behind this technique is to proceed by trying to refute as many reasonable hypotheses as possible rather than to confirm what initially appears to be the most likely hypothesis. The most likely hypothesis is then the one with the least amount of inconsistent information—not the one with an abundance of supporting relevant information.

When to Use It

ACH is appropriate for almost any analysis where there are alternative explanations for what has happened, is happening, or is likely to happen. Use it when the judgment or decision is so important that you cannot afford to be wrong or when you need a systematic approach to avoid being surprised by an unforeseen outcome. ACH is particularly appropriate when dealing with controversial issues and when analysts need to leave an audit trail to show what relevant information they considered and how they arrived at their analysis. If other analysts and decision makers disagree with the analysts' conclusions, an ACH matrix can help identify the precise area of disagreement. Subsequent discussion can then focus on the most important substantive differences.

ACH is most effective when there is a robust flow of data to absorb and evaluate. It is well suited for addressing questions about technical issues in the chemical, biological, radiological, and nuclear arenas, such as, "For which weapons' system is this part most likely being imported?" or, "Which type of missile system is Country X importing or developing?" The technique is useful for managing criminal investigations and determining which line of analysis is correct. ACH is particularly helpful when an analyst must deal with the potential for denial and deception, as it was initially developed for that purpose.

The technique can be used by a single analyst but is most effective with a small team that can challenge team members' evaluations of the relevant information. It structures and facilitates the exchange of information and ideas with colleagues in other offices or agencies.

An ACH analysis requires a modest commitment of time; it may take a day or more to build the ACH matrix. Once all the relevant information has been collected, it may take several hours to work through all the stages of the analytic process before writing up the conclusions. Usually a facilitator or a colleague previously schooled in the use of the technique helps guide analysts through the process, especially if it is the first time they have used the methodology.

Value Added

Analysts are commonly required to work with incomplete, ambiguous, anomalous, and sometimes deceptive data. In addition, strict time constraints and the need to "make a call" often conspire with natural human cognitive biases to cause inaccurate or incomplete judgments. If the analyst is already generally knowledgeable on the topic, a common procedure is to develop a favored hypothesis and then search for relevant information to confirm it. This is called Satisficing or going with the first answer that seems to be supported by the evidence.

Satisficing is efficient because it saves time and often works. However, Confirmation Bias, which impels an analyst to look only for information that is consistent with the favored or lead hypothesis or widely accepted school of thought, is often at work in the background, as the analyst has made no investment in protection against surprise. Satisficing allows analysts to accept data as true without assessing its credibility or questioning fundamental assumptions because it helps create a more coherent story (Evidence Acceptance Bias). If engaged in Satisficing, analysts often bypass the analysis of alternative explanations or outcomes, which should be fundamental to any complete analysis. As a result, Satisficing fails to distinguish that much of the relevant information seemingly supportive of the favored hypothesis is also consistent with one or more alternative hypotheses. It often fails to recognize the importance of what is missing (i.e., what should be observable if a given hypothesis is true but is not there).

ACH improves the analyst's chances of overcoming these challenges by requiring analysts to identify and then try to refute as many reasonable hypotheses as possible using the full range of data, assumptions, and gaps that are pertinent to the problem at hand. The method for analyzing competing hypotheses takes time and attention in the initial stages, but it pays big dividends in the end. When analysts are first exposed to ACH and say they find it useful, it is because the simple focus on identifying alternative hypotheses and how they might be disproved prompts analysts to think seriously about evidence, explanations, or outcomes in ways that had not previously occurred to them.

The ACH process requires the analyst to assemble the collected information and organize it in a useful way, so that it can be readily retrieved for use in the analysis. This is done by creating a matrix with relevant information down the left side and hypotheses across the top. Each item of relevant information is then evaluated as to whether it is consistent or inconsistent with each hypothesis. The results are then used to assess the evidentiary and logical support for and against each hypothesis. This can be done manually, but it is much easier and better to use an Excel spreadsheet or ACH software designed for this purpose. Various ACH software applications can be used to sort and analyze the data by type of source and date of information, as well as by degree of support for or against each hypothesis.

ACH helps analysts produce a better analytic product by

Maintaining a record of the relevant information and tracking how that information relates to each hypothesis. Capturing the analysts' key assumptions when the analyst is coding the data and recording what additional information is needed or what collection requirements are needed. Enabling analysts to present conclusions in a way that is organized and transparent as it documents how conclusions were reached. Providing a foundation for identifying indicators that can then be monitored and validated to determine the direction in which events are heading. Leaving a clear audit trail as to how the analysis was done, the conclusions reached, and how individual analysts may have differed in their assumptions or judgments.

ACH Software

ACH started as a manual method at the CIA in the mid-1980s. The first professionally developed and tested ACH software was created in 2005 by the P alo A lto R esearch C enter (PARC), with federal government funding and technical assistance from Richards J. Heuer Jr. and Randolph Pherson. Randolph Pherson managed its introduction into the U.S. Intelligence Community. The PARC version, though designed for use by an individual analyst, was commonly used by a co-located team of analysts. Members of such groups reported,

The technique helped them gain a better understanding of the differences of opinion with other analysts or between analytic offices. Review of the ACH matrix provided a systematic basis for identification and discussion of differences between participating analysts. Reference to the matrix helped depersonalize the argumentation when there were differences of opinion.

A collaborative version of ACH called Te@mACH ® was developed under the direction of Randolph Pherson for Globalytica, LLC, in 2010. It has most of the functions of the PARC ACH tool but allows analysts in different locations to work on the same problem simultaneously. They can propose hypotheses and enter data on the matrix from multiple locations, but they must agree to work from the same set of hypotheses and the same set of relevant information. The software allows them to chat electronically about one another's assessments and assumptions, to compare their analysis with that of their colleagues, and to learn what the group consensus was for the overall problem solution.

Other government agencies, research centers, and academic institutions have developed versions of ACH. One version called Structured Analysis of Competing Hypotheses, developed for instruction at Mercyhurst College, builds on ACH by requiring deeper analysis at some points.

The use of collaborative ACH tools ensures that all analysts are working from the same database of evidence, arguments, and assumptions, and that each member of the team has had an opportunity to express his or her view on how that information relates to the likelihood of each hypothesis. Such tools can be used both synchronously and asynchronously and include functions such as a survey method to enter data that protects against bias, the ability to record key assumptions and collection requirements, and a filtering function that allows analysts to see how each person rated the relevant information. 5

The Method

To retain five or seven hypotheses in working memory and note how each item of information fits into each hypothesis is beyond the capabilities of most analysts. It takes far greater mental agility than the common practice of seeking evidence to support a single hypothesis already believed to be the most likely answer. The following nine-step process is at the heart of ACH and can be done without software.

Identify all possible hypotheses that should be considered. Hypotheses should be mutually exclusive; that is, if one hypothesis is true, all others must be false. The list of hypotheses should include a deception hypothesis, if that is appropriate. For each hypothesis, develop a brief scenario or "story" that explains how it might be true. Analysts should strive to create as comprehensive list of hypotheses as possible. Make a list of significant relevant information , which means everything that would help analysts evaluate the hypotheses, including evidence, assumptions, and the absence of things one would expect to see if a hypothesis were true. It is important to include assumptions as well as factual evidence, because the matrix is intended to be an accurate reflection of the analyst's thinking about the topic. If the analyst's thinking is driven by assumptions rather than hard facts, this needs to become apparent so that the assumptions can be challenged. A classic example of absence of evidence is the Sherlock Holmes story of the dog barking in the night. The failure of the dog to bark was persuasive evidence that the guilty party was not an outsider but an insider whom the dog knew. Create a matrix and analyze the diagnosticity of the information. Create a matrix with all hypotheses across the top and all items of relevant information down the left side. See Figure 7.6a for an example. Analyze the "diagnosticity" of the evidence and arguments to identify which points are most influential in judging the relative likelihood of the hypotheses. Ask, "Is this input Consistent with the hypothesis, is it Inconsistent with the hypothesis, or is it Not Applicable or not relevant?" This can be done by either filling in each cell of the matrix row-by-row or by randomly selecting cells in the matrix for analysts to rate. If it is Consistent, put a "C" in the appropriate matrix box; if it is Inconsistent, put an "I"; if it is Not Applicable to that hypothesis, put an "NA." If a specific item of evidence, argument, or assumption is particularly compelling, put two "C's" in the box; if it strongly undercuts the hypothesis, put two "I's."

When you are asking if an input is Consistent or Inconsistent with a specific hypothesis, a common response is, "It all depends on . . ." That means the rating for the hypothesis is likely based on an assumption. You should record all such assumptions when filling out the matrix. After completing the matrix, look for any pattern in those assumptions, such as the same assumption being made when ranking multiple items of information. After the relevant information has been sorted for diagnosticity, note how many of the highly diagnostic Inconsistency ratings are based on assumptions. Consider how much confidence you should have in those assumptions and then adjust the confidence in the ACH Inconsistency Scores accordingly.

Review where analysts differ in their assessments and decide if the ratings need to be adjusted (see Figure 7.6b ). Often, differences in how analysts rate an item of information can be traced back to different assumptions about the hypotheses when doing the ratings. Refine the matrix by reconsidering the hypotheses. Does it make sense to combine two hypotheses into one, or to add a new hypothesis that was not considered at the start? If a new hypothesis is added, go back and evaluate all the relevant information for this hypothesis. Additional relevant information can be added at any time.

Figure 7.6A Creating an ACH Matrix

Description Figure 7.6B Evaluating Levels of Disagreement in ACH Draw tentative conclusions about the relative likelihood of each hypothesis, basing your conclusions on an analysis regarding the diagnosticity of each item of relevant information. Proceed by trying to refute hypotheses rather than confirm them. Add up the number of Inconsistency ratings for each hypothesis and note the Inconsistency Score for each hypothesis. As a first cut, examine the total number of "I" and "II" ratings for each hypothesis. The hypothesis with the most Inconsistent ratings is the least likely to be true and the hypothesis or hypotheses with the lowest Inconsistency Score(s) is tentatively the most likely hypothesis. The Inconsistency Scores are broad generalizations, not precise calculations. ACH is a tool designed to help the analyst make a judgment, but not to actually make the judgment for the analyst. This process is likely to produce correct estimates more frequently than less systematic or rigorous approaches, but the scoring system does not eliminate the need for analysts to use their own good judgment. The "Potential Pitfalls" section below identifies several occasions when analysts need to override the Inconsistency Scores. Analyze the sensitivity of your tentative conclusion to see how dependent it is on a few critical items of information. For example, look for evidence that has a "C" for the lead hypothesis but an "I" for all other hypotheses. Evaluate the importance and credibility of those reports, arguments, or assumptions that garnered a "C." Consider the consequences for the analysis if that item of relevant information were wrong or misleading or subject to a different interpretation. If all the evidence earns a "C" for each hypothesis, then the evidence is not diagnostic. If a different interpretation of any of the data would cause a change in the overall conclusion, go back and double-check the accuracy of your interpretation. Report the conclusions. Consider the relative likelihood of all the hypotheses, not just the most likely one. State which items of relevant information were the most diagnostic, and how compelling a case they make in identifying the most likely hypothesis. Identify indicators or milestones for future observation. Generate two lists: one focusing on future events or what additional research might uncover that would substantiate the analytic judgment, and a second that would suggest the analytic judgment is less likely to be correct or that the situation has changed. Validate the indicators and monitor both lists on a regular basis, remaining alert to whether new information strengthens or weakens your case.

Potential Pitfalls

A word of caution: ACH only works when all participating analysts approach an issue with a relatively open mind. An analyst already committed to a "right answer" will often find a way to interpret relevant information to align with or make consistent with the preexisting belief. In other words, as an antidote to Confirmation Bias, ACH is like a flu shot. Getting the flu shot will usually keep you from getting the flu, but it won't make you well if you already have the flu.

The Inconsistency Scores generated for each hypothesis are not the product of a magic formula that tells you which hypothesis to believe. The ACH software takes you through a systematic analytic process, and the Inconsistency Score calculation that emerges is only as accurate as your selection and evaluation of the relevant information.

Because it is more difficult to refute hypotheses than to find information that confirms a favored hypothesis, the generation and testing of alternative hypotheses will often increase rather than reduce the analyst's level of uncertainty. Such uncertainty is frustrating, but it is usually an accurate reflection of the true situation. The ACH procedure has the offsetting advantage of focusing attention on the few items of critical information that cause the uncertainty or, if they were available, would alleviate it. ACH can guide future collection, research, and analysis to resolve the uncertainty and produce a more accurate judgment.

Analysts should be aware of five circumstances that can cause a divergence between an analyst's own beliefs and the Inconsistency Scores. In the first two circumstances described in the following list, the Inconsistency Scores seem to be wrong when they are correct. In the next three circumstances, the Inconsistency Scores may seem correct when they are wrong. Analysts need to recognize these circumstances, understand the problem, and adjust accordingly.

Assumptions or logical deductions omitted. If the scores in the matrix do not support what you believe is the most likely hypothesis, the matrix may be incomplete. Your thinking may be influenced by assumptions or logical deductions not included in the list of relevant information or arguments. If so, they should be added so the matrix fully reflects everything that influences your judgment on this issue. It is important for all analysts to recognize the role that unstated or unquestioned (and sometimes unrecognized) assumptions play in their analysis. In political or military analysis, for example, conclusions may be driven by assumptions about another country's capabilities or intentions. A principal goal of the ACH process is to identify those factors that drive the analyst's thinking on an issue so that these factors can be questioned and, if appropriate, changed. Insufficient attention to less likely hypotheses. If you think the scoring gives undue credibility to one or more of the less likely hypotheses, it may be because you have not assembled the relevant information needed to refute them. You may have devoted insufficient attention to obtaining such relevant information, or the relevant information may simply not be there. If you cannot find evidence to refute a hypothesis, it may be necessary to adjust your thinking and recognize that the uncertainty is greater than you had originally thought. Definitive relevant information. There are occasions when intelligence collectors obtain information from a trusted and well-placed inside source. The ACH analysis can label the information as having high credibility, but this is probably not enough to reflect the conclusiveness of such relevant information and the impact it should have on an analyst's thinking. In other words, in some circumstances, one or two highly authoritative reports from a trusted source in a position to know may support one hypothesis so strongly that they refute all other hypotheses regardless of what other less reliable or less definitive relevant information may show. Unbalanced set of evidence. Evidence and arguments must be representative of the entire problem. If there is considerable evidence on a related but peripheral issue and comparatively few items of evidence on the core issue, the Inconsistency Score may be misleading. Diminishing returns. As evidence accumulates, each new item of Inconsistent relevant information or argument has less impact on the Inconsistency Scores than does the earlier relevant information. For example, the impact of any single item is less when there are fifty items than when there are only ten items. To understand this, consider what happens when you calculate the average of fifty numbers. Each number has equal weight; adding a fifty-first number will have less impact on the average than if you start with only ten numbers and add one more. Stated differently, the accumulation of relevant information over time slows down the rate at which the Inconsistency Score changes in response to new relevant information. Therefore, the numbers may not reflect the actual amount of change in the situation you are analyzing. When you are evaluating change over time, it is desirable to delete the older relevant information periodically, or to partition the relevant information and analyze the older and newer relevant information separately.

Some other caveats when using ACH include the following:

The possibility that none of the relevant information identified is diagnostic. Not all relevant information is identified. Some of the relevant information is inaccurate, deceptive, or misleading. The ratings are subjective and therefore subject to human error. When the analysis is performed by a group, the outcome can be biased by Groupthink or the absence of healthy group dynamics.

Relationship to Other Techniques

ACH is often used in conjunction with other techniques. For example, Cluster Brainstorming, Nominal Group Technique, Multiple Hypothesis Generation, or the Delphi Method can identify hypotheses or relevant information for inclusion in the ACH analysis. They can also help analysts evaluate the significance of relevant information. Deception Detection may identify an opponent's motive, opportunity, or means to conduct deception or to identify past deception practices; information about these factors should be included in the list of ACH-relevant information. The Diagnostic Reasoning technique is incorporated within the ACH method. The final step in the ACH method identifies Indicators for monitoring future developments.

The ACH matrix is intended to reflect all relevant information and arguments that affect one's thinking about a designated set of hypotheses. That means it should also include assumptions identified by a Key Assumptions Check, discussed earlier in this chapter. Conversely, rating the consistency of an item of relevant information with a specific hypothesis is often based on an assumption. When rating the consistency of relevant information in an ACH matrix, the analyst should ask, "If this hypothesis is true, would I see this item of relevant information?" A common thought in response to this question is, "It all depends on. . . ." This means that, however the consistency of that item of relevant information is rated, that rating is likely based on an assumption—whatever assumption the rating "depends on." These assumptions should be recorded in the matrix and then considered in the context of a Key Assumptions Check.

The Delphi Method ( chapter 8 ) can double-check the conclusions of an ACH analysis. In this process, outside experts are asked separately to assess the probability of the same set of hypotheses and to explain the rationale for their conclusions. If the two different groups of analysts using different methods arrive at the same conclusion, confidence in the conclusion increases. If they disagree, their lack of agreement is also useful, as one can then seek to understand the rationale for the different judgments.

ACH and Argument Mapping (described later in this chapter) are both used on the same types of complex analytic problems. They are both systematic methods for organizing relevant information, but they work in fundamentally different ways and are best used at different stages in the analytic process. ACH is used during an early stage to analyze a range of hypotheses to determine which is most consistent with the broad body of relevant information. At a later stage, when the focus is on developing, evaluating, or presenting the case for a specific conclusion, Argument Mapping is the appropriate method. Each method has strengths and weaknesses, and the optimal solution is to use both.

Origins of This Technique

Richards Heuer originally developed the ACH technique at the CIA in the mid-1980s as one part of a methodology for analyzing the presence or absence of Soviet deception. It was described publicly in his book, Psychology of Intelligence Analysis , first published in 1999; 6 Heuer and Randolph Pherson helped the Palo Alto Research Center gain funding from the federal government during 2004 and 2005 to produce the first professionally developed ACH software. Randolph Pherson managed its introduction into the U.S. Intelligence Community. Globalytica, LLC, with Pherson's assistance, subsequently developed a collaborative version of the software called Te@mACH ® . An example of an Analysis of Competing Hypotheses can be found at https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art11.html .

7.7 Inconsistencies Finder™

The Inconsistencies Finder™ is a simpler version of Analysis of Competing Hypotheses that focuses attention on relevant information that is inconsistent with a hypothesis, helping to disconfirm its validity.

When to Use It

The Inconsistencies Finder™ can be used whenever a set of alternative hypotheses exists, or has recently been identified, and analysts need to do the following:

Carefully weigh the credibility of multiple explanations, or alternative hypotheses, explaining what has happened, is happening, or is likely to happen. Evaluate the validity of a large amount of data as it relates to each hypothesis. Challenge their current interpretation of the evidence (or, alternatively, the interpretation of others). Create an audit trail.

Value Added

The process of systematically reviewing the relevant information and identifying which information or evidence is inconsistent with each hypothesis helps analysts do the following:

Identify the most diagnostic information. Focus on the disconfirming evidence. Dismiss those hypotheses with compelling inconsistent information. Flag areas of agreement and disagreement. Highlight the potential for disinformation or deception.

Instead of building a case to justify a preferred solution or answer, the Inconsistencies Finder™ helps analysts easily dismiss those hypotheses with compelling inconsistent information and focus attention on those with the least disconfirming information. An analytic case can then be built that supports this most likely hypothesis—or hypotheses.

The technique is not an answer generator. It should be viewed as a thinking tool that helps you frame a problem more efficiently. Unlike ACH, the technique does not help analysts identify the most diagnostic information for making their case.

The Inconsistencies Finder™ aids the production of high-quality analysis in much the same way ACH mitigates cognitive biases and intuitive traps by helping analysts do the following:

Avoid leaping to conclusions. Move beyond "first impressions." Challenge preconceived ideas. Uncover unknowns and uncertainties.

The Method

Create a matrix with all the hypotheses under consideration listed in separate columns along the top of the matrix. Make a list of all the relevant information (including significant evidence, arguments, assumptions, and the absence of things) that would be helpful in evaluating the given set of hypotheses. Put each piece of information in a separate row down the left side of the matrix. Working in small teams, analyze each item for consistency/inconsistency against the given hypotheses. Review each piece of information against each hypothesis. Analysts can move across the matrix row by row to evaluate each hypothesis against all the relevant information moving from column to column.

Place an "I" in the box that rates each item against each hypothesis if you would not expect to see that item of information if the hypothesis were true. Place a "II" in the box if the presence of the information makes a compelling case that the hypothesis cannot be true. For example, if a suspect had an unassailable alibi proving he or she was at a different location at the time a crime was committed, then he or she could not be the perpetrator. Add up all the "I's" (Inconsistent ratings) in each hypothesis column. Assign one point to each "I" and two points to each "II." Rank order the credibility of the hypotheses based on the total number of points or "I's" that each hypothesis receives. The higher the score, the less likely the hypothesis. Assess if the "I's" noted in each column make a compelling case to dismiss that hypothesis. Work your way through the "I's" beginning with the hypothesis with the most "I's" to the hypothesis with the fewest or no "I's." Identify the hypothesis(es) with the least Inconsistent information and make a case for that hypothesis(es) being true.

7.8 Deception Detection

Deception is an action intended by an adversary to influence the perceptions, decisions, or actions of another to the advantage of the deceiver. Deception Detection uses a set of checklists to help analysts determine when to look for deception, discover whether deception actually is present, and figure out what to do to avoid being deceived. As Richards J. Heuer Jr. has argued, "The accurate perception of deception in counterintelligence analysis is extraordinarily difficult. If deception is done well, the analyst should not expect to see any evidence of it. If, on the other hand, deception is expected, the analyst often will find evidence of deception even when it is not there." 7

When to Use It

Analysts should be concerned about the possibility of deception when the following occurs:

The analysis hinges on a single critical piece of information or reporting. Key information is received at a critical time—that is, when either the recipient or the potential deceiver has a great deal to gain or to lose. Accepting new information would cause the recipient to expend or divert significant resources. Accepting new information would require the analyst to alter a key assumption or key judgment. The potential deceiver may have a feedback channel that illuminates whether and how the deception information is being processed and to what effect. Information is received from a source whose bona fides are questionable. The potential deceiver has a history of conducting deception.

Value Added

Most intelligence analysts know not to assume that everything that arrives in their inbox is valid, but few know how to factor such concerns effectively into their daily work practices. Considering the deception hypothesis puts a major cognitive burden on the analyst. If an analyst accepts the possibility that some of the information received may be deceptive, then all evidence is open to question and no valid inferences can be drawn from the reporting. This fundamental dilemma can paralyze analysis unless the analyst uses practical tools to determine when it is appropriate to worry about deception, how best to detect deception in the reporting, and what to do in the future to guard against being deceived.

It is very hard to deal with deception when you are really just trying to get a sense of what is going on, and there is so much noise in the system, so much overload, and so much ambiguity. When you layer deception schemes on top of that, it erodes your ability to act.

—Robert Jervis, "Signaling and Perception in the Information Age," in The Information Revolution and National Security (August 2000)

The measure of a good deception operation is how well it exploits the cognitive biases of its target audience. The deceiver's strategy usually is to provide some intelligence or information of value to the person being deceived in the hope that he or she will conclude the "take" is good enough and should be disseminated. As additional information is collected, the Satisficing bias is reinforced and the recipient's confidence in the information or the source usually grows, further blinding the recipient to the possibility that he or she is falling prey to deception. The deceiver knows that the information being provided is highly valued, although over time some people will begin to question the bona fides of the source. Often, this puts the person who developed the source or acquired the information on the defensive, and the natural reaction is to reject any and all criticism. This cycle is usually broken only by applying structured techniques such as Deception Detection to force a critical examination of the true quality of the information and the potential for deception.

Deception Detection is a useful tool analysts can employ to avoid cognitive biases and heuristics, such as seeking only the information that is consistent with the lead hypothesis (Confirmation Bias), accepting data as true without assessing its credibility because it helps "make the case" (Evidence Acceptance Bias), and judging the frequency of an event by the ease with which instances come to mind (Availability Heuristic). It also safeguards an analyst against several classic mental mistakes, including giving too much weight to first impressions or initial data that appears important at the time (Relying on First Impressions), assuming the same dynamic is in play when something appears to be in accord with past experiences (Projecting Past Experiences), and accepting or rejecting everything someone says because the analyst strongly likes or dislikes the person (Judging by Emotion).

The Method

Analysts should routinely consider the possibility that opponents or competitors are attempting to mislead them or hide important information. The possibility of deception cannot be rejected simply because there is no evidence of it; if the deception is well done, one should not expect to see evidence of it. Some circumstances in which deception is most likely to occur are listed in the "When to Use It" section. When such circumstances occur, the analyst, or preferably a small group of analysts, should assess the situation using four checklists that are commonly referred to by their acronyms: MOM, POP, MOSES, and EVE (see box on pp. 173–174).

Analysts have also found the following "rules of the road" helpful in anticipating the possibility of deception and dealing with it: 8

Avoid overreliance on a single source of information. Seek and heed the opinions of those closest to the reporting. Be suspicious of human sources or human subsources who have not been seen or when it is unclear how or from whom they obtained the information. Do not rely exclusively on what someone says (verbal intelligence); always look for material evidence (documents, pictures, an address, a phone number, or some other form of concrete, verifiable information). Be suspicious of information that plays strongly to your own known biases and preferences. Look for a pattern of a source's reporting that initially appears to be correct but later and repeatedly turns out to be wrong, with the source invariably offering seemingly plausible, albeit weak, explanations to justify or substantiate the reporting. At the onset of a project, generate and evaluate a full set of plausible hypotheses, including a deception hypothesis, if appropriate. Know the limitations as well as the capabilities of the potential deceiver.

Relationship to Other Techniques

Analysts can combine Deception Detection with Analysis of Competing Hypotheses to assess the possibility of deception. The analyst explicitly includes deception as one of the hypotheses to be analyzed, and information identified through the MOM, POP, MOSES, and EVE checklists is included as evidence in the ACH analysis.

Origins of This Technique

Deception—and efforts to detect it—has always been an integral part of international relations. An excellent book on this subject is Michael Bennett and Edward Waltz, Counterdeception Principles and Applications for National Security (Boston: Artech House, 2007). The description of Deception Detection in this book was previously published in Randolph H. Pherson, Handbook of Analytic Tools and Techniques, 5th ed. (Tysons, VA: Pherson Associates, LLC, 2019). A concrete example of Deception Detection at work can be found at https://www.apa.org/monitor/2016/03/deception .

Deception Detection Checklists

Motive, Opportunity, and Means (MOM)

Motive: What are the goals and motives of the potential deceiver? Channels: What means are available to the potential deceiver to feed information to us? Risks: What consequences would the adversary suffer if such a deception were revealed? Costs: Would the potential deceiver need to sacrifice sensitive information to establish the credibility of the deception channel? Feedback: Does the potential deceiver have a feedback mechanism to monitor the impact of the deception operation?

Past Opposition Practices (POP)

Does the adversary have a history of engaging in deception? Does the current circumstance fit the pattern of past deceptions? If not, are there other historical precedents? If not, are there changed circumstances that would explain using this form of deception at this time?

Manipulability of Sources (MOSES)

Is the source vulnerable to control or manipulation by the potential deceiver? What is the basis for judging the source to be reliable? Does the source have direct access or only indirect access to the information? How good is the source's track record of reporting?

Evaluation of Evidence (EVE)

How accurate is the source's reporting? Has the whole chain of evidence, including translations, been checked? Does the critical evidence check out? Remember, the subsource can be more critical than the source. Does evidence from one source of reporting (e.g., human intelligence) conflict with that coming from another source (e.g., signals intelligence or open-source reporting)? Do other sources of information provide corroborating evidence? Is the absence of evidence one would expect to see noteworthy?

7.9 Argument Mapping

Argument Mapping is a technique that tests a single hypothesis through logical reasoning. An Argument Map starts with a single hypothesis or tentative analytic judgment and then graphically separates the claims and evidence to help break down complex issues and communicate the reasoning behind a conclusion. It is a type of tree diagram that starts with the conclusion or lead hypothesis, and then branches out to reasons, evidence, and finally assumptions. The process of creating the Argument Map helps identify key assumptions and gaps in logic.

An Argument Map makes it easier for both the analysts and the recipients of the analysis to clarify and organize their thoughts and evaluate the soundness of any conclusion. It shows the logical relationships between various thoughts in a systematic way and allows one to assess quickly in a visual way the strength of the overall argument. The technique also helps the analysts and recipients of the report to focus on key issues and arguments rather than focusing too much attention on minor points.

When to Use It

When making an intuitive judgment, use Argument Mapping to test your own reasoning. Creating a visual map of your reasoning and the evidence that supports this reasoning helps you better understand the strengths, weaknesses, and gaps in your argument. It is best to use this technique before you write your product to ensure the quality of the argument and refine it if necessary.

Argument Mapping and Analysis of Competing Hypotheses (ACH) are complementary techniques that work well either separately or together. Argument Mapping is a detailed presentation of the argument for a single hypothesis; ACH is a more general analysis of multiple hypotheses. The ideal is to use both, as follows:

Before you generate an Argument Map, using ACH can be helpful way to take a closer look at the viability of alternative hypotheses. After looking at alternative hypotheses, you can then select the best one to map. After you have identified a favored hypothesis through ACH analysis, Argument Mapping helps check and present the rationale for this hypothesis.

Value Added

An Argument Map organizes one's thinking by showing the logical relationships between the various thoughts, both pro and con. An Argument Map also helps the analyst recognize assumptions and identify gaps in the available knowledge. The visualization of these relationships makes it easier to think about a complex issue and serves as a guide for clearly presenting to others the rationale for the conclusions. Having this rationale available in a visual form helps both the analyst and recipients of the report focus on the key points rather than meandering aimlessly or going off on irrelevant tangents.

When used collaboratively, Argument Mapping helps ensure that a variety of views are expressed and considered, helping mitigate the influence of Groupthink. The visual representation of an argument also makes it easier to recognize weaknesses in opposing arguments. It pinpoints the location of any disagreement, serves as an objective basis for mediating a disagreement, and mitigates against seeking quick and easy answers to difficult problems (Mental Shotgun).

An Argument Map is an ideal tool for dealing with issues of cause and effect—and for avoiding the trap that correlation implies causation (Confusing Causality and Correlation). By laying out all the arguments for and against a lead hypothesis—and all the supporting evidence and logic—it is easy to evaluate the soundness of the overall argument.

The process also helps analysts counter the intuitive traps of Ignoring Base Rate Probabilities by encouraging the analyst to seek out and record all the relevant facts that support each supposition. Similarly, the focus on seeking out and recording all data that support or rebut the key points of the argument makes it difficult for the analyst to overdraw conclusions from a small sample of data (Overinterpreting Small Samples) or to continue to hold to an analytic judgment when confronted with a mounting list of evidence that contradicts the initial conclusion (Rejecting Evidence).

The Method

An Argument Map starts with a hypothesis—a single-sentence statement, judgment, or claim about which the analyst can, in subsequent statements, present general arguments and detailed evidence, both pro and con. Boxes with arguments are arrayed hierarchically below this statement; these boxes are connected with arrows. The arrows signify that a statement in one box is a reason to believe, or not to believe, the statement in the box to which the arrow is pointing. Different types of boxes serve different functions in the reasoning process, and boxes use some combination of color-coding, icons, shapes, and labels so that one can quickly distinguish arguments supporting a hypothesis from arguments opposing it. Figure 7.9 is a simple example of Argument Mapping, showing some of the arguments bearing on the assessment that North Korea has nuclear weapons.

These are the specific steps involved in constructing a generic Argument Map:

Write down the lead hypothesis—a single-sentence statement, judgment, or claim at the top of the argument tree. Draw a set of boxes below this initial box and list the key reasons why the statement is true along with the key objections to the statement. Use green lines to link the reasons to the primary claim or other conclusions they support. Use green lines to connect evidence that supports the key reason. ( Hint: State the reason and then ask yourself, "Because?" The answer should be the evidence you are seeking.) Identify any counterevidence that is inconsistent with the reason. Use red lines to link the counterevidence to the reasons they contradict. Identify any objections or challenges to the primary claim or key conclusions. Use red lines to connect the objections to the primary claim or key conclusions. Identify any counterevidence that supports the objections or challenges. Use red lines to link the counterevidence to the objections or challenges it supports. Specify rebuttals, if any, with orange lines. An objection, challenge, or counterevidence that does not have an orange-line rebuttal suggests a flaw in the argument. Evaluate the argument for clarity and completeness, ensuring that red-lined opposing claims and evidence have orange-line rebuttals. If all the reasons can be rebutted, then the argument is without merit.

Potential Pitfalls

Argument Mapping is a challenging skill. Training and practice are required to use the technique properly and to gain its benefits. Detailed instructions for effective use of this technique are available at the website listed below under "Origins of This Technique." Assistance by someone experienced in using the technique is necessary for first-time users. Commercial software and freeware are available for various types of Argument Mapping. In the absence of software, using a self-stick note to represent each box in an Argument Map drawn on a whiteboard can be helpful, as it is easy to move the self-stick notes around as the map evolves and changes.

Origins of This Technique

The use of Argument Mapping goes back to the early nineteenth century. In the early twentieth century, John Henry Wigmore pioneered its use for legal argumentation. The availability of computers to create and modify Argument Maps in the later twentieth century prompted broader interest in Argument Mapping in Australia for use in a variety of analytic domains. The short description here is based on material in the Austhink website: http://www.austhink.com/critical/pages/argument_mapping.html .

Description Figure 7.9 Argument Mapping: Does North Korea Have Nuclear Weapons? Source: Diagram produced using the bCisive Argument Mapping software from Austhink, www.austhink.com .

Notes

1. See the discussion in chapter 2 contrasting the characteristics of System 1, or intuitive thinking, with System 2, or analytic thinking.

2. Karl Popper, The Logic of Science (New York: Basic Books, 1959).

3. Stuart K. Card, "The Science of Analytical Reasoning," in Illuminating the Path: The Research and Development Agenda for Visual Analytics , eds. James J. Thomas and Kristin A. Cook (Richland, WA: National Visualization and Analytics Center, Pacific Northwest National Laboratory, 2005), https://pdfs.semanticscholar.org/e6d0/612d677199464af131c16ab0fa657d6954f2.pdf

4. See Popper, The Logic of Science .

5. A more detailed description of Te@mACH ® can be found on the Software tab at http://www.globalytica.com . The software is in the process of being rehosted in 2019.

6. Richards J. Heuer Jr., Psychology of Intelligence Analysis (Washington, DC: CIA Center for the Study of Intelligence, 1999; reprinted by Pherson Associates, LLC, Reston, VA, 2007).

7. Richards J. Heuer Jr., "Cognitive Factors in Deception and Counterdeception," in Strategic Military Deception , eds. Donald C. Daniel and Katherine L. Herbig (New York: Pergamon Press, 1982).

8. Heuer, "Cognitive Factors in Deception and Counterdeception"; Michael I. Handel, "Strategic and Operational Deception in Historical Perspective," in Strategic and Operational Deception in the Second World War , ed. Michael I. Handel (London: Frank Cass, 1987).

Descriptions of Images and Figures

Back to Figure

Data from the timeline are as follows. February 7: Range instrumentation radar first active. February 12: Airframes observed on ground transport. February 13: TELs observed at suspected launch site. February 16: Transporters observed at launch site. February 24: Telemetry first active. February 28: Military establishes communication links. March 2: Missiles observed on training pads. March 11: Propellant handling activity observed. March 13: Azimuth markers observed on launch pads. March 18: Transporters moved to launch area. March 23: Airframe revealed. April 1: Propellant loading underway. April 3: Navigational closure area announced, and TEL and equipment transloading. April 5: Military assets deployed to support launch. April 6: Missiles launched.

Back to Figure

The cells in each row show the impact of the variable represented by that row on each of the variables listed across the top of the matrix. The cells in each column show the impact of each variable listed down the left side of the matrix on the variable represented by the column.

Variables 2 and 4 in the cross-impact matrix have the greatest effect on the other variables, while variable 6 has the most negative effect.

Variable 1

Variable 2

Variable 3

Variable 4

Variable 5

Variable 6

Variable 1

Nil

Neutral

Positive

Neutral

Strong negative

Neutral

Variable 2

Neutral

Nil

Negative

Strong positive

Positive

Positive

Variable 3

Strong positive

Negative

Nil

Positive

Neutral

Negative

Variable 4

Neutral

Strong positive

Neutral

Nil

Positive

Negative

Variable 5

Strong negative

Positive

Neutral

Positive

Nil

Neutral

Variable 6

Negative

Positive

Strong negative

Negative

Negative

Nil

Back to Figure

Idea is generated by a group, who performs structured brainstorming to produce 1 to 3 alternative hypotheses. A list of possible alternative hypotheses leads to idea evaluation and consolidation, such as formation of affinity groups 1 through 4, and their related hypotheses. The final list consists of hypotheses. Text for the group reads, "Is the group sufficiently diverse?" Text for structured brainstorming reads, "Prompt creativity by using situational logic, historical analogies, and theory." Text for the list of possible hypotheses reads, "Does this initial list take into account all the key forces and factors?" Text for the affinity group reads, "Create groups of similar hypotheses. Ask if the opposite could be true to generate new ideas." Text for the hypotheses list reads, "Are the hypotheses mutually exclusive? Is the list comprehensive? Did you clarify each hypothesis by asking who, what, how, when, where, and why?"

Back to Figure

The illustration shows a quadrant. The top side is labeled centralized; the right side is labeled religious; the bottom side is labeled decentralized; and the left side is labeled secularized. H1: Centralized state and secularized society. H2: Centralized state and religious society. H3: Decentralized state and secularized society. H4: Decentralized state and religious society.

Back to Figure

The matrix lists relevant information and hypothesis, buttons for rating credibility, and a column for listing notes, assumptions, and credibility justification. At the top of the matrix, a color legend shows the level of disagreement. The column consists of a tool for reordering hypotheses from most to least likely. The row consists of a tool for moving the most discriminating information to the top of the matrix. The cells consists of options for access chat and viewing analyst ratings. The number of inconsistencies in the hypotheses is also displayed.

Back to Figure

The contention is that North Korea has nuclear weapons. The objection to this is that North Korea does not have technical capacity to produce enough weapons-grade fissile material. A rebuttal to this is that North Korea was provided key information and technology by Pakistan around 1997. The reasoning to the contention is that North Korea has exploded nuclear weapons in tests. The evidence to this is that North Korea claimed to have exploded test weapons in 2006 and 2009, and that there are seismological evidence of powerful explosions.