凯德·梅茨.(2023.2021).2024013深度学习革命.(桂曙光译).中信出版社 => 0101 感知机：最早的神经网络之一

Cade Metz.(2007).2024013Genius-Makers => Preface

## 1701. Facebook 的无能

在俄罗斯，有些人的工作就是试图利用我们的系统。这是一场军备竞赛，对吗？

马克·扎克伯格每天都穿同样的服装：一件鸽子灰色的棉质 T 恤，搭配一条蓝色牛仔裤。他觉得这给了他更多的精力来管理 Facebook。他喜欢称之为「一个社区」，而不是一家公司或一个社交网络。「我真的想清理我的生活，让我可以尽可能少地做决定，除了决定如何最好地为这个社区服务，」他曾经这么说过，「实际上，有一堆心理学理论认为，即使围绕你穿什么、早餐吃什么或诸如此类的事情做出一些小决定，也会让你感到疲惫，消耗你的精力。」1 但当他 2018 年 4 月在美国国会作证时，他穿着一套深蓝西装，系着「Facebook 蓝」的领带。2 有些人称这身装扮为他的「很抱歉」套装，3 有些人说他的头发很奇怪地高高耸在额头上，让他看起来像一个忏悔的和尚。4

2018 年 3 月，美国和英国的报纸报道称，英国初创公司剑桥分析公司（Cambridge Analytica）曾经获取了 5 000 多万人的 Facebook 的私人资料，然后在 2016 年美国大选前，特朗普的竞选团队利用这些数据锁定了选民。5 这条披露的消息引发了媒体、公众支持者和立法者铺天盖地的指责，这些指责在过去几个月里已经开始针对扎克伯格和 Facebook 了。被传唤到国会山后，扎克伯格在两天内忍受了 10 个小时的作证。6 他回答了近 100 名议员提出的 600 多个新老问题，包括剑桥分析公司的数据泄露、俄罗斯干涉选举、假新闻以及经常在 Facebook 上传播的仇恨言论，这些言论在缅甸和斯里兰卡等地煽动了暴力。7 扎克伯格一再道歉，尽管他看起来似乎并不总是表现出了歉意。无论在私下还是在公开场合，扎克伯格的行为都有一种近乎机器人的感觉，他会异常频繁地眨眼睛，并不时在喉咙里发出无意识的咔嗒声，似乎像是机器出了某种故障。

第一天，扎克伯格在参议院的作证进行到一半时，来自南达科他州的资深参议员、共和党人约翰·图恩（John Thune）质疑扎克伯格道歉的效果，称这位 Facebook 创始人花了 14 年时间为一个又一个令人震惊的错误公开道歉。8 扎克伯格承认这一点，但他说，Facebook 现在意识到它应该以一种新的方式运作，它不仅需要提供网络信息共享软件，还需要主动监管共享的内容。他说：「我认为，在很多问题上 —— 不仅仅是数据隐私，还有假新闻和外国对选举的干涉 —— 我们现在知道，要发挥更积极的作用，对我们的责任要有更广泛的认识。我们打造工具还不够，还要确保它们可以永远使用。」9 图恩说，他很高兴扎克伯格听取了这条信息，但他想确切知道 Facebook 将如何解决这个极其困难的问题。以仇恨言论为例，这看似简单，实则不然。从语言上讲，它通常很难定义，有时相当微妙，而且在不同的国家会表现出不同的特征。

作为回应，扎克伯格回顾了 Facebook 发展的早期，展示了他和他的员工在作证前几天准备的一些文件。他说，当他 2004 年在宿舍里将 Facebook 上线时，人们可以在社交网络上分享他们想要分享的任何东西。然后，如果有人将共享的内容标记为不合适，公司会查看并决定是否应该将其删除。他承认，在过去的几十年里，这已经成了 Facebook 自己庞大的影子行动，有超过两万名合同工在社交网络上努力审查被标记的内容，这些内容来自 20 多亿人。但是，他说，人工智能正在将一切变得可能。

尽管像伊恩·古德费洛这样的研究人员认为深度学习会加剧假新闻问题，但扎克伯格将其描述为解决方案。他告诉参议员图恩，人工智能系统已经能以近乎完美的准确性识别恐怖主义宣传。他说：「今天，当我们坐在这里时，我们在 Facebook 删除了 99% 的‘伊斯兰国 ' 和基地组织内容，我们的人工智能系统会在任何人看到之前就标记出来。」10 他承认其他类型的有害内容更难识别，包括仇恨言论。但是他相信人工智能可以解决这个问题。他说，在 5～10 年内，人工智能甚至可以识别仇恨言论的细微差别。他没有说的是，即使是人类也无法就什么是仇恨言论、什么不是仇恨言论达成一致。

### 01

在此两年之前，2016 年夏，在 AlphaGo 击败李世石之后，在唐纳德·特朗普击败希拉里·克林顿之前，扎克伯格坐在 20 号楼里的一张会议桌旁，这里是门洛帕克 Facebook 园区的新中心。这栋楼由弗兰克·盖里（Frank Gehry）设计，是一栋又长又平的钢架建筑，占地超过 43 万平方英尺，是一座足球场的 7 倍大。屋顶是中央公园，9 英亩的面积，有草地、树木和砾石人行道，Facebook 的员工们可以随时在这里休息或散步。大楼里面是一个可容纳 2 800 多名员工的大空间，里面摆满了桌椅和笔记本电脑。你如果站在正确的位置，可以从一端看到另一端。

扎克伯格正在举行公司的年中业务回顾。每个部门的领导都会走进他所在的房间，讨论各自在今年上半年的进展情况，然后出去。当天下午，负责该公司人工智能研究的团队跟随首席技术官迈克·斯科洛普夫一起进来，由杨立昆做演示，详细介绍了他们在图像识别、翻译和自然语言理解方面的工作。扎克伯格听的时候没怎么说话，斯科洛普夫也没有。然后，当演示结束时，一行人走出房间，斯科洛普夫告诉杨立昆，他所说的一切都没有任何意义。「我们只需要一些能表明我们比其他公司做得更好的东西，」他说，「我不管你怎么做，我们只需要赢得一场比赛，只要启动一场我们知道可以赢的比赛。」

「视频。我们可以赢得视频。」一名同事替他说。

「看到了吗？你可以学到一些东西！」斯科洛普夫对着杨立昆咆哮道。

扎克伯格希望全世界将 Facebook 视为一位创新者，视为谷歌的竞争对手，这将有助于公司吸引人才。随着反垄断的幽灵在硅谷抬头，公司还需要一个避免被拆分的理由 —— 或者说公司里的很多人是这么认为的。Facebook 的想法是，公司可以向监管者表明，它不仅仅是一个社交网络，不仅仅是人与人之间的一种联系，还是一家开发对人类未来至关重要的新技术的公司。Facebook 人工智能实验室是一个公关机会。这就是为什么斯科洛普夫告诉满屋子的记者，Facebook 正在打造可以解决围棋问题的人工智能，这个项目背后的大想法将会在公司传播。这也是为什么扎克伯格和杨立昆试图在几周后抢占 DeepMind 的围棋里程碑。但是，作为 Facebook 实验室的负责人，杨立昆并不是一个追逐月球拍摄瞬间的人，他不是戴密斯·哈萨比斯或埃隆·马斯克。在该领域工作了几十年后，他认为人工智能研究是一项耗时更长、速度更慢的工作。

事实证明，斯科洛普夫向满屋子记者描述的一些大想法对公司的未来至关重要。但未来不会像他想象的那样光明，这些创意和想法也没有看起来那么伟大，也从未以他预料的方式在整个公司传播。

在为横跨纽约和硅谷的 Facebook 人工智能实验室招募了几十名顶尖研究人员之后，斯科洛普夫设立了第二个组织，负责将实验室的技术付诸实践。这就是所谓的「应用机器学习团队」。起初，这个团队将人脸识别、语言翻译和自动生成图像标题等功能带到了世界上最大的社交网络，但后来，它的使命改变了。2015 年底，在巴黎及其周边地区的协同袭击中，伊斯兰武装分子造成 130 人死亡和 400 多人受伤，马克·扎克伯格发了一封电子邮件，询问该团队可以做些什么来打击 Facebook 上的恐怖主义。在接下来的几个月里，他们分析了数千条涉及违反其政策的恐怖组织的 Facebook 帖子，并提出了打造一个可以自动标记新的恐怖主义宣传的系统。然后，人工承包商会检查系统标识出的内容，并最终决定是否应该予以删除。扎克伯格告诉参议院，Facebook 的人工智能可以自动识别来自「伊斯兰国」和基地组织的内容，他指的正是这项技术。但是，其他人质疑这项技术太过复杂和微妙。

2016 年 11 月，当扎克伯格还在否认 Facebook 在虚假新闻传播中扮演的角色时，迪安·波默洛提出了一项挑战。30 年前，在神经网络的帮助下，他曾在卡内基 —— 梅隆大学制造了一辆自动驾驶汽车。现在，波默洛在 Twitter 上发布了他所谓的「假新闻挑战」，他赌 1 000 美元，认为没有研究人员能打造一个自动系统来区分真假。11「我会给任何人 20∶1 的赔率（每次下注最高 200 美元，总计 1 000 美元），赌他们无法开发出一种能够区分互联网上真实与虚假新闻的自动化算法。」他写道。12 他知道目前的技术无法胜任这项任务，这需要非常微妙的人类判断。任何能够可靠地识别出假新闻的人工智能技术都要越过一个更大的里程碑。「这将意味着人工智能已经达到了人类的水平。」13 他说。他也知道，假新闻是个人的观点，辨别真假是个见仁见智的问题。如果人类不能就什么是假新闻、什么不是假新闻达成一致，他们要如何训练机器识别假新闻？新闻本质上是客观观察和主观判断之间的一种对立关系。波默洛说：「很多情况下，没有正确的答案。」14 最初，出现了一系列的活动来回应他的挑战，但它们都没有任何结果。

在波默洛发出挑战的第二天，由于 Facebook 继续否认自身存在的问题，该公司在门洛帕克的公司总部举行了一场新闻圆桌会议。15 杨立昆也在现场，一名记者问他，人工智能能否检测到假新闻和其他在社交网络上迅速传播的有害内容，包括视频直播中的暴力。两个月前，曼谷一名男子在直播 Facebook 视频时上吊自杀。杨立昆以一个伦理难题作为回应。他说：「过滤和审查之间如何权衡？经验和体面的自由之间如何取舍？这项技术要么已经存在，要么可以被开发出来。但接下来的问题是，部署它有什么意义？这不是我的职责所在。」16

随着来自公司外部的压力越来越大，斯科洛普夫开始转移他的应用机器学习团队内部的资源，努力清理社交网络上的有害内容，从色情图片到虚假账号。到 2017 年中，检测不受欢迎的内容在团队工作中所占的比重超过了其他任何任务。斯科洛普夫称之为「清理第一优先级」。与此同时，该公司继续扩大审查内容的人工承包商的数量。人工智能还不够。

因此，在剑桥分析公司数据泄露后 17 ，当扎克伯格在国会作证时 18 ，他不得不承认 Facebook 的监控系统仍然需要人工的帮助。他们可以标记某些类型的图像和文本，比如一张裸照或一篇恐怖主义宣传文章，但一旦他们这样做了，人工审查员 —— 主要是在海外工作的大型承包商 —— 必须介入审查每一篇帖子，并决定是否应该予以删除。无论人工智能工具在非常特殊的情况下有多么精确，它们仍然缺乏人类判断的灵活性。例如，它们很难区分色情图片和母亲哺乳婴儿的照片。而且它们不是在处理一个静态的情况：Facebook 建立的系统能够识别日益多样化的有害内容，但社交网络上还会出现新的有害内容，而这些系统并没有接受识别新的有害内容的训练。当来自加州的民主党参议员黛安娜·范斯坦（Dianne Feinstein）问扎克伯格，他将如何阻止外国破坏分子干涉美国大选时，他再次指向了人工智能，但他承认情况很复杂。「我们部署了新的人工智能工具，可以更好地识别可能试图干扰选举或传播错误信息的虚假账号。但是，这些攻击的本质是，你知道，有些俄罗斯人的工作就是试图利用我们的系统，」他说，「所以这是一场军备竞赛，对吗？」19

### 02

2019 年 3 月，扎克伯格在国会山作证之后一年，一名枪手在新西兰克赖斯特彻奇的两座清真寺里杀害了 51 个人，他当时在 Facebook 上直播了这次袭击。Facebook 花了一个小时的时间从社交网络上删除了这段视频，但在这一个小时之内，这段视频传遍了整个互联网。几天之后，迈克·斯科洛普夫在 Facebook 总部与两名记者坐下来交流，讨论公司在人工智能的帮助下识别和删除不受欢迎内容的努力。20 在半个小时的时间里，他用彩色记号笔在白板上画了一些图表，展示了公司如何自动识别大麻和摇头丸广告。然后，记者问到了克赖斯特彻奇的枪击事件。他停顿了将近 60 秒，然后哭了起来。「我们正在努力解决这个问题，」他说，「系统不会在明天得到完善，但我不想 6 个月之后再有这样的对话。我们可以做得更好。」

在另外几次采访中，他一次又一次地流泪，讨论 Facebook 工作的规模和难度，以及随之而来的责任。经历过这一切，他仍然坚持认为人工智能是答案；随着时间的推移，人工智能可以缓解公司的困难局面，并最终将徒劳无功的苦差事转变为可以管控的情况。但当被追问时，他承认这个问题永远不会完全消失。他说：「我确实认为这有个结局，但我不认为答案是‘一切都解决了 '，我们都可以收拾行李回家了。」

与此同时，打造这种人工智能要付出非常人性化的努力。当克赖斯特彻奇的视频在 Facebook 上出现时，该公司的系统没有标记它，因为这段视频看起来不像系统接受训练时识别的任何东西，它看起来像一款第一人称视角的电子游戏。通过一些狗攻击人、人踢猫、人用棒球棒攻击别人的图像，Facebook 已经训练了系统，使其对网站上的图像暴力进行识别，但是这个新西兰的暴力视频不一样。「那些图像看起来跟这个视频都不太像。」斯科洛普夫说。跟团队中的其他人一样，这段视频他也看了几遍，他试图理解如何建立一个能够自动识别的系统。「我希望自己没有看过它。」

随着大麻广告在社交网络上出现，斯科洛普夫和他的团队建立了可以识别的系统。然后新的、不同的内容方式出现了，随着循环的继续，他们建立了识别这些新内容的新方法。在这个过程中，研究人员也打造了能够自行产生错误信息的系统。这包括 GAN 和图像生成的相关技术，还包括一项在 DeepMind 开发的被称为 WaveNet 的技术，这项技术可以产生逼真的声音，甚至可以用于复制某个人的声音，比如唐纳德·特朗普的声音。

这演变成了一场人工智能对人工智能的游戏。随着另一场选举的临近，斯科洛普夫发起了一场竞赛，敦促来自产业界和学术界的研究人员打造人工智能系统，以识别「深度造假」，即由其他人工智能系统生成的虚假图像。问题是，哪一方会赢？对伊恩·古德费洛这样的研究人员来说，答案是显而易见的。错误信息会赢。毕竟，GAN 的设计模式，就是打造一个可以欺骗任何探测器的开发器。甚至在比赛开始之前，它就赢了。

在 Facebook 公布竞赛结果几周后，另一名记者再次问杨立昆，人工智能能否阻止虚假新闻的传播。他说：「我不确定是否有人拥有获取新闻真实性的技术。真实性，尤其是在涉及政治问题时，基本上是一种个人观点的事。」他补充说，即使你能打造出一台能合理完成这项工作的机器，很多人也会说打造这台机器的技术专家有偏见，他们会抱怨用来训练它的数据有偏见，他们就是不接受。「即使这项技术存在，」他说，「在实际的部署中，它可能也不是一个好主意。」