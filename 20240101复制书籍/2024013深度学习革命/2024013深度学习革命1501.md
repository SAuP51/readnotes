凯德·梅茨.(2023.2021).2024013深度学习革命.(桂曙光译).中信出版社 => 0101 感知机：最早的神经网络之一

Cade Metz.(2007).2024013Genius-Makers => Preface

## 1501. 神经网络的偏见

谷歌照片，你们搞砸了。我的朋友不是大猩猩。

2015 年 6 月的一个星期天，雅基·阿尔西内（Jacky Alciné）坐在他和弟弟两人同住的房间里，在网上浏览一长串关于黑人娱乐电视奖的 Twitter 消息。他们的公寓位于美国布鲁克林皇冠高地，因为没有接通有线电视，他看不了电视里的颁奖典礼，但至少可以在笔记本电脑上阅读不断涌入的 Twitter 评论。在他吃了一碗米饭后，一位朋友给他发了一个网络链接，上面有他发布到新的「谷歌照片」上的一些快照。22 岁的阿尔西内是一名软件工程师，他过去曾使用过这项服务，但自从谷歌几天前发布新的版本后，他还没有使用过。新的「谷歌照片」可以分析你的快照，并根据每张照片中的内容自动将它们归类到不同的文件夹中。一个文件夹可能是「狗」，一个是「生日聚会」，还有一个是「海滩旅行」。这也是一种浏览图片并快速搜索的方式。如果你输入「墓碑」，那么谷歌可以自动找到所有包含墓碑的照片。当阿尔西内点击链接并进入「谷歌照片」时，他惊讶地发现照片已经被重新分类了 —— 出现了一个「大猩猩」的文件夹。他不知道这是什么意思，于是打开了文件夹，发现里面有 80 多张照片，这些照片是他大约一年前在附近的前景公园的一场音乐会上为一位朋友拍摄的。这位朋友是一名非裔美国人，谷歌给她贴上了「大猩猩」的标签。

如果谷歌只是错误地标记了一张照片，他可能就算了，但这是 80 多张照片。他截了一张图，并发布到 Twitter 上，他认为 Twitter 是「世界上最大的自助餐厅」，是一个任何人都可以出现，并凭任何事情去引起任何人注意的地方。他写道：「谷歌照片，你们搞砸了。我的朋友不是大猩猩。」1 一名谷歌员工几乎立即给他发了一封私信，请求登录他的账号，这样公司就能明白哪里出了问题。在媒体上，谷歌花了几天时间道歉，称其在采取迅速行动，以确保这种情况永远不会再次发生。「谷歌照片」服务中的「大猩猩」标签被完全删除了，这是该公司多年来的惯例。5 年后，「谷歌照片」仍然禁止任何人搜索「大猩猩」这个词。

问题是，谷歌训练了一个神经网络来识别大猩猩，给它输入了成千上万张大猩猩的照片，却没有意识到其副作用。神经网络可以自行学习工程师永远无法编入机器的任务，但在训练这些系统时，工程师有责任选择正确的数据。更重要的是，在训练完成后，即使这些工程师对他们的选择很谨慎，他们也可能无法理解机器所学到的一切，因为训练规模如此之大，涉及如此多的数据和如此多的计算。作为一名软件工程师，雅基·阿尔西内明白问题所在。他把训练系统比作制作千层面，「如果你很早就把材料弄乱了，整件事就毁了，」他说，「人工智能也是如此，你必须非常有意识地对待输入的东西。否则，过程很难撤销。」

### 01

2012 年夏，在「小猫论文」发表后不久，杰夫·辛顿正式成为「谷歌大脑」实验室的实习生（64 岁），在一张团队的照片中，他和杰夫·迪恩举着一张巨大的小猫数字图像。2 大约有十几名研究人员围在他们周围，其中有马特·泽勒，他是一个穿着黑色短袖 Polo 衫和褪色的蓝色牛仔裤的年轻人，笑容满面，头发蓬乱，下巴上有几天未刮的胡茬儿。泽勒在同年夏天到「谷歌大脑」实习之前，曾在纽约大学的深度学习实验室学习。一年后，他跟随辛顿、克里哲夫斯基和萨特斯基弗的脚步，赢得了 ImageNet 竞赛。很多人称赞他是这个行业最热门领域的「摇滚明星」。阿兰·尤斯塔斯打电话给他，向他提供了一份谷歌的高薪工作，但正如泽勒经常告诉记者的那样，他拒绝了尤斯塔斯，转而创办了自己的公司。

这家公司就是 Clarifai，设立在离纽约大学深度学习实验室不远的一间小办公室里。公司开发的技术可以自动识别数字化图像中的物体，可以在电商网站上搜索鞋子、连衣裙和手袋的照片，或者在监控摄像头传来的视频片段中识别人脸。这个想法复制了谷歌和微软等公司过去几年在各自的人工智能实验室中打造的图像识别系统，然后将其出售给警察部门、政府行政部门和其他企业。

2017 年，在公司成立 4 年后，德博拉·拉吉（Deborah Raji）坐在纽约曼哈顿下城区办公室的一张办公桌前。一盏无情的荧光灯笼罩着她、办公桌、角落里的啤酒冰箱，以及所有其他 20 多位戴着耳机、盯着超大电脑屏幕的人。拉吉盯着一个满是人脸的屏幕，那是公司训练人脸识别软件用的一些图像。当她一页又一页地滚动这些人脸图片时，她看到了问题所在。拉吉是一名来自加拿大渥太华的 21 岁黑人女性。大多数图像 —— 超过 80%—— 是白人的。几乎同样引人注目的是，这些白人中超过 70% 是男性。拉吉认为，当该公司根据这些数据训练其系统时，它可能会在识别白人方面做得很好，但在识别有色人种方面会惨败，可能在识别女性时也会如此。

这个问题是普遍的。马特·泽勒和 Clarifai 还在打造一款被称为「内容审核系统」的工具，该工具可以自动识别并删除人们发布到社交网络上的大量图片中的色情内容。该公司在两组数据上训练了这个系统：从色情网站上提取的数千张淫秽照片，以及从照片服务商那里购买的数千张大众级图片。他们的想法是，该系统能够学会区分色情图片和普通图片。问题是，大众级图片是白人为主，色情图片却不是。正如拉吉很快意识到的那样，该系统正在学习将黑人识别为色情。「我们用来训练这些系统的数据很重要，」她说，「我们不能只是盲目地选择来源。」

这个问题的根源可以追溯到几年前，至少是从照片服务商那里选择图片的时候，Clarifai 将这些图片输入其神经网络。同样的问题也困扰着所有的流行媒体：这是同质的。现在的风险是，人工智能研究人员在训练自动化系统时使用这样的数据会放大这个问题。对拉吉来说，这显而易见，但对公司的其他人来说，情况并非如此。选择训练数据的人 —— 马特·泽勒和他为 Clarifai 招聘的工程师 —— 大多是白人。因为他们自己大多是白人，所以他们没有意识到其数据是有偏见的。谷歌的大猩猩标签本应该给这个行业敲响警钟，事实上却没有。

其他有色人种的女性把这个根本问题公之于众。蒂姆尼特·格布鲁（Timnit Gebru）在斯坦福大学学习人工智能，师从李飞飞，她出生在埃塞俄比亚，是一对移民到美国的厄立特里亚夫妇的女儿。在 NIPS 大会上，当她进入主会场观看第一场演讲时，看着坐在观众席上的数百人一排又一排的面孔，她惊讶地发现，他们虽然有些是东亚人，有些是印度人，还有一些是女性，但绝大多数是白人男性。那年有超过 5 500 人参加了会议，她只看到了 6 名黑人，他们都是她认识的男性。这不是一场美国或加拿大的会议，而是在巴塞罗那召开的一次国际大会。德博拉·拉吉在 Clarifai 发现的问题遍及产业界和学术界。

当格布鲁回到帕洛阿尔托时，她告诉丈夫自己所看到的一切，她决定不能忽视这些。在回来的第一个晚上，她盘腿坐在沙发上，拿着笔记本电脑，她把这个难题写在了 Facebook 帖子上：

我不担心机器接管世界。我担心人工智能圈子里的群体思维、狭隘和傲慢，尤其是在当前对该领域人员的大肆炒作和需求的情况下。这些事情已经引发了一些我们现在就应该担忧的问题。

机器学习被用来计算谁应该承担更高的利率，谁更可能犯罪并因此获得更严厉的判决，谁应该被视为恐怖分子等。一些我们认为理所当然的计算机视觉算法仅适用于具有特定外在特征的人。我们不需要推测未来会发生的大规模破坏。人工智能只服务于世界人口的一小部分，创造它的人也来自世界人口的极小部分。某些人口会受到它的主动伤害，不仅因为算法对他们不利，还因为算法的工作是自动化的。这些人被主动排除在高薪领域之外，这使他们从劳动力市场中消失。我听过很多人谈论多样性，好像这是某种慈善事业。我看到一些公司甚至个人都把其用作公关噱头，但仅仅是口头说说而已。因为这是日常用语，所以你应该说「我们重视多样性」。人工智能需要被视为一个系统，创造这项技术的人是这个系统的重要组成部分。如果很多人被主动排除在外，那么这项技术只会让少数人受益，同时损害很多人。

这份迷你宣言传遍了整个圈子。在接下来的几个月里，格布鲁创建了一个名为「人工智能中的黑人」（Black in AI）的新组织。博士毕业后，她被谷歌聘用。第二年，以及此后的每一年，「人工智能中的黑人」都在 NIPS 大会设立了自己的研讨会。那时，NIPS 已经不叫 NIPS 了。在很多研究人员抗议这个名字助长了对女性的敌视之后，会议组织者将名字改成了 NEURips。3

有一位名叫乔伊·布拉姆维尼（Joy Buolamwini）的年轻计算机科学家是格布鲁的学术合作者，她是剑桥麻省理工学院的一名研究生，最近在英国获得了罗兹奖学金。布拉姆维尼来自一个学者家庭，她的祖父和父亲都专攻药物化学。她出生在加拿大艾伯塔省的埃德蒙顿，这是她的父亲完成博士学位的地方。在她的成长过程中，她一直跟随父亲去研究所需的地方，包括非洲和美国南部的实验室。20 世纪 90 年代中期，当她还在上小学的时候，她参观了父亲的实验室，父亲提到自己正在尝试用神经网络来进行新药研发，她不知道这意味着什么。在大学主修了机器人和计算机视觉之后，她被人脸识别吸引，神经网络以一种完全不同的方式重新出现在她的人生中。文献上说，由于深度学习技术，人脸识别正在走向成熟，然而，当她在使用的时候，她发现实际上并没有。这变成了她论文的内容。「这不仅仅涉及人脸分析技术，还包括对人脸分析技术的评估，」她说，「我们如何确定进步？谁来决定进步意味着什么？我看到的主要问题是，我们用来决定进步情况的标准和衡量方式可能具有误导性，而且其误导性是因为抽样不足，大多数人严重缺乏代表性。」

那年 10 月，一位朋友邀请她和其他几位女士去波士顿过夜，这位朋友说：「我们要做面膜。」她的朋友指的是去当地一家水疗中心做护肤面膜，但布拉姆维尼以为是万圣节面具（mask）。所以那天早上，她带着一个白色的塑料万圣节面具去了办公室。她忙着完成某门功课的一个项目，几天之后面具仍然在她的桌子上放着。她在尝试让人脸检测系统跟踪她的脸，但无论做什么，她都无法让系统正常工作。在沮丧中，她从桌子上拿起白色面具并戴在头上。系统立刻识别出了她的脸 —— 至少是识别出了面具。「黑皮肤，白面具，」她说，这与 1952 年精神病学家弗朗茨·法农（Frantz Fanon）对历史种族主义的批判类似，「这个比喻变成了事实。你必须符合一个标准，而这个标准不是你。」

很快，布拉姆维尼开始研究一些分析人脸、识别年龄和性别等特征的商业化服务，包括微软和 IBM 的工具。随着谷歌和 Facebook 将人脸识别技术引入智能手机应用程序，微软和 IBM 加入了 Clarifai 的队伍，为企业和政府机构提供类似的服务。布拉姆维尼发现，当这些服务读取肤色较浅的男性照片时，性别识别错误率只有 1%。4 但是，照片中的人皮肤越黑，识别的出错率就越高，5 对于黑皮肤的女性图片，出错率特别高。微软的出错率约为 21% 6 ，IBM 是 35% 7 。

她的研究成果发表于 2018 年冬天，并很快引发了大家对人脸识别技术，尤其是在执法中使用该技术的更大反弹。其中的风险在于，该技术会错误地将某些群体识别为潜在的罪犯。一些研究人员认为，如果没有政府的监管，这项技术就无法得到适当的控制。很快，大公司别无选择，只能承认这种舆论风潮。在麻省理工学院的研究发表之后，微软首席法务官表示，由于担心可能会不合理地侵犯人们的权利，该公司不再向执法机构出售该技术，他还公开呼吁政府进行监管。那年 2 月，微软在华盛顿州支持了一项法案，该法案要求，使用人脸识别技术的公共场所需要张贴通告，并要求政府机构在寻找特定人员时获得法院命令。显而易见的是，微软并没有支持提供更强有力保护的其他立法，但态度至少开始转变。

德博拉·拉吉还在 Clarifai 的时候，就注意到布拉姆维尼在种族和性别偏见方面的工作，于是她联系了布拉姆维尼，她们开始合作，拉吉最终进入了麻省理工学院。她们合作的项目甚至包括测试美国第三大科技巨头亚马逊的人脸识别技术。亚马逊已经超越了其网络零售商的根基，成为云计算领域的主导者和深度学习领域的主要参与者。2019 年，该公司开始以「亚马逊识别」（Amazon Rekognition）的名义向警察局和政府机构推销其人脸识别技术，其早期客户包括佛罗里达州的奥兰多警察局和俄勒冈州的华盛顿县警长办公室。8 然后，布拉姆维尼和拉吉发表了一项新的研究成果，表明亚马逊的人脸识别服务也很难识别女性和深色皮肤人脸的性别。9 根据这项研究，亚马逊人脸识别技术将女性误认为男性的概率为 19%，将深色皮肤的女性误认为男性的概率是 31%。10 对浅肤色的男性来说，错误率为零。

但亚马逊的回应与微软和 IBM 不同。亚马逊也呼吁政府对人脸识别进行监管，但它没有接洽拉吉和布拉姆维尼，而是通过私人电子邮件和公共博客帖子攻击她们。亚马逊高管马特·伍德（Matt Wood）在一篇博客中写道：「面对新技术的焦虑，答案不是去运行一种与服务所设计的使用方式不一致的‘测试 '，并通过新闻媒体放大测试的错误和误导性结论。」11 他对该研究以及《纽约时报》描述该研究的文章提出了质疑。这种做法源于驱动亚马逊发展的根深蒂固的企业文化。这家公司坚持认为，外界的声音不会扰乱它自己的信仰和态度。但在驳斥这项研究的同时，亚马逊也驳斥了一个非常现实的问题。「我学到的是，如果你是一家市值万亿美元的公司，你不必知道真相，」布拉姆维尼说，「如果你是街头的恶霸，你说的就是事实。」

### 02

那个时候，梅格·米切尔已经在谷歌内部打造了一支致力于「道德人工智能」的团队。米切尔是微软研究院早期在深度学习领域探索时的一分子，当她接受《彭博新闻》的采访并说人工智能遇到了「人海」问题时，她引起了这个圈子的注意，她估计自己在过去 5 年里与数百名男性和大约 10 名女性合作过。12「我绝对相信，性别对我们提出的问题类型有影响，」她说，「你把自己置于近视的境地。」13 蒂姆尼特·格布鲁加入了谷歌跟她一起工作，着眼于偏见、监控和自动化武器的兴起，她们要为人工智能技术打造坚实的伦理框架。另一位谷歌人梅雷迪思·惠特克（Meredith Whittaker）是该公司云计算部门的产品经理，在他的帮助下，包括谷歌、Facebook、微软等在内的公司联盟在纽约大学设立了一个名为「人工智能伙伴关系」的组织。像生命未来研究所（由迈克斯·泰格马克在麻省理工学院创立）和人类未来研究所（由尼克·波斯特洛姆在牛津大学创立）这样的组织也关注人工智能的伦理问题，但他们关注的是遥远未来的生存威胁，而新一波的伦理学家关注的是更为紧迫的问题。

对米切尔和格布鲁来说，偏见问题是整个科技行业更大问题的一部分。女性努力在所有科技领域发挥自己的影响力，但在工作场所面临着极端偏见，有时还会受到骚扰。在人工智能领域，这个问题更为突出，也可能更为危险。这就是为什么她们给亚马逊写了一封公开信。

在信中，她们驳斥了马特·伍德和亚马逊对布拉姆维尼和拉吉的抨击。14 她们坚持要求公司重新考虑其方法，15 并指出所谓的政府监管只是虚张声势。16 她们写道：「没有法律规定或要求的标准来确保‘亚马逊识别 ' 的使用方式不会侵犯公民自由。我们呼吁亚马逊停止向执法部门出售‘亚马逊识别 '。」17 她们的公开信获得了谷歌、DeepMind、微软和学术界的 25 名人工智能研究人员的签名。18 其中一位是约书亚·本吉奥。「当只有我们自己反对这家大公司的时候，这很可怕，」拉吉说，「当我们的工作获得了这个圈子的支持时，这令人感动。我觉得这不再只是我和布拉姆维尼与亚马逊的对抗。这是研究 —— 艰苦的科学研究 —— 与亚马逊的对抗。」