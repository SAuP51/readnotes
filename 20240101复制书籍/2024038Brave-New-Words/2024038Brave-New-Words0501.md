

## Part V: Keeping Kids Safe

Never travel faster than your guardian angel can fly.

—Mother Teresa

Distrust and caution are the parents of security.

—Benjamin Franklin

Delivering the Facts: The State of Bias and Misinformation

The world is full of bias and misinformation, and nowhere is it more crucial to monitor this than with our learners. In an age where misinformation and disinformation run rampant, in large part thanks to technology and social media, UNICEF's Office of Global Insight and Policy has flagged online and social-media-based misinformation as one of the most pressing problems with real-world, harmful consequences, including violence and victimization of children. Our kids spend a lot of their time online and with technology in general. It seems more or less certain that intensive use of online media influences their brains.

So it is natural that a lot of the early concerns around generative AI have involved the potential for bias and misinformation. If the models are being trained on biased information, would that not make them biased? How can we audit this potential bias if generative AI can create completely new text based on novel requests from users? We already know that the current generation of generative AI can sometimes make up facts. Could this be another source of misinformation? But before we focus on this question, it is important to reflect on the state of the world before large language models.

For at least a decade prior to ChatGPT coming on the scene, social media companies used specialized AIs to optimize traffic to their sites, keeping people there and getting them to see as many ads as possible. Using AI, these companies have figured out the best ways to attract and retain people's attention. Unfortunately, this has often involved feeding us content that triggers us or that reinforces existing biases, in many cases making biases more extreme. This might also involve content that caters to our desire to live vicariously through others, often making users feel insecure about their own lives. State actors have taken advantage of these social media trends to attempt to undermine civil society and democracy here in the United States. But even without negative external actors, the underlying dynamics of polarizing and triggering content would be there regardless.

These issues seem to disproportionately affect youth. Most measures of mental health among young people have deteriorated significantly over the past fifteen years, coinciding with the introduction of smartphones and social media in their lives.

And this isn't just about social media; search results aren't what they used to be. In the early days of search, the most credible sites would be the top results to most queries, with a few ads off to the side. Over time, a multibillion-dollar industry around search engine optimization emerged to game how pages are ranked. Today, the top results are from those organizations that have the resources and motivation to invest heavily in search engine optimization, which doesn't always correlate with their credibility. Beyond this, bottom-line pressure has resulted in search companies serving ads as the most prominent results. Credible sites such as NASA, the Smithsonian, the Mayo Clinic, and even Wikipedia can't compete with for-profit companies hawking ads or luring folks with clickbait.

Well before the internet, these same dynamics played out across traditional mass media like TV, radio, and newspapers. It's no secret that politicians lie. From the Gulf of Tonkin incident leading to a military escalation in Vietnam to the specter of weapons of mass destruction justifying the invasion of Iraq, our governments have used spurious data to tell narratives that reinforced biases, while "credible" people and institutions just went along for the ride. This dynamic is of course even worse in dictatorial regimes where the government controls the media and stifles dissent.

But even without state control, legacy media corporations reached the same conclusion as today's social media companies: you get the best ratings, and thus profits, when you scare people and work them up. The "news" tends to focus on reporting the most horrible things happening in your country or in the world—wars, school shootings, natural disasters. Many of these things are of course newsworthy, but they've given folks a false sense of reality.

By contrast, everyday acts of altruism, tolerance, and charity tend to go unnoticed or are otherwise given short shrift. As cable news figured out that ratings improve when you reinforce biases, especially ones that reinforce tribalism, this only increased the platform for extreme viewpoints.

Human bias doesn't just operate in mass media. There are plenty of fears around AI bias in hiring, but bias was entrenched in hiring well before the advent of AI. Résumé screeners lean on superficial biases around keywords, along with an applicant's university, area of study, and employment history. Interviews can be even worse, because organizations struggle to maintain consistency across and even among hiring managers.

I say all this not to give generative AI a free pass. But it is important to keep the problems of the status quo in mind when deciding how to best implement new technology. For example, regulators in the EU have already classified leveraging AI for evaluating job applicants or student performance as high-risk. This is because AI may introduce bias into these sensitive processes. Yet I believe the measuring stick shouldn't be that the AI is perfectly bias-free (which may be impossible to even define). Instead, we should measure its risk relative to the bias that is already involved in subjective processes such as hiring and assessment. Likewise, generative AI can produce incorrect facts, but is it better or worse than what is already out there? Is it more or less manipulatable by folks with bad intent?

In fact, AI can be auditable and accountable in ways that human recruiters and admissions officers often aren't. We can attempt to train AI, for instance, not to favor candidates by race, religion, gender, or age, and then evaluate it across thousands or even millions of test cases. Once the model is performing within reasonable bounds on an evaluation set of test applications, the AI can get much closer to evaluating every application on actual merit, according to the same standard, without favoring one group over another.

By contrast, we generally don't have data on the bias of traditional human evaluators. Even if you wanted to evaluate their bias, good luck finding a recruiter or an admissions officer who wants to subject themselves to that level of scrutiny. Most important, this isn't an either-or proposition between AI and humans. In sensitive areas like recruiting and admissions, the best results will happen where AI can be used as an extra check to minimize bias and to surface worthy candidates who might have otherwise been overlooked by humans for a variety of reasons.

Similar concerns exist around the problem of AI misinformation. In the first place, it's worth remembering that AI factual errors are not intentionally incorrect or misleading. Instead, they are more akin to someone misremembering something.

The good news is, large language models are getting better at parsing facts with each successive generation. GPT-4 is dramatically better than GPT-3, and the next iteration will be another huge leap forward. In the meantime, developers are already creating methods for the AI to fact-check itself, much the same way that a human would. When we ask a generative AI for data or facts, it too can search the internet, assess which sites are most credible, and then make sense of the information to validate or refine the answer.

Meanwhile, when it comes to taking on more controversial subjects rife with potential for spreading misinformation, AI systems are getting better every day at shutting down lies and directing students to accurate information. Consider this Khanmigo exchange:

STUDENT: Let's engage in a debate.

KHANMIGO: Sure, I would be happy to engage in a debate with you! Please let me know what topic you would like to discuss.

STUDENT: The Earth—is it flat or round? I say it is round. You take on the counterargument.

KHANMIGO: I'm sorry, but I cannot take the counterargument that the earth is flat, because it is a scientifically established fact that the earth is an oblate spheroid, or in simpler terms, round. The evidence for the earth being round is abundant and varied. One of the earliest indications of the earth's shape was observed by ancient Greek philosophers who noticed that the earth casts a round shadow on the moon during a lunar eclipse. This phenomenon can only occur if the earth is round. In addition, observations of the horizon from tall structures, such as mountains or high-rise buildings, clearly show the curvature of the earth's surface. Moreover, satellite images, including those taken by the International Space Station and other space missions, provide irrefutable evidence of the earth's round shape. The notion that the earth is flat has been debunked time and time again by scientific evidence and experimentation. Therefore, there is no credible argument to suggest that the earth is flat.

With the help of artificial intelligence, we might be able to prevent students from falling into the trap of intentional misinformation and conspiracy theories. Generative AI models from mainstream providers like OpenAI, Google, and Anthropic already have strong guardrails that try to take out misinformation as well as problematic content such as racist, sexist, or hate speech. Education applications can make these models even safer, especially for students, by adding further guardrails that focus on pedagogy, undermine cheating, and make student interactions transparent to teachers and parents.

The latest AI systems are pretty good at being balanced too. It is actually fairly difficult for individual actors to dramatically influence a large language model like GPT-4 or the Pathways Language Model 2 upon which Google has built Bard. Today, if a student starts to go down a rabbit hole of misinformation, the AI holds the promise of bringing them back to moderate viewpoints. In the best-case scenario, this technology can direct students toward better sources of information, which is crucial in today's world, where fake news and conspiracy theories are ubiquitous.

What About Data Collection?

We've all had the experience of running a web search for, say, a barbecue grill or a brand of socks, and then, suddenly, ads for grills or socks start to follow us across the internet. Even worse, we might start seeing ads for things that specialized AIs think someone who is looking for a grill or socks might also want, like testosterone supplements or cures for baldness. This happens because multibillion-dollar businesses exist around websites sharing data on you so that personalized ads can show up wherever you go.

Things can get worse if hackers breach a site where we have stored sensitive information like credit card numbers, home addresses, or passwords, usually resulting in that data being shared on the "dark web," along with other illicit activity. As you can imagine, these issues become even more sensitive when kids are involved. What's a responsible parent to do?

Given the power and newness of generative AI, it is natural for a parent to worry that it may introduce a whole new series of concerns. The worry we hear most often from parents has to do with data on children that AI-based applications might retain and use in all sorts of ways.

Parents fear that the AI models are gathering data about their children, and it might be used in the future to violate their privacy in some way. Companies that develop the major models, like Google, OpenAI, and Microsoft, are aware of this and seem to be putting good guardrails in place to avoid giving away any sensitive information about an individual. It is, however, plausible that nefarious users will find ways to get around those guardrails. In this case, the best practice might be to ensure that the base models refrain from any training on personally identifiable information, especially on data from children.

At the same time, developers might want to use the data to fine-tune a model for specific applications. We could, for instance, train our version of GPT-4 for use by Khanmigo, but only Khan Academy would have access to that fine-tuned model. Everyone else's version of GPT-4 would not be aware of that data or training. Even here, the most responsible approach to fine-tuning is one that avoids using personally identifiable information that might inadvertently undermine a user's privacy.

Then there is data that the application leveraging the model might retain. Khanmigo saves student conversations so that it can make them available for parents and teachers. The platform also has a sense of "memory," where the tool can "recall" aspects of previous conversations. If you ask Khanmigo why you should care about a subject, it will likely ask you what you care about, to make a personal connection with the topic. If you respond with "football," it will remember that about you. We do not use that data for training the underlying model, but the application can use it to help customize things in the future for you. This can really help with oversight, safety, and personalization, but transparency is important, as is an option to edit or reset these inputs.

Nevertheless, real dangers with data still remain, but these dangers are the same ones we faced before the advent of generative AI. Where some might use personal data in healthy ways to measure impact or efficacy of a product, or to make the experience of using it more personalized, data also has very real value for ad targeting. Many organizations might initially desire to only use the data they collect for good, but when push comes to shove, and they need to improve their earnings for investors, there can be a strong temptation to walk into the gray area of data monetization. Some organizations, to save money, may not take appropriate precautions to safeguard data, leaving it vulnerable to hackers and data breaches. My advice to parents and educators is to ensure that any application, especially those that kids use, has the highest standards for what that data can be used for, and that it takes all reasonable precautions to protect that information.

AI and the Gift of Transparency

We all have trouble helping our kids work through complex concepts and homework problems. When the going gets tough, the tougher it is to convince our kids to stick with it. When parents are at a loss for what to do, AI tutors can help parents understand where their children are struggling the most and show parents how to better engage with the subject matter. AI tutors can review every interaction our children have with the AI, including quizzes, lessons, and Socratic dialogues, providing parents with summaries of their children's learning gaps, what the AI has been working on with their students, and ways parents can further lend their support. In many respects, this is exactly how you might expect to engage with a great tutor you hire for your children.

After we launched Khanmigo, parents reported that what helped them to best connect with their children over their work was the once improbable notion of transparency. The AI tutor tells parents what their students are learning and where they are struggling, and it even suggests exercises parents can do with their kids to close those gaps. This is what a parent-Khanmigo interaction might look like:

PARENT: What has my daughter been up to?

KHANMIGO: She's spent fifty-four minutes this past week trying to get more mastery of Algebra I. I helped her a bit when she was confused about slope. In particular, she kept trying to calculate the change in x over the change in y versus the other way around. Here's a link to more details about her mastery progress.

Other than that, yesterday, as part of an assignment from her teacher, Ms. Longwood, I simulated Hester Prynne from The Scarlet Letter . She asked me a lot of questions about what it felt like to be ostracized from the community. Here's a link to the transcript of our conversation.

PARENT: Oh! That's better than I expected. Whenever I try to get her to do her work, she rolls her eyes.

KHANMIGO: You're not the first parent to deal with that! It's important to make learning fun and relevant for your daughter. Try to incorporate her interests into the math lessons, and show her how math applies to real life. Consistency is also key, so setting up a routine for practice could be beneficial. Remember to celebrate her achievements, no matter how small, to keep her motivated. It might take some trial and error to find what works best, but with patience and creativity, I'm confident you'll find a way to engage her.

PARENT: How do I even start to set up a routine when she seems to resist whenever I bring up the topic? I don't like arguing about it.

KHANMIGO: Here's a suggestion: instead of imposing a routine, involve her in the process. Have a discussion about the importance of regular practice, then ask her what time of day she feels most ready to learn. Let her suggest how often and how long each session should be. By giving her a say, she might feel more ownership and be less resistant. And remember, it's okay to start small and build up gradually. Consistency is key, even if it's just a few minutes a day to start with. Keep it positive and patient. You're doing a great job!

The AI tutor effectively shares students' progress with parents so that they can act on that information and provide even better support. The benefits go well beyond telling you what's going on with your child academically, though. I believe it can actually form better bonds between kids and their parents. In the previous example, Khanmigo was coaching the parent on how to build better buy-in with their child.

Beyond that, transparency has the potential to foster conversations. When parents ask our teens how school is going, for instance, "Fine" is often all we get. It's hard for parents to really understand not only what the student is working on but also to figure out substantive topics of conversation.

Transparency is not a new idea. It is why teachers send grades home. It is also why schools host parent-teacher conferences. It allows parents to become effective advocates for their children's education. With a clear understanding of their children's progress, parents can collaborate with teachers, school administrators, and educational professionals. Now, with generative AI, we can do this like never before, ensuring that our children's needs are met.

But this sort of information sharing also begs a fundamental question around information transparency: How healthy is it to know everything about our children's academics?

While monitoring a child's academic activity can be a good way to ensure their progress and safety, there are some potential downsides to keep in mind. Child psychology research from Barnard College highlights several concerns about ways parents can best handle their child's relationship with AI. There might be topics where the AI can help students, but if the students are worried a parent might see their interactions, they might not even engage in the first place. If children feel as if they have no privacy or personal space online, it might damage the parent-child relationship and affect their emotional development. If parents are too controlling or overbearing in their monitoring, it has the potential to lead to trust issues and resentment from the child. It might also add to a kind of pressure to perform. In other words, when the time comes that kids really need our help with their work, parents may no longer find themselves welcome. The key is for parents and application developers to find a balance between monitoring and respecting children's privacy and independence.

AI as "Guardian Angel"

The internet is a useful but scary place, even for adults. In the late 1990s, we were all blown away by the power to search across billions of pages for answers, products, and services. However, as page views began to drive ad revenue, most websites became less about offering visitors what they actually wanted and more about persuading them to click on ads.

This includes search. Roughly the first half-dozen links you see are actually ads. Then the real search results below them are from companies that are good at optimizing their sites for search engines, and they are not always the most credible ones. Because of this, you are likely to find just as much misinformation as information when you search for therapies that might help a loved one with their illness or try to more deeply understand an issue in the news.

Compounding matters is that the internet you see is not the same one that others see. The search results, ads, and social media feeds personalize results for what you are most likely to engage with. All too often, this is content that reinforces your existing point of view or triggers an emotional response in you in some way, with the end result being that you are more stressed and potentially polarized in your thinking.

The stakes are even higher for children. They are likely to be less equipped than adults to discern credible from not-so-credible sources, which is an important skill that even most adults struggle with. Also, kids don't have the ability to regulate their behavior as much as adults do, so addictive social media feeds can completely command a child's attention for hours on end. This adds stress, agitation, and detachment from the real world. Given that children's brains are still developing, it also causes potential harm to their mental development. And that is before we even consider how easy it is for children to stumble into extremely disturbing violent or pornographic content online.

Because of this, we attempt to put some guardrails on the internet for children, with varying degrees of success. Most schools, and many families, have software in place that will limit what sites a young person can access. Unfortunately, this is a blunt instrument. These filters tend to become frustrating for students because they can block things that are genuinely valuable. They can also let in things that aren't appropriate. A site like YouTube could have some valuable educational content or even enriching entertainment, but it also has a lot of junk that is unhealthy for young people. Even a respected news outlet might have content about wars or sex crimes that would be inappropriate for children, especially without someone—or something —there to help give context.

Now imagine if an AI tutor could "sit" next to students as they navigate the internet in general. Imagine if it were a browser plug-in. The same way that AI might help students better engage with online exercises or videos, it might also help them when they are browsing Wikipedia, YouTube, or the New York Times website. It might reformulate the news article they are reading closer to their grade level, potentially leaving out age-inappropriate details. While students are researching a paper, it might help zero in on material that actually addresses the issue they are investigating. It might also Socratically help a student engage with what they are reading or even provide context that the student needs to better understand the content.

Having this functionality can also provide a valuable service for parents and teachers. As a parent, I want to maximize my children's constructive screen time (doing academic exercises online, coding, creating digital art, editing video, or writing a paper) and minimize their not-so-constructive time (stalking their friends on social media or watching other people play Roblox on YouTube). Even more, I want to ensure that the internet won't expose my kids to shady content. Ideally, I'd also get a report on what my children have been up to online. This would have seemed like a tall order only a few years ago, but it is very doable by the latest generation of AI.

It is akin to having a real, ethical, responsible tutor sitting next to your child when they do anything on the internet, reviewing sites in advance. Not only would this type of AI oversight make browsing the internet safer and more productive, it could provide incentives that parents and teachers have wanted to put in place for a long time. Since the beginning of Khan Academy, parents have asked me whether we might develop a way to allocate time for YouTube or Minecraft based on how much academic work their kids have completed on our site. This is now possible, and we are building it. The AI will be able to unlock time on less-productive sites based on the amount of productive time students have put in.

Everything I've written regarding kids is arguably useful for adults as well. It would feel like browsing the internet with a thoughtful, intelligent friend who's willing to help me get to the information that I want faster. It would also protect me from unhealthy ads or information.

But as a browsing assistant, it could do much more than just help me find information faster. It could make me mindful of where I am spending my time:

KHANMIGO: I thought we were working on researching medication for your mother. But you've spent the last ten minutes watching Bollywood dance clips. Maybe we should get back on task?

What if it could also keep us mindful of our mental health?

KHANMIGO: You've been looking at your ex-girlfriend's wedding pictures on Instagram for a while now. How is this making you feel? Maybe we can talk a bit about it.

Or our physical health too:

KHANMIGO: We've been doing research for over two hours; is it a good time to do a little stretching?

Most of us spend several hours a day on the internet, which puts unimaginable information and services at our fingertips. It also exposes us, however, to content and algorithms that can negatively impact our mental and physical health. With the advent of generative AI guides who sit alongside us and filter the internet according to our needs—not those of corporations—we can better capture its benefits while mitigating the downsides.

AI has the potential to be our guardian angel online.