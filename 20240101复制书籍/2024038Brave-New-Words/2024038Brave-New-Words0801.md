## Part VIII: AI, Assessments, and Admissions

Not everything that can be counted counts, and not everything that counts can be counted.

—William Bruce Cameron

Evaluation is creation: hear it, you creators! Evaluating is itself the most valuable treasure of all that we value. It is only through evaluation that value exists: and without evaluation the nut of existence would be hollow. Hear it, you creators!

—Friedrich Nietzsche

The Future of K–12 Assessments

It has become fashionable to bash standardized tests in the United States. Every state has "summative" assessments at the end of each school year to measure how students and schools are performing. People often criticize these tests for being too narrow—they primarily use multiple-choice questions that target a subset of what is actually important in life. This can create pressure for educators to similarly narrow their focus in the classroom.

That's not all, though. Others argue that these tests take time away from learning and are not really actionable. By the time the scores come in over the summer, or at the beginning of the following school year, kids have moved on to a new grade with a new teacher. In addition, students have very little incentive to care about showing their best work on a test that is not connected to their grades. Also, demographic differences in performance can lead to claims of bias against some groups or schools. As education has become more politically charged, the lack of transparency into what these tests actually assess makes people skeptical.

Let's take a step back, however. When people raise objections to standardized tests, I like to interrogate what part they don't like and whether they are throwing out the baby with the bathwater. If they are against assessments altogether, I ask them how we can get better at anything without measuring it. And if we are going to measure, standardizing that measurement is arguably fairer, since it applies the same standard to everyone (versus "unstandardized" assessment). If the issue is with the perceived narrowness of what these tests measure, wouldn't the answer be to broaden the scope of the assessments to make them richer rather than abandon them entirely? Likewise, if the critique is around actionability or transparency, couldn't we make the tests more actionable and transparent?

Most of all, even though standardized tests might be imperfect, does removing them really make things more or less equitable? If a school serving underrepresented groups doesn't know where and how their students may be falling behind, how can they begin to fix the problem? Is it somehow better for educators, students, and families to not know their gaps? Eventually, these deficits will surface regardless. This is likely to happen years later, in college or in the workforce, when it is much harder to fix after years of falling behind.

I'd rather think about how we can improve standardized tests rather than try to remove them entirely. Solutions have existed even before generative AI. Let's consider the critique that there isn't much that teachers can do with the standardized testing results. Well, if the assessments could feed into a software platform for personalized practice—for example, having students work on addressing different weak points based on the standardized testing data—the information becomes actionable. Over time, the personalized practice software would make sense of a student's previous assessments and provide that student with more targeted recommendations.

Khan Academy has, in fact, done this for some standardized tests for many years now. That is, we use standardized testing information to better differentiate practice in a classroom, improving student outcomes. In a study of more than three hundred thousand students using standardized test scores to inform personalized practice on our platform, " students who engaged . . . during the 2021–22 school year at the recommended dosage of 30+ minutes per week exceeded growth projections by 26% to 38%, depending on grade."

Continuous standardized assessments while students are learning also avoids taking away valuable instruction time. Every exercise on our platform is standardized, and we have millions of data points about how students from different grade levels and demographics perform on them. Rather than just having students take a traditional state summative test once or twice a year, they can regularly practice their skills on our platform. Educators can then use the generated data to measure student learning in a standardized way. This gives a more accurate and regular read on how a student is performing. That data then becomes actionable by driving student recommendations for further learning. This type of continuous assessment offers higher-quality data points on a much more regular basis. Where traditional standardized tests might cover fifty to one hundred questions once or twice a year, continuous assessments can glean this much information every week without them even feeling like a separate evaluation. This approach also addresses the issue with student motivation: you are more likely to care if your daily practice is also assessing you in a standardized way behind the scenes, as that work impacts your class progress and grades.

Much of the political angst over assessments and what is happening inside the classroom is due to external stakeholders like parents and politicians not being able to directly observe what students are experiencing. Instead, they rely on second- and thirdhand accounts, which might not be entirely accurate, or they fail to understand how high-level standards tangibly manifest in a test or classroom.

The lack of transparency and flexibility of traditional standardized assessment stems from both the expense of creating the assessment items and the fact that they must remain secure; if any of them leak, the entire assessment can become invalid. On the other hand, if you have easily accessible online platforms that adaptively assess from a large bank of items—think hundreds of thousands of questions—you can let more stakeholders try out the assessment whenever they want without undermining it. This is because an adaptive assessment will give every student a different sequence of questions based on how they performed on previous ones. It's unlikely that two students will ever see the same set of questions.

Generative AI holds the potential to help with all of this. Large language models aren't yet good enough to make high-quality assessment items completely on their own, but they can help a human question writer/reviewer become much more productive. Eventually, this will allow us to produce many more items with the same resources, enabling a new wave of even more transparent and accessible assessments.

This still leaves the question of how to deepen and broaden the skills that standardized assessments can measure. While multiple- choice or numeric-entry questions can get you pretty far when assessing some types of skills, they can't really capture how well you write, engage with a problem, or think creatively. Historically, these more nuanced tasks have been prohibitively expensive to assess widely. To gauge even basic open-ended questions, you need expert human reviewers working with complex rubrics and systems to ensure consistency. Richer assessments akin to a thesis defense for a PhD or a job interview have historically been impossible to do at scale.

This is about to change. The latest generation of large language models holds the potential to allow us to conduct this type of rich assessment economically and universally.

Consider reading comprehension: Today, students read a passage and then answer a few multiple-choice questions based on it. These questions might ask something about, say, the author's intent, followed by four choices. In the coming years, we will increasingly see assessments use generative AI to engage students about their views or the author's intent without the need for multiple choices. It will ask students to just write or speak their thoughts, and the AI will be able assess that response in a consistent way. Even better, it will be able to engage in a conversation with students about why they feel that way and discuss the evidence they are drawing on. The entire assessment will resemble a fluid, wide-ranging conversation with a thoughtful, empathetic, and fun mentor. Parts of it might involve role-playing or trying to work through a simulation. It wouldn't necessarily even have to be separate from learning. The same AI tutor that is there to help you would also build up evidence of what you know and don't know.

This goes beyond language and reading comprehension. In math, the AI can ask students to explain their reasoning or develop a proof. In science, it might assess how well a student can design an experiment or critique a research paper, arguably two of the most powerful elements to becoming a scientist. AI-driven simulations can assess student problem-solving skills. As AI takes on increasing visual capabilities, it will be able to critique and assess visual works, pictures, or videos of a presentation.

Of course, using AI for assessment can rightfully make folks wary. What if the AI has biases that are not immediately apparent? What if it makes mistakes? I try to compare this kind of hypothetical to the status quo. Current assessments are written by thoughtful but fallible human beings with their own biases. We already know that by not leveraging AI, we are limiting ourselves to a much narrower type of assessment that arguably magnifies a bias toward prioritizing easy-to-measure skills over ones that are harder to measure but perhaps more important. Historically, when we have been able to administer richer assessments, like in PhD oral thesis defenses or job interviews, they are inconsistent and rife with more bias than any current standardized exam. Generative AI allows us to capture the best of both worlds: standardization and scale with richness and nuance. Because of its potential accessibility, stakeholders will have a far easier time trying it out and auditing it themselves.

I am not saying that we should blindly assume that any AI assessment is going to be good. In fact, I am afraid a lot of people are going to create some very bad ones, rife with bias. I also believe, however, that with proper care, transparency, and guardrails, we can mitigate the risks and develop assessments that are far richer, more accurate, and fairer than those that we have today. This will have positive consequences for the education system as a whole, reopening the aperture of what makes a quality education. By measuring skills long thought to be immeasurable, such as communication, creativity, and curiosity, it will naturally motivate the system to care a lot more about developing the whole person.

The AI of College Admissions

The classic components considered in college admissions are grades, standardized tests, extracurriculars, essays, and letters of recommendation. AI will change how most if not all of these factors are valued, developed, and evaluated.

I've already argued that generative AI is going to transform schoolwork and grading in the classroom; students will be able to do much richer assignments, and teachers will have more support grading them. I've also discussed how standardized assessment is likely to change. Assessment will be deeper, much more continuous, and indistinct from learning. Over time, either standardized tests like SATs and ACTs will move in this direction, or new assessments will enter the space to take advantage of the opportunity.

Other than extracurriculars, the remaining components—essays and recommendations—both involve writing. This is the most obvious place where large language models introduce some very big ethical questions. Teachers and guidance counselors may use generative AI to write their recommendation letters. Students are likely to use large language models to generate impressive-sounding essays that misrepresent their actual writing ability or creativity. This poses a challenge for admissions officers to accurately evaluate the validity of applicants' work.

And yet heads of admissions at top universities tell me that the advent of generative AI has simply shined the spotlight on inequities that they have known about since long before large language models arrived. Take the Varsity Blues scandal, for example. This was a case in which wealthy celebrity parents paid hundreds of thousands of dollars to hire an unethical college admissions coach who not only wrote application essays but went as far as completely fabricating extracurricular activities, including photos. While this is an extreme example, an entire industry exists around college admissions coaches that only affluent families can afford. The going rate in Silicon Valley, where I live, is roughly four hundred dollars per hour for the top coaches. This can amount to tens of thousands of dollars to assist one student through a college admissions cycle. What do these coaches do? The more ethical ones advise students on how to approach extracurriculars and essay topics, help families think through good college options, and provide students with thoughtful feedback on early essay drafts. At the more unethical side of the spectrum, they might provide so much editing of a student's essay that they are essentially writing it for them. Either way, these affluent students receive significant help. Even if they do not hire a coach, many of these families have a lot of insider knowledge about the byzantine college admissions process and use it to give their children a leg up.

Tools like ChatGPT are obviously much more accessible to a broad group of people who never could have afforded high-priced college admissions coaches. And like these coaches, generative AI can be used for ethical and unethical purposes, as well as everything in between. It has now opened the door for everyone to play in the ethical gray area that was once only the domain of the affluent.

Something similar is likely to happen when it comes to recommendations. High-priced admissions coaches can't write recommendations, but guidance counselors and teachers serving wealthier students tend to have a lot more knowledge of how to give their students the best shot at university admission. Wealthier schools also tend to have smaller classes in which teachers and guidance counselors can get to know their students better and have more time to spend on each student's recommendation. Now someone writing a reference can work with generative AI tools to better express the strength of an applicant.

So, on the positive side, generative AI can help close the gap between the rich and poor. Now everyone—not just the affluent—needs to decide how much help is too much help. On the negative side, less-ethical students are likely to push the envelope, putting the more-ethical students at a disadvantage. Meanwhile, admission directors need to wrestle with whether this entire exercise of writing essays even still provides a credible signal for admissions.

To address that, it's worth questioning why essays and recommendations are part of admissions in the first place. In most countries, admission to highly selective universities is a fairly objective process. In India, entry to the hyperselective Indian Institutes of Technologies (IITs) is based solely on the Joint Entrance Exam (JEE). IITs admit the students with the top test scores, permitting quotas for some underrepresented groups. Not only do the highest scorers get their pick of IIT campuses, they also get to select their majors first. In India, this is a deliberate attempt to steer clear of the corruption that has often infected other institutions in the country. Nothing remotely subjective like essays, recommendations, or extracurriculars is involved.

On the other hand, admissions directors at highly selective universities in the United States will talk about subjective things like "building a community of diverse future leaders." Yes, they index on test scores and grades to some degree, but many of these institutions could fill their freshmen classes many times over with students who have perfect test scores and GPAs. Put another way, at some top schools, half of the applicant pool will have grades and test scores indicating that they could more than succeed academically if admitted, yet the university will only be able to admit 3–6 percent of them. This leads to a highly subjective process of trying to gauge the student's personality and backstory through essays, extracurriculars, and recommendations. Have the students overcome obstacles? Do they seem collaborative? Are they likely to make an impact on the world one day? These are big, deep questions to ask about young people who are seventeen or eighteen years old. I think many people are skeptical of how well admissions officers can judge these qualities based on some essays and recommendations that are subject to significant outside influence.

Extracurriculars are arguably a more tangible display of a student's leadership or commitment to community, but this, too, can be hard to judge. Did the student win the international science fair on their own? Is it a coincidence that their experiment studied heart disease and their mother is an academic cardiologist? Was that volunteer work really substantive or just something that sounds impressive?

This has all led to a randomness in American competitive college admissions that is clear to anyone who has been involved in the process. Many of the brightest, most collaborative, and poised people have been rejected far more than one might expect. The assumption is usually that they weren't sufficiently represented by glowing recommendations or unique essays. On the other hand, visit any highly selective college and you will meet many impressive young people. You are also likely to meet many who are struggling academically or do not seem to embody traits like humility, collaboration, or leadership. Most assume that these students were unusually good at constructing a paper narrative about themselves and gaming the system—or that their family was good at hiring someone who did this for them.

But what if we could have more standardized ways of evaluating "soft skills" like leadership, collaboration, empathy, and community service? Even better, what if this were coupled with ensuring deep academic competency? It turns out that this predates AI, but AI is going to take things to another level.

In 2020, I launched Schoolhouse.world to give anyone free, live tutoring via Zoom. This was more needed than ever, considering how many students were falling behind because of the COVID-19 pandemic. We have been able to keep it free by enlisting vetted volunteers to do the tutoring. The first step of the vetting process is to ensure the volunteers have mastery of the material they are going to tutor. They take the appropriate assessments while a separate tool records their face and screen. The volunteers have to explain their reasoning out loud. If they get at least a 90 percent on the assessment, the video is submitted for peer review. Assuming that everything looks good, they are allowed to start their tutoring journey, which still involves more vetting and training on the craft of tutoring itself. It's a rigorous method, ensuring quality tutors. After every tutoring session, students rate the volunteers. The volunteers have a transcript page that summarizes all the subjects they are certified in, the number of sessions they have run, their average rating, and any other qualitative feedback from the community that they'd like to highlight.

Jim Nondorf, the head of admissions at the University of Chicago, reached out to me soon after, asking if they could use the Schoolhouse.world tutor transcript for college admissions. His rationale was that any high school student who was a highly rated tutor for, say, calculus, surely knows the material well, especially considering our rigorous vetting process. Even more, if they have done many tutoring sessions and are highly rated, they are also likely to have strong leadership, communication, and empathy skills, not to mention their commitment to helping others by spending hours tutoring for free. We thought this was a great idea, and that fall, the University of Chicago made the Schoolhouse.world transcript an optional part of their application process. By the next admissions cycle, MIT had signed up as well. Fast-forward three years, and the list has grown to eighteen universities, including Yale, Brown, Caltech, Georgia Tech, and Columbia, with more added each year.

They all value the Schoolhouse.world transcript for the same reason that Jim Nondorf does: it is a dynamic and standardized way of measuring both subject-matter competency and communication, empathy, community service, and leadership. Unlike in the past, when admission officers didn't have a lot to go on if a student said that they did regular community service, on the Schoolhouse transcript, the extent and quality of their service is quantified in a standardized way. It is pretty much impossible to fake being a high-quality tutor over hundreds of sessions. Because of this, I learned during early conversations with several of these schools that students submitting these transcripts generally have a higher acceptance rate than the broader pool. A side benefit to all this is that it also provides a strong incentive for ambitious high schoolers to become tutors and help others.

How does AI play into this? First of all, Schoolhouse.world is already using AI to give volunteer tutors feedback on their tutoring sessions. The AI can "observe" Zoom sessions via the transcripts and give the tutors pointers on how they can improve. In the near future, it will give tutors real-time tips on how to serve their students better. Eventually, it will be able to provide narrative assessments of the tutor's style and capability on the Schoolhouse.world transcript, providing yet another rich input for admissions officers. Most important, the Schoolhouse.world example starts to point to how we can reimagine admissions altogether with AI.

Rather than essays or recommendations alone, what if the AI could do extensive text- or voice-based interviews with students, guidance counselors, and teachers? A protocol like ours could ensure that the interviewee is alone and not being fed answers by anyone. Eventually, the AI might make use of the video as well, which would be hard for a person to game. The interviewing AI would be aware of the student's grades, SAT/ACT scores, and extracurricular activities and then use that information to provide accurate references. Students could still submit essays and recommendations, but the AI could dig deep into interviewees to ensure that students authentically know what they are talking about.

Admissions interviews, typically conducted by alumni living in the same area as the student, are not conducted uniformly across all prospective students, and those that take place are incredibly inconsistent with one another. They can be useful for admissions officers to screen out applicants with obvious red flags, but they aren't super useful for comparing the bulk of students who all seem exceptional on paper. AI allows this process to become far more scalable, consistent, and auditable. In this context, the AI can consistently summarize its interactions and rate them in multiple dimensions based on a rubric created by the admissions office.

There is even the possibility that AI agents can vouch for the student themselves, just like a teacher who knows the student well. Think about it this way: an AI platform like Khanmigo has been working with you for some period of time. Whether you have used it for a month or for many years of schooling, it knows your strengths and your passions and can plausibly render a dynamic picture of who you are. When it is time to apply to college, the AI can write a recommendation letter for you. The letter is standardized across every student who uses the platform, only it has different memories based on its experiences with each learner. Imagine if everyone in the country had the same teacher. This teacher would actually be a pretty good arbiter. If we wanted to take this to the extreme—and it is not clear that we do—the AI recommender could talk to the AI interviewer on the admissions side to see if there is a good fit.

I know this raises fears of bias in both directions. There are some biases you want. You want the process to be biased toward thoughtful, collaborative young people who could be tomorrow's humble future leaders. You of course don't want it to be biased along lines of gender, race, religion, or geography. A 100 percent bias-free solution might be impossible, but that shouldn't be the hurdle. Instead, any AI system needs to be demonstrably better than the status quo, which usually involves all sorts of bias. This is not hypothetical. In a 2018 Supreme Court case, it was clearly established that Harvard admissions officers consistently rated Asian American applicants lower on personality traits, oftentimes arbitrarily overruling the observations of in-person interviewers. Harvard's admissions process scored applicants in five categories—"academic," "extracurricular," "athletic," "personal," and "overall"—ranking students from 1 to 6, with 1 being the best. White applicants got higher personal ratings than Asian Americans, with 21.3 percent of white applicants getting a 1 or 2 compared to 17.6 percent of Asian Americans. Alumni interviewers gave Asian Americans personal ratings comparable to those of white applicants, but the admissions office issued them the worst scores of any racial group.

It took a major lawsuit for this data to surface. Most of the time, the biases embedded in this very opaque process are well hidden. The power of an AI-based interviewer and assessor is that they can be audited. You can test them with identically qualified applicants with different demographics and publish the results to ensure consistency across race, gender, or background.

Rather than introducing new problems in college admissions, AI is forcing us to realize existing deficiencies while offering the possibility for positive change. Used thoughtfully, perhaps with a bit of educated bravery, it might enable us to move to a fairer and more transparent world.

Part IX

Work and What Comes Next

The one who plants trees, knowing that he will never sit in their shade, has at least started to understand the meaning of life.

—Rabindranath Tagore

Learn the rules like a pro, so you can break them like an artist.

—Pablo Picasso

Employment in an AI World

Many people fear that AI is going to lead to mass layoffs in favor of new, AI-powered tools that can do jobs faster, cheaper, and more efficiently than humans. Some companies have already paused hiring for roles that they think artificial intelligence will replace in the coming years. In 2023, IBM announced that it was suspending or slowing back-office hiring by 30 percent over a five-year period for jobs that could ultimately be done by AI. IBM's revelation suggests that the future of work is going to roll out differently, with back- or middle-office jobs disappearing, together with non-client-facing roles involving tasks such as creating budgets, managing data, and organizing records. Reading the tea leaves, we can see where this might be going.

What will jobs in an AI-infused marketplace look like and how do we prepare our learners for them? Since ChatGPT came on the scene, many in the know have been saying that you won't get replaced by AI, but you might get replaced by someone else using AI.

Writers and copywriters using AI could potentially be three to five times as productive. The same will go for software engineers who are using AI copilots to debug and fill in large portions of their code. Graphic designers will be able to make fifty variations of a logo by tweaking a series of text-based prompts. Given this increase in productivity, are we going to need as many copywriters, engineers, and graphic designers?

I suspect it will be a mixed bag. Because we are at such a technological inflection point that allows us to do so much more with generative AI, the demand for engineers, especially ones who are five to ten times more productive, is only going to go up. We've seen this happen in the past. In the early 2000s, accelerating globalization allowed a lot of software engineering work to be outsourced to places such as India. As a young engineer at the time, I thought I had to go to business school and change to a career in finance to avoid being disrupted by low-cost labor from abroad.

I was wrong. Since then, software engineering salaries have gone up much faster than the rate of inflation. This is because smartphones and the rise of the internet created an environment conducive to new software-based solutions. Generative AI is creating an even riper environment for further innovation. From my vantage point, because of generative AI, there is endless work for engineers who can creatively apply these technologies to solve new problems in nearly every industry.

On the other hand, I'm not as bullish for, say, the people currently writing news summaries about daily stock market fluctuations. If it isn't already happening, these types of tasks will soon be done by generative AI. The copywriters and technical writers who are going to survive are going to be the ones who lean in most on AI to increase their productivity. The other 90 percent are going to have to find something else to do.

The good news is, generative AI will require new kinds of work. One of the hottest jobs today is being a prompt writer or prompt engineer. Two years ago, no one knew what those jobs were. It turns out that an open-minded and creative copywriter could transition quite well into some of these roles. Generative AI is also making us envision entirely new opportunities around safety, security, and antibias. I suspect that as more organizations wrestle with how to apply this technology, new opportunities will continue to emerge.

In the same way that teachers are using AI to facilitate their more rote or mundane responsibilities, positions from HR to management will begin to task AI with producing hiring letters or meeting reports. On the surface, this sounds good and helpful, yet I also can't help but consider the larger consequences.

It isn't just about individual jobs. AI-induced natural selection is also going to be happening at the corporate level. If we have two companies—one that's smaller, leaner, and more automated, and another that's bigger, slower, and more dependent on human labor—over time, the smaller company will be able to offer an equivalent or better product for a lower price and start to gain more market share from the bigger company, which ultimately leads to a net loss of jobs. This is a common trend that we see in many industries, and this will continue as automation and technology advance at a rapid pace. On the one hand, the smaller company can provide goods or services more efficiently and at a lower cost, which can be beneficial for consumers. On the other hand, the job losses can be difficult for those who are directly affected.

This is already starting to unfold in dramatic fashion. In 2006, the education technology company Chegg, Inc. launched as a service that assisted nearly three million customers with homework, digital and physical textbook rentals, and online tutoring. Just two and a half years before ChatGPT, Forbes called the company the most valuable ed-tech business in America. Once ChatGPT arrived, Chegg pivoted and began to incorporate AI into its platform, but it was no match for ChatGPT. When Chegg announced its quarterly earnings in May 2023, the CEO admitted that the service had been struggling to keep up with OpenAI's offering, because so many students were using the large language model for help with their homework. The company admitted it no longer had any idea how much money it was going to make that year because of the influence of ChatGPT. This bombshell admission led to Chegg's stock plummeting almost 50 percent. Since the arrival of generative AI, many have worried that these tools will upend established businesses. As distant as this fear once was, it remained speculative until it suddenly became a bit too real, with stories like this.

These sentiments are not mine alone.

"It's going to affect every industry differently, every person differently, and every job differently," Wharton's Ethan Mollick says. "The job that's least affected by AI, according to the early-stage research we have, is roofing, and yet I've talked to a couple roofers who're like, ‘Actually, roofing is going to change, too, because we can now do all of our proposals with AI help.' "

When you develop artificial intelligence that can understand language, recognize patterns, and solve problems, as well as AI that can diagnose illnesses, make stock market trades, compose music, fight lawsuits, understand emotions, analyze genetic code, handle insurance claims, engineer, and write articles, it is not hard to believe that more changes are coming. The successful strategy will not be to resist but to adapt.

How to Prepare Kids to Thrive in the AI-Future Workplace

It is several months before we launch Khanmigo, and I am speaking at the Stanford Computer Science Department along with a professor named Chris Piech. He tells me a story about a young Stanford student he was advising who came into his office. "She was really upset, saying how she just learned how to code and now artificial intelligence was able to do it better," Piech says. "She felt like she was already marginalized."

For the past twenty years, software engineering and data science have been the hottest two jobs for young people right out of college. We encourage them to go into these professions if they want to be part of the future. We advocate for students to learn how to code to become part of the digital economy. But what we now know is that generative AI can do this well.

This creates tension. We are afraid to allow kids to use AI tools in school in order to prevent cheating, despite the fact that they will have future jobs requiring close symbiosis with these same tools. This is compounded by what Bill Gates calls another "confounding paradox." We now have a tool that makes it easier for those who want to learn, but sadly, in some cases, it makes people wonder if they need those skills at all. Why should students learn skills that AI can do better?

"So what did you tell her?" I ask Piech.

"I said I believe the ability to code is going to be really important. I told her if she wants to create any type of major solution, including a solution that uses generative AI, even if the generative AI can write pieces of code, you really need to know how those pieces can fit together."

In other words, he says, it will be important to learn to work in concert with generative AI. In order to build anything, you are still going to need to know how the pieces fit together and how they actually work.

Entry-level employees who understand AI, and use it, will be far more efficient than those who don't. My fourteen-year-old son loves to code and aspires to someday make video games. I think he has a great shot at this, but I also encourage him to use generative AI to tackle more ambitious projects than he would ordinarily be able to do on his own. He is going to be able to make games that, before generative AI, would have required a team of half a dozen professional engineers to create.

It is well documented that generative AI can write with real competency. This does not mean that one should not learn how to write. If one of my children came to me and said they wanted to be a screenwriter, I would tell them to get really good at writing screenplays. Then I would encourage them to use generative AI to take on more and more enterprising projects. You need to be good at a craft in order to know what high quality looks like. Even more, an individual with a strong sense of story and film will no longer have to stop at the screenwriting phase. A screenwriter could actually produce the movie with the help of generative AI. This technology can already produce music and videos. It can even edit raw footage. A film that used to cost a hundred million dollars and take several years to make will likely soon be doable by a handful of film students with a fraction of that budget. We do not know how much better these systems are going to get, but even the current generation of AI is starting to cause a disruption along these lines in the film industry, as well as in many others.

Early controlled studies on productivity improvement due to AI by the Wharton School are seeing 30 to 80 percent performance improvement on many high-powered white-collar analytical tasks. This includes stronger and more concise writing, analysis, consulting, and programming.

"If you want to be in these fields, AI is, and will remain, a part of your life," Mollick says. "You need to figure out if you can use AI to be ten times more productive—meaning there remains a need for humans to be in the loop. If you are trying to be a copy editor, a coder, or even a roofer, you will need to be a centaur, only instead of being half human and half horse, you must be half human and half large language model."

To meet the demands of this new world of work, educators are going to have to, explicitly or implicitly, continue making their students familiar with these tools and the power that they have. Generative AI is what's called a general-purpose technology, one that comes along very rarely. Think of the transformations brought about by steam power, computers, or the internet. Generative AI is likely to transform our lives faster, and more profoundly, than any of these previous inflection points.

The Industrial Revolution was all about specialization of labor. We created assembly lines and workers then specialized in certain jobs on that assembly line. That trend has continued ever since, as complex organizations have developed complex systems. The benefits of specialization are going to continue in an AI world.

"Exactly what the job market of tomorrow looks like is very hard to predict, but the deeper the skill set, whether it's medical consultation, scientific thinking, or customer support, the more value it's going to have, even in a world where productivity will be enhanced by AI," Bill Gates tells me. Not only is there more reason than ever for kids to continue to learn about their fields of interest, he says, but students need to accelerate learning these skills, and to learn them as well as possible. "Entry-level jobs are going to require people to understand how to use large language models and all of the tools they offer. You'll need them to create everything from invoices to business plans. The workplace is going to encourage its workforce to come up with the best product it can. The higher your skill level is, the more your skill will retain a substantial value in the workforce. It's the workforce plus the AI, working together."

Yet, in the world we are entering, it is not just about specialization. Those who succeed might be deep in one or two areas, but they also need entrepreneurial expertise in a broad set of domains in order to put all of the pieces together.

This is nothing new. When I started Khan Academy, I knew how to code, so I was able to start prototyping it without needing any money or help. I was also good at putting together tools that already existed and seeing utility in them that others did not. I used YouTube for videos and instant messaging to communicate with the cousins I was tutoring. I was a hedge fund analyst, so even though I had never run a nonprofit before, I knew enough about finance and accounting to get it off the ground. People who are able to embark on a project this way will always have an advantage as entrepreneurs, but there is a limit to how far they can go on their own. At some point, they need to raise money and hire people. Just as software and the internet facilitated remarkable growth (I was able to scale Khan Academy to one hundred thousand users as a side hobby on my own), generative AI will allow the next generation of entrepreneurs to go even further.

We are entering a world where we are going back to a pre–Industrial Revolution, craftsmanlike experience. A small group of people who understand engineering, sales, marketing, finance, and design are going to be able to manage armies of generative AI and put all of these pieces together.

When economists talk about the factors of production, they talk about things like capital, labor, land, and other resources such as energy. But they also talk about entrepreneurship. From an economics point of view, entrepreneurship is really the creativity of knowing how to put resources together in order to create value. So how do we prepare every student to be this type of entrepreneur?

Step one is to get out of the way. I believe all human beings are born highly creative and entrepreneurial. Unfortunately, our Industrial Revolution–designed education system unintentionally suppresses both traits. Kids learn to sit in rows, make no noise, and take notes. They are spoon-fed knowledge and forced to learn in lockstep. Both academically and socially, nonconformity is punished. When students are young adults and most capable of being creative entrepreneurs, the system instead bogs them down with hours of busywork that squeezes out any time for their passions.

Step two is a little bit more traditional. As Bill Gates mentioned, the successful workers of the future will be those with deep and broad skills. The "three Rs" of reading, writing, and arithmetic are more important than ever. On top of that, a solid appreciation and understanding of history, art, science, law, and finance would round out someone well. Luckily, we now have the technological tools to ensure the mastery of these skills without having to enforce the lockstep learning of the industrial age.

Finally, it's more important than ever that students have strong communication, collaboration, and empathy skills. Traditional entrepreneurship tends to invoke ideas of starting a business, but what I am describing goes much further and includes a more personal vision. It is an ability to look at the various parts of your job, and to see any problem that needs solving, to know where you must focus your research and to understand the pieces that you need to put in place in order to solve it. To thrive in an AI world, everyone needs to be this type of entrepreneur, even if they are working for someone else. Schools can make this happen by putting students in the driver's seat more often, using AI tools to better support student mastery of core skills and free up time for student agency and creativity.

Matchmaking Between Job Seekers and Employers

In 1999, for a cover story on the new millennium, Computerworld magazine asked me for predictions that might come to pass in the coming ten to twenty years. I was surprised that they were asking me, a recent college graduate who was a new product manager at Oracle, given that the other nineteen people they interviewed were titans of technology like Bill Gates and Larry Ellison. The editor in chief had attended my college graduation, where I spoke as class president; he thought it would be interesting to get a more youthful perspective in the mix.

Regardless, I didn't want to waste the opportunity and ran with the most far-out ideas that I thought might be plausible. I talked about a future in which we would all have artificially intelligent personal agents to represent us in "cyberspace." The agents would purchase things for you and broker transactions, even matchmake between employers and employees (or romantic partners). Twenty-three years went by and, for the most part, my predictions didn't come to pass. AI hadn't advanced to the necessary degree.

Fast-forward to now, and this latest generation of generative AI holds the very real promise of making my predictions seem conservative. As optimistic as I was about progress in AI in 1999, what we have started to see in the early 2020s far surpasses anything that I could have imagined happening in my lifetime. Among many other things, AI, as our personal agent, will soon post, find, apply for, and even acquire jobs for us.

Traditionally, for every job opening at our company, we write job descriptions. Today, a hiring manager must first take the time to write that job description or work alongside someone in HR to create it. Once we post a job, we typically get several hundred résumés for every opening. Then we have our talent acquisition team go through those résumés, which is incredibly tedious work. Because these screeners can realistically spend only a few seconds on each résumé, their eyes likely focus on things like name brands of previous employers, keywords in previous job titles, or degrees from well-known universities. I can imagine that sometimes they are in the mood to dig a bit deeper, but sometimes they aren't. And this is before we come to any personal biases they bring to the table. This process is likely to miss a lot of great applicants, especially those who didn't spend time at well-known companies or schools.

Regardless, our recruiters flag a small pool of candidates for phone screens and possibly more interviews. If they pass the thirty-minute phone screen based on the recruiter's subjective judgment, the candidate will then be interviewed by four to six team members in the function that they are applying for. Due to scheduling complexity, this takes days or weeks. It's also quite expensive: six one-hour interviews from team members who make the equivalent of one hundred dollars an hour costs us six hundred dollars, before even considering the time to prep and debrief. To avoid inconsistency, we give interviewers a framework of questions. Yet, at the end of the day, the preferences and the mood of the interviewer likely dictate where the conversation goes. Eventually, we hope to get a signal that we have found a top choice and cross our fingers that we are making a good hire. It is a similar process almost everywhere.

This is an imperfect process on both sides. We probably overlooked some great candidates, and it took a lot of time and energy to eventually make an offer, knowing that it may not even be a perfect match. On top of that, by its very nature the process was not completely consistent. I also wouldn't be surprised if people's personal biases tilted things for or against certain applicants based on things that were not relevant to the job.

If all parties had infinite time and energy, the recruiters and the hiring managers would engage in in-depth conversations with every person interested in that job. They would apply the same standard consistently and be in the same mood and have the same level of enthusiasm for every candidate. Even better, we would be able to audit this process for bias by running test candidates through it. On top of that, in an ideal world it would take hours—not days or weeks—to come to a decision.

This might seem unrealistic, but it may now very well be possible to approach this ideal.

Large language models can already streamline the standard process in fairly obvious ways. Recruiters can use them to help draft job posts and interview questions. Candidates can use them to create cover letters and résumés. This, however, is just the tip of the iceberg.

In the future, if you are looking for a job, an application leveraging a large language model will create an interactive résumé that communicates with the AI that has posted the job you are applying for. Instead of just submitting a résumé and a cover letter into a void, every job candidate might be able to automatically have a rich conversation with the AI recruiter, an experience that potentially gives each person a much fairer shot.

You will not even have to wait for the employer's bot to schedule time with you. Rather, the employer's recruiter bot can talk to your agent bot. This agent will have learned to represent you accurately based on your employment history and extensive interviews that it has conducted with you.

Because it has been with you since you were a student, and because you've given it permission to access your entire work and education history, your skill sets, your interests, and even work samples you have produced, it will ask you what you are looking for in a job and the types of roles you might like to explore. It can even help coach you to think through career and education possibilities that match your life goals. Consider it as a supercharged life coach that learns to represent you to potential employers.

From there, it will look for the right positions on your behalf by talking to other AI bots that have posted these jobs. As a job seeker, your AI job agent could read literally every job posting that is out there. If you are looking to switch careers but do not have the right experience that 99 percent of employers want, your AI job agent could find the 1 percent willing to give you a shot. Your AI job agent might then report back that it just talked to a thousand employers and found a number that really value the fact that you are an outsider. Imagine a generative artificial intelligence that can solicit people over LinkedIn and reply to you with opportunities.

These bots, in theory, can have infinite conversations with each other and eventually glean a signal about a best fit for both parties. At the end of the day, if I am the hiring manager, my AI recruiting assistant will offer me the top five to ten people it thinks I should talk to based on the simulated conversations it has had with all the candidates' AI agents.

This isn't limited to just screening candidates. The AI recruiting assistant will be capable of engaging with the references that the candidate has provided. Based on this, it can further refine its recommendations.

Even during the live interview with the candidate, the AI recruiting assistant could whisper into your ear good follow-up questions or provide real-time feedback to ensure that you're interviewing as fairly and consistently as possible.

The job application and hiring process will become far more equitable, faster, and less resource-intensive for everyone. Each person, or at least their AI agent, gets an interview with the hiring AI. Every company gets a chance to get to know you through your AI agent. They will go through your entire job history. I can imagine a world in which the traditional résumé may no longer be relevant or useful because your AI agent will do a much better job of representing you.

It's worth acknowledging that this might make some people uneasy. In fact, one of the biggest fears around AI is the bias it might introduce while screening résumés or interviewing candidates. I'll be the first to admit that it will be near impossible to create a system that is free of bias. Yet I'd argue that AI will be an improvement when it can be demonstrably less biased and more consistent than the status quo, which is subjective and full of bias. Yes, we should heavily scrutinize any AI systems that claim they can assist in the recruiting process, but I also think that eventually you will have the tools that have not only made the process more inclusive and efficient but have also made it far less biased.

Where This Leaves Us and Where It Will Take Us: A Call for Educated Bravery

Ihave a confession. I once thought I would be an AI researcher. I viewed, and still view, intelligence and perception—which are two different things—as the biggest mysteries of the universe. I was fascinated by the idea of being able to build something as smart, or even smarter, than any of us. I had read nearly every science-fiction book on the topic. I loved thinking about how we could prove whether another being was truly sentient. After all, we can only directly perceive our own perception. It is really a leap of faith that other creatures—including other people in our life—are truly sentient versus just acting like they are. The best way to understand intelligence, I once thought, was to construct machines that are capable of it.

When I was a freshman at MIT in 1994, I was lucky to have direct access to several of the titans of AI at the time. I sought out Patrick Henry Winston to be my freshman adviser. He was the director of the MIT Artificial Intelligence Laboratory and author of the canonical textbook on artificial intelligence at the time. I took his class, Introduction to Artificial Intelligence. I also took Marvin Minsky's class, Society of Mind. Minsky was Winston's mentor and the founder of the Artificial Intelligence Laboratory. He also won computer science's highest award—the Turing Award—for "his central role in creating, shaping, promoting, and advancing the field of Artificial Intelligence." His ideas were considered foundational for the field of artificial neural networks. He was also the AI adviser to Stanley Kubrick when he made perhaps the most famous AI film of all time, 2001: A Space Odyssey .

These professors were incredibly intelligent, creative, and inspiring, but I found myself disappointed in where the field was and how slowly it seemed to be developing. The most impressive AI systems that could play games like chess were just good at anticipating decisions several moves ahead. No matter how proficient these systems got, no one really believed that they would be intelligent in the same way we are. Artificial neural nets were compelling from a philosophical point of view, but they weren't really capable of doing anything truly mind-blowing at the time. There hadn't been any big, new ideas in twenty or thirty years. Little did I know that this was the tail end of what would later be considered an "AI winter" among researchers.

So I decided to move on. I still loved computer science and thought that I would eventually try to start some type of tech company. But questions around intelligence and, by extension, education continued to draw me in because they seemed so fundamental to the advancing of society. The summer after my junior year, I received a fellowship to create software that allows students to learn and practice math at their own time and pace. Sound familiar?

I started to believe that people had a lot of latent, unused potential. For every person born with the raw material to be Albert Einstein or Marie Curie, how many get the education and support to do so? What if, with broader, more accessible education, we could increase by a factor of ten or one hundred the number of people capable of making the next major scientific, artistic, or entrepreneurial leap for us all? How many more diseases might we cure? How much faster might we explore the cosmos?

My curiosity wasn't just about fostering genius. If everyone had access to truly great education, I wondered, how many more billions of people might attain purpose and meaning in their lives?

But practical reality was there in the background. I grew up in a single-mother household. My parents separated shortly after I was born, and I only met my father once before he died when I was fourteen. He was a pediatrician and came from a prominent family of politicians and academics in Bangladesh, but we never received any financial support because I think he was barely making it himself. When he died, my sister and I inherited a Nissan Sentra that had more debt on it than it was worth. The only narrative I can piece together is that he and my mother were wildly incompatible, as they had an arranged marriage, and he likely suffered from depression. For most of my life, my mother was a cashier at various convenience stores, making enough money to be slightly below the poverty line. MIT was generous with financial aid, but I still had about thirty thousand dollars in debt upon graduation. The tech boom was heating up, and when I found out that I could make eighty thousand dollars a year as a new computer science graduate, which was about five times what my mother was making, I could not pass the opportunity by and took a job at Oracle Corporation.

I later went to business school and found myself as an analyst at a hedge fund. My then fiancée and now wife would give me grief about how I wasn't doing anything helpful for humanity with my talents and education. I found investing to be intellectually fascinating, though. It allowed me to study how the world worked, along with the animal spirits of the market. I also needed the money. I had further debt to pay from business school. I also knew that I was going to support my mother and other family members, and I was pretty determined to not perpetuate the financial insecurity that I grew up with. If I'm honest, I'm still more insecure about this than most of my friends. I would also tell folks that I was only going to do this until I was independently wealthy so that I could start a school on my own terms. I had some ideas about one day being the Dumbledore at a school that focused on putting students at the center and giving much more time and space for them to explore their passions.

It was at that time, in 2004, when I had family from New Orleans visiting me in Boston after my wedding. It came out of a conversation with my aunt that my twelve-year-old cousin Nadia was having trouble in math, and I offered to tutor her remotely. That led to the beginning of Khan Academy, which at its essence has been all about trying to scale the type of personalized learning that I did with Nadia to hundreds of millions of learners, across subjects, grades, and geographies.

Through the years, many people have asked me why I set up Khan Academy as a nonprofit. After all, my previous career was very for-profit, and I live in the middle of Silicon Valley, where scalable tech-enabled solutions can be worth a lot of money. Many have been skeptical whether a nonprofit could even compete with for-profit companies. There were two notions I couldn't get out of my head, however. First, I tend to believe in market forces, but there are a few sectors—namely, education and health care—where the outcomes of market forces don't always align with our values. Education and health care are two areas where our shared values tell us that, ideally, family resources shouldn't be a limiting factor in accessing the best possible opportunities. Most of us believe that every mind and life deserves to reach its full potential.

The second notion was more grandiose, if not outright delusional. One of my favorite sets of books is the Foundation series by Isaac Asimov. It takes place tens of thousands of years in the future, when humanity has colonized the galaxy, unified under one empire. Within that empire, an academic by the name of Hari Seldon has developed a new field called psychohistory—something of a combination of history, economics, and statistics—that can probabilistically predict large-scale historic trends. This science tells him that the Galactic Empire will enter a ten-thousand-year dark age within the next few hundred years. This will be ten thousand years of war, famine, and lost knowledge. His calculations show that nothing can prevent the coming dark ages, but it can be shortened. So he starts a foundation at the periphery of the galaxy to preserve knowledge and technology, which can then be used to shorten the coming chaos to "only" one thousand years. The book series mainly focuses on how the ensuing hundreds of years actually play out.

When I first read the Foundation series in middle school, I found it inspiring to think along those time scales. It was also the first moment I truly appreciated that the strength of a civilization doesn't lie in its physical size, power, and wealth. Those are just by-products of where the real strength lies: a society's culture, know-how, and mindset.

Jump ahead to when Khan Academy was beginning, and I realized that very few people in our society think on a scale of more than a few years or decades, much less hundreds or thousands of years. Beyond this, the internet was clearly the transformational technology of our time, but no real institutions were being built with it. I began to wonder whether Khan Academy might just be able to become one of the first of them; something that could help educate billions of people for hundreds of years to come. It would be like Hari Seldon's foundation, except in our case, we could uplift humanity so that the present moment would feel like a dark age when looked back upon from fifty or a hundred years in the future. We only have one life—why not swing for the fences?

As Khan Academy grew and scaled from tens to hundreds of millions of people, that dream seemed to feel less and less delusional. Amazing people came out of the woodwork to help us. By the fall of 2009, I had quit my hedge fund job to work on Khan Academy full time. Ten months later, my family was quickly depleting our savings. Our first child was born, and I was having trouble sleeping because of financial stress and, to some degree, the shame of giving up a lucrative job for something that didn't seem to have a future. At what seemed like Khan Academy's darkest moment, Ann Doerr—who is now our chairperson—and John Doerr miraculously showed up and donated enough money for me to keep going. Since then, hundreds of thousands of people have donated to support us. Despite being a nonprofit, we have been able to build a team that rivals those of the most resource-rich tech companies. Hundreds of incredibly talented people have committed a major part of their careers to be part of the Khan Academy team, often taking considerable pay cuts to do so. Thousands of volunteers all over the world have now translated Khan Academy into over fifty languages. Inspirational leaders like Bill Gates, Reed Hastings, and Elon Musk have become some of our biggest supporters and advocates. This journey seems so serendipitous that it has become something of an inside joke among the Khan Academy team that perhaps benevolent aliens are helping us so that, through education, we can prepare humanity for first contact.

This narrative seemed to be reinforced when Sam Altman and Greg Brockman of OpenAI reached out to us before anyone else with a technology that seemed to tie together every thread of my journey. GPT-4 was built on years of important innovations from many people and companies, but it was the first AI technology that truly made me wonder whether I was dreaming (or perhaps living in a simulation). It surpassed anything that, back in 1994, aspiring AI researcher Sal could have ever imagined happening in his lifetime. More important, it was the potential missing piece to our goal of delivering a truly world-class education for anyone, anywhere. I realized that as thrilling as it would be to be an AI researcher now, it was even more exciting to think about how the technology could be applied to help human potential.

This is not something to be taken lightly; there is real urgency here. Despite making us far more productive as a whole, this technology also has the potential to displace or disrupt many industries and jobs. The traditional labor pyramid—with less-skilled manual labor forming the bottom layer, bureaucratic white-collar jobs making up the middle layer, and highly skilled knowledge work and entrepreneurship making up the top—no longer applies. Robotics, including self-driving cars and trucks, is going to dramatically reduce the need for humans in that bottom layer. Generative AI can clearly perform large aspects of the work of the middle, white-collar layer and even parts of today's most skilled professions. A society in which all the productivity and resulting wealth accrues to only the tippy-top of the traditional labor pyramid, likely concentrated in Silicon Valley, with many others out of work, will not be a stable one. It might lead to massive wealth redistribution efforts. This scenario is dystopian because most people aren't looking for a handout. Rather, they want to have a sense of purpose and a feeling of contributing to the world.

The real solution is to invert that labor pyramid so that most people can operate at the top and use AI and other technology for their own productivity and entrepreneurship. The only way we have a hope of doing this is to use the same AI technology to lift the skills of a large chunk of humanity in the coming decades.

Few people may view the Star Trek universe through an economic lens, but doing so provides a window into a world that might soon be upon us. All of classical economics is based on the notion of scarcity—namely, that there isn't usually enough of anything to give everyone everything they want or need. Because of that, we use markets and pricing to allocate those goods, services, and resources to where they might result in the highest benefit. In Star Trek , however, there isn't much scarcity. Technology has allowed that society to replicate any food they want, transport themselves thousands of miles in the blink of an eye, communicate over light-years, and travel among the stars. All of humanity in that world has been fully educated so that they can participate in this bounty. Everyone is an explorer, researcher, engineer, artist, doctor, or counselor. Generative AI has the potential to allow many dimensions of our own society to be similarly low scarcity or highly abundant. Do we have the will to take us to the utopia of Star Trek ?

If we don't, societies will increasingly fall prey to populism. People with time but no sense of purpose or meaning don't tend to be good for themselves or others. They are susceptible to the ideas of demagogues. Generative AI can be used to move us in this negative direction by reinforcing "fake news" with fabricated videos and images. It can be used by governments to police their own populations much more tightly than anything George Orwell imagined in 1984 . For decades it has been possible to put cameras and sensors throughout a city and to tap phone lines, but it was difficult to monitor all the information and make sense of it. AI could soon flag any recording or observation that seems like disobedience to the eyes of the state. Big Brother will not only be watching but will also have comprehension.

Without proper countermeasures and AI literacy, people will also fall victim to increasingly sophisticated fraud. In the near future, expect to get phone calls, or even video chats, from something that looks like your family member, telling you that they are in an emergency and that you need to wire them money.

AI will increasingly play a role in national security. Foreign enemies will have the capability to orchestrate increasingly sophisticated attacks on infrastructure using this technology—attacks that potentially involve manipulating human beings. AI-generated images of people waiting in line to get their deposits could go viral on social media and start a run on banks. State and nonstate actors will use generative AI within social media to try to influence the outcomes of our elections and make us more divided as a society. The best tacticians in the battles of the future are likely to be AI, not human.

These very real possibilities may motivate some to advocate for slowing down innovation. Honestly, even I find the pace of its development dizzying. But the genie is out of the bottle, and the bad actors are not about to slow down because we want them to. Today, the good actors have the edge, but it really is a race. The countermeasure for every risk is not slowing down; it is ensuring that those favoring liberty and empowering humanity have better AI than those on the side of chaos and despotism.

This moment can be an existential risk or an existential opportunity for us. People have every right to be both scared and hopeful of what this leap in technology and innovation means. I do not, however, think our fate is subject to the flip of a coin. Rather, each of us is an active participant in the decision about how we will use AI moving forward. If we act with fear, the rule followers might pause, but the rule breakers, from totalitarian governments to criminal organizations, are going to accelerate their development of AI. The only way that we can ensure that we are closer to reaching a utopian Star Trek scenario is if we double down our efforts on using large language models for the good of society.

This is not a drill: generative AI is here to stay. The AI tsunami has drawn back from the shore, and it is now barreling toward us. Faced with the choice between running from it or riding it, I believe in jumping in with both feet, while taking proper precautions so that we don't get hit with the flotsam.

Each of us has an obligation to make sure that we use this technology responsibly. This means that as developers we must put the necessary guardrails on it to protect our children. When problems arise, we should apply reasonable regulations, regulations that don't give an edge to rule breakers. All the while, we must accelerate our efforts and make sure that we are developing the technology with the right intent and the right pedagogy. This will allow us to accelerate the improvement of human purpose and potential. Let's use AI to create a new golden age for humanity, a time that will make today look like a dark age. From my vantage point, nothing could be more inspiring and important than that.

Acknowledgments

Thank you to Umaima Marvi for being the best life partner, whose support has been instrumental in every step of this journey.

Imran, Diya, and Azad, who inspire me every day to try to make myself and the world a little bit better. Masooda Khan, for raising me as a single mother and teaching me to persist when things get tough. Farah Khan, for being my first teacher and mentor. Naseem Marvi, for being an amazing mother-in-law, listener, and friend. Polly, for being the ideal foot warmer during much of the book-writing process.

Nadia, Arman, Ali, Azad, and Nazrat Rahman, for helping to plant the seed for all this. Dan Wohl, for being a great boss and mentor who was cool with me working on Khan Academy on the side while working for him.

Ann (our board chair) and John Doerr for believing in and supporting this effort from the beginning.

Shantanu Sinha, Ben Kamens, Jason Rosoff, and Bilal Musharraf, Khan Academy's first team beyond me that helped turn it into a real organization.

Bill Gates, Jorge Paulo and Susanna Lemann, Carlos Rodriguez-Pastor, Reed Hastings, Dan Benton, Scott Cook, Signe Ostby, Ratan Tata, Carlos Slim, Tony Slim, Eric Schmidt, Elon Musk, David Siegel, Laura Overdeck, John Overdeck, Laurene Powell Jobs, David Stiles Nicholson, Carlyse Ciocca, Erica and Feroz Dewan, Ray and Barbara Dalio, Bob Hughes, Jack Little, Jeanne O'Keefe, Craig Santos, Charles, Liz, Chase, and Elizabeth Koch, Brian Hooks, Janine and Jeff Yass, Ravenel Curry, Laure and Guillaume Pousaz, Ross Annable, Lonnie Smith, Mark and Debra Leslie, Chuck Kung and Lisa Guerra, Larry Cohen, Sundar Pichai, James Manyika, Satya Nadella, Shantanu Narayen, Dharmesh Shah, Jack Dorsey, Jeb Bush, Sean O'Sullivan, Ted Mitchell, Patricia Levesque, Curtis Feeny, Sanjiv Yajnik, Fareed Zakaria, Arne Duncan, Tom Friedman, Diane Greene, Walter Isaacson, Todd Rose, David Coleman, Sameer Sampat, Dianne Seeman, Yuri Milner, Henry McCance, Geraldine Acuna-Sunshine, Craig McCaw, Susan McCaw, Tim Reynolds, Scott Heimlich, Eduardo Cetlin, Gisèle Huff, Jerry Hume, Darren Woods, Robert Bradway, Gary Wilson, Jeff and Tricia Raikes, Bobby Kotick, Mason and Logan Angel, Angela Duckworth, Ethan Mollick, Chris Anderson, and Francis Ford Coppola, for being incredible supporters, advisers, and mentors.

Greg Brockman, Sam Altman, and Jessica Shieh, for your partnership on this AI journey.

The entire Khan Academy team, including the individuals below who were instrumental in our early concepts and launch:

Engineering: Paul Morgan, Shawn Jansepar, Sujata Salem, Jason Chancey, Pepper Miller, Mark Sandstrom, Sean Driedger-Bauer, John Resig, Kelli Hill, Chase Carnaroli, Jason Voll, Jack Zhang, Hunter Liu, RJ Corwin, Salman Omer, Zachary Plummer, Alice Pao, Jeanette Head, Brian Genisio, Jonathan Price, Liz Faubell, Mahtab Sabet, Robert Pippin, Sarah E.S. Proffitt, Walt Wells, Matthew Curtis, Ned Redmond, Nicole Watts, Rachel Roberts, Sarah Third, David Braley, Kathy Phillip, Luke Smith, Andrew Pagan, Alex Morelli, Maddy Andrade-Ozaette, Amos Latteier, Elise McCrorie, Divya Chandrasekar, Emily Janzer, Ian Powell, Adam Berkan, Adam Goforth, Patrick McGill, Matt Morgan, Boris Lau, Erik Helal, Michael Mendoza, Nathan Dobrowolski, Kevin Barabash, Gerardo Gonzalez, Gina Valderrama, Danielle Whyte, Tim McCabe, Craig Silverstein, Miguel Castillo, Reid Mitchell, Cat Yannish.

Product, content, and design: Kristen DiCerbo, Ricky Chandarana, Adrienne Hunter Wong, Dave Travis, Laurie LeDuc,  Daniel De Angulo, Sarah Robertson, Gintas Bradunas, Tommy Day, Susan August, Elvira Valdez, Corey Kollbocker, Jess Hendel, Heather Meston, Charlie Auen, Jeff Dodds, Nick Kokkinis, Anya Bila, Jonah Goldsaito, Lan Borg, Karen Shapiro.

Thank you to Stacey Olson, Vicki Zubovic, Regina Ross, Julian Roberts, Rachel Boroditsky, Julia Cowles, Sandeep Bapna, Jeremy Schifeling, Jason Hovey, Ted Chen, Craig Silverstein, Diana Olin, Jordan Peavey, Evan Rahman, Eirene Chen, Barb Kunz, Felipe Escamilla, Jesse Ambrose, for supporting and leading key parts of our work.

Special thanks to visionary education leaders including Dr. Katie Jenner, Commissioner Frank Edelblut, Superintendent Peggy Buffington, Dr. Jose Fuentes, Alan Usherenko, Tim Nellegar, and all the remarkable educators and students at Newark Public Schools, School City of Hobart, and our partner districts in Indiana and across the country.

My thanks to Richard Pine, Eliza Rothstein, Inkwell Management, Ibrahim Ahmad, Lee Kravetz, Carolyn Coleburn, Yuleza Negron, Bridget Gilleran, Molly Fessenden, Barb Kunz, Elizabeth Pham Janowski, Alex Cruz-Jimenez, Carrie Cook, Suzanne Roberts, Tom Greene, Tomer Altman, Joanna Samuels, Roger Studley, Eric Berson, Mimi Kravetz.

Huge thanks to Jeremiah Hennessy for convincing me to quit my day job in 2009.

I'd like to thank the hundreds of thousands of people who have donated to make Khan Academy possible and the hundreds of millions of learners, parents, and teachers who have chosen to use Khan Academy to uplift themselves and those they care about.

Last, but not least, thank you benevolent aliens for helping us help prepare humanity for first contact. Onward!