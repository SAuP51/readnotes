Ethan Mollick.(2024).2024029Co-Intelligence_Living-and-Working-with-AI.Penguin Publishing Group => 

9 AI AS OUR FUTURE

This book may seem as if it is full of science fiction, but everything I am describing has already happened. We have created a weird alien mind, one that isn't sentient but can fake it remarkably well. It is trained on the vast archives of human knowledge, and also on the backs of low-paid workers. It can pass tests and act creatively, with the potential to change how we work and learn; but it also makes up information regularly. You can no longer trust that anything you see, or hear, or read was not created by AI. All of that already happened. Humans, walking and talking bags of water and trace chemicals that we are, have managed to convince well-organized sand to pretend to think like us.

What happens next is science fiction—or rather, science fictions, because there are many possible futures. I see four clear possibilities for what will happen in the next few years in the world of AI. The implications of each possibility, however, are less clear. I want to lay out each possibility for you, and what the world may look like as a result.

Let us start with the most unlikely future, which, disturbingly enough, is not the possibility of AGI. Far less likely is the possibility that AI has already reached its limits, but that is where we will start.

Scenario 1: As Good as It Gets

What if AIs stop making huge leaps forward? Sure, there may be small improvements here or there, but in this future, they are vanishingly small compared to the huge leaps that we saw from GPT-3.5 and GPT-4. The AI you are using now really is the best you will ever use.

From a technical perspective, this seems like an unrealistic outcome. There is no reason to suspect that we have hit any sort of natural limit in the ability of AIs to improve. That doesn't mean LLMs will inevitably keep getting smarter: researchers have identified many possible problems with their underlying architecture and training that may, at some point, limit how capable they can become. For example, the AI systems may run out of data to train on; or the cost and effort of scaling up the computing power to run AIs may become too large to justify. But there is little evidence that the limitations have already been reached, and even if they were, there are other tweaks and changes that could be made to LLMs to squeeze more out of the systems for years to come. And LLMs are just one approach to AI; other successor technologies may overcome these limits.

Slightly more possible is a world where regulatory or legal action stops future AI development. Maybe AI safety experts convince governments to ban AI development, complete with threats of force against any who dare breach these limits. But, given that most governments are only just beginning to consider regulation, and there's a lack of international consensus, it seems extremely unlikely that a global ban will happen soon or that regulation will make AI development grind to a halt.

Nonetheless, this scenario seems to be the one most people and organizations are planning for. And I understand that denial. Most people didn't ask for an AI that can do many tasks previously reserved for humans. Teachers did not want to see almost every form of homework instantly be solvable by a computer. Employers did not want highly paid tasks that are only meaningful when done by humans (performance reviews, reporting) to be done by machines instead. Government officials did not want a perfect disinformation system released without any useful countermeasures. The world got much stranger, very quickly.

So it is not surprising that so many people are trying to deal with the implications of AI by assuming that nothing is going to change, by banning it permanently, or even imagining that the changes brought by AI can be easily contained. As we have seen, those policies are not likely to work. Worse yet, the substantial benefits of AI are going to be greatly reduced by trying to pretend it is just like previous waves of technology, where changes take decades to occur.

Even if AI doesn't advance further, some of its implications are already inevitable. The first set of certain changes from AI is going to be about how we understand, and misunderstand, the world. It is already impossible to tell AI-generated images from real ones, and that is simply using the tools available to anyone today. Video and voice are also trivially easy to fake. The online information environment is going to become completely unmanageable, with fact-checkers overwhelmed by the flood. It is only slightly harder to create fake images now than to take real photographs. Every image of a politician, celebrity, or a war could be made up—there is no way to tell. Our already fragile consensus about what facts are real is likely to fall apart, quickly.

Technological solutions are unlikely to save us. Attempts to track the provenance of images and videos by watermarking AI creations can be defeated with relatively simple alterations to the underlying content. And that assumes that the people faking images and videos are using commercial tools—identifying AI-generated content will become even harder as governments develop their own systems and open-source models proliferate. Perhaps, in some future, AI can help us filter through the chaff, but AIs are notoriously unreliable at detecting AI content, so this seems unlikely as well.

There are really only a couple of ways that this could work out. Perhaps there will be a resurgence of trust in mainstream media, which might be able to act as arbiters of what images and stories are real, carefully tracking the provenance of each story and artifact. But that seems unlikely. A second option is that we further divide into tribes, believing the information we want to believe and ignoring as fake any information we don't want to pay attention to. Soon, even the most basic facts will be in dispute. This growth of ever more insular information bubbles seems much more likely, accelerating the pre-LLM trend. A final option is that we turn away from online sources of news entirely, because they are so polluted with fake information that they stop becoming useful. Regardless of which direction we head in, even without advances in AI, the way we relate to information will change.

Our personal relationship with AI will change as well. Current systems are already good enough to seem human, and with small amounts of tuning, research has shown that AIs can be even more engaging, perhaps worryingly so. One large experiment on a platform with millions of users shows that training a model to produce results that keep people chatting leads to 30 percent more user retention and much longer conversations. That suggests that, even without technological advancement, chatting with bots is going to get significantly more compelling. Current systems are not good enough to be deep conversation partners, but we may start to see individuals choosing to interact more with AI than with humans.

And other trends we have already discussed are now also inevitable. Even assuming no further improvement in LLMs, AIs will have a large impact on the tasks of many workers, especially those in highly paid creative and analytical work. However, AIs in their present state leave plenty of room for Cyborg tasks, and human ability exceeds AI abilities in many cases. While work will change if AI did not develop further, it would likely operate as a complement to humans, relieving the burden of tedious work and improving performance, particularly among low performers. That doesn't mean some jobs and industries will not be under threat—most translation work, for example, is likely to be largely displaced by AI—in most cases, though, AI would not replace human labor. Current systems are not good enough in their understanding of context, nuance, and planning.

That is likely to change.

Scenario 2: Slow Growth

AI has been increasing in ability at an exponential pace, but most exponential growth in technology eventually slows down. AI could hit that barrier soon. Practically, that means that rather than increasing in capacity by ten times a year, growth slows, increasing maybe 10 percent, or 20 percent, a year instead. There are many reasons this might happen. Ballooning training costs and regulatory requirements are possibilities. So is the possibility that we will soon hit technical limits for Large Language Models, as a number of scientists, including professor (and chief AI scientist at Meta) Yann LeCun, have argued. This will require us to find new technological approaches to developing AI in order to continue. However it happens, this slower improvement would still represent an impressive rate of change, though one we can understand. Think of how televisions get a bit better every year. You don't need to throw out your old TV, but new ones are likely quite a bit better and cheaper than the one you bought a few years ago. With this sort of linear change, we can see the future coming and plan for it.

Everything that happens in Scenario 1 still happens. Bad actors still use AI to fake online information, but as time goes on, the ability of AI to do more complex work makes them more dangerous. Your email inbox becomes flooded with precisely targeted messages that are personalized to you (ad companies are already doing personalized videos for millions of users via AI), some of which are scams or phishing attempts. You receive phone calls in the voice of loved ones asking for bail money. During the next war, every Defense Department official begins receiving very specific threatening text messages featuring AI-generated videos of their family. Incompetent criminals and terrorists take advantage of the ability of AI to raise performance to become more effective killers.

These are scary possibilities, but because AI is advancing in a measured pace, the worst bad outcomes don't come to pass. Early incidents where AIs are used to generate dangerous chemicals or weapons could result in effective regulation to slow down the proliferation of dangerous uses. Coalitions of companies and governments, or perhaps open-source privacy advocates, could have the time to develop usage rules that allow people to establish their identity in a way that can be verified, removing some of the threat of impersonation.

And every year the personas generated by AIs get more realistic, pushing the frontiers further. Video games are populated by AI-generated nonplayer characters, and the first personalized AI movies begin to appear, in which you can select the way scenes or characters play out. It becomes more normal to use an AI therapist, and interacting with a mix of real humans and AI chatbots becomes a regular way we do business. Again, the slower growth of AI provides an opportunity for society to adjust to this change. Laws require AI content to be labeled, and social norms around using chatbots as friends continue to make sure that most people spend their time with real human beings.

Work becomes ever more transformed. Each year, AI models do more than they were capable of the year before, creating waves that ripple across industry after industry. First, the $100 billion a year call-center market is transformed as AI agents start to supplement human ones. Next, most advertising and marketing writing are done primarily by AI, with limited guidance from human Cyborgs. Soon, AI is performing many analytical tasks and doing increasing amounts of coding and programming work. By and large, though, the slower pace of change means that this wave of disruption looks like that of past General Purpose Technologies. Tasks change more than jobs, and more jobs are created than destroyed. A major focus on retraining and focusing skills on working with AI helps mitigate the worst risks.

But the first society-wide benefits also begin to appear. Innovation has been slowing alarmingly in recent decades. In fact, a recent, convincing, and depressing paper found that the pace of invention is dropping in every field, from agriculture to cancer research. More researchers are required to advance the state of the art. In fact, the speed of innovation appears to be dropping by 50 percent every 13 years, slowing down economic growth.

Part of the issue appears to be a growing problem with scientific research itself: there is too much of it. The burden of knowledge is increasing, in that there is too much to know before a new scientist has enough expertise to start doing research themselves. This is also why half of all pioneering contributions in science now happen after age forty, when it used to be that younger scientists were the ones who achieved breakthroughs. Similarly, start-up rates of STEM PhDs are down 38 percent in the last 20 years. The nature of science is growing so complex that PhD founders now need large teams and administrative support to make progress, so they go to big firms instead. Thus, we have the paradox of our Golden Age of science. More research is being published by more scientists than ever, but the result is actually slowing progress! With too much to read and absorb, papers in more crowded fields are citing new work less and canonizing highly cited articles more.

But there are already signs that AI can help. Research has successfully demonstrated that it is possible to correctly determine the most promising directions in science by analyzing past papers with AI, ideally combining human filtering with the AI software. And other work has found that AI shows considerable promise autonomously conducting scientific experiments, finding mathematical proofs, and more. It may be that the advances in AI can help us overcome the limitations of our merely human science and lead to breakthroughs in how we understand the universe and ourselves. Many of the original AI enthusiasts are, in fact, counting on the power of AI to figure out ways to radically extend and improve human lives. While linear growth in AI capabilities may not be able to achieve this lofty goal, if it is even achievable, it may help restart the slowing engines of progress.

It is possible to see this scenario as gently turning up the temperature over time. AI comes to take a larger and larger role in our lives but gradually enough that disruption is manageable. And we also start to see some of the major benefits of AI as well: faster scientific discovery, increased productivity growth, and more educational opportunities for people all over the world. The results are mixed, but largely positive. And humans remain in control of the direction that AI takes.

But AI has not been advancing in a linear way.

Scenario 3: Exponential Growth

Not all technological growth slows down quickly. Moore's Law, which has seen the processing capability of computer chips double roughly every two years, has been true for fifty years. AI might continue to accelerate in this way. One reason this might occur is the so-called flywheel—AI companies might use AI systems to help them create the next generation of AI software. Once this process starts, it may be hard to stop. And at this pace, AI becomes hundreds of times more capable in the next decade. Humans are not very good at visualizing exponential change, and so our vision starts to rely far more on science fiction and guesswork. But we can expect massive changes everywhere. Everything in Scenario 2 happens, but at a much, much, much faster pace that we find correspondingly more difficult to absorb.

In this scenario, risks are more severe and less predictable. Every computer system is vulnerable to AI hacking, and AI-powered influence campaigns are everywhere. AIs, still controlled by humans, generate dangerous new pathogens and chemicals, helping governments and terrorist cells achieve new methods of destructiveness. There were already signs of this occurring with primitive, pre-LLM AIs: AI researchers building a tool to find new drugs to save lives realized it could do the opposite, generating new chemical warfare agents. Within six hours, it invented deadly VX nerve gas . . . and worse things. With widespread, powerful AIs, militaries and criminals use AI to amplify their efforts. And unlike in the previous scenario, our current governmental systems do not have time to adjust in the usual way.

Instead, these AI bad actors are held in check by "good" AIs. But there is an Orwellian tinge to this solution. Everything we see needs to be filtered through our own AI systems to remove dangerous and misleading information, creating its own risk of filter bubbles and bad information. Governments use AI to crack down on AI-powered crime and terrorism, creating the danger of AI-tocracy, as ubiquitous surveillance enables both dictators and democracies to establish more control over the citizens. The world looks more like a cyberpunk struggle between authorities and hackers, all using AI systems.

AI companions become far more compelling to speak with than most other people, and can communicate seamlessly with us in real time, a change that happens faster than anyone expected. Loneliness becomes less of an issue, but new forms of social isolation emerge, in which some people would rather interact with AIs than with humans. AI-powered entertainment provides incredibly customized and unique experiences that mix games, stories, and movies. This doesn't mean that everyone becomes an introvert, speaking only to artificial intelligences. They are still not sentient in this scenario, and humans will still want to do human things with other people.

And in working with people, AI can help unlock human potential. AI therapists and assistants help people who want to improve themselves do so in new ways. The ability to use AI to accomplish tasks in days that would otherwise take years allows new types of entrepreneurship and innovation to flourish. I have already spoken to physicists and economists who are able to do much more focused research because AI serves as both a source of inspiration and a way of outsourcing time-consuming, pricey programming and grant-writing tasks. Perhaps AI companions will help us all achieve goals that were previously out of reach. And it is probably good that this is possible, because we are all likely to have more free time under this scenario.

With exponential change, AIs a hundred times better than GPT-4 start to actually take over human work. And not just office work, either, as there is some early evidence that LLMs may help us overcome the barriers that have made building working robots so challenging. AI-powered robots and autonomous AI agents, monitored by humans, could potentially drastically reduce the need for human work while expanding the economy. The adjustment to this shift, if it were to occur, is hard to imagine. It will require a major rethinking of how we approach work and society. Shortened workweeks, universal basic income, and other policy changes might become a reality as the need for human work decreases over time. We will need to find new ways to occupy our free time in meaningful ways, since so much of our current life is focused around work.

In some ways, however, this shift has already been occurring. In 1865 the average British man worked 124,000 hours over his lifetime, as did people in the US and Japan. By 1980, British workers spent only 69,000 hours at work, despite living longer. In the US, we went from spending 50 percent of our lives working to 20 percent. Work hours have improved more slowly since 1980. Still, UK workers now work 115 hours less a year than they did then, a decline of 6 percent. And similar changes are happening all over the world. Much of that extra time has been filled up with school, which is unlikely to change quickly, even if AI becomes much more capable, but we have also found many other ways to use our leisure. Adjusting to working less may be less traumatic than we think. No one wants to go back to working six days a week in Victorian factories, and we may soon feel the same way about five days a week in grim cubicle-filled offices.

Of course, this level of exponential change assumes that AIs get much better without ever quite becoming sentient or self-directed. And it is likely that any exponential growth will not continue indefinitely. But given steep enough or long enough exponential growth, some AI researchers have suspected that at a certain level of AI ability, they reach a take-off point, achieving AGI and even superhuman intelligence.

Scenario 4: The Machine God

In this fourth scenario, machines reach AGI and some form of sentience. They become as smart and capable as humans. Yet there is no particular reason that human intelligence should be the upper limit. So these AI, in turn, help design smarter AIs still. Superintelligence emerges. In the fourth scenario, human supremacy ends.

The end of human dominance need not be the end of humanity. It may even be a better world for us, but it is no longer a world where humans are at the top, ending a good two-million-year run. Achieving this level of machine intelligence means that AIs, not humans, are in charge. We have to hope they are properly aligned to human interests. They may then decide to watch over us as "machines of loving grace," as the poem goes, solving our problems and making our lives better. Or they can view us as a threat, or an inconvenience, or a source of valuable molecules.

Honestly, though, nobody knows what will happen if we successfully build a superintelligence. The results will be world-shaking. And if we don't get all the way to superintelligence, even a truly sentient machine would challenge much of what we think about what it means to be human. They would be true alien minds in every possible way, and they would challenge our place in the universe no less than finding aliens on another planet would.

There is no theoretical reason why this can't happen, but also no reason to suspect that it might. There are world experts on AI who argue both positions. But the truth is that we don't know if there is a straight road from today's LLMs to building a true AGI. And we don't know if AGI would help or hurt us, or how it would do either. Enough serious experts believe this risk is real that we need to take it seriously. For example, one of the godfathers of AI, Geoffrey Hinton, left the field in 2023, warning of the danger of AI with statements like "It's quite conceivable that humanity is just a passing phase in the evolution of intelligence." Other AI researchers speak of their p(doom), the chance that AI leads to human extinction. If the AI "doomers" are right, large-scale regulation to halt AI development forever is the only option—as unlikely as that seems.

But I think too much consideration of this fourth scenario also makes us feel powerless. If we focus solely on the risks or benefits of building super-intelligent machines, it robs us of our abilities to consider the more likely second and third scenarios, worlds where AI is ubiquitous but very much in human control. And in those worlds, we get to make choices about what AI means.

Rather than being worried about one giant AI apocalypse, we need to worry about the many small catastrophes that AI can bring. Unimaginative or stressed leaders may decide to use these new tools for surveillance and for layoffs. The less fortunate in developing countries may be disproportionally hurt by the shift in jobs. Educators may decide to use AI in ways that leave some students behind. And those are just obvious problems.

AI does not need to be catastrophic. In fact, we can plan for the opposite. J. R. R. Tolkien wrote about exactly this, a situation he termed a eucatastrophe, so common in fairy tales: " the joy of the happy ending: or more correctly of the good catastrophe, the sudden joyous ‘turn' . . . is a sudden and miraculous grace: never to be counted on to recur." Correctly used, AI can create local eucatastrophes, where previously tedious or useless work becomes productive and empowering. Where students who were left behind can find new paths forward. And where productivity gains lead to growth and innovation.

The thing about a widely applicable technology is that decisions about how it is used are not limited to a small group of people. Many people in organizations will play a role in shaping what AI means for their team, their customers, their students, their environment. But to make those choices matter, serious discussions need to start in many places, and soon. We can't wait for decisions to be made for us, and the world is advancing too fast to remain passive. We need to aim for eucatastrophe, lest our inaction makes catastrophe inevitable.

9 AI 作为我们的未来

本书看起来可能像是充满了科幻内容，但我描述的这一切都已经发生了。我们创造了一种奇特的外星意识，它虽然没有真正的感知能力，但能出色地假装具备这种能力。它是在庞大的人类知识库上训练出来的，同时也依赖于低薪工人的劳动。它可以通过测试并展现创造力，有潜力改变我们的工作和学习方式；但它也经常编造信息。现在，你无法再相信你所看到、听到或阅读的任何内容不是由 AI 创建的。这一切都已经发生了。我们人类，作为行走和交谈的水和微量化学物质的集合体，已经成功地让组织良好的沙子假装像我们一样思考。

接下来会发生什么是科幻小说，确切地说，是许多种科幻小说，因为未来有很多种可能性。我看到在未来几年内 AI 领域有四种明确的可能性。每种可能性的影响却不那么明确。我想为你一一列出这些可能性，以及由此可能出现的世界。

让我们从最不可能的未来开始，令人不安的是，这并不是指通用人工智能（AGI）的可能性。更不可能的是 AI 已经达到了极限，但这正是我们要开始讨论的。

情景 1：尽善尽美如果 AI 不再取得巨大进展会怎样？当然，可能会有一些小的改进，但在这个未来，它们与我们从 GPT-3.5 和 GPT-4 看到的巨大进步相比微不足道。你现在使用的 AI 可能就是你能用到的最好的 AI。

从技术角度来看，这种结果似乎不太现实。没有理由认为我们在 AI 改进能力上已经达到了某种自然极限。这并不意味着大语言模型（LLM）会必然越来越智能：研究人员已经发现其底层架构和训练过程可能存在许多问题，这些问题可能在某些时候限制其能力。例如，AI 系统可能会面临训练数据不足的问题；或者扩展 AI 所需的计算能力的成本和努力可能变得过于巨大而难以承受。然而，目前几乎没有证据表明这些限制已经达到了，即使达到了，仍然可以通过其他调整和改进来进一步优化大语言模型，从中挖掘更多潜力。而且，大语言模型只是 AI 的一种方法；未来可能会有其他技术克服这些限制。

更有可能的是，通过监管或法律手段来阻止未来的 AI 发展。也许 AI 安全专家会说服政府禁止 AI 的发展，甚至对任何敢于突破这些限制的人采取武力威胁。但是，考虑到大多数政府才刚刚开始讨论监管问题，而且缺乏国际共识，全球禁令很快发生或监管导致 AI 发展停滞的可能性极低。

尽管如此，大多数人和组织似乎正为这种情景做准备。我理解这种否认。大多数人并没有要求一个能够完成许多原本由人类完成任务的 AI。教师们不希望看到几乎所有形式的家庭作业都可以瞬间被计算机解决。雇主们不希望高薪且有意义的任务（如绩效评估、报告）由机器来完成。政府官员不希望一个完美的虚假信息系统在没有任何有效对策的情况下被发布出来。世界变得非常快速且陌生。

因此，很多人试图通过假设什么都不会改变、永久禁止 AI 或者认为 AI 带来的变化可以轻易控制来应对 AI 的影响，这也就不足为奇了。正如我们所见，这些政策往往无法奏效。更糟糕的是，如果我们假装 AI 只是像过去的技术浪潮那样，需要几十年才能产生变化，那么 AI 的巨大潜力将被大大削弱。

即使 AI 不再进一步发展，它的一些影响已经不可避免。AI 带来的第一个显著变化将是对我们理解和误解世界的方式产生影响。现在，使用任何人都能获得的工具，已经难以分辨 AI 生成的图像和真实图像。视频和声音也非常容易伪造，在线信息环境将变得难以管理，事实核查员将被信息洪流淹没。现在创造假图像只比拍真实照片稍微难一点。每一张政治家、名人或战争的图片都可能是捏造的 —— 没有办法辨别。我们对什么是真实事实的脆弱共识可能会迅速瓦解。

技术解决方案可能无法拯救我们。试图通过为 AI 生成的内容打上水印来追踪图像和视频的来源，这种方法可以通过对底层内容进行简单修改来破解。而且，这还假设伪造图像和视频的人使用的是商业工具 —— 随着政府开发自己的系统和开源模型的普及，识别 AI 生成的内容将变得更加困难。或许，未来某天，AI 可以帮助我们过滤这些虚假信息，但 AI 在检测 AI 内容方面是出了名的不可靠，所以这也似乎不太可能。

实际上，这种情况只有几种可能的结果。或许主流媒体的信任度会重新上升，主流媒体可能会充当裁决者，判断哪些图像和故事是真实的，仔细追踪每个故事和文物的来源。但这似乎不太可能。第二种可能是我们进一步分裂成部落，相信我们想相信的信息，并忽略我们不想关注的信息，认为它们是假的。不久后，即使是最基本的事实也会受到质疑。这种越来越封闭的信息泡沫的增长似乎更有可能，加速了前大语言模型（LLM）趋势。最后一种可能是我们完全远离在线新闻来源，因为它们充满了虚假信息，变得没有用了。不论我们走向哪个方向，即使没有 AI 的进步，我们与信息的关系也会改变。

我们与 AI 的个人关系也会改变。当前的系统已经足够好，似乎很像人类，并且经过少量的调整，研究表明 AI 可以变得更具吸引力，甚至可能令人担忧。一项在拥有数百万用户的平台上进行的大规模实验显示，训练模型以产生保持用户聊天的结果，可以使用户保留率提高了 30%，并且对话时间更长。这表明，即使没有技术进步，与机器人聊天也会变得显著更具吸引力。当前的系统还不足以成为深度对话的伙伴，但我们可能会开始看到个人选择更多地与 AI 互动，而不是与人类互动。

我们之前讨论的其他趋势现在也已经不可避免。即使大语言模型（LLM）不再进一步发展，人工智能（AI）也会对许多工人的任务产生巨大影响，特别是那些高薪创意和分析工作的工人。然而，目前的 AI 仍然为人机协作任务留有大量空间，而且在许多情况下，人类能力仍然优于 AI。即使 AI 不再进一步发展，它也可能作为人类的补充，减轻繁琐工作的负担，提高工作表现，尤其是对于低效率的工人。这并不意味着某些工作和行业不会受到威胁 —— 例如，大多数翻译工作可能会被 AI 取代 —— 但在大多数情况下，AI 不会完全取代人类劳动力。目前的系统在理解上下文、细微差别和规划方面还不够好。

这一切可能会改变。

情景 2：缓慢增长

AI 的能力正在迅速提升，但大多数技术的快速增长最终会放慢脚步。AI 可能很快就会遇到这个瓶颈。实际上，这意味着 AI 的能力增长不再是每年十倍，而是每年可能只增长 10% 或 20%。这种情况可能由多种原因导致，比如不断增加的训练成本和监管要求。另一种可能性是我们很快会遇到大语言模型的技术极限，正如包括 Meta 的首席 AI 科学家 Yann LeCun 教授在内的许多科学家所指出的那样。这将迫使我们寻找新的技术方法来继续开发 AI。尽管增长速度减缓，这种变化依然显著，但更容易理解。就像每年电视机都会稍微改进一样。你不需要扔掉旧电视，但新电视可能比几年前的更好且更便宜。通过这种线性变化，我们可以预测未来并做好准备。

情景 1 中的所有情况依然会发生。坏人仍然会使用 AI 伪造在线信息，但随着时间的推移，AI 的复杂工作能力让他们变得更危险。你的电子邮件收件箱会被精准定位的信息淹没，这些信息是专门为你个性化的（广告公司已经在通过 AI 为数百万用户制作个性化视频），其中一些是诈骗或网络钓鱼信息。你可能会接到冒充亲人的电话，要求支付保释金。在下一场战争中，每个国防部官员都会收到威胁短信，其中包含 AI 生成的其家人的视频。不称职的罪犯和恐怖分子会利用 AI 技术，使他们成为更有效的杀手。

这些确实是令人担忧的可能性，但由于 AI 正在稳步发展，最坏的情况不会发生。在 AI 被用于生成危险化学品或武器的早期事件中，可能会引发有效的监管措施，从而减缓这些危险用途的扩散。公司和政府的联盟，或开源隐私倡导者，将有时间制定使用规则，使人们能够以可验证的方式建立身份，从而减少冒名顶替的威胁。

每年 AI 生成的人物形象越来越逼真，不断推进技术的前沿。视频游戏中充满了 AI 生成的非玩家角色，第一部个性化的 AI 电影也开始出现，在这些电影中，你可以选择场景或角色的演绎方式。使用 AI 治疗师变得越来越普遍，与真人和 AI 聊天机器人互动成为我们日常生活的一部分。同样，AI 的缓慢发展为社会适应这一变化提供了机会。法律要求 AI 内容必须标明标签，社交规范也在不断调整，确保大多数人依然花时间与真人相处。

工作的变革越来越明显。每年，AI 模型的能力都在提升，逐渐在各个行业引起变化。首先，年收入达 1000 亿美元的呼叫中心市场发生了转变，AI 智能体开始辅助人类员工工作。接着，大多数广告和营销文案主要由 AI 撰写，仅需要人类 Cyborgs（生化人）的少量指导。很快，AI 将承担更多的分析任务，并进行更多的编程和编码工作。不过，总体而言，变化的速度较为缓慢，这使得这次变革看起来像过去的通用技术。工作任务的变化大于工作岗位的变化，创造的新岗位多于被取代的岗位。通过重点进行再培训和提高与 AI 协同工作的技能，可以缓解最严重的风险。

然而，全社会的好处也开始显现。近几十年来，创新速度显著放缓。事实上，最近的一篇有说服力且令人沮丧的论文指出，从农业到癌症研究，各个领域的发明速度都在下降。要推动技术进步，需要更多的研究人员。实际上，创新速度似乎每 13 年下降 50%，这也导致经济增长放缓。

科学研究本身的问题似乎在日益加剧：研究过多。知识负担越来越重，新科学家在开始独立研究前需要掌握太多的知识。这也是为什么现在所有开创性科学贡献的一半发生在四十岁以后，而过去这些突破多由年轻科学家实现。同样，STEM 博士的创业率在过去 20 年下降了 38%。科学的复杂性增加，博士创始人现在需要大型团队和行政支持才能取得进展，所以他们去了大公司。因此，我们面临科学黄金时代的悖论。更多的科学家发表了更多的研究，但结果实际上在减缓进展！由于内容过多难以消化，拥挤领域的论文引用新工作的次数减少，反而更多地引用那些被高度引用的文章。

但已有迹象表明 AI 可以提供帮助。研究已经成功地证明，通过使用 AI 分析过去的论文，可以正确确定最有前途的科学方向，理想情况下将人类筛选与 AI 软件结合。其他研究发现，AI 在自主进行科学实验、寻找数学证明等方面显示出相当的潜力。AI 的进步或许能帮助我们克服仅靠人类科学的局限，带来对宇宙和自我理解的突破。事实上，许多早期的 AI 爱好者期望 AI 的力量能找到根本延长和改善人类生活的方法。尽管 AI 能力的线性增长可能无法实现这一崇高目标（如果它可以实现），但可能有助于重新启动逐渐放缓的进步引擎。

可以把这种情况比作温度随时间缓慢上升。AI 在我们生活中扮演的角色越来越大，但变化是逐步的，因此混乱是可控的。同时，我们也开始看到 AI 带来的一些主要好处：科学发现速度加快、生产力提升，以及全球范围内更多的教育机会。虽然结果有好有坏，但总体来说是积极的。人类仍然掌握着 AI 的发展方向。

然而，AI 的进步并不是线性的。

场景 3：指数增长并不是所有的技术进步都会很快放缓。摩尔定律表明计算机芯片的处理能力大约每两年翻一番，这一规律已经持续了五十年。AI 可能会以类似的方式继续加速。这其中的一个原因是所谓的飞轮效应 ——AI 公司可能会利用 AI 系统来帮助他们开发下一代 AI 软件。一旦这个过程开始，可能很难停止。按照这样的速度，AI 在未来十年内将变得强大数百倍。人类不擅长想象指数级的变化，因此我们的预测更多地依赖于科幻和猜测。但我们可以预见到处都会发生巨大的变化。场景 2 中的所有事情都会发生，但速度快得多，以至于我们难以适应。

在这种情况下，风险变得更加严重且难以预测。每个计算机系统都可能受到 AI 黑客的攻击，而 AI 驱动的影响活动无处不在。即便是由人类控制的 AI 也能生成危险的新病原体和化学品，帮助政府和恐怖组织实现新的破坏性手段。早在大语言模型（LLM）出现之前，就已经有迹象表明这种情况的发生：AI 研究人员开发了一种寻找新药物的工具，却发现它也能生成新的化学战剂。在六小时内，这个工具就发明了致命的 VX 神经毒气…… 以及更危险的物质。随着强大的 AI 广泛应用，军队和犯罪分子都在利用 AI 来增强他们的行动。与之前的情况不同，我们现有的政府系统没有时间以通常的方式进行调整。

相反，这些 AI 恶意行为者被「友好」AI 制约。但这种解决方案有点像奥威尔小说中的情节。我们看到的所有信息都需要通过我们自己的 AI 系统过滤，以去除危险和误导性的信息，这又带来了信息过滤泡沫和错误信息的风险。政府利用 AI 打击 AI 驱动的犯罪和恐怖主义，带来了 AI 专制的危险，因为无处不在的监视使得独裁者和民主国家都能够对公民实施更多的控制。世界看起来更像是当局和黑客之间的赛博朋克风格的斗争，都在使用 AI 系统。

AI 伙伴变得比大多数人与人交流更有吸引力，并且可以实时无缝地与我们对话，这种变化比任何人预期的都要快。孤独感不再是问题，但随之而来的是新的社会隔离形式，有些人宁愿与 AI 互动而不是与人类交流。AI 驱动的娱乐提供了极为个性化和独特的体验，融合了游戏、故事和电影。这并不意味着每个人都变成了只与 AI 交流的内向者。在这种情况下，AI 仍然没有意识，人类仍然愿意与其他人一起做事。

在与人类合作时，AI 可以帮助释放人类的潜力。AI 治疗师和助手以新的方式帮助那些希望自我提升的人。使用 AI 几天内完成原本需要几年时间的任务，这种能力使新的创业和创新形式蓬勃发展。我已经与物理学家和经济学家交谈过，他们能够进行更集中的研究，因为 AI 既是灵感的来源，也是外包耗时、昂贵的编程和撰写资助申请任务的工具。也许 AI 伙伴会帮助我们实现以前无法达到的目标。这可能是好事，因为在这种情况下，我们将有更多的空闲时间。

随着技术的快速发展，性能比 GPT-4 高出一百倍的 AI 可能会开始接管人类的工作。而且不仅仅是办公室工作，有一些早期证据表明，大语言模型（LLM）可能帮助我们克服构建工作机器人所面临的障碍。由 AI 驱动的机器人和由人类监控的自主 AI 智能体，有可能显著减少对人类工作的需求，同时推动经济发展。如果这种转变真的发生，我们将需要重新思考工作和社会的意义。缩短工作周、普遍基本收入等政策变化可能会成为现实，因为对人类工作的需求逐渐减少。我们需要找到新的有意义的方式来充实我们的空闲时间，因为目前我们的生活很大程度上围绕工作展开。

实际上，这种转变已经在某些方面开始了。1865 年，英国男性在一生中平均工作 124,000 小时，美国和日本的人也是如此。到 1980 年，英国工人的工作时间减少到了 69,000 小时，尽管他们的寿命更长。在美国，工作占据人们生活时间的比例从 50% 降到了 20%。自 1980 年以来，工作时间的减少速度有所放缓，但英国工人每年的工作时间比那时减少了 115 小时，下降了 6%。类似的变化正在全球发生。大部分额外时间被用于学习，即使 AI 变得更智能，这一现象也不太可能迅速改变，但我们也找到了许多其他方式来利用我们的闲暇时间。适应减少工作时间可能不会像我们想象的那样困难。没有人愿意回到维多利亚时代的工厂里每周工作六天，未来我们可能也会对每周五天在布满隔间的办公室里工作感到同样的厌倦。

当然，这种指数级的变化假设 AI 会变得越来越强大，但不会完全拥有知觉或自我意识。而且，任何指数增长都不太可能无限期地持续下去。然而，一些 AI 研究人员认为，如果指数增长足够快或持续足够长时间，AI 在某个能力水平上可能会达到一个临界点，实现通用人工智能（AGI）甚至超人智能。

场景 4：机器之神在第四种场景中，机器达到了通用人工智能（AGI）和某种形式的知觉。它们变得像人类一样聪明和有能力。然而，并没有理由认为人类智力应该是上限。因此，这些 AI 会帮助设计更聪明的 AI，最终出现超人智能。在这个场景中，人类的主导地位结束了。

人类主导地位的结束并不意味着人类的灭绝。这可能会是一个更好的世界，但不再是人类处于顶端的世界，结束了我们长达两百万年的主导地位。实现这种水平的机器智能意味着 AI 而不是人类将掌控局面。我们希望它们的利益能与人类高度一致。它们可能会像诗中所描述的「慈爱的机器」那样，照顾我们，解决我们的问题，让我们的生活更美好。或者，它们可能会认为我们是威胁、不便，或者只是有价值的分子来源。

然而，老实说，没有人确切知道如果我们成功构建一个超人智能会发生什么。结果将是震撼世界的。即使我们没有完全达到超人智能，即使是真正有知觉的机器也会挑战我们对人类意义的许多看法。它们将以各种方式成为真正的「外星大脑」，对我们在宇宙中的地位提出挑战，这种挑战不亚于在另一个星球上发现外星人。

没有理论上的障碍说明这事不能发生，但也没有理由怀疑它会发生。关于这个问题，世界上的 AI 专家们意见不一。但事实是，我们不知道从今天的大语言模型（LLM）到构建真正的通用人工智能（AGI）是否有一条明确的路径。我们也不知道 AGI 会帮助我们还是伤害我们，或者它具体会如何影响我们。许多严肃的专家认为这种风险是存在的，我们需要认真对待。例如，AI 领域的先驱之一 Geoffrey Hinton 在 2023 年离开了这个领域，并警告 AI 的危险，他说「人类很可能只是智能进化中的一个过渡阶段」。其他 AI 研究人员则谈论他们的「p（doom）」，即 AI 导致人类灭绝的可能性。如果 AI 的「末日论者」是对的，那么大规模的监管以永久停止 AI 发展将是唯一的选择 —— 尽管这看起来不太可能。

但是，我认为过多考虑这种灾难性情景会让我们感到无力。如果我们只关注构建超级智能机器的风险或好处，我们就无法考虑更可能发生的第二和第三种情景，即 AI 无处不在但仍然在人类控制之下的世界。在这些世界中，我们可以选择 AI 的意义。

与其担心一次巨大的 AI 灾难，我们更需要担心 AI 可能带来的许多小灾难。缺乏想象力或压力重重的领导者可能会决定使用这些新工具进行监视和裁员。发展中国家的弱势群体可能会因为工作变动而受到不成比例的影响。教育工作者可能会以不利于某些学生的方式使用 AI。而这些只是显而易见的问题。

AI 不一定会带来灾难。事实上，我们可以预见并规划相反的结果。J. R. R. Tolkien 曾描述过一种情景，他称之为 eucatastrophe（善终），在童话故事中很常见：「幸福结局的喜悦，或者更准确地说，是一种好的灾难，突然的快乐转折…… 是一种突如其来的奇迹般的恩赐：无法指望它会再次出现。」如果使用得当，AI 可以在局部创造出 eucatastrophes，使之前枯燥或无用的工作变得有意义和富有成效。学生们也可以找到新的前进道路，而生产力的提高则会带来增长和创新。

对于一种广泛应用的技术来说，决策的权力不应仅限于少数人。许多组织中的人将共同决定 AI 对他们的团队、客户、学生及环境的影响。但要使这些选择真正有意义，必须在许多地方尽快展开认真讨论。我们不能等待别人为我们做出决定，因为世界进展太快，我们不能再被动。我们需要以善终为目标，否则我们的不作为将使灾难不可避免。