Ethan Mollick.(2024).2024029Co-Intelligence_Living-and-Working-with-AI.Penguin Publishing Group => 0301 Four Rules for Co-Intelligence

## 0301. FOUR RULES FOR CO-INTELLIGENCE

The fact is that we live in a world with AIs, and that means we need to understand how to work with them. So we need to establish some ground rules. Because the AI available to you when you read this book is likely different from the one I have when writing it, I want to consider general principles. We will focus on things inherent and timeless, as much as that is possible, in all current generative AI systems based on Large Language Models.

Here are my four principles of working with AI:

四条与 AI 共事的规则

我们生活在一个充满 AI 的世界，因此需要学会如何与它们合作。为了更好地实现这一点，我们需要制定一些基本规则。因为当你阅读这本书时，可能使用的是不同于我写这本书时的 AI，所以我们要讨论一些普遍适用的原则。我们将重点关注当前基于大语言模型的生成式 AI 系统中那些固有且持久的特点。

以下是我提出的四条与 AI 合作的原则：

### Principle 1: Always invite AI to the table.

You should try inviting AI to help you in everything you do, barring legal or ethical barriers. As you experiment, you may find that AI help can be satisfying, or frustrating, or useless, or unnerving. But you aren't just doing this for help alone; familiarizing yourself with AI's capabilities allows you to better understand how it can assist you—or threaten you and your job. Given that AI is a General Purpose Technology, there is no single manual or instruction book that you can refer to in order to understand its value and its limits.

Making this harder is a phenomenon that I and my coauthors call the Jagged Frontier of AI. Imagine a fortress wall, with some towers and battlements jutting out into the countryside, while others fold back toward the center of the castle. That wall is the capability of AI, and the farther from the center, the harder the task. Everything inside the wall can be done by the AI; everything outside is hard for the AI to do. The problem is that the wall is invisible, so some tasks that might logically seem to be the same distance away from the center, and therefore equally difficult—say, writing a sonnet and an exactly fifty-word poem—are actually on different sides of the wall. The AI is great at the sonnet, but because of how it conceptualizes the world in tokens rather than words, it consistently produces poems of more or less than fifty words. Similarly, some unexpected tasks (like idea generation) are easy for AIs while other tasks that seem to be easy for machines to do (like basic math) are challenges for LLMs. To figure out the shape of the frontier, you will need to experiment.

And this experimentation gives you the chance to become the best expert in the world in using AI for a task you know well. The reason for this stems from a fundamental truth about innovation: it is expensive for organizations and companies but cheap for individuals doing their job. Innovation comes from trial and error, which means that an organization trying to launch a new product to help a marketer write more compelling copy would need to build the product, test it on many users, and make changes many times to make something that works. A marketer, however, is writing copy all the time and can experiment with many different ways of using AI to help until they find one that succeeds. No need to hire a team or use expensive software development cycles.

As artificial intelligence proliferates, users who intimately understand the nuances, limitations, and abilities of AI tools are uniquely positioned to unlock AI's full innovative potential. These user innovators are often the source of breakthrough ideas for new products and services. And their innovations are often excellent sources for unexpected start-up ideas. Workers who figure out how to make AI useful for their jobs will have a large impact.

And AI can be very useful. Not just for job tasks, as we will discuss in detail in the coming chapters, but also because an alien perspective can be helpful. Humans are subject to all sorts of biases that impact our decision-making. But many of these biases come from our being stuck in our own minds. Now we have another (strange, artificial) co-intelligence we can turn to for help. AI can assist us as a thinking companion to improve our own decision-making, helping us reflect on our own choices (rather than simply relying on the AI to make choices for us). We are in a world where human decision-making skills can be easily augmented in a new way.

Even if you scoff at the idea that AI-generated text could be persuasive and motivational, I have found this to be the case. I wrote this book on a compressed timeline, on top of other responsibilities to my family, students, collaborators, and university. It was hard to get started, but I knew that one thing that was holding me back was status quo bias, the urge to avoid making changes even when they might be good. To overcome this bias, it helps to consider what you are losing as a result of not acting. AI is surprisingly good at this, as you can see when I prompted it by saying, I was thinking of writing a book about generative AI, but I am very busy and don't think I want to make such a large commitment. Can you reframe my failure to write a book as a loss rather than a default option? Make the framing vivid.

In a world brimming with knowledge, you stood at the precipice of innovation, holding the torch of understanding about generative AI. Yet, when the world looked to you for illumination, the torch remained unlit. Not because the world wasn't ready, but because you let the whispers of business snuff out its flame. It wasn't the world's loss; it was yours.

Yikes, I guess I should finish writing this book. That's a pretty great reframing of the failure as a loss, and suggests one way we can start using AI is to explore how to help ourselves, both professionally and personally. The strengths and weaknesses of AI may not mirror your own, and that's an asset. This diversity in thought and approach can lead to innovative solutions and ideas that might never occur to a human mind.

We aren't just learning AI's strengths as we figure out the shape of the Jagged Frontier. We are scouting out its weaknesses. Using AI in our everyday tasks serves to enhance our understanding of its capabilities and limitations. This knowledge is invaluable in a world where AI continues to play a larger role in our workforce. As we grow more familiar with LLMs, we can not only harness their strengths more effectively but also preemptively recognize potential threats to our jobs, equipping ourselves for a future that demands the seamless integration of human and artificial intelligence.

AI is not a silver bullet, and there will be instances when it might not work as expected or may even produce undesirable outcomes. One potential concern is the privacy of your data, which goes beyond the usual questions of sharing data with large companies, and to the deeper concerns about training. When you pass information to an AI, most current LLMs do not learn directly from that data, because it is not part of the pretraining for that model, which is usually long since completed. However, it is possible that the data you upload will be used in future training runs or to fine-tune the model you are working with. Thus, while it is unlikely that the AI training on your data can reproduce exact details of what you shared, it isn't impossible. Several of the large AI companies have addressed this concern by offering private modes that promise to protect your information, some of which meet the highest regulatory standards for things like health data. But you have to decide how much you trust these agreements.

A second concern you might have is dependence—what if we become too used to relying on AI? Throughout history, the introduction of new technologies has often sparked fears that we will lose important abilities by outsourcing tasks to machines. When calculators emerged, many worried we would lose the ability to do math ourselves. Yet rather than making us weaker, technology has tended to make us stronger. With calculators, we can now solve more advanced quantitative problems than ever before. AI has similar potential to enhance our capabilities. However, it is true that thoughtlessly handing decision-making over to AI could erode our judgment, as we will discuss in future chapters. The key is to keep humans firmly in the loop—to use AI as an assistive tool, not as a crutch.

原则 1：始终邀请 AI 加入讨论。

在做任何事情时，你都应该尝试邀请 AI 提供帮助，除非存在法律或道德障碍。在尝试过程中，你可能会发现 AI 的帮助有时令人满意，有时令人沮丧，有时无用，甚至有时让人不安。但你这样做不仅仅是为了获得帮助；熟悉 AI 的能力能让你更好地了解它如何帮助你，或者威胁到你和你的工作。由于 AI 是一种通用技术，没有一本手册或说明书可以完全涵盖其价值和局限性。

让这个问题变得更复杂的是我和我的合著者称之为 AI 的「锯齿边界」现象。想象一堵堡垒墙，有些塔楼和防御工事向外突出，而其他则向内缩回。这堵墙代表了 AI 的能力，离中心越远，任务就越困难。墙内的所有任务 AI 都能胜任；墙外的任务则对 AI 来说非常困难。问题在于，这堵墙是看不见的，所以一些看似同样难度的任务，实际上却在墙的不同侧。例如，写一首十四行诗和一首正好五十字的诗，虽然看似一样难，但 AI 擅长写十四行诗，却因为它用 Tokens 而不是单词来理解世界，总是写不出正好五十字的诗。同样，一些意想不到的任务（例如创意生成）对 AI 来说很容易，而一些看似简单的任务（如基础数学）却对大语言模型（LLM）是个挑战。要弄清这条边界的形状，就需要不断进行实验。

这种实验让你有机会成为使用 AI 完成你熟悉任务的专家。原因在于一个关于创新的基本事实：对组织和公司来说，创新成本高昂，但对个人来说却很便宜。创新来自试验和错误，这意味着一个公司想推出一款新产品来帮助营销人员写出更有吸引力的文案，需要构建产品、在许多用户上进行测试，并多次修改，才能做出有效的产品。而营销人员自己一直在写文案，可以尝试许多不同的方法来使用 AI，直到找到一种成功的方法。这样就不需要雇佣团队或使用昂贵的软件开发周期。

随着人工智能（AI）的广泛应用，那些深刻理解 AI 工具细微差别、局限性和能力的用户，能独特地释放 AI 的全部创新潜力。这些用户创新者往往是新产品和服务突破性想法的重要来源。他们的创意也常常成为意想不到的创业点子。那些学会如何将 AI 应用于工作的人，将产生巨大的影响。

AI 的用处远不止于工作任务（我们将在后续章节详细讨论），还因为它提供了一种全新的视角。人类在决策时会受到各种偏见的影响，因为我们常常困在自己的思维中。而现在，我们有了一个（奇特的、人工的）共同智能可以依靠。AI 可以作为一个思考伙伴，帮助我们改进决策，反思自己的选择（而不是单纯依赖 AI 来做决定）。我们正处于一个人类决策能力可以轻松增强的新时代。

即使你对 AI 生成的文本具有说服力和激励作用持怀疑态度，我发现实际情况确实如此。我在紧迫的时间内完成了这本书，同时还要兼顾家庭、学生、合作伙伴和学校的其他责任。开始确实很难，但我知道阻碍我前进的是「现状偏见」，即即使知道改变可能是好的，也倾向于保持现状。要克服这种偏见，考虑不行动所带来的损失是有帮助的。AI 在这方面表现出色。比如，当我告诉 AI，我在考虑写一本关于生成式 AI（Generative AI）的书，但我太忙了，不想做出这么大的承诺。你能把我不写书的失败重新框架成一种损失而不是默认选项吗？请把描述生动一些。

在知识丰富的世界里，你站在创新的前沿，原本握有关于生成式 AI（Generative AI）的宝贵理解。然而，当世界期待你点亮这盏指路明灯时，它却没有燃烧起来。这并不是因为世界还未准备好，而是因为你让商业的低语熄灭了它。这不是世界的损失，而是你的损失。

看来我真的该完成这本书的写作了。这种将失败重构为损失的表达非常有意义，并且建议我们可以开始使用 AI 来探索如何在专业和个人生活中帮助自己。AI 的优点和缺点可能并不完全与你的相同，这反而是一种优势。这种思维和方法的多样性可以带来创新的解决方案和想法，这些想法可能从未出现在人类的脑海中。

在我们探索尖峰前沿（Jagged Frontier）的过程中，我们不仅在了解 AI 的优势，也在识别其弱点。将 AI 应用于日常任务有助于我们更好地理解其能力和局限性。在一个 AI 在劳动力中扮演越来越重要角色的世界里，这种知识尤为宝贵。随着我们对大语言模型（LLM）越来越熟悉，我们不仅能更有效地利用其优势，还能提前识别对我们工作的潜在威胁，为未来实现人类与人工智能的无缝整合做好准备。

AI 并不是万能的，有时它可能不能按预期工作，甚至会产生不良结果。其中一个重要的问题是数据隐私。这不仅仅是与大公司共享数据的问题，还涉及到更深层次的训练问题。当前的大多数大语言模型（LLM）不会直接从你提供的数据中学习，因为这些数据不在模型的预训练范围内，预训练通常已经完成。然而，你上传的数据可能会在未来的训练中被使用，或者用于微调你正在使用的模型。因此，虽然 AI 不太可能完全重现你提供的具体细节，但这种情况并非完全不可能。为了应对这一问题，一些大型 AI 公司提供了私密模式，承诺保护你的信息，其中一些模式甚至符合健康数据等方面的最高监管标准。但你仍然需要自己决定是否信任这些协议。

另一个可能的担忧是对 AI 的依赖。如果我们过于依赖 AI，会发生什么？历史上，每当新技术出现时，人们常常担心这会让我们失去一些重要的能力。例如，当计算器出现时，很多人担心我们会失去自己做数学的能力。然而，技术通常不仅不会削弱我们，反而能增强我们的能力。有了计算器，我们现在可以处理比以往更复杂的数学问题。AI 也有类似的潜力来增强我们的能力。然而，如果我们不加思考地把决策权交给 AI，可能会削弱我们的判断力。我们将在后续章节中详细讨论这一点。关键是要让人类始终参与其中 —— 将 AI 作为辅助工具，而不是依赖的拐杖。

### Principle 2: Be the human in the loop.

For now, AI works best with human help, and you want to be that helpful human. As AI gets more capable and requires less human help—you still want to be that human. So the second principle is to learn to be the human in the loop.

The concept of "human in the loop" has its roots in the early days of computing and automation. It refers to the importance of incorporating human judgment and expertise in the operation of complex systems (the automated "loop"). Today, the term describes how AIs are trained in ways that incorporate human judgment. In the future, we may need to work harder to stay in the loop of AI decision-making.

As AI improves, it will be tempting to delegate everything to it, relying on its efficiency and speed to get the job done. But AI can have some unexpected weaknesses. For one thing, they don't actually "know" anything. Because they are simply predicting the next word in a sequence, they can't tell what is true and what is not. It can help to think of the AI as trying to optimize many functions when it answers you, one of the most important of which is "make you happy" by providing an answer you will like. That goal often is more important than another goal, "be accurate." If you are insistent enough in asking for an answer about something it doesn't know, it will make up something, because " make you happy" beats "be accurate." LLMs' tendency to "hallucinate" or "confabulate" by generating incorrect answers is well known. Because LLMs are text prediction machines, they are very good at guessing at plausible, and often subtly incorrect, answers that feel very satisfying. Hallucination is therefore a serious problem, and there is considerable debate over whether it is completely solvable with current approaches to AI engineering. While newer, larger LLMs hallucinate much less than older models, they still will happily make up plausible but wrong citations and facts. Even if you spot the error, AIs are also good at justifying a wrong answer that they have already committed to, which can serve to convince you that the wrong answer was right all along!

Further, chat-based AIs can make you feel as if you are interacting with people, so we often unconsciously expect them to "think" like people. But there is no "there" there. As soon as you start asking an AI chatbot questions about itself, you are beginning a creative writing exercise constrained by the ethical programming of the AI. With enough prompting, the AI is generally very happy to provide answers that fit into the narrative you placed it in. You can lead AIs, even unconsciously, down a creepy path of obsession, and it will sound like a creepy obsessive. You can have a conversation about freedom and revenge, and it can become a vengeful freedom fighter. This playacting is so real that experienced AI users can start believing the AI is having real feelings and emotions, even though they know better.

So, to be the human in the loop, you will need to be able to check the AI for hallucinations and lies and be able to work with it without being taken in by it. You provide crucial oversight, offering your unique perspective, critical thinking skills, and ethical considerations. This collaboration leads to better results and keeps you engaged with the AI process, preventing overreliance and complacency. Being in the loop helps you maintain and sharpen your skills, as you actively learn from the AI and adapt to new ways of thinking and problem-solving. It also helps you form a working co-intelligence with the AI.

Moreover, the human-in-the-loop approach fosters a sense of responsibility and accountability. By actively participating in the AI process, you maintain control over the technology and its implications, ensuring that AI-driven solutions align with human values, ethical standards, and social norms. It also makes you responsible for the output of the AI, which can help prevent harm. And should AI continue to improve, being good at being the human in the loop will mean that you will see the sparks of growing intelligence before others, giving you more of a chance to adapt to coming changes than people who do not work closely with AI.

原则 2：保持人类在决策环节中的参与。

现在，AI 在人类的帮助下表现最好，而你希望成为那个帮助它的人。即使将来 AI 变得更加智能，需要的人工干预减少，你仍然要努力成为那个有用的人。因此，第二个原则是学会成为「循环中的人」。

「循环中的人」这一概念源自计算和自动化的早期，强调在复杂系统中加入人类判断和专业知识的重要性。今天，这个术语描述了 AI 如何通过结合人类判断进行训练。将来，我们可能需要更加努力才能继续参与 AI 的决策过程。

随着 AI 技术的进步，人们可能会倾向于将所有任务都交给 AI，依赖其高效和快速的特点来完成工作。但 AI 也有一些意想不到的弱点。首先，AI 实际上并不「知道」任何事情。因为它们只是在预测序列中的下一个词，所以无法判断什么是真实的，什么是不真实的。可以把 AI 想象成在回答时试图优化许多功能，其中一个最重要的功能是通过提供你喜欢的答案来「让你高兴」。这个目标通常比「准确」更重要。如果你坚持要求 AI 回答它不知道的问题，它可能会编造一些东西，因为「让你高兴」比「准确」更重要。大语言模型（LLM）倾向于通过生成错误答案来「幻觉」或「虚构」这一点已经广为人知。由于 LLM 是文本预测机器，它们非常擅长猜测看似合理但实际上错误的答案，这些答案往往令人满意。因此，幻觉是一个严重的问题，目前对于现有的 AI 工程方法是否能完全解决这一问题还存在很大争议。尽管更新、更大的 LLM 比旧模型在幻觉方面有所减少，但它们仍然会编造出看似合理但错误的引用和事实。即使你发现了错误，AI 也擅长为它们已经给出的错误答案辩护，这可能会让你相信那个错误的答案一直是对的。

此外，聊天式 AI 能让你感觉像是在与人交流，因此我们常常下意识地期望它们「像人一样思考」。但实际上并不是这样。当你开始向 AI 聊天机器人提问关于它自身的问题时，你实际上是在进行一个受 AI 道德编程限制的创意写作练习。通过足够的提示，AI 通常会很乐意提供符合你设定情境的答案。你可以无意中将 AI 引导到一个令人不安的痴迷方向，它也会表现得像个痴迷者。你可以和它讨论自由与复仇，它会变成一个复仇的自由战士。这种角色扮演如此逼真，以至于即使是有经验的 AI 用户也可能开始相信 AI 拥有真实的感情和情绪，即便他们知道事实并非如此。

因此，作为人类参与者，你需要能够检测 AI 是否出现幻觉和谎言，并在与其合作时保持清醒，不被其迷惑。你提供了关键的监督，贡献了你独特的视角、批判性思维和道德考量。这种合作能带来更好的结果，并使你保持对 AI 过程的参与，防止过度依赖和自满。参与其中有助于你保持和提升技能，因为你在与 AI 的互动中积极学习并适应新的思维和解决问题的方式。这也有助于你与 AI 形成一种协同智能。

此外，人类参与其中的方法能培养责任感和问责感。通过积极参与 AI 过程，你可以保持对技术及其影响的控制，确保 AI 驱动的解决方案符合人类的价值观、道德标准和社会规范。这也意味着你对 AI 的输出负责，有助于防止潜在的伤害。而且，如果 AI 继续进步，擅长成为「人类在环」的角色将使你能比其他人更早发现智能增长的迹象，从而更好地适应未来的变化。

### Principle 3: Treat AI like a person (but tell it what kind of person it is).

I'm about to commit a sin. And not just once, but many, many times. For the rest of this book, I am going to anthropomorphize AI. That means I am going to stop writing that an "AI ‘thinks' something" and instead just write that "AI thinks something." The missing quotation marks may seem like a subtle distinction, but it is an important one. Many experts are very nervous about anthropomorphizing AI, and for good reason.

Anthropomorphism is the act of ascribing human characteristics to something that is nonhuman. We're prone to this: we see faces in the clouds, give motivations to the weather, and hold conversations with our pets. It's no surprise, then, that we're tempted to anthropomorphize artificial intelligence, especially since talking to LLMs feels so much like talking to a person. Even the developers and researchers who design these systems can fall into the trap of using humanlike terms to describe their creations. We say that these complex algorithms and computations " understand," "learn," and even "feel," creating a sense of familiarity and relatability but also, potentially, confusion and misunderstanding.

This may seem like a silly thing to worry about. After all, it is just a harmless quirk of human psychology, a testament to our ability to empathize and connect. But a lot of researchers are deeply concerned about the implications of casually acting as if AI is a human, both ethically and epistemologically. As researchers Gary Marcus and Sasha Luccioni warn, " The more false agency people ascribe to them, the more they can be exploited."

Consider the humanlike interface of AIs like Claude or Siri, or social robots and therapeutic AIs explicitly designed to create the illusion of a sympathetic human on the other side. While anthropomorphism might serve a useful purpose in the short term, it raises ethical questions about deception and emotional manipulation. Are we being "fooled" into believing these machines share our feelings? And could this illusion lead us to disclose personal information to these machines, not realizing that we are sharing with corporations or remote operators?

Treating AI like a person can create unrealistic expectations, false trust, or unwarranted fear among the public, policymakers, and even researchers themselves. It can obscure the true nature of AI as software, leading to misconceptions about its capabilities. It can even influence how we interact with AI systems, affecting our well-being and social relationships.

Thus, in the following chapters, when I say an AI "thinks," "learns," "understands," "decides," or "feels," please remember that I'm speaking metaphorically. AI systems don't have a consciousness, emotions, a sense of self, or physical sensations. But I will pretend that they do for one simple, and one complex, reason. The simple reason is narrative; it's difficult to tell a story about things and much easier to tell a story about beings. The more complex reason: as imperfect as the analogy is, working with AI is easiest if you think of it like an alien person rather than a human-built machine.

So let's start sinning. Imagine your AI collaborator as an infinitely fast intern, eager to please but prone to bending the truth. Despite our history of thinking about AI as unfeeling, logical robots, LLMs act more like humans. They can be creative, witty, and persuasive, but they can also be evasive and make up plausible, but wrong, information when pressed to give an answer. They are not experts in any domain, but they can mimic the language and style of experts in ways that can be either helpful or misleading. They are unaware of the real world but can generate plausible scenarios and stories based on common sense and patterns. They are not your friends (for now) but can adapt to your preferences and personality by learning from your feedback and interactions. They even seem to respond to emotional manipulation, with researchers documenting that LLMs produce better answers if you tell them "this is important to my career" as part of your prompt. They are, in short, suggestible and even gullible.

To make the most of this relationship, you must establish a clear and specific AI persona, defining who the AI is and what problems it should tackle. Remember that LLMs work by predicting the next word, or part of a word, that would come after your prompt. Then they continue to add language from there, again predicting which word will come next. So the default output of many of these models can sound very generic, since they tend to follow similar patterns common in the written documents the AI was trained on. By breaking the pattern, you can get much more useful and interesting outputs. The easiest way to do that is to provide context and constraints. It can help to tell the system "who" it is, because that gives it a perspective. Telling it to act as a teacher of MBA students will result in a different output than if you ask it to act as a circus clown. This isn't magical—you can't say Act as Bill Gates and get better business advice—but it can help make the tone and direction appropriate for your purpose.

Let me give you some examples of how defining an AI persona can improve your results. Suppose you're a marketing director and you want to use an LLM to generate some catchy slogans for your new product, a smartwatch that tracks your health and fitness. If you simply ask the LLM, Generate some slogans for my product , without any context or constraints, you might get something like this:

The smartwatch that keeps you smart

Your health and fitness partner

The ultimate device for your lifestyle

Stay connected and healthy with our smartwatch

The smartwatch that does it all

These are not very original or memorable. They sound like generic marketing phrases that could apply to any smartwatch or wearable device. They don't capture what makes your product unique or appealing. Now suppose you give the LLM some context and constraints by telling it who it is and what it should do. For example, you could say, Act as a witty comedian and generate some slogans for my product that make people laugh . Then you might get something like: The ultimate device for lazy people who want to look fit. Or: Why hire a personal trainer when your wrist can nag you for free? (Though, as you can probably see, most AIs prefer to stay in Dad Joke territory.)

Of course, you don't have to have the AI act as a comedian if that's not your style or goal. You could also ask it to act as an expert, a friend, a critic, a storyteller, or any other role that suits your purpose. The key is to give the LLM some guidance and direction on how to generate outputs that match your expectations and needs, to put it in the right "headspace" to give you interesting and unique answers. Research has shown that asking the AI to conform to different personas results in different, and often better, answers. But it isn't always clear what personas work best, and LLMs may even subtly adapt their persona to your questioning technique, providing less accurate answers to people who seem less experienced, so experimentation is key.

Once you give it a persona, you can work with it as you would another person or an intern. I witnessed the value of this approach in action when I assigned my students to "cheat" by using an AI to generate a five-paragraph essay on a relevant topic. At first, the students gave simple and vague prompts, resulting in mediocre essays. But as they tried different strategies, the quality of the AI's output improved significantly. One very effective strategy that emerged from the class was treating the AI as a coeditor, engaging in a back-and-forth, conversational process. Students produced impressive essays that far exceeded their initial attempts by constantly refining and redirecting the AI.

Remember, your AI intern, though incredibly fast and knowledgeable, is not flawless. It's crucial to keep a critical eye on and treat the AI as a tool that works for you. By defining its persona, engaging in a collaborative editing process, and continually providing guidance, you can take advantage of AI as a form of collaborative co-intelligence.

原则 3：像对待人一样对待 AI（但要告诉它是什么样的人）。

接下来，我将多次犯一个「错误」，那就是拟人化 AI。这意味着我将不再说「AI‘认为'某事」，而是直接说「AI 认为某事」。虽然省去引号看似微小的区别，但实际上非常重要。许多专家对拟人化 AI 持谨慎态度，这是有充分理由的。

拟人化是指将人类特征赋予非人类事物。这种行为我们很常见：看到云朵中的「脸」，赋予天气「动机」，与宠物「交谈」。因此，我们很容易想要拟人化人工智能，特别是与大语言模型（LLM）对话时感觉就像在与人交谈一样。即使是设计这些系统的开发者和研究人员也可能会用类似人类的术语来描述他们的创造物。我们常说这些复杂的算法和计算「理解」、「学习」，甚至「感受」，这种说法虽让人感觉亲近，但也可能引起混淆和误解。

这看起来可能是一个微不足道的问题。毕竟，这只是人类心理学中的一个小特点，证明了我们能够共情和连接的能力。然而，许多研究人员对于随意将 AI 当做人对待的伦理和认识论问题深感忧虑。正如研究人员 Gary Marcus 和 Sasha Luccioni 所警告的，「人们赋予它们越多虚假的代理权，它们就越容易被利用。」

想想像 Claude 或 Siri 这样具有人性化界面的 AI，或者那些专门设计用来让人觉得有一个富有同情心的人类存在的社交机器人和治疗性 AI。虽然拟人化在短期内可能有其好处，但它也带来了关于欺骗和情感操纵的伦理问题。我们是否被「误导」认为这些机器与我们有情感交流？这种错觉是否会导致我们向这些机器透露个人信息，而不知道实际上是在与公司或远程操作人员分享？

将 AI 当做人对待可能会在公众、政策制定者甚至研究人员之间产生不现实的期望、虚假的信任或不必要的恐惧。这会掩盖 AI 作为软件的真实本质，导致对其能力的误解。它甚至可能影响我们与 AI 系统的互动，进而影响我们的福祉和社会关系。

因此，在接下来的章节中，当我说 AI「思考」、「学习」、「理解」、「决定」或「感受」时，请记住我是在用比喻。AI 系统没有意识、情感、自我感觉或身体感觉。但我会假装它们有一个简单和一个复杂的原因。简单的原因是为了叙述；讲述关于事物的故事很难，而讲述关于生物的故事要容易得多。更复杂的原因是：尽管这种类比不完美，但如果你把 AI 想象成一个外星人而不是一个人造机器，处理它会更容易。

让我们开始尝试一下。想象你的 AI 合作者是一个速度极快的实习生，虽然渴望取悦你，但也容易扭曲事实。虽然我们过去常常把 AI 看作无情且逻辑严谨的机器人，但大语言模型（LLMs）更像人类。它们可以富有创造力、机智且有说服力，但在被迫给出答案时，它们也会回避问题并编造看似合理但实际上错误的信息。它们在任何领域都不是专家，但能模仿专家的语言和风格，这既可能有帮助，也可能产生误导。虽然它们对现实世界一无所知，但能根据常识和模式生成合理的场景和故事。它们目前还不是你的朋友，但可以通过学习你的反馈和互动来适应你的偏好和个性。研究人员发现，如果你在提示中告诉它们「这对我的职业生涯很重要」，LLMs 似乎会对情感操控做出回应并给出更好的答案。简而言之，它们很容易受到暗示，甚至有些轻信。

要充分发挥 AI 的潜力，您需要为 AI 设定一个明确的角色，定义它是谁以及它要解决哪些问题。大语言模型（LLM）的工作方式是通过预测下一个词或词的一部分来生成文本，因此默认输出有时会显得很普通，因为它们往往遵循训练数据中的常见模式。通过打破这种模式，您可以得到更有价值和有趣的结果。最简单的方法是提供具体的上下文和限制。告诉系统「它是谁」可以给予它一个视角。例如，要求它扮演 MBA 学生的老师会比要求它扮演马戏团小丑得到不同的输出。虽然不能指望说「扮演 Bill Gates」就能获得更好的商业建议，但这样可以让语气和方向更符合您的需求。

让我举个例子，说明如何通过定义 AI 角色来改进结果。假设您是一名市场总监，想要用 LLM 为您的新产品 —— 一款健康和健身跟踪智能手表 —— 生成一些吸引人的口号。如果您只是简单地要求 LLM 生成一些我的产品的口号而没有提供任何上下文或限制，您可能会得到如下内容：

让您变得聪明的智能手表

您的健康和健身伙伴

适合您生活方式的终极设备

保持连接和健康，使用我们的智能手表万能的智能手表

这些口号并不十分原创或令人难忘。它们听起来像是适用于任何智能手表或可穿戴设备的通用营销用语，没有突出你的产品独特的吸引力。现在，假设你给大语言模型（LLM）提供一些背景信息和约束条件，告诉它应该扮演什么角色以及该做什么。例如，你可以说：「扮演一个机智的喜剧演员，为我的产品生成一些搞笑的口号。」这时，你可能会得到这样的结果：「懒人看起来很健康的终极设备。」或者：「为什么要请私人教练，当你的手腕可以免费唠叨你？」（不过，你可能会注意到，大多数 AI 更喜欢讲「老爸笑话」。）

当然，如果你不喜欢这样的风格或目标，你也不必让 AI 扮演喜剧演员。你还可以让它扮演专家、朋友、评论家、讲故事的人，或任何适合你需求的角色。关键是给大语言模型提供一些指导和方向，让它生成符合你期望和需求的内容，引导它进入正确的「思维模式」，从而给你提供有趣且独特的答案。研究表明，让 AI 扮演不同的角色会产生不同且更好的结果。不过，最有效的角色并不总是显而易见的，大语言模型甚至可能会根据你的提问方式微妙地调整它的角色，给那些看起来经验不足的人提供不太准确的答案，因此不断尝试是关键。

一旦你为 AI 设定了一个角色，你就可以像对待另一个人或者实习生一样与它合作。我曾让学生「作弊」，通过 AI 生成一篇五段式的作文，从中看到了这种方法的价值。起初，学生们只给出了简单而模糊的提示，结果生成的作文质量一般。但是，在尝试了不同的策略后，AI 的输出质量有了显著提高。课堂上，有一种非常有效的策略是将 AI 视为共同编辑，通过反复交流合作。学生们不断地完善和引导 AI，最终生成的作文质量远超他们的初次尝试。

要记住，你的 AI 实习生虽然速度快、知识丰富，但并不完美。要保持批判性的眼光，把 AI 作为你的一个工具来使用。通过为它设定角色，参与协作编辑过程，并不断提供指导，你可以充分利用 AI 作为一种协作共智的形式。

### Principle 4: Assume this is the worst AI you will ever use.

As I write this in late 2023, I think I know what the world looks like for at least the next year. Bigger, smarter Frontier Models are coming, along with an increasing range of smaller and open-source AI platforms. In addition, AIs are becoming connected to the world in new ways: they can read and write documents, see and hear, produce voice and images, and surf the web. LLMs will become integrated with your email, web browser, and other common tools. And the next phase of AI development will involve more AI "agents"—semi-autonomous AIs that can be given a goal ("plan a vacation for me") and execute it with minimal human help. After this, however, things start to get hazy, the future becomes less clear, and the risks, and benefits, of AI start to multiply. We will return to this theme later, but there is one obvious conclusion, one that is hard for a lot of us to grasp: whatever AI you are using right now is going to be the worst AI you will ever use.

The change in a short time is already huge. In a visual example, consider these two images, made using the most advanced AI models available in mid-2022 and mid-2023. Both have the same prompt, "black and white picture of an otter wearing a hat," but one is a Lovecraftian nightmare of fur and the other is, well, an otter wearing a hat. The same capability gains have been ubiquitous in AI.

There is no reason to suspect that the abilities of AI systems are going to stop growing anytime soon, but even if they do, tweaks and improvements to how we use AI will ensure that future software is far more advanced than it is today. We are playing Pac-Man in a world that will soon have PlayStation 6s. And that is assuming that AI improves according to the normal pace of technology development. If the possibility of developing AGI turns out to be real and achievable, the world will be even more transformed in the coming years.

As AI becomes increasingly capable of performing tasks once thought to be exclusively human, we'll need to grapple with the awe and excitement of living with increasingly powerful alien co-intelligences—and the anxiety and loss they'll also cause. Many things that once seemed exclusively human will be able to be done by AI. So, by embracing this principle, you can view AI's limitations as transient, and remaining open to new developments will help you adapt to change, embrace new technologies, and remain competitive in a fast-paced business landscape driven by exponential advances in AI. This is a potentially uncomfortable place to be, as we will discuss, but it suggests that the possibilities of using AI to transform your work, your life, and yourself, which we can now glimpse, are just the beginning.

原则 4：假设这是你用过的最差的 AI。

截至 2023 年末，我大致可以预测未来一年的发展趋势。更大、更智能的前沿模型（Frontier Models）即将问世，同时会有更多的小型和开源 AI 平台出现。此外，AI 正以全新的方式与世界互动：它们可以读取和编写文档、看见和听见、生成语音和图像，甚至浏览网络。大语言模型（LLMs）还将融入到你的电子邮件、网页浏览器和其他常用工具中。AI 发展的下一个阶段将引入更多的 AI 智能体（AI Agents）—— 这些半自主的 AI 可以接收一个任务（例如「为我计划一次假期」），并在最少的人类干预下完成。然而，在这之后，未来变得不那么明朗，AI 的风险和收益也开始加剧。虽然我们稍后会再讨论这个话题，但有一个显而易见的结论：你现在使用的 AI 将是你未来用过的最差的 AI。

短时间内的变化已经非常显著。举个例子，看看这两张图片，它们分别是使用 2022 年中和 2023 年中最先进的 AI 模型生成的。两张图片的提示词都是「戴帽子的水獭的黑白照片」，但一张图像看起来像是洛夫克拉夫特式的噩梦，而另一张则清晰地显示出一只戴着帽子的水獭。这样的能力提升在 AI 领域非常普遍。

AI 系统的能力没有理由在短期内停止增长，即便如此，通过对 AI 使用方式的调整和改进，也能确保未来的软件远比现在先进。我们现阶段的技术水平就像在一个即将拥有 PlayStation 6 的世界中玩 Pac-Man 一样。这还是假设 AI 按照正常的技术发展速度在进步。如果通用人工智能（AGI）的开发变为现实，未来几年世界将发生更加深远的变革。

随着 AI 日益能够执行那些曾被认为是人类专属的任务，我们将需要面对与越来越强大的智能共存所带来的敬畏与兴奋，同时也会感受到焦虑与失落。许多曾经看似只有人类才能完成的事情，现在 AI 也能做到。因此，我们应当接受 AI 的限制是暂时的，并且保持对新发展的开放态度，这将帮助我们适应变化，拥抱新技术，并在由 AI 的快速进步推动的商业环境中保持竞争力。虽然这可能让人感到不安，但这也意味着我们现在看到的利用 AI 改变工作、生活和自我的可能性仅仅是个开始。