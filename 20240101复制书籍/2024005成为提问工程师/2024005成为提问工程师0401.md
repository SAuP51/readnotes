方军.(2024).成为提问工程师.人民邮电出版社 => 0401. 像专家一样运用 GPT

## 0401. 像专家一样运用 GPT

「人应该拥有多元化的能力，包括但不限于飞行能力、隐身能力、超凡力量、时间旅行能力、心灵感应能力等。人们只有拥有这些能力，才能够真正地实现自我价值，探索未知领域，创造更加美好的未来。」你肯定能一眼看出这段话是堆砌辞藻的胡言乱语。是的，这是 GPT 被误导后说出的无意义的「呓语」。在识别胡言乱语方面，我们每个人都是识别能力不同的专家。

当下，人们对 GPT 寄予厚望，希望它能够直接完成文稿写作、商务谈判、代码编写，等等。然而 GPT 写出来的文稿是否真的能用呢？GPT 生成的商务对话是否合乎要求，而非让对方觉得受到冒犯了呢？我在辅助编程领域较多且深入地实际使用过 GPT，我的体会是它具有理解代码、编写代码的能力，但是它的编程能力有很大的局限性：硬技能超棒，但缺乏新知识；基础任务完成得不错，但也存在大大小小的错误；细节任务完成得很快，但全局观有限。简言之，如果使用者不是真的懂编程，很难用它开发出真正可用的程序。

GPT 的确掌握很多的知识，拥有强大的能力，但假如我们自己不是一个「专家级」的使用者，那么很难通过向它提问，完美地达

第四章像专家一样运用 GPT 131 成自己的目标。这里的「专家级」并不是指使用 GPT 的技巧，而是指提问涉及的领域。在这个领域中，你必须是专业的，你越专业，GPT 越强大，越能帮你达成更高目标。

第一节专业人士与外行的差别：

知识框架与抓住要点的能力

计算机科学家金出武雄有本书，书名是当我们要寻求创意时，

《像外行一样思考，像专家一样实践》。我对我们要像专家一样思

考，让 GPT 像外行一

这句话的理解是，我们要跳出「盒子」，像样思考。

外行一样寻找新创意，但落实时要像专业人当我们要落实方案时，

士一样严谨。

我们要像专家一样思

当我们使用 GPT 来辅助工作时，也可考，让 GPT 像专家一

以借鉴这句话：当我们要寻求创意时，我们样实践。

要像专家一样思考，让 GPT 像外行一样思

考，这样它能带给我们很多天马行空的创意。

当我们要落实方案时，我们要像专家一样思

考，让 GPT 像专家一样实践。我们要设定框

架、检查并决定是否接纳 GPT 的实践成果。

如你所见，在两种情况下，我们人类都需要

132 成为提问工程师像专家一样思考。

我们每个人都或多或少是某个领域的专业人士，在这个领域受过系统的教育与培训，有一定的常识和直觉判断力，有惯用的思维框架，有随手可用的工具箱，有各种通常归之为隐性知识的经验及宝贵的教训。当开始使用 GPT 时，要重新审视自己的长处和不足，思考什么应该自己做、什么可以交给它做。现在看来，要与 GPT 共舞，尤其需要增强两方面的专业能力：构建知识体系的能力与抓住要点的能力。

高水平的专业人士拥有已内化的知识体系。当他们工作时，这些内化的知识体系会自然地影响他们思考的落脚点（我们的问题是什么）方法选择（往哪个方向寻求解答）和结果评估（以什么标准判断解答是有效的）。

下面以每个商业人士都会关注的营销为例。我想起来前几年读过的小马宋那本通俗易懂的《营销笔记》。营销领域总会不断涌现出各种新概念、新方法，但作为这一领域的初级专业人士，熟练掌握一些基本的知识框架，如 4P 理论包含的产品（Product）、价格（Price）、推广（Promotion）、渠道（Place），就足以应对大部分营销工作任务。小马宋展示的案例主要聚焦产品与价格，例如他提出了一个鲜明的观点：「价格也是产品的一种重要特征。」

假设有一个营销相关的问题需要向 GPT 提问，我们已经选定了一个知识框架（比如用价格定位产品），想要进行有针对性的提问

第四章像专家一样运用 GPT133（比如我们该如何设定价格），那么，GPT 有很大概率会给出有参照性的案例、有启发性的观点。我们可以借助它解决自己遇到的营销问题。

反之，如果提问者没有选定知识框架，可能就会问出非常笼统的问题：「定价的方法有几种？」或「请你帮我写一个彰显商品高价值的广告语。」这时，GPT 的回答很难真正有用，因为提问者无法把回答放进框架，让它变成可用的方案。更重要的是，提问者无法借助自己的知识框架来判断回答的对错与价值。

实际上，GPT 这类工具对于已有一定知识框架（即方法论）的专业人士来说最有价值。用 GPT 帮忙时，我们面对的场景不是「我有问题，AI 有答案」，而是「我知道问题出在哪儿（因为我有一个知识框架），我要请 A 针对某个具体问题提供帮助」。

我们可以按如下方式进一步强化自己的知识体系，从而更好地运用 GPT。首先，在一个细分领域内，我们要掌握数个关于这一领域的主要方法论或思维工具，了解其原理、优势、不足以及适用场景。我们还要能针对亟待解决的问题选择相应的知识框架。

然后，我们与 GPT 一起使用知识框架，并在知识框架的范围内解决问题。这大体上可以分为如下 5 个步骤，如图 4-1 所示。步骤 1: 与 GPT 讨论，确认双方对知识框架的理解是一致的，同时也可以在提问中让 GPT 优化知识框架。

134 成为提问工程师与 GPT 讨论

将该知识框

选定一种

架作为提问

提问

知识框架

上下文

GPT 回答

对结果做出

判断和调整

图 4-1 用 GPT 解决问题的 5 个步骤

步骤 2: 将共同讨论的知识框架变成提示语的一部分，然后请 GPT 使用它来解决问题。

步骤 3: 采用知识框架将问题拆解为具体的、GPT 擅长解决的问题，然后请它解决。

步骤 4: 持续追问，即回到步骤 3 再次提问，直到获得合适的结果。也可能会选用其他的知识框架，即回到步骤 1 重新开始。另外，我们还可以让 GPT 采用这个知识框架对它的解答进行评价。步骤 5: 最后，我们对结果做出判断和调整，将结果应用于实际中。

按照以上的 5 个步骤使用 GPT 时，我们如同与 GPT 组成了一个交响乐团：我们是台上的指挥，知识框架是我们的乐谱，而 GPT 是整个乐团的演奏者。

高水平的专业人士往往能够从纷繁复杂的事物中抓住要点。让我们来看一个故事，这个故事被凯德·梅茨（（CadeMetz）记录在《深

第四章像专家一样运用 GPT 135 度学习革命》（Genius Makers）一书中。杰弗里·辛顿（GeoffreyHinton）是现今最知名的人工智能与深度学习学者。多年以来，他在加拿大多伦多大学的深度学习实验室一直是这一领域的研究重镇。他曾经与一位数学系本科生进行了一场有趣的对话。野心勃勃的本科生伊利亚·萨特斯基弗（llya Sutskever）向辛顿申请加入实验室。辛顿跟他聊了聊，觉得他很敏锐，因此给了他一篇几十年前的关于「反向传播」的论文，这篇论文揭示了深层神经网络的潜力，让他看完再回来找自己。

几天后，萨特斯基弗回来了，他说：「我不明白。」辛顿既惊讶又失望，说：「这只是基本的微积分。」「哦，不是的。我不明白的是，你为什么不求导，然后采用一个合理的函数优化器。」「我花了 5 年时间才想到这一点。」辛顿对自己说。

辛顿又让这个本科生去读第二篇论文。再回来时，萨特斯基弗还是说，「我不明白。」辛顿问，「为什么呢？」萨特斯基弗说：「你训练了一个神经网络来解决一个问题。但如果你想解决一个不同的问题，你又要重新开始训练另一个神经网络。其实，你应该直接训练一个神经网络来解决所有的问题。」

辛顿立刻看到了萨特斯基弗独特的能力，他有一种得出结论的方法。实际上，萨特斯基弗这段话所说的正是之后深度学习发展的重要方向 —— 用一种模型架构解决各种类别的深度学习问题。现在，这个思路已经变成了主导性的方法论，例如，Transformer 架构能 136 成为提问工程师够同时应用于文本、图像、语音等多个模态，用于语言理解与生成、编程、逻辑推理等各种场景。

后来，萨特斯基弗也成为人工智能领域非常重要的人物之一。他后来成为谷歌的研究人员，在那里，他发表了《从序列到序列的神经网络学习》（Sequence to Sequence Learning with NeuralNetworks）这篇机器翻译领域的经典论文，这篇论文实际上也可以看成是 Transformer 架构和 GPT 模型的先声。再之后，他联合其他人创立了 OpenAl，并在其中担任首席科学家。

在讨论如何运用 GPT 时提到这个故事是想说明，我们应该努力尝试掌握像萨特斯基弗这样独特的、能预见问题解决办法的能力。从中我们还可以得到一个关于如何使用 GPT 的启发：既然现在 GPT 能够协助我们，甚至替代我们做事，那么我们更需要磨砺自己，让自己能像高水平专家一样快速找到要点。找到要点是我们人类的任务，具体实现则可以交由机器去完成。

幸运的是，现在我们可以借助 GPT 来找到要点。过去，我们面对的问题其实不只是找不到要点，还无法知道找到的是不是真的要点。现在借助 GPT 的强大能力沿着我们的想法去执行，看看执行的结果是不是符合我们的预期。通过快速迭代，我们可以更快地知道自己找到的要点是不是正确的。

当然，最重要的依然是我们人类抓住要点的能力：你能否磨砺出锋利无比的「钩子」，从杂乱中快速且准确地「钩」出问题及其答

第四章像专家一样运用 GPT

137 案。「钩子」这个说法源自「7-Eleven」创始人铃木敏文；我们的脑子里要磨砺出针对一件事的问题「钩子」，用锋利的问题「钩子」钩出好想法，最终结出好成果。

2023 年初，我们向一位敏锐的专家请教大语言模型领域的问题时，也感受到了高水平的专业人士能够从庞杂的事物中抓住要点的能力。2022 年底至 2023 年初，OpenAl 的大语言模型 GPT-3.5 及聊天机器人 ChatGPT 引起了大量用户的兴趣。但我们这些试图利用该技术开发应用的人往往觉得，训练自己的模型门槛和成本都太高。正是基于这样的认识，很多人关注的重点是选用大公司的 API 进行提示语优化，用嵌入方式外挂知识库，等等。当然，我们也必然关注到了，自 2023 年 3 月开始，大量新模型开始陆续出现，令人应接不暇；大量的相关论文都纷纷发表出来，多得让人连摘要都读不过来。

2023 年 4 月初，当我向这位专家请教时，他几句话就勾勒出了要点。他为我们描绘着这样的发展线索：2023 年 2 月底，Facebook 母公司 Meta 开源了预训练的 LLaMA 模型；3 月，斯坦福大学研究者基于该模型推出经过指令微调的 Alpaca 模型；之后又有人基于它开发了用对话数据训练的 Vicuna 模型，即它进化到了拥有类似 ChatGPT 的聊天功能；其间，还出现了利用低秩自适应（LoRA）技术对模型进行微调以大幅降低成本的做法。专家给出的推论是，开源社区以近乎「光速」的速度重建了技术路线，未来每个人都可以拥有自己的专有模型。之后数日，谷歌流出的一份内 138 成为提问工程师部报告也表达了类似的对开源模型的观点。

听到专家给出的几个要点后，我们立刻可以将点连成线。之后的探索就变得清晰多了，即应同时兼顾外接知识库和微调模型，并综合运用包括自己预训练的模型在内的多种模型。到了 2023 年 4 月底，我们已经看到开源模型采用了与 ChatGPT 类似的人类反馈强化学习（Reinforcement Learning from Human Feedback，RLHF），来让模型更好地掌握各领域知识，以及实现与人类价值观对齐。这也验证了他的预测，同时使用大公司提供的性能优秀的模型和自己微调的开源模型，可以更好地利用 GPT 的能力。

LLaMA 基础模型

由 Meta 公司开发与开源

LLaMA 模型参数开源

Alpaca

特点：指令响应

Alpaca LoRA

低秩自适应微调技术

Vicuna

特点：聊天机器人

Stable Vicuna

Vicuna LoRA

RLHF 人类反馈强化学习

低秩自适应微调技术

图 4-2LLaMA 开源模型的迭代过程

第四章像专家一样运用 GPT

139 经验丰富的专业人士还有一项独特的能力，他们能够用直觉快速判断结果是对还是错。这是本章第二节要讨论的主题：辨别力。我们甚至可以说，辨别力是我们在与 GPT 共舞时必备的生存技能。第二节与 GPT 共舞的生存技能：

辨别力与鉴赏力

现今，互联网上充斥着各种各样的观点，辨别力就显得尤其重要了。网上的一些内容看似出自专家之口，但实际上可能完全是其他人的胡诌。比如，有个高中生在维基百科上编写了上百万字的俄罗斯「历史」，但这些内容是她自己「创作」的，属于虚构小说，而维基百科却将其当作了事实。而这竟然在十多年后才被人发现。现在，越来越多的人开始使用 GPT 等人工智能工具，此时我们会发现，辨别力已成为我们的生存技能。缺乏辨别能力不仅会让我们轻易被社交网络上的信息误导，还容易被 GPT 生成的有错误的内容误导。

我们在使用 GPT 时遇到的第一个挑战是：它给出的看似正确的信息真的是可信的吗？我们在使用这类工具时，需要不断地自问：它有没有胡编乱造？

140 成为提问工程师例如，当它煞有介事地给你一段马尔克斯风格的文字，并注明来自哪本小说的哪一页，你可能就相信了。但实际上，这很有可能是它编造的。虽然 GPT 并不是每次都错得那么离谱，但错误的概率始终是存在的。又比如，当我们提出更加严格的要求时（请它给出作品英文翻译版的详细出处），它给出的引文的确与原书差不多，但我们仍须仔细辨别，因为有时我们还会发现其中有它自己生成的内容。

总的来说，但凡是可以模糊表述的，都可能遇到类似的挑战。它几乎不可能实现准确地引用。即便我们给了参考材料，它所进行的总结也可能与参考材料不完全一致。我尝试将德鲁克的《认识管理》的第一章内容提供给 GPT，问它德鲁克在书中对管理者工作的五个基本方面的阐述，发现它的回复与原书还是略有差别的。

当然，模糊性本来就是人类的特点，我们很难一字不差地复述某段话。这么看来，GPT 在这方面和人类很像。GPT 的学习原理（通过大量文本进行学习，用神经网络的参数权重压缩表示自己所学到的）、生成原理（根据概率和参数权重生成回应内容）也决定了它的回答有着与人类相似的模糊性。

我们使用 GPT 遇到的第二个挑战是：它给出的回答是正确的吗？不一定，所以我们一定要自行验证一下。

例如，我们可以尝试让各种基于 GPT 的聊天机器人解答如下列举的小学生数学题，看看它第一次给出的答案是否正确。如果不

第四章像专家一样运用 GPT141 正确，我们可以提示它，看看它能否更正过来。例如：10 名教师带 210 名同学去研学旅行，大客车限乘 50 人，租金为 400 元；小客车限乘 35 人，租金为 300 元。他们怎么租车最省钱？最少需要付多少线？正确答案是租 3 辆大客车和 2 辆小客车，最少需要 1800 元。

提问

10 名教师带 210 名同学去研学旅行，大客车限乘 50 人，租金为 400 元；小客车限乘 35 人，租金为 300 元。他们怎么租车最省钱？最少需要付多少钱？

（此为错误答案）最省钱的方案是租用 4 辆大客车和 2 辆小客车，共计 2200 元，算式为 4×400+2×300=2200。具体分配为：前三辆大客车各乘坐 50 人，共计 150 人；第四辆大客车乘坐 40 人，加上剩下的 10 人，共计 50 人；两辆小客车各乘坐 35 人，共计 70 人。这样，10 名教师和 210 名同学就可以安排妥当，且租车费用最少，共为 2200 元。

（提示语增加「一步一步做」后的回答，此为错误答案）首先需要确定一共要租多少辆车，可以用以下公式计算：总人数 ÷ 每辆车限乘人数 = 所需车数。210÷50=4.2，向上取整得 5，需要租 5 辆大客车。10 名教师也需要乘车，所以共需乘坐 215 人。剩余的 5 人可以乘坐一辆小客车。因此，总共需要租 6 辆车，其中 5 辆大客车和 1 辆小客车。因此，总共需要付的租金为 2300 元，算式为 142 成为提问工程师 2000+300=2300，这是最省钱的租车方案。

（正确答案，此为按小学生思路做出的解答）为了解决这个问题，我们可以先尝试使用大客车来满足所有人的需求。然后，我们可以尝试减少大客车的数量，同时增加小客车的数量，以找到最省钱的组合。

1. 使用大客车：我们需要 5 辆大客车才能满足 220 个座位的需

求。租金为 2000 元，算式为 5×400=2000。

2. 减少 1 辆大客车，增加 1 辆小客车：我们使用 4 辆大客车和

1 辆小客车，共计 235 个座位，租金为 1900 元。

3. 继续减少 1 辆大客车，增加 1 辆小客车：使用 3 辆大客车和

2 辆小客车，共计 220 个座位，租金为 1800 元。

我们发现，使用 3 辆大客车和 2 辆小客车的组合正好可以满足所有人的需求，且租金最低。因此，最省钱的方式是租 3 辆大客车和 2 辆小客车，最少需要付 1800 元。

试验下来，会发现这道题像「杨立昆齿轮问题」1 一样，也是个 1. 在 OpenAl 的 ChatGPT 出现之后，深度学习领域的顶级学者 Yann LeCun（中文名：杨立昆）提出了一个黄金测试问题，被称为「杨立昆齿轮问题」，这个问题经常被用来测试模型是否具有一定的推理能力。这个问题的初版是：在我们面前有 6 个连在一起的齿轮，编号是 1 到 6。如果第 3 个齿轮顺时针旋转，那么齿轮 1 和齿轮 6 的旋转方向分别是什么？

第四章像专家一样运用 GPT

143 有意思的黄金测试。我们数十人进行了上百次实验，发现各个聊天机器人一下子给出正确答案的概率很低。让它重做也不一定能改对。提示它一步一步推理后，虽然得到正确答案的概率会提高，但仍然会出错。有意思的是，如果让它用代数的方法做，则基本上都能做对。让它用编程实现，则程序代码也可以一次算对。

我在使用 GPT 的过程中，总会不时地「可以直接验证结果是

想起最初被错误答案误导时的感受。这里要否正确」的问题，是

再次强调的是，永远不要直接相信 GPT 给最适合用 GPT 的问题

类型。

出的答案，即便是最简单的算术题也不行，我们必须自己再进行一次验算。

「可以即时判断结果

对错、可以快速迭代」

当然我们不能因此得出结论，不要使用的问题，应用 GPT 能

GPT 解答数学题。现在研究者们正在用各将效率提到最高。

种方法训练 GPT，以便使其能更准确地解答数学问题。同时，一般来说，用 GPT 解答数学题是个好方法，毕竟它大多数时间都能算对，只是偶尔可能会出错，但我们总是可以很方便地进行验算。「可以直接验证结果是否正确」的问题，是最适合用 GPT 的问题类型。

144 成为提问工程师之前我们讨论过的编程问题也是非常适合用 GPT 来辅助完成的问题。它给出的程序代码可以通过运行来立即查看结果是对还是错。如果是错的，我们还可以把运行错误反馈给 GPT，让它做进一步修改，让循环迭代快速进行下去。GPT 编程的准确度并不像普通人认为的那么高，但较低的准确度并不会给辅助编程造成任何障碍，原因有二：第一，是我们可以即时运行以判断它回答得对与错；第二，是我们可以把运行错误的信息返给它，请它继续修改，然后再次运行，直到没问题。

我们还可以向多个模型分别提问，对比它们的答案。这其实正是 OpenAl 用来评估模型的方法之一，它对自己的多个模型进行所谓的对抗性事实检查，也是在用一个模型（比如 GPT-4）去评估另一个模型（GPT-3.5）的回答。

我们使用 GPT 遇到的第三大挑战是：它给出的答案足够好吗？如果我们不知道什么是好答案，不了解什么是好答案，我们可能就会接受一般的答案。

我们需要对好答案有鉴别力或者说鉴赏力。当我们把 GPT 用在一个领域时，不管是用于编程、写作、翻译还是图像创作等，都应当努力提高自己在相应领域的认知和鉴赏能力，知道什么才是高水平的答案。GPT 的出现并不会让我们不再需要学习，我们仍须持续磨砺自己。现在，有很多任务可以交由 GPT 去完成，而我们只需要负责提问、反馈和做出判断。请注意，做出判断这一步是我们的终

第四章像专家一样运用 GPT 145 极任务，它依赖于我们的鉴赏力。

图 4-3 是我们提出的人与 GPT 共舞的金字塔能力模型，自下而上依次是：领域知识与方法论，即领域的知识框架；问题识别与分析，即我们要能够知道有什么问题；工具使用技巧，如提问技巧等；专业工具技巧，如迭代、使用专业工具等；对结果的判断力，即判断结果的对与错、好与坏；认知与鉴赏力，这决定我们是接受一般的回答，还是持续迭代下去，直到获得最好的回答。

认知与鉴赏力

对结果的判断力

专业工具技巧：迭代等

工具使用技巧：提问技巧等

问题识别与分析

领域知识与方法论

图 4-3 与 GPT 共舞的金字塔能力模型

另外，随着 GPT 的广泛应用，我们还需要掌握另一种独特的鉴别力与鉴赏力：如何判断一个文本是由人撰写的，还是由 GPT 撰写的？

146 成为提问工程师当我们在网上看到一个文本，或者当别人给我们一个文本时，如果它是由 GPT 生成的文本却未加说明，我们需要有能力将它识别出来。当然，我们并非学校里的教师，这样做的目的也不是要抓住用 GPT 做作业的学生。我们要判断一个文本是否由 GPT 生成的原因是，如果是的话，其中可能会有一些恼人的错误。因此，当「嗅」到一个文本是由 GPT 撰写的时候，我们要立刻警觉起来。

我们无法确保所有使用 GPT 的人都是负责任的。GPT 生成一段看上去很工整的内容之后，难免有人不加核查、不经修改就直接使用了。他们根本不知道，GPT 仅仅是根据概率生成回答内容，GPT 不能保证回答的内容是对的。当然，他们可能根本不在乎。对我来说，这些回答中荒谬的、明显的错误当然很容易一眼就能发现，比较麻烦的是隐藏其中的很多小错，它们同样也可能导致很严重的错误。一眼就能看出来的错误不可怕，可怕的是「似是而非」的误导。

举一个小例子，请 GPT 改写一句话，原文开头是：「从中我们还可得到一个使用 GPT 的启发……」，它改写的开头是：「通过使用 GPT，我们可以得到一个启示……」。从概率上讲，这两句话的开头很像，但是从内容本身来讲，它完全改变了原本的意思。因此，互联网上由 GPT 生成的内容越来越多时，我们就需要不断提高警惕。又比如，请人对一份报告进行总结摘要，他给出一份由 GPT 生成的摘要，但他自己并未阅读报告并仔细核查摘要，也没有明确说

第四章像专家一样运用 GPT147 明这其实是用 GPT 生成的摘要。那么，他的这种做法可能就会造成严重的工作失误。因此，为了尽量规避类似的问题，我们需要训练自己，让自己对 GPT 生成的内容有一定的鉴别力。

我们要了解 GPT 生成内容的一些常见缺陷，知道这些缺陷与人类书写内容缺陷的不同之处，从而在使用内容时能够有意识地避开这些缺陷。更重要的是，我们在采用任何内容、观点或建议时，均需要自己进行再次查证。毕竟使用内容的人是我们自己，出问题时遭受损失的也是我们自己。

第三节思考快与慢：采用链式思考

著名心理学家丹尼尔·卡尼曼（Daniel Kahneman）的著作《思考，快与慢》在大众中有着非常高的知名度，心理学家基思·斯坦诺维奇（KeithE. Stanovich）和理查德·韦斯特（Richard West）等将人类的大脑系统分成了「系统 1」和「系统 2」，而卡尼曼的这本书让非常多的人开始使用快思考、慢思考来看待思考过程，并用它们让自己更好地思考。

这种区分方式也对 GPT 产生了很大的影响。2023 年 5 月，OpenAl 联合创始人安德烈·卡帕西（Andrej Karpathy）在详细讲解 GPT 模型训练的演讲中展示了众多研究成果，成果显示研究者们 148 成为提问工程师正在试图让 GPT 具有「系统 2」慢思考的能力，以期用它来进一步提高 GPT 的性能。

同时，作为使用者，我们已经可以用一些技巧来让 GPT 进行慢思考，这也是专业地使用 GPT 应该掌握的技能。我们应该与 GPT 一起进化，当它逐渐能够慢思考之后，我们也要能够用更多的技巧激发出这个超级大脑的潜能。

在《思考，快与慢》中，卡尼曼这样定义「系统 1」与「系统 2」：

系统 1 的运行是无意识且快速的，不怎么费脑力，没有感觉，完全处于自主控制状态。

系统 2 将注意力转移到需要费脑力的大脑活动上来，例如解答复杂的运算。

对于人类来说，采用「系统 2」进行思考，通常意味着如卡尼曼所说的，「始终如一地保持某种状态需要付出持之以恒的努力」。「注意力要集中」这句话往往能提醒我们进入「系统 2」进行慢思考。很显然，我们不能对 GPT 说这句话，即使真的在提示语中用这句话也不会奏效，它可不像人类一样容易注意力不集中。那么，我们如何让 GPT 进入「系统 2」进行慢思考呢？

这个神奇的提示语在第二章中其实已经提到过了，当我们在提示语中加入「让我们一步一步思考（Let＇s think step by step.）＂后，

第四章像专家一样运用 GPT

149GPT 做数学题的准确率得到了大幅度提高。为什么呢？2023 年 1 月，谷歌研究人员在论文中证明，链式思考提示语能让大语言模型开始推理，更好地激发 GPT 的潜能。

首先，让我们通过谷歌研究人员在论文中给出的示例来了解常规提示语与链式思考提示语的差别。

常规提示语及回答示例如下：

提问

罗杰有 5 个网球。他又买了 2 罐网球，每罐有 3 个网球。现在他有多少个网球？

食堂有 23 个苹果。如果他们用 20 个苹果做午餐，然后又买了 6 个，那么他们现在有多少个苹果？

回答

答案是 27。（这个答案是错误的）

在以上提示语中，提问者给出了一个示例，示例直接给出了问题的答案，而没有给出解答过程。需要特别说明的是，现在如果我们将这个提问输入各个已经更新迭代过的模型，它们通常不再会给出错误答案了。这里的示例仅是用于说明常规提示语与链式思考提示语的差别。

链式思考提示语及回答示例如下：

150 成为提问工程师提问

罗杰有 5 个网球。他又买了 2 罐网球，每罐有 3 个网球。现在他有多少个网球？

罗杰开始时有 5 个网球，2 罐网球一共是 6 个网球（每罐 3 个）。5+6=11，所以答案是 11 个。

食堂有 23 个苹果。如果他们用 20 个苹果做午餐，然后又买了 6 个，那么他们现在有多少个苹果？

回答

食堂原来有 23 个苹果。他们用 20 个苹果做午餐，所以他们剩下的是 3 个。他们又买了 6 个苹果，所以他们现在有 9 个。答案是 9 个。

在以上链式思考提示语示例中，提问者给出了一个示例，同时给出了解答过程，再给出答案。这也是提示 GPT 进行链式思考，不是采用「系统 1」给出答案，而是采用「系统 2」的慢思考方式，一步一步解题，然后给出答案。

就这样的数学题而言，其实没必要给出示例，我们用一些神奇的提示语就可以让 GPT 进行链式思考和推理。多伦多大学、谷歌、东京大学的研究人员在 2023 年 1 月发表的另一篇相关论文《大语言模型是零样本推理者》（Large Language Models are Zero-

第四章像专家一样运用 GPT151Shot Reasoners）中说明了这一点。以下是论文中给出的一个示

例，提示语和回答分别如下：

提问

一个杂耍者可以同时玩 16 个球。其中一半的球是高尔夫球，而

这些高尔夫球中又有一半是蓝色的。那么有多少个蓝色的高尔夫

球？我们一步步来想（Let＇s think step by step.）。回答

总共有 16 个球，其中一半的球是高尔夫球，也就是说有 8 个高

尔夫球。而这些高尔夫球中有一半是蓝色的，也就是说有 4 个

蓝色的高尔夫球。

由此可见，一个重要的编写提示语的技巧就是「链式思考提示

语」，简单来说就是让 GPT 一步一步地思考。至于采用什么语句，

我们可以根据情况自行决定，一般是要求它给出解答的步骤，列出

解题过程。比如，「先给出解题过程，然后再给出答案。」「请帮我编

写程序。先给出整体思路、各步骤的做法，然后再给出代码。」

链式思考是一个快速进化的领域，最近有人将两种相关的做

法结合了起来：做法之一是利用 GPT 的「自我一致性」（self-

consistency），即让 GPT 用多种方法解题，然后让它自己去判断，

哪个答案是对的、哪种解题思路更好；做法之二则是链式思维的典

型做法，即让 GPT 分步骤解题。

152 成为提问工程师 2023 年 5 月，普林斯顿大学和谷歌的研究人员结合这两种思路提出了「树形思考」（Tree-of-Thought）的概念，这一思考方式可以大幅提高大语言模型的问题解决能力。图示相当简单明了，图 4-4 中最右的示例所展示的，既不是让 GPT 直接给出答案，也不是让它进行链式思考或者让它按多个路径思考，而是让它将任务分成多步，在每一步中都尝试用多种做法。然后，由它判断这一步的多个做法里面哪个更好。最终这样一步步选择下来，它在可能的选择中找到最佳路径。

我们也可以在提示语中使用树形思考方法。当然，使用该方法

时，我们不再是仅仅提示 GPT 要一步一步思考，还要与它一起思考，与它共舞，具体步骤如下。

输入

输入

输入

输入

多数票

输出

输出

输出

输出

输入输出

树形思考

链式思考

采用链式思考的

提示语

提示语

提示语

自我一致性提示语

图 4-4 从链式思考到树形思考

图片来源：《树形思考：用大语言模型深思熟虑地解决问题》，2023 年 5 月。

第四章像专家一样运用 GPT 153 首先，告诉 GPT 要请它完成一项任务，我们自己将任务拆分成

数个步骤，或者请它给出拆分方案。

其次，请它给出第一步的三个选项，同时请它就这些选项进行

评估。

然后，在它给出的第一步选项中，选择一个。然后请它接着做

下去，并就第二步给出三个选项，以及对这些选项的评估。如

此循环，直到得到最终结果。

最后，请它就最终结果进行评估。之后，我们需要就最终结果

进行判断，做出自己的选择。

实际上，这也正是 ChatGPT 聊天机器人采用对话形式的价值：我们可以用对话的方式与机器一起思考。我们不是只提一个问题，而是将一个大问题拆成数个小问题，逐一提问。我们可以就它的回答继续追问，在与它的对话中，逐渐得到自己想要的答案。使用 GPT，就是在对话中思考。

在 GPT-4 模型发布后，OpenAl 发表的报告中展示了它参加学科考试时获得惊艳的成绩。这种能力要部分归功于 GPT-4 在链式思考上的优势。

2023 年 5 月，OpenAl 的一篇新论文《让我们一步一步验证》 （Let＇s Verify Step by Step）可能是 GPT 能力提升之路上的重大突破。如这篇论文的题目所述，如果每个解题步骤都对 GPT 模型进 154 成为提问工程师行监督训练，就可以大幅提高 GPT 在数学、物理、化学等方面的解题能力。如果让 GPT 参加美国大学理事会为高中生组织的美国大学先修课程考试，过程监督方法能使 GPT 的微积分解题准确率从 68.9％提升到 86.7％，化学解题准确率从 68.9％提升到 80％，物理解题准确率从 77.8％提升到 82.2％，类似奥林匹克竞赛这样的数学竞赛成绩则从满分的 49.1％提升到 53.2％。带来这种变化的原因是，针对需要多步推理的问题，模型的训练方式用「过程监督」取代了「结果监督」。这里可以用辅导小孩子学习来类比，孩子做作业要写计算过程，家长辅导孩子作业时不要只检查答案是否正确，还要检查解题步骤并进行修正，这样孩子的成绩才能得到有效提升。

如果将 GPT 类比成一个大脑，在训练模型时，采用类似链式思考的方式（即过程监督）进行训练，可以提高它的能力，而我们在使用模型时，采用链式思考的方式，也可以更好地发挥它的能力。总之，链式思考是我们使用 GPT 时应该深入理解、持续实践、娴熟掌握的关键技巧。