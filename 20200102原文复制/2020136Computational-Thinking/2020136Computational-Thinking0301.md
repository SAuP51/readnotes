# 0301. Computing Machines

The intolerable labour and fatiguing monotony of a continued repetition of similar arithmetical calculations, first excited the desire, and afterwards suggested the idea, of a machine, which, by the aid of gravity or any other moving power, should become a substitute for one of the lowest operations of human intellect.

—— Charles Babbage (July 3, 1822, letter to Humphry Davy)

Now we will look at the evolution of computing machines and the dimension of computational thinking needed to design and understand them. The primary practical motivation for building computing machines was always to speed up computation and eliminate the errors inherent in human computing.

People have always been fascinated by the idea of building devices that automated aspects of human behavior or human thinking. For millennia, craftsmen built automata for art and amusement, such as animated animal figures, music boxes, player pianos, and human-like figurines mimicking people’s behavior. The Mechanical Turk, a chess-playing automaton, created a sensation in 1770 because it seemed to mechanize chess play, then considered a high-order mental skill. It was later revealed to be an elaborate hoax. But it titillated the curiosity of inventors who wondered if they could really build a chess-playing machine. Some philosophers believed that automata for calculation, another revered human mental skill, might be more feasible because the rules of basic arithmetic were much clearer and simpler than the rules and strategies of chess.

## 3.1 The Rise of Computing Machines

When experts can codify, as procedural steps, what they know about calculation and reasoning, their knowledge becomes useful to many non-experts, who can obtain the results without error simply by following directions. But no matter how precise the procedure, human operators are prone to making mistakes. They are forgetful, they do not fully understand every computational operation, they are easily distracted, and they are quickly bored by a long routine calculation. No matter how simple and unambiguous the steps, human computers make mistakes. A lot of them. One study of 40 volumes of old mathematical tables found 3,700 errors, and another found 40 errors on just one page.

For this reason, inventors through the ages sought computing machines and aids for calculation that would allow humans to complete longer computations with fewer errors. This was a slow process. The slide rule was invented around 1620. By sliding sticks marked with logarithmic scales past each other, it implemented the method of multiplication based on summing logarithms. But the slide rule could not add or subtract. Blaise Pascal designed a calculator in 1642; it could add and subtract, but could not multiply or divide. Attempts by others to extend Pascal’s design to permit multiplication failed.

The slide rule found its home among engineers and the arithmetic calculator among mathematicians and accountants. Over the following centuries, these kinds of machines were gradually improved. By the 1930s, Keuffel and Esser Company was the primary supplier of log-trig slide rules and Marchant was the primary supplier of mechanical calculators that did all four arithmetic operations. Many slide-rule and mechanical calculator companies were swept away by the avalanche of change unleashed by the electronic computer revolution in the 1950s. New companies such as Hewlett-Packard and Texas Instruments started to produce all-electronic desktop calculators that could perform all slide-rule and arithmetic functions. The coup de grâce came in 1972 with the HP-35 programmable handheld calculator, which replaced the slide-rule on the engineer’s belt.

Despite their popularity, the slide rule and calculating machine had two serious limitations. First, they could not perform long chains of calculations; human operators had to do that. Second, these tools could only be used for a single purpose. The electronic digital computer overcame these limitations with a radical idea: software stored internally in the machine’s memory. Software could perform long calculations and could easily be adjusted to change the operation of the underlying machine.

Precursors to the idea of software originated well before the electronic computing age. In the early 1700s, French textile weavers experimented with machines that could weave complex patterns using an automatic loom. One of the more well known of these machines was the Jacquard loom, which was controlled by long chains of punched cards; a hole in a card let a hook through, lifting a thread that became part of a single line of the weave. Jacquard’s automatic loom revolutionized textile weaving. Jacquard’s cards were a form of external, changeable software that controlled the operation of the loom.

The idea of controlling machines with punched cards appealed to Herman Hollerith, who designed a machine to tabulate the data from the 1890 US Census. He recorded each citizen’s data as a pattern of holes punched in a card, representing characteristics such as sex, address, and ethnic origin. The tabulating machine selected out cards meeting given characteristics and tallied statistics for the selected group of citizens. With Hollerith’s machine, the Census Bureau completed its analysis of 63 million records in one year, far faster and cheaper than any previous census. In the following years, the same technology was adopted for myriad data processing tasks: keeping track of health of tens of thousands of soldiers, agricultural censuses, rail freight waybills, and so on.

Before seeing where tabulating machines led, we would like to back up 50 years to the significant development by Charles Babbage and Ada Lovelace: the general-purpose computer.

## 3.2 The Babbage Machines

Charles Babbage designed two significant computing machines in his long career. His Difference Engine (ca. 1820) automated the calculation of mathematical tables such as tables of logarithms or sines. His Analytical Engine (ca. 1840) was a general-purpose computer capable of any computable function.

In Babbage’s day, experts prepared books of tables of important functions such as the logarithms of all six-digit numbers. These tables were commonly used for mathematical calculations; for example, one could multiply two numbers by looking up and adding their logarithms. These tables were computed by hand using difference formulas that calculated each line of the table from the previous line. Babbage knew that these hand-computed books contained many errors, and those errors sometimes led to serious consequences. For example, he argued that errors in the navigation tables used by the British Navy caused shipwrecks. He wished to eliminate the errors by replacing humans with machinery that does not get tired, bored, or distracted. He conceived of a machine that he called Difference Engine to calculate and print tables of numbers. Intrigued, the British government gave him funds to develop it.

Babbage spent the better part of the next 20 years trying to build his machine. It was a much bigger challenge than he thought: the mechanical engineering methods of the day were not able to produce thousands of gears and levers with the precision needed to avoid skipping or jamming. In the 1830s he conceived of a new design called the Analytical Engine, which would need fewer parts and would be more powerful — capable of calculating any mathematical function. But by that time, the government distrusted him over his failure to deliver a Difference Engine and refused to back his Analytical Engine project. He pursued that project with scraps of funding until his death in 1871, but never completed it. His visionary ideas lay dormant for the next 80 years.

The Analytical Engine took instructions from punched cards, an idea from Jacquard’s loom. The punched cards contained a program that would instruct the machine to automatically compute a mathematical function. It was able to decide what to do based on earlier results (selection) and repeat parts of its program (looping). It had separate units for separate functions of the machine: input, processing, memory, and output. It composed machine instructions from microprograms.

Babbage collaborated with a gifted English mathematician, Ada Lovelace, who designed algorithms for the Analytical Engine. One of her example programs calculated a sequence of rational numbers called Bernoulli numbers. Babbage and Lovelace are often regarded as the first programmers. What is more, Lovelace saw Babbage’s machine as more than a number calculator; for her it was a processor of any information that can be encoded in symbols. She called the study of such programs「the science of operations.」Her insight that computing machines can calculate not only over numbers, but over symbols that can stand for anything in the world, anticipated by a hundred years a key tenet of the modern computer age. Lovelace saw the computer as an information machine.

The vision of both Babbage and Lovelace was groundbreaking. Their designs introduced many ideas today considered as features that distinguish computational thinking from other kinds of thinking. Besides representing programs in a changeable external medium, the Analytical Engine embodied many aspects of modern computers: digital representation of data, programming, machine-executable algorithms, control structures for choosing cases and looping, arithmetic-logic unit, and microprogramming to break machine instructions into low-level logic gate operations. Ironically, some central insights of the computer age were born in the age of steam.

## 3.3 The Stored-Program Computer

Babbage’s logical designs for his computer could not be realized on the era’s technology, but many decades later, the dawning age of electronics opened up new possibilities. The period from the late 1930s was one of intense experimentation to build computing machines. Konrad Zuse built a computer in Germany in 1938, but the German government did not take it seriously and it made little impact. Howard Aiken, in partnership with IBM and sponsored by the US Navy, built the Mark I at Harvard in 1944. It was an electromechanical computer that straddled the mechanical world governed by Newton’s laws of motion and the light-speed world governed by Maxwell’s laws of electromagnetism. Its programs and input data were stored externally on punched paper tapes.

At the Moore School of Electrical Engineering at the University of Pennsylvania, John Mauchly and Presper Eckert — with support from the US Army — designed what is perhaps the most famous among the first electronic computers. Their ENIAC machine went into operation in 1945 and was used to calculate artillery-firing tables and explore the feasibility of the thermonuclear weapon. The ENIAC (Electronic Numerical Integrator and Computer) took its program from an external wire patch board; programming it was tedious. The ENIAC machine was very influential as a proof-of-concept of fully electronic computing: it worked, it was fast, and it inspired better machines soon after. Its engineers founded Univac, the first commercial company to offer an electronic computer.

In 1945, the ENIAC team, joined by John von Neumann, met to design a better machine based on their experience. Aside from the ENIAC being difficult to program, its memory was limited, and it used many thousands of vacuum tubes (18,000 of them) that gradually wore out. For their new design, the team separated the machine into three main subsystems: the central processing unit (CPU) for performing the arithmetic and logical operations, the memory for storage, and the input-output (I/O) unit for communicating with the external world. To speed up the computer, they designed a CPU that took its instructions from memory, not external punched cards or tapes, thus initiating the「stored program computer」idea. By a quirk of history, this way of organizing a machine became known as the「von Neumann architecture」because von Neumann took the notes on their meetings and distributed them. He claimed to be the note taker, not the designer. The von Neumann architecture emerged as a consensus, the plan for almost all commercial computers from that time to the present. The notion that a CPU traces out an instruction sequence among instructions stored in memory has become a central tenet of computational thinking.

## 3.4 Computational Thinking and Machines

Let us now examine the various precepts of computational thinking that these early machines and their operating systems gave us.

### 3.4.1 Digital Representations with Signals and Binary Codes

To be processable, data must be represented as signals in the machine or as measurable disturbances in the structure of storage media. There is no information without representation. Arithmetic operations such as add and subtract must be represented as rules for transforming signals. One early way to represent a decimal digit was a ring of 10 dual-triode vacuum tubes simulating a 10-position wheel. This scheme was much more expensive than a 4-tube binary representation of the same digit. Proposals to represent decimal digits with 10 distinct voltages were dismissed because of the complexity of the circuits. Engineers quickly settled on using binary codes to represent numbers because binary-coded arithmetic used many fewer components than decimal-coded arithmetic, and because circuits to distinguish two voltage values were much more reliable than circuits to distinguish more than two values. Moreover, storage could easily be built from available two-state technology such as acoustic delay lines, magnetic cores, flip-flop circuits, or phosphor patches on a cathode-ray screen. The decision to abandon decimal arithmetic and use binary codes for everything in the computer led to very simple, much more reliable circuits and storage media. The term「bit」came into standard use as shorthand for「binary digit.」Today no one can think about contemporary computers without thinking about binary representations.

It is important to keep in mind that internally the computer does not process numbers and symbols. Computer circuits deal only with voltages, currents, switches, and malleable materials. The patterns of zeroes and ones are abstractions invented by the designers to describe what their circuits do. Because not every binary code is a valid description of a circuit, symbol, or number, the designers invented syntax rules that distinguished valid codes from invalid ones. Although the machine cannot understand what patterns mean, it can distinguish allowable patterns from others by applying the syntax rules.

We cannot overemphasize the importance of physical forms in computers — such as signals in circuits or magnetic patches on disks — for without these physical effects we could not build a computer. Although computer programs appear to be abstractions, they cannot work without the machines harnessing physical phenomena to represent and process binary numbers. For this reason, it is safe to say that every dataset, every program, and every logic circuit layout is a「strategic arrangement of stuff.」

### 3.4.2 Boolean Algebra and Circuit Design

Because of Claude Shannon’s insight that George Boole’s logic precisely described electronic switching circuits, today we cannot think about computers without thinking about Boolean algebra. Boolean algebra helps us understand how the hardware implements the machine instructions generated by a compiler. But Boolean algebra is an abstraction. Sometimes it hides physical race conditions caused by multiple signals following different paths to the same output; race conditions can cause errors by causing the circuits to deviate from their Boolean formulas. This confounds programmers who are only aware of the abstractions and not the circuitry, and for that reason cannot find the errors by studying their programs.

### 3.4.3 The Clocked CPU Cycle for Basic Computational Steps

The physical structure of computers consists of registers, which store bit patterns, and logic circuits, which compute functions of the data in the registers. It takes time for these logic circuits to propagate signals from their input registers to their output registers. If new inputs are provided before the circuits settle, the outputs are likely to be misinterpreted by subsequent circuits. Engineers solved this problem by adding clocks to computers. At each clock tick the output of a logic circuit is stored in its registers. The interval between ticks is long enough to guarantee that the circuit is completely settled before its output is stored. Computers of the von Neumann architecture cannot function without a clock. Today computers are rated by their clock speeds — for example, a「3.8 GHz processor」is one whose clock ticks 3.8 billion times a second.

The existence of clocks gives a precise physical interpretation to the「algorithmic steps」in the digital realm. Every algorithmic step must be completed before the next step is attempted. The machine supports this by guaranteeing each instruction will be correctly finished before the next instruction is attempted. (There are a few types of computers that do not use clocks, but they will not be discussed here.) Clocks are essential to support our notion of computational steps and guarantee that the computer performs them reliably.

### 3.4.4 Control Flow

From the time of Babbage and Lovelace, programmers have realized that the machine must be able to decide which instructions are next. They do not always follow a linear sequence. In the von Neumann architecture, the address of the next instruction is stored in a CPU register called the program counter (PC), which is updated after each instruction. The default is to execute the next instruction in sequence (PC set to PC+1). One common deviation from linearity is to branch to another instruction at a different memory location, say X. The decision to branch is governed by a condition C (such as「is A equal to B?」) and the jump from one part of the program to another part is implemented by an instruction that says「if C then set PC to X.」This method of controlling the program counter so that the program execution jumps to a different part of the code is manifested in computational thinking as the if-then-else construct in programming languages.

### 3.4.5 Loops: Small Programs Making Big Computations

If all our programs were nothing more than decision trees of instruction sequences each selected by if-then-else, they could never generate computations longer than the number of instructions in the program. The loop allows us to design computations that are much longer than the size of the program. A loop is a sequence of instructions that are repeated over and over until a stopping condition is satisfied. A loop can be implemented with an if-then-else that branches back to the loop’s start when the stopping condition is false. A common programming error is a faulty stopping condition that does not exit the loop. That behavior is called an「infinite loop.」

Alan Turing proved that there is no algorithm for inspecting a program to determine if any of its loops is infinite. This makes debugging a challenging problem that cannot be automated. Programmers spend a great deal of time looking for mistakes in their programs.

Some programs are built on purpose to loop forever. This is very common in service processes on the Web. The service process waits at a homing position for an incoming request; it then executes code to fulfill the request and returns to its homing position. While this facilitates designing service processes, it does not remove the challenge of proving that the service process always returns to its homing position.

### 3.4.6 The Address-Contents Distinction

Stored-program computing machines introduced a distinction between an address (a name) and a value (associated with the name). In a program, a variable X names a memory location holding a value. In classical algebra, X is an unknown value. In a program, the statement「X=3」means「store the value 3 in the memory location named X.」Contrast this with the meaning of「X=3」in classical algebra, which is「the unknown X has the value 3.」In a program,「X=3」is a command; in algebra it is a fact. This distinction is part of our computational thinking. Novice programmers who do not make this distinction often draft programs that do not work.

### 3.4.7 Subprograms

By the late 1940s, designers of computers realized that a common practice of programmers would be to write code for standard functions that could be invoked from anywhere in their programs. For example, an expert programmer could write code for a SORT function that anybody else can use to arrange a list of numbers in ascending order. To enable the efficient invocation of such subprograms, the designers included a new kind of branch instruction in their machines. An instruction「CALL X」would remember the current value of the program counter (PC) and then set PC to X, thereby transferring control to the subprogram stored in memory at location X. On completion, the subprogram would execute a「RETURN」instruction that restored the remembered PC value, enabling the original program to resume operation from the point of call.

The idea of subprograms has become an important principle of computational thinking. Hardware designers have given us efficient implementations. Subprograms appear in programming languages as「subroutines,」「functions,」and「procedures.」It is taken for granted today that programs are divided into modules implemented as subprograms.

### 3.4.8 Universal Machines

In 1936, Alan Turing introduced the idea of a universal machine — a computer that could simulate any other computer, given the program of the other computer. The universal machine itself was not very complicated. This idea was implicit in the designs of machines dating back to Babbage’s Analytical Engine: designers build one base machine that can run many programs. The base machine is an example of a universal machine. Today this is taken for granted: software designers assume that compilers and operating systems will make their software work on a basic underlying universal machine.

Sometimes people equate the idea of a universal machine with a stored program computer. They are not the same. Babbage’s Analytical Engine was a universal machine whose programs were external decks of punched cards. The ENIAC was a universal machine whose programs were external patch boards. After 1945, computers were universal machines that stored their programs in internal memory.

The stored program computer makes it possible to switch the interpretation of a set of bits in memory between data and instruction. The very same patterns in the computer memory can be bits that represent things (data), as well as bits that do things (instructions). A compiler, for example, generates machine code as output data; the CPU can immediately interpret those data as executable instructions. Some early machines allowed programs to modify their own code to achieve greater efficiency. But most operating systems prohibited this by making machine code read-only: that allows the sharing but not the changing of code. The older idea of self-modifying programs is far from dead: malware today constantly modifies its own code to escape detection by antivirus software.

### 3.4.9 Fault Tolerance and Data Protection

Logic circuits regularly experience errors from physical causes. For example, the state of a component might be unpredictable if conflicting signals arrive at the same time, or if the clock is too fast to allow some components to settle into new states, or if components deteriorate and fail over time. Circuit engineers spend a lot of time on fault tolerance. They have generally done a good job because hardware is sufficiently reliable that users do not worry about errors in the hardware.

In the 1950s design engineers began to think about multiple-access computers that would be shared within a user community. Correspondingly, CT expanded from single-user computations to multi-user computations. Multi-user systems had to guarantee that no user could access another’s data without explicit permission. This setup would provide the significant benefit of allowing users to share programs and data and would reduce the cost per user by spreading costs across many users. Designers of the first operating systems achieved this by isolating each executing program in a private region of memory defined by a base address and length. The base-length numbers were placed in a CPU register so that all memory accesses from the CPU were confined to the defined region of memory. This idea of partitioning memory and setting up the hardware so that it was impossible for a CPU to access outside its private memory was crucial for data protection. It not only protected user programs from each other; it could be used to protect users from untrusted software, which could be confined into its own memory region.

Users of machines and networks today are aware they are sharing their machines and networks with many others. They assume that the operating systems and networks are enforcing the isolation principle by keeping the executing programs in private memory regions. When they download new software they do not trust, they expect their operating system to isolate the new software in a memory region called a「sandbox.」

Although it has been in our computational thinking for a long time that operating systems isolate programs, many computer chips designed in the 1980s dropped out the memory bound checks in order to achieve greater speed. Many security specialists are now regretting this omission. New generations of hardware may once again enforce the security checks that CT experience leads users to believe are present.

## 3.5 Beyond the von Neumann Architecture

One of the popular modern definitions of computational thinking is「formulating problems so that their solutions can be expressed as computational steps carried out by a machine.」This definition is closely tied to the framework of the von Neumann architecture. In effect, the definition is a generalization of the operation of the CPU in a von Neumann machine.

After half a century, the von Neumann architecture has been approaching its limits. There are two main reasons. One is that the underlying chip technology, which has been doubling its component count every two years according to Moore’s law, can no longer absorb the continuous reductions in component size. Soon components will be so small they cannot comprise enough atoms to allow them to function properly. The impending end of Moore’s law has motivated extensive research into alternative architectures.

The other reason is that the separation of processor and memory in von Neumann architecture creates massive data traffic between processor and memory. One technology invented to lessen the processor-memory bottleneck is the cache, which retains data in the CPU rather than returning it to memory. Another technology intersperses processor and memory in a cellular array to spread the data load among many smaller processor-memory channels. A third technology is special purpose chips — ones that do a particular job exceptionally well but are not general-purpose computers themselves. An example is the graphics processing units (GPUs) now permeating every computer with a graphics display. Special purpose processors are themselves the subject of extensive research.

Two new categories of computer architecture have been getting special attention. Both are potential disruptors of today’s computational thinking. One is the neural network, which has been the powerhouse behind recent advances in artificial intelligence. A neural network maps large bit patterns (for example, the bits of a photograph) into other bit patterns (for example, labeled faces in the photograph). The input signals travel through multiple layers where they are combined according to assigned weights. An external algorithm trains the network by presenting it with a large number of input-output pairs and assigning the internal weights so that the network properly maps each input to its corresponding output. Training a network is computationally intensive, taking anywhere from many hours to several days. A trained network is very fast, giving its output almost instantly after the input is presented. Graphics-processing chips have been successful in achieving fast response of a trained neural network. Although machines capable of only pattern matching and recognition are not general-purpose (universal) computers, they have produced amazing advances in automating some human cognitive tasks, such as recognizing faces. However, there is no mechanism for verifying that a neural network will give the proper output when presented with an input not in its training set. It is very jarring to our computational thinking to be unable to「explain」how a computational network generated its conclusion.

The invention of the fully-electronic stored program computer changed the very concept of computing and created a fresh world of computational concepts that had few counterparts or precursors. The concepts, practices, and skills for designing programs and computers quickly diverged from mathematics and logic. It was a profound change.

The other computer architecture getting special attention uses quantum mechanical effects to process data. These quantum machines represent bits with electron spins and connections with quantum effects such as entanglement. Quantum computers can perform some computations much faster than von Neumann computers. One such computation is factoring a large composite number into its two constituent primes. The intractability of factoring on von Neumann architectures has been the principle behind the security of the RSA cryptosystem, which is currently the most secure cryptosystem in wide use. Quantum computers threaten to break its security. Because their operation is nothing at all like that of the von Neumann computers, most people trained in computer science rather than physics find it very difficult to understand the operation of these machines or how to program them.

These two examples illustrate how each kind of machine has an associated style of computational thinking and is quite good at particular kinds of problems. A person with advanced knowledge in CT would be familiar with these architectures and, as part of the design process, would select the best architecture for solving the problem. At the same time, particular machine types can also induce a kind of「blindness」 — for example, designers schooled in the basic von Neumann architecture think in terms of instructions and have trouble understanding how a quantum computer works.

Until the 1940s, computing was seen largely as an intellectual task of humans and a branch of mathematics and logic. The invention of the fully electronic stored program computer changed the very concept of computing, and it created a fresh world of computational concepts that had few counterparts or precursors. The concepts, practices, and skills for designing programs and computers quickly diverged from mathematics and logic. It was a profound change.

And until the 1940s, computational thinking was embedded in the tacit knowledge and state-of-the-art practices of many different fields, including mathematics, logic, engineering, and natural sciences. After the 1940s, computational thinking started to become the centerpiece of the new profession that designed information machines to do jobs humans never thought were possible.