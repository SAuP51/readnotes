# 0401. Computer Science

The question「What can be automated?」is one of the most inspiring philosophical and practical questions of contemporary civilization.

—George Forsythe (1969)

In the 1950s academics started to advocate for the formation of computer science programs in universities to meet a rising hunger for learning the new technology. Many precepts of CT were refined and perfected in computer science departments since that time. We turn now to the story of how CT developed in the universities.

Before we begin, we would like to point out a few key aspects of the academic environment in which CT developed. First and foremost, computing is a technical field blending engineering, science, and mathematics. Most computing students come to university to learn a profession of software and hardware designers, not to obtain a general education. Employers also come to university to recruit graduates for jobs. Thus, the CT that evolved along with academic computing has always had a strong component around design and has been strongly influenced by what employers say they need.

But that is not all. Universities are organized into a set of departments by discipline and a scattering of cross-disciplinary institutes and centers. The departments fiercely protect their identities, budgets, and space. Because their budgets depend on students enrolled, they are protective of their enrollments. And because enrollments depend on reputation and reputation on research productivity, university departments are protective of their research domains.

Another important shaping aspect of academia is the practice of seeking consensus on all decisions. Everybody wants a say, whether it is hiring a new person, awarding tenure, deciding what courses will be offered, approving possibly overlapping courses proposed by other departments, or approving the formation of new programs or departments.

This is the atmosphere in which new CS departments and academic computational thinking were formed. The founders worried about curriculum and industry demand in the context of a set of consensus-seeking departments fiercely guarding their prerogatives, always concerned with public image and identities.

The new departments proposed by the founders were split off from their existing departments. Their home departments often did not support the split because they would lose students, budget, and identity. The founders encountered a lot of resistance from other departments that did not deem a new department focused on computer technology to be legitimately science or engineering, or see that it would provide a unique intellectual perspective. Forging a consensus favoring formation of a new department was a challenge. Thus, the founders spent a good deal of time debating about the substance of computing, why it was different and new, and how it would benefit the other fields. They built a good case and were successful. Slowly the number of computer science departments grew, from 1 in 1962 to around 120 in 1980 in the US alone. Eventually in the late 1990s computer science took off as people finally realized the computing revolution is real. Today nearly every university has a computer science department.

Computer science departments are found in schools of science, engineering, and even business. Why so many homes? The answer echoes those early political fights: the new departments were established in the schools that were most welcoming. Because most of the departments were in schools of science and engineering, by the 1980s, computer scientists were labeling their field「CS&E.」That mouthful was simplified in the 1990s as「computing」became a popular shorthand for CS&E and its European counterpart「informatics.」In the 1990s some universities went further and established separate schools of computing, a movement that continues to grow today. What a turnaround!

Two academic computer societies were formed in the early days: the IEEE-CS (Institute of Electrical and Electronics Engineers Computer Society) in 1946, and the ACM (Association for Computing Machinery) in 1947. Because of their diligence to develop and promote curriculum recommendations, there are a series of snapshots of the computing curriculum at regular intervals—1968, 1978, 1989, 1991, 2001, and 2013. These snapshots show how the concerted efforts of computing pioneers to articulate a unique identity for computer science led them to recognize computational thinking as a distinguishing aspect from the beginning. In hindsight, we can discern four eras describing how universities thought about computing and how those views of computational thinking changed:

Phenomena surrounding computers (1950s–1970s)

Programming as art and science (1970s)

Computing as automation (1980s)

Computing as pervasive information processes (1990s to present)

We will discuss these eras in following sections.

These four stages of CT development in the universities were strongly shaped by the initial resistance to computer science from other fields: academic computer scientists spent a lot of effort clarifying and justifying their field. But computer science was not always the receiver of resistance. There were two important instances when computer science was the giver. One was the computational science movement in the 1980s, which was eschewed by many computer scientists. A common reaction to an announcement by the physics or biology department that they were setting up a computational science branch would be a howl of protest that those departments were impinging on the territory of computing. Some computer scientists believed that physics and biology, having now recognized the importance of computing, were trying to hijack the field they once vociferously opposed. Eventually computer scientists got over this and now work collaboratively with computational sciences. We will talk about computational science in chapter 7.

A similar process happened with software engineering. The computing departments that viewed themselves as science were not receptive to the practices of teaching and doing projects common in engineering. Software engineering had trouble gaining a foothold in those departments. There was an ongoing debate in computer science for a long time about whether software engineering is part of computer science or should be its own department. We will talk about that in chapter 5.

Phenomena Surrounding Computers

The developers of early automatic computers realized quickly that the new machines required a way of thinking and designing that differed from anything already existing in science or engineering. The ACM and IEEE started journals for the young field in the early 1950s. The Moore School, home of the ENIAC project, was an early starter of computing education in 1946 with a two-month intensive course on「theory and techniques for design of electronic digital computers.」In the 1950s the Moore School offered a multi-discipline degree in computing that included numerical analysis, programming, and programming language design. Other schools started their own programs.

These early efforts to establish computing as an academic discipline were slow to gain traction. The impediment was more than a cautionary hesitancy to see if computers were here to stay; it was a deep doubt about whether computing had academic substance beyond mathematics, electrical engineering, and physics. Outsiders typically saw the computing field of the 1950s as an impenetrable and anarchistic thicket of idiosyncratic technology tricks. What is more, the different perspectives to thinking about computing were disunited: those who designed computing machines were mostly unaware of important developments in the theory of computing such as Turing on computable numbers, Church on lambda calculus, Post on string manipulation, Kleene on regular expressions, Rabin and Scott on nondeterministic machines, and Chomsky on the relation between grammars and classes of automata.1

Academics who proposed full-fledged computer science departments or programs in research universities met stiff resistance. Many critics did not believe in the value of computing’s new ways: common objections included lack of unique intellectual content and lack of adequate theoretical basis. Purists argued that computers were human-made artifacts and not natural occurrences, and thus their study could not be counted among the noble natural sciences. On top of all that, many doubted whether computing would last. Until there was a consensus among many departments, no one could found a computer science department.

This tide began to change in 1962, when Purdue established the first computer science department and Stanford followed soon thereafter. Over the next two decades the number of departments grew slowly but steadily to well over a hundred just in the US. Even so, many academics continued to question whether computer science was a legitimate field of science or engineering.

A major shift in the question about the legitimacy of computing happened in 1967, when three well-recognized computer scientists—Allen Newell, Alan Perlis, and Herbert Simon—published a famous letter in Science addressing the question. They wrote:「Wherever there are phenomena, there can be a science to describe and explain those phenomena. Thus, ... botany is the study of plants, ... zoology is the study of animals, astronomy the study of stars, and so on. Phenomena breed sciences. ... There are computers. Ergo, computer science is the study of computers. The phenomena surrounding computers are varied, complex, rich.」2 From this basis they quickly dismissed six objections, including the one that computers are human-made and are therefore not legitimate objects of a science. Herb Simon, a Nobel laureate in economics, so objected to the notion that there could be no science surrounding human-made objects that he wrote a now-classic book titled Sciences of the Artificial refuting this idea.3 He gave an example from time-sharing systems (computers that allow many simultaneous users): The early development of time-sharing systems could not have been guided by theory as there was none, and most predictions about how time-sharing systems would behave were astonishingly inaccurate. It was not possible to develop a theory of time-sharing systems without actually building those systems; after they were built, empirical research on their behavior led to a rich theoretical base about them. In other words, CT could not approach problems from one direction only—the engineering aspects and scientific-mathematical aspects of computing evolved in a synergistic way to yield a science that was not purely a natural science.

The notion of computing as the study of phenomena surrounding computers quickly gained traction, and by the end of the 1960s was taken as the definition of computing. A view of the field’s uniqueness started to form around that notion. The term「algorithmic thinking」was used to describe the most obvious aspect of new kind of thinking. The field’s unique aims, typical problems, methods of solving those problems, and kinds of solutions were the basis of CT.

The computing pioneers expanded computational thinking beyond what they inherited from the long history of computation. They focused on the construction principles of programs, computing machines, and operating systems. They worked out a large number of computing concepts that are today taken for granted, including named variables, control structures, data structures, data types, formal programming languages, subroutines, compilers, input-output protocols, instruction pipelines, interrupt systems, computing processes, memory hierarchies, caches, virtual memory, peripherals, and interfaces. Programming methodology and computer systems architecture were main drivers in the development of computational thinking. By 1970, most computer scientists said that computing’s characteristic ways of thinking and practicing—which today are called computational thinking—embrace all the knowledge and skills relating to computers.

Computational thinking divided early into a hardware flavor and a software flavor. The hardware flavor was followed by computer engineers in the engineering school; the software flavor by software designers and computing theorists in the science school.

Programming as Art and Science

The 1960s were a maturing period for computing that produced considerable richness in the ways computer scientists thought about their work and their field. The subfield of operating systems was born in the early 1960s to bring cheap, interactive computing to large user communities—CT acquired a systems attitude. The subfield of software engineering was born in the late 1960s from a concern that existing models of programming were incapable of developing reliable and dependable production software—CT acquired an engineering attitude. The subfield of networking was born in 1967 when the ARPANET project was started—CT acquired a networking attitude.

With a solid, reliable technology base in place, the field’s attention shifted to programs and programming. Many programming languages came into existence along with standard ways of programming. A huge interest in formal verification of programs welled up, seeking a theory-based way to demonstrate that programs were reliable and correct. A similar interest in computational complexity also welled up, seeking analytical ways to assess just how much computational work the different algorithms required.

Computer programs are expressions of algorithms in a formal language that, when compiled to machine-executable form, control the actions of a machine. Programs are central to nearly all of computing: Most professionals and researchers in computing work in some way or another with programs. On the first stored-program computers of the 1940s, programming was done in assembly languages that converted short abbreviated codes for instructions line-by-line to machine code that computers can run. For example, the instruction「ADD R1,R2,R3」would place the sum of registers R1 and R2 into register R3. That instruction was converted to machine code by substituting binary codes for ADD, R1, R2, and R3. Writing programs in assembly language was very tedious and error-prone.

Programming languages were invented to provide precise higher-level expressions of what the programmer wanted, which could then be unambiguously translated by a compiler to machine code. This greatly simplified the job of programming, making it much more productive and much less error-prone. The first widely adopted programming languages introduced a plethora of new CT concepts that had few or no counterparts in other intellectual traditions.

Most programming languages were aimed at helping automate important jobs such as analyzing scientific data and evaluating mathematical models (FORTRAN in 1957), making logical deductions (LISP in 1958), or tracking business inventories and maintaining customer databases (COBOL in 1959). A few languages aimed at allowing people to communicate precise specifications of algorithms that could be incorporated into other languages. The ALGOL language (1958) was developed from this perspective.

The idea that languages cater to particular ways of thinking about problems came to be called「programming paradigms.」For example, imperative programming saw programs as series of modules (called「procedures」) whose instructions commanded the machine. FORTRAN, COBOL, and ALGOL all fit this category. Object-oriented programming treated programs as collections of relatively self-sufficient units,「objects,」that interact with each other and with the outside world by exchanging messages. Later languages such as Smalltalk and Java fit this category. Functional programming treated programs as sets of mathematical functions that generate output data from input data. LISP is an example.

These programming paradigms were seen in the 1970s as different styles of algorithmic thinking. They all sought programs that are clear expressions for humans to read and perform correctly and efficiently when compiled and executed. Donald Knuth, in his major works The Art of Computer Programming and Literate Programming, and Edsger Dijkstra in his work on structured programming, epitomized the idea that computing is about algorithms in this sense. By 1980, most computer scientists said that computational thinking is a set of skills and knowledge related to algorithms and software development.

But things got tricky when the proponents of algorithmic thinking had to describe what algorithmic thinking was and how it differed from other kinds of thinking. Knuth compared the reasoning patterns in mathematics textbooks and computing textbooks, identifying typical patterns in both.4 He concluded that algorithmic thinking differed from mathematical thinking in several aspects: by the ways in which it reduces complex problems to interconnected simple ones, emphasizes information structures, pays attention to how actions alter the states of data, and formulates symbolic representations of reality. In his own studies, Dijkstra differentiated computer scientists from mathematicians by their capacity for expressing algorithms in natural as well as formal languages, for devising notations that simplified the computations, for mastering complexity, for shifting between abstraction levels, and for inventing concepts, objects, notations, and theories when necessary.5

Today’s descriptions of the mental tools of CT are typically much less mathematical in their orientation than were many early descriptions of algorithmic thinking. Over time, many have argued that programming and algorithmic thinking are as important as reading, writing, and arithmetic—the traditional three Rs of education—but the proposal to add them (as a new combined「R」) to that list has yet to be accepted. Computing’s leaders have a long history of disagreement on this point. Some computing pioneers considered computing’s ways of thinking to be a generic tool for everyone, on a par with mathematics and language.6 Others considered algorithmic thinking to be a rather rare, innate ability—present with about one person in fifty.7 The former view has more support among educators because it embraces the idea that everyone can learn computational thinking: CT is a skill to be learned and not an ability that one is born with.8

The programming and algorithms view of computing spawned new additions to the CT toolbox. The engineering-technology side provided compilers (for converting human-readable programs to executable machine codes), parsing methods (for breaking programming language statements into components), code optimization, operating systems, and empirical testing and debugging methods (for finding errors in programs). The math-science side provided a host of methods for algorithms analysis such as O-notation for estimating the efficiency of algorithms, different models of computation, and proofs of program correctness. By the late 1970s it was clear that computing moved on an intellectual trajectory with concepts, concerns, and skills very different from other academic disciplines.

Computing as Automation

Despite all its richness, the view of computing as the study and design of algorithms was seen as too narrow. By the late 1970s, there were many other questions under investigation. How do you design a new programming language? How do you increase programmer productivity? How do you design a secure operating system? How do you design fault-tolerant software systems and machines? How do you transmit data reliably over a packet network? How do you protect systems against data theft by intruders or malware? How do you find the bottlenecks of a computer system or network? How do you find the response time of a system? How do you get a system to do work previously done by human operators? The study of algorithms focused on individual algorithms but rarely on their interactions with humans or the effects of their computations on other users of systems and networks. It could hardly provide complete answers to these questions.

The idea emerged that the common factor in all these questions, and the soul of computational thinking, was that computing enabled automation in many fields. Automation generally meant one of two things: the control of processes by mechanical means with minimal human intervention, or the carrying out of a process by a machine. Many wanted to return to the 1960s notion that automation was the ultimate purpose of computers and among the most intriguing questions of the modern age. Automation seemed to be the common factor among all of computer science, and CT seemed to be about making automation efficient.

In 1978 the US National Science Foundation launched a comprehensive project to map what is essential in computing. It was called the「Computer Science and Engineering Research Study」(COSERS). In 1980 they released What Can Be Automated?, a thousand-page tome that examined numerous aspects of computing and its applications from the standpoint of efficient automation.9 That study answered many of the questions above, and for many years, the COSERS report offered the most complete picture of computing and the era’s computational thinking. It is still a very relevant resource for anyone who wants an overview, written by famous computing pioneers, of many central themes, problems, and questions in computing.

Well into the 1990s, the computing-as-automation idea was adopted in books, research reports, and influential policy documents as the「fundamental question underlying computing.」This idea resonated well with the history of computational thinking: As we discussed in the previous chapters, automatic computing realized the dream of applied mathematicians and engineers to calculate rapidly and correctly without relying on human intuition and judgment. Theoreticians such Alan Turing were fascinated by the idea of mechanizing computing. Practitioners saw their programs as automations of tasks. By 1990,「What can be automated?」became a popular slogan in explanations of computing to outsiders and a carrying theme of computational thinking.

Ironically, the question of「what can be automated」led to the undoing of the automation interpretation because the boundary between what can and cannot be automated is ambiguous. What was previously impossible to automate might now be possible thanks to new algorithms or faster hardware. By the 1970s, computer scientists had developed a rich theory of computational complexity, which classified problems according to how many computational steps algorithms solving them needed. For example, searching an unordered list of N items for a specific item takes time proportional to N steps. Sorting a list of N items into ascending order is more complex: it takes time on order of N2 steps by some algorithms and on order of N log N steps by the best algorithms. Printing a list of all subsets of N items takes time proportional to 2N. The search problem is of「linear difficulty,」the sorting problem is of「quadratic difficulty,」and the printing problem is of「exponential difficulty.」Search is fast, enumeration is slow; computational complexity theorists call the former「easy」and the latter「hard.」

To see how vast the difference is between easy and hard problems, imagine that we have a computer that can do 1 billion (109) instructions per second. To search a list of 100 items would take 100 instructions or 0.1 microseconds. To enumerate and print all the subsets of 100 items would take 2100 instructions, a process that would take around 1014 years. That is 10,000 times longer than the age of the universe, which is very roughly around 1010 years old. Even though we can write an algorithm to do that, there is no computer that could complete the job in a reasonable amount of time. Translating this to automation, an algorithm to automate something might take an impossibly long time. Not everything for which we have an algorithm is automatable in practice. Over time, new generations of more powerful machines enable the automation of previously intractable tasks.

Heuristic algorithms make the question of computational hardness even more interesting. The famous knapsack problem asks us to pack a subset of items into a weight-limited knapsack to maximize the value of items packed. The algorithm for doing this is similar to the enumeration problem and would take an impossibly long time for most knapsacks. But we have a rule-of-thumb (a「heuristic」) that says「rate each item with its value-weight ratio, and then pack in order of decreasing ratios until the knapsack is full.」This rule of thumb packs very good knapsacks fast, but not necessarily the best. Many hard problems are like this. There are fast heuristic algorithms that do a good job but not necessarily the best. We can automate them only if we find a good heuristic algorithm.

The early findings about what things cannot be done in computing, either because they are impossible or just too long, led to pessimism about whether computing could help with most practical problems.10 Today the mood is much more optimistic. A skilled computational thinker uses a sophisticated understanding of computational complexity, logic, and optimization methods to design good heuristic algorithms.

Although all parts of computing contribute to automation, the field of artificial intelligence (AI) has emerged as a focal point in computing for automating human cognitive tasks and other human work. The CT toolbox accumulated heuristic methods for searching solution spaces of games, for deducing conclusions from given information, and for machine-learning methods that find problem solutions by generalizing from examples.

Computing as Pervasive Information Processes

The spread of computing into many fields in the 1990s was another factor in the disintegration of the automation consensus of computational thinking in the academic world. Scientists who ran simulations or evaluated mathematical models were clearly thinking computationally but their interest was not about automating human tasks. A computational interpretation of the universe started to gain a foothold in sciences (see the next section,「The Universe as a Computer」). The nail went into the automation coffin when scientists from other fields started saying around 2000 that they worked with naturally occurring information processes. Biologists, for example, said that the natural process of DNA transcription was computational. There was nothing to automate; instead they wanted to understand and then modify the process.

Biology is not alone. Cognitive scientists see many brain processes as computational. Chemists see many chemical processes as computational and have designed new materials by computing the reactions that yield them. Drug companies use simulations and search, instead of tedious lab experiments, to find new compounds to treat diseases. Physicists see quantum mechanics as a way to explain all particles and forces as information processes. The list goes on. What is more, many new innovations like blogging, image recognition, encryption, machine learning, natural language processing, and blockchains are all innovations made possible by computing. But none of the above was an automation of any existing process—each created an altogether new process.

What a radical change from the days of Newell, Perlis, and Simon! Then the very idea of computer science was attacked because it did not study natural processes. Today much of computing is directly relevant to understanding natural processes.

The Universe as a Computer

Some researchers say there is another stage of evolution beyond this: the idea that the universe is itself a computer. Everything we think we see, and everything we think, is computed by a natural process. Instead of using computation to understand nature, they say, we will eventually accept that everything in nature is computation. In that case, CT is not just another skill to be learned, it is the natural behavior of the brain.

Computer science’s self-story as the field that studies automation faded by the turn of the century. The nail went into the automation coffin when scientists from other fields started saying that they worked with naturally occurring information processes.

Hollywood screenwriters love this story line. They have taken it into popular science-fiction movies based on the notion that everything we think we see is produced for us by a computer simulation, and indeed every thought we think we have is an illusion given by a computation. It might be an engaging story, but there is little evidence to support it.

This claim is a generalization of a distinction familiar in artificial intelligence. Strong AI refers to the belief that suitably programmed machines can be literally intelligent. Weak AI refers to the belief that, through smart programming, machines can simulate mental activities so well they appear intelligent without being intelligent. For example, virtual assistants like Siri and Alexa are weak AI because they do a good job at recognizing common commands and acting on them without「understanding」them.

The pursuit for strong AI dominated the AI agenda from the founding of the AI field in 1950 until the late 1990s. It produced very little insight into intelligence and no machines came close to anything that could be considered intelligent in the same way humans are intelligent. The pursuit for specialized, weak AI applications rose to ascendance beginning in the 1990s and is responsible for the amazing innovations with neural networks and big data analysis.

Similar to the weak-strong distinction in AI, the「strong」computational view of the universe holds that the universe itself, along with every living being, is a digital computer. Every dimension of space and time is discrete and every movement of matter or energy is a computation. In contrast, the「weak」computational view of the universe does not claim that the world computes, but only that computational interpretations of the world are very useful for studying phenomena: we can model, simulate, and study the world using computation.

The strong computational view is highly speculative, and while it has some passionate proponents, it faces numerous problems both empirical and philosophical. Its rise is understandable as a continuation of the ongoing quest to understand the world through the latest available technology. For instance, in the Age of Enlightenment, the world was compared to the clockwork. The brain has successively been compared to the mill, the telegraph system, hydraulic systems, electromagnetic systems, and the computer. The newest stage in this progression is to interpret the world is not a classical computer but a quantum computer.