# 0701. Computational Science

The sciences do not try to explain, they hardly even try to interpret, they mainly make models.

—— John von Neumann (1955)

Computational science refers to the branches of every scientific field that specializes in using computation, such as computational physics, bioinformatics, and digital humanities. Although numerical methods have been a feature of science for centuries, simulation of complex systems was rarely viable before computers. Scientists developed mathematical models, usually expressed as sets of differential equations, but unless they could find closed-form solutions to the equations, the complexity of the models usually blocked them from any effective method to calculate the results. Although computers slowly began to invade all fields of science in the 1950s, the supercomputers in the 1980s were a tipping point in mustering the computing power to solve a rapidly increasing number of these equations by simulation. This led to an explosion of simulation models in science, some of which made discoveries that earned Nobel Prizes. By the mid-1980s, many scientists were counting computer simulation as a new way to do science, alongside the traditional ways of theory and experiment.

In the 1980s, scientists from many fields came together to formulate「grand challenge problems」 — problems for which their models gave solutions that required massive computations. By extrapolating Moore's law on the doubling chip speed every two years, they were able to predict with considerable accuracy when computation was going to yield solutions of these challenges. For example, aeronautics engineers projected that by 1995 they could design a safe commercial airliner using simulation as a substitute for wind tunnel testing — and the Boeing company achieved this with its 777 aircraft, which flew its first test flights in 1994.

Computer simulations got so good they could be used as experimental platforms. With simulations, scientists were able to explore the behavior of complex systems for which there were no analytical models. Simulations also opened the door for a new way of exploring the inner workings of the nature: by interpreting natural processes as information processes and simulating them in order to understand how they work.

The computational turn of science and its new methods and tools were widely adopted and the change was radical. Computational methods were described as the most significant scientific paradigm shift since quantum mechanics. The computational-science revolution ushered in a new wave of computational thinking. But unlike the previous waves of CT — which were initiated by computer scientists — scientists in other fields initiated the new CT wave. Computational science became a major driving force in the development of CT outside computing.

During the 1980s and the 1990s, computational thinking provided the mental toolbox for the new computational sciences — co-developed across many fields. In fields where natural phenomena could be interpreted as information processes, CT became a must-have skill for researchers. In an ironic twist, where previous scientists had argued that computing is not a science because there are no natural information processes, the new generation of computational scientists found information processes all over nature. And like computer scientists of the 1950s and 1960s, computational scientists learned CT from the practice of designing computations to explore phenomena and solve problems in their fields.

In this chapter, we describe how computational thinking became central to sciences, explain a number of CT practices in computational science, and discuss the new ways in which computational scientists interpret their subject matter. The electronic computing age brought some remarkable advances to science in three aspects: simulation, information interpretation of nature, and numerical methods.

## 7.1 Science and Computation: Old Friends

Science and computation have been old friends for centuries. Through most of the history of science and technology, two sorts of scientist roles have been common. One is the experimenter, who gathers data to explore and isolate phenomena, describe recurrences, and reveal when a hypothesis works and when it does not. The other is the theoretician, who designs mathematical models to explain what is already known and uses the models to make predictions about what is not known. The two roles were active in the sciences well before computers came on the scene.

2『科学家里的三类角色，做一张任意卡片。（2021-05-25）』—— 已完成

Both roles used computation. The experimenters produced data that had to be analyzed, classified, and fit to known mathematically formulated laws. The theoreticians used calculus to formulate mathematical models of physical processes. In either case, they could not deal with very large problems because the computations were too extensive and complex.

A third role emerged: scientists who saw new opportunities using computers as simulators that neither the experimenters nor the theoreticians used. The computing pioneers at the Moore School, home of the ENIAC, argued early on that computer simulation could make any computer into a laboratory. They saw the evaluation of models and the production of data for analysis as a new frontier of science. Crossing that frontier required new ways of incorporating modeling and simulation into research, as well as new kinds of computational thinking directly relevant to science.

Large-scale modeling and simulation required significant upgrades to mathematical software. Numerical analysts, a branch of early computer scientists, were heavily involved in the quest to improve mathematical software to efficiently calculate mathematical models on computers. They were especially concerned with representing numbers and performing long calculations in machines that could only offer finite precision; controlling round-off errors and increasing computational speed were major concerns.

In the late 1980s, John Rice, a pioneer of mathematical software, estimated that mathematical software had improved in performance by a factor of 10^12 since the 1950s. Of that improvement, 10^6 was due to faster hardware and another 10^6 due to better algorithms. Moore's law was only part of the reason numerical methods got better. The ingenuity of the numerical analysts did the rest.

The idea of using calculus to evaluate mathematical models must have seemed obvious to the modelers because their equations were typically differential equations. Many physical processes could be described by relating the value of a function at a point to the values of the function at neighboring points. For example, a modeler who knew that the rate of change of function f(t) was another function g(t) could calculate the values of f(t) in a series of small time steps of size Δt with the difference equation f(t+Δt) = f(t) + g(t)Δt. The sequence of Δt-separated time points is a time-series sample of the function. This idea is easily extended to functions over space coordinates (x,y) by relating f(x,y) to f(x+Δx,y) and f(x,y+Δy) on a two-dimensional grid. John von Neumann, the polymath who helped design the first stored program computers, described algorithms for solving systems of differential equations on discrete grids.

Because of the complexity of computations involved in these simulations, high-performance supercomputers became very important in the sciences. Only those computers had sufficient power to numerically solve differential equations over complex grids. With supercomputers, computational scientists cracked the grand challenge problems articulated in the late 1980s.

For centuries, theory and experiment were the two modes of doing science. Supercomputers changed this, opening a new approach to doing science based on computational exploration and modeling. It was the most significant scientific paradigm shift since quantum mechanics. The computational science revolution ushered in a new wave of computational thinking.

As computing invaded science, something unexpected happened. Instead of computing becoming more like other sciences, other sciences became more like computing. Scientists who used computers found themselves thinking differently — computationally — and routinely designing new ways to advance science. By simulating air flows around a wing with the Navier-Stokes equation discretized to a grid surrounding an aircraft, aeronautical engineers eliminated the need for wind tunnels and many test flights. Astronomers simulated the collisions of galaxies. Macroeconomists simulated scenarios in national and global economies. Chemists simulated the deterioration of space probe heat shields on entering an atmosphere. Simulation allowed scientists to reach where theory and experiment could not. It became a new way of doing science. Scientists became computational explorers as well as experimenters and theoreticians.

Just as numerical analysis enabled better simulation, better simulation enabled another new scientific paradigm: information process interpretation of phenomena in the world. Much can be learned about a physical process by interpreting it as an information process and simulating the information process on a computer. For example, it has become a mainstay of modern biology, notably with sequencing and editing genes. [1] For the quantities modeled, the real process behaves as if it were an information process. The simulation and interpretive approaches are often combined, as when the information process provides a simulation for the physical process it models.

For centuries, theory and experiment were the two modes of doing science. Supercomputers changed this, opening a new approach to doing science based on computational exploration and modeling. It was the most significant scientific paradigm shift since quantum mechanics. The computational science revolution ushered in a new wave of computational thinking.

The term「computational science」and its associated term「computational thinking」came into use during the 1980s. In 1982, Kenneth Wilson received a Nobel Prize in physics for developing computational models that produced startling new discoveries about phase changes in materials. He designed computational methods to evaluate the equations of renormalization groups, which he used to observe how a material changes phase, such as the direction of the magnetic force in a ferrimagnet. He launched a campaign to win recognition and respect for computational science. He argued that all scientific disciplines had「grand challenge」problems that would yield to massive computation. [2] He and other visionaries used the term「computational science」for the emerging branches of science that made computation their primary method. Many of them saw computation as a new paradigm of science, complementing the traditional paradigms of theory and experiment. Convinced by the benefits computational thinking would bring to science, they launched a political movement to secure funding for computational science research, culminating in the High Performance Computing Act (HPCA) passed in 1991 by the US Congress, and bringing computational thinking in science into public view.

It is noteworthy that computational science and computational thinking in science emerged from within the scientific fields — they were not imported from computer science. In fact, computer scientists were slow to join the movement. Whereas numerical analysts often felt like outcasts from mathematics in the 1950s, and outcasts from computing in the 1970s, they were natural participants in computational science. Fortunately, this mood did not last; numerical analysts are important members of the computing field.

Computation has proved so productive for advancement of science and for engineering that virtually every field of science and engineering has developed a「computational」branch. In many fields the computational branch has grown to be critical for the field. For example, biology is seen as an information science.3 Chemists design molecules and simulate them to find out how they would fare under real conditions. Pharmaceutical companies test molecules by simulation to learn if they would be effective against certain diseases. Computational methods are spreading into traditionally non-experimental fields, such as humanities and social sciences. This trend will continue. Computation will invade deeper into every field.

Because CT has advanced science — by providing better methods of numerical analysis, advanced simulations, and the information interpretation of physical processes — many people will decide to learn the skills required of computational designers and thinkers.

1 Baltimore (2001).

2 Wilson (1989).

3 Baltimore (2001).

## 7.2 Computational Thinking in Science

Computational thinking in science has two aspects. First, mental skills facilitate the design of computational models for natural processes and for methods of evaluating models. The phrase「modeling and simulation」comes up frequently for this aspect of CT in science. Computing terminology gained favor among computational scientists because it distinguished the new computational methods of conducting science from the traditional methods of theory and experiment.

The second aspect of CT in science is a skill of interpreting the world in terms of information processes. Instead of asking computing's question — Can an information process be efficiently automated? — computational scientists ask: Can a simulated information process replicate a real process? What kind of information process creates an observed phenomenon? What computational mechanism is behind an observed process? For instance, many biologists study DNA and protein interactions in terms of information processes with the hope of designing future DNA that heals diseases and lengthens life. Physicists hope that by interpreting physics as information processes, they can learn about hard-to-detect particles from simulations of particles.

We see then that CT in computational science has a different orientation from CT in computer science. Much of computational science is concerned with using modeling and simulation to explore phenomena, test hypotheses, and make predictions in its respective fields. Much of computer science is concerned with designing algorithms to solve problems. Scientists and engineers who design simulations are often not formulating problem statements; they are investigating the behaviors of phenomena. Computing people are often not using simulations to understand how nature works; they are designing software to do jobs for users.

Computing people and scientists looking to collaborate ought to keep this distinction in mind. The collaboration will work better if the computer people develop an understanding of the science domain, and the scientists an understanding of the computing domain. For example, one of us (Peter) personally witnessed a disconnect between computational and computer scientists in the 1980s. A team of PhD computational fluid dynamics scientists invited PhD computer scientists to join them, only to discover that the computer scientists did not understand enough fluid dynamics to be useful. They were not able to think computational fluid dynamics with the same facility as the fluid dynamicists. The fluid dynamics scientists wound up treating the computer scientists like programmers rather than peers, much to the chagrin of the computer scientists.

## 7.3 Computational Models

The term「computational model」can also be a source of misunderstanding. To a scientist, computational models are sets of equations, often differential equations, that describe a physical process; the equations can be used computationally to generate numerical data about the process. Simulations are often the algorithms that do this. In contrast, a computational model in computing means an abstract machine that runs programs written in a programming language. The Turing machine is frequently cited in computing as the fundamental theoretical model of all computation, even though it is too primitive to be useful for most purposes.

Scientists routinely use abstract machines in the computing sense because every one of the familiar programming languages is associated with an abstract machine. For example, the FORTRAN language presents an abstract machine that is particularly good at evaluating mathematical expressions. The Java language presents an abstract machine that hosts a large number of autonomous「objects」that concurrently send and receive messages from each other. The C++ language also has objects but is closer to the actual machine and thus gives more efficient executable code.

The computational models in computational science are realized as abstract machines that bring a replica of a natural information process to life. The simulations are the executions of programs that implement those abstract machines.

## 7.4 Modeling and Simulation

Computational science has a rich trove of methods for modeling, simulating, and interpreting natural processes. We will consider five examples that illustrate the range and we will point out some key CT features of the models and the simulations.

### 7.4.1 Mandelbrot Set

Many simulations walk through all the points on a grid, computing a function at each point, and then visualizing the result by assigning colors to the numbers on the grid points. The Mandelbrot set is a good example of a computation that reveals behaviors no one suspected by inspecting the equations. In the Mandelbrot visualization, for each point on a grid, the computer calculates a series of values based on a simple equation over complex numbers, and assigns colors to those points: if the calculated series converges (stays within some limits), color the point black, and if the series diverges, color it blue or yellow. Now repeat this for all points on the grid. [4]

When each point's color is assigned to a pixel, the Mandelbrot set appears on a graphics screen. No one suspected that such a simple computation would yield such a beautiful, mysterious object (see the figure below). One can select a small square anywhere on the graphic, zoom in on it, cover it with a grid and calculate all its grid-point colors — and see more copies of the Mandelbrot set appear at smaller scales. Each new zoom reveals more sets. It never ends. Mandelbrot called this self-replicating behavior at all scales「fractals.」

The fractal idea (self-similarity at different scales of measurement) was the key to Ken Wilson's renormalization group algorithms that yielded new discoveries in physics when simulated on a supercomputer, and it won him a Nobel Prize. The fractal idea is used in visualization systems to compute realistic graphic images, such as trees or horizons, rapidly.

4 For the more mathematically inclined, the Mandelbrot set is the points in the complex plane at which the series of values of a function converges. A complex number is represented as a+bi, where i=sqrt(-1) and i2 = -1. The equation of the series is z(n+1) = z2(n)+c where z(n) and c are complex numbers. Having chosen a value of c, compute a series of z(n)-values starting with z(0)=c. (You may need to go to an algebra refresher for algorithms to multiply complex numbers.) If the z(n) sequence converges (stays within a short radius of c for all n), color the chosen value of c black. If it diverges color c blue or yellow. Now repeat this for all c points on a grid.

### 7.4.2 Telephone Engineers

When the first telephone exchanges were designed in the early 1900s, telephone engineers confronted a serious design issue. In a town of K customers, there are potentially K2 connections. Guaranteeing every customer could connect to any other customer at any time they desired would be hopelessly complex and expensive, especially since most of the time most of the customers are not talking at all. To control the complexity and cost, engineers decided to build switches that would handle up to N calls at once (N is substantially less than K). This of course brings a risk that a customer cannot get a dial tone if the exchange is already carrying N calls. The design question was how to choose N so that the probability of encountering the busy signal is small, for example 0.001. A random walk computational model yields an answer. The model has states n = 0, 1, 2, ... , N representing the number of calls in progress up to a maximum of N, here N = 10. Requests to initiate new calls are occurring randomly at rate λ. Individual callers hang up randomly at rate μ. Each new-call arrival increases the state by 1 and each hang-up decreases it by 1. The state diagram in the figure below represents the movement through the possible states. Telephone engineers define p(n) the fraction of time the system is in state n and can prove a difference equation p(n) = (λ/nμ)p(n–1). They calculate all the probabilities by guessing p(0), calculating each p(n) from its predecessor p(n-1), and then normalizing so that the sum of all p(n) is 1. Then they find the largest N so that p(N) is below the target threshold. For example, if they find p(N) = 0.001 when N = 10, they predict that a new caller has a chance 0.001 of not getting a dial tone when the exchange capacity is 10 calls.

A key idea here was modeling the physical process with a state space representing all the possible states of the system, connected by transitions representing the random rates of flow between pairs of states. By invoking a flow balance principle — total flow into a state equals total flow out — engineers got a set of equations relating the proportions of time p(s) each state s is occupied. They can then calculate the values of p(s) by applying the equations. This form of modeling is very common in queueing theory and system performance evaluation because all the measures of interest, such as throughput, response time, and overflow probabilities, are easy to calculate from the p(s).

### 7.4.3 Doctor's Waiting Room

Engineers have also used state space models to build controllers of systems. In this example (see the figure below), a doctor wishes to build an electronic controller for her office, which consists of a four-person waiting room and a one-person treatment room. Patients enter the waiting room and sit down. As soon as the doctor is free, she calls the next patient into the treatment room. When done, the patient departs by a separate door. The doctor wants an indicator lamp to glow in the treatment room when patients are waiting, and another to glow in the waiting room when she is busy treating someone. The engineer designing the controller uses a computational model with states (n,t) where n = 0,1,2,3,4 is the number in the waiting room and t = 0,1 is the number in the treatment room. The controller implements the state diagram above. The indicator lamp in the treatment room glows whenever n > 0, and the lamp in the waiting room whenever t > 0. State transitions occur at three events: patient arrival (a), patient departure (d), and patient called by the doctor (c). Sensors located in the three office doors signal these events.

In this case the model is not used to evaluate probabilities of state occupancies, but to plan the states and transitions of an electronic circuit. It is of course possible to interpret the state diagram as in the previous example, where a, b, and c are flow rates between the states.

### 7.4.4 Aircraft Simulation

Aeronautics engineers use simulations from computational fluid dynamics to model airflows around proposed aircraft. They have become so good at this that they can test new aircraft designs without wind tunnels and space shuttle designs without test flights. The first step is to build a 3-D mesh of the space surrounding the aircraft (see the figure on the following page). The spacing of the grid points is smaller near the fuselage where the changes in air movement are greatest. Then the differential equations of airflow are converted to difference equations on the mesh, and a supercomputer grinds out the profiles of the flow field and forces on each part of the aircraft over time. The numerical results are converted to shaded images (as shown in the figure on the next page) to visualize where the stresses on the aircraft are greatest.

This form of modeling is common in science. A physical process is modeled as differential equations that relate the values of the process at a point in space to the values of the process at close neighbors. The space in which the process is to be studied is modeled with a mesh. The difference equation is used to relate each mesh point value to its immediate neighbors. A graphical display converts the field of values on the grid to a colored picture. The whole mesh can be recomputed for the next time step, giving an animated visualization.

### 7.4.5 Genetic Algorithms

Since the 1950s, various geneticists experimented with computer simulations of biological evolution, studying how various traits are passed on and how a population evolves to adapt to its circumstances. In 1975 John Holland adapted the idea of these simulations as a general method for finding near optimal solutions to complex problems in many domains. The idea, depicted in the flow diagram in the figure below, is to develop a population of candidate solutions to the problem, encoded as bit-strings. Each bit-string is evaluated by a fitness function and the most-fit members of the population are selected for reproduction by mutation and crossover. A bit-string is modified by mutation when one or several of its bits are randomly flipped. A pair of bit-strings are modified by crossover by selecting a random breakpoint and exchanging the two tails of the strings. This generates a new population. The process is iterated many times until there are no further improvements in the most-fit individuals or until the computational budget is exhausted. This process is surprisingly good at finding near-optimal solutions to optimization problems whose direct solutions would otherwise be intractable.

## 7.5 Grand Challenges and Wicked Problems

Computing has changed dramatically since the time when computational modeling grew up. In the 1980s, the hosting system for grand-challenge models was a supercomputer. Today the hosting system is the cloud — a massively distributed system of data and processing resources around the world. Commercial cloud services allow users to mobilize immense storage and processing power they need just when they need it. In addition, users are no longer constrained to deal with finite computations — those that start, compute, deliver their output, and stop. Instead devices now tap endless flows of data and processing power as needed and users count on the whole thing to keep operating indefinitely. With so much cheap, massive computing power, more people can be computational designers and tackle grand-challenge problems.

Yet there are important limits to what all this computing power can do. One limit is that most computational methods have a sharp focus — they are very good at the particular task for which they were designed, but not for seemingly similar tasks. That limit can often be overcome with a new design that closes a gap in the old design. Facial recognition is an example. A decade ago, methods of detecting and recognizing faces in images were not very good — people had to look at the images themselves. Today, deep learning (neural network) algorithms have been used to design very reliable automated face recognizers, overcoming the earlier gap. These recognizers are trained by showing them a large number of cases of labeled images. But recognizers are「fragile」in the sense that no one knows how the machine will do when presented with inputs outside the training sets. Overcoming fragility has motivated computational scientists to look at machines that learn without training sets. A recent example is a machine that learned to play the board game Go by competing against other machines, eventually becoming good enough to beat the world's highest-ranked Go player in a five-game match.

Self-learning machines have raised another concern: explainability. Designers and users want to know how the machine reached its conclusion. The idea that a machine can reach a conclusion makes sense when algorithms are seen as step-by-step procedures because the result can be explained by examining the steps followed. But when the algorithms are not step-by-step procedures, as with face recognizers and Go, that is not possible. All there is inside is an inscrutable, complex mass of connections. It is really the same problem with fellow humans — how do we explain why we do certain things? If asked directly, we may not know, and it certainly cannot be figured out by dissecting our brains. Other ways are needed to know when machines can be trusted and when not. Machine learning–related computational thinking is still in its infancy.

Another limit to what can be done with computing power concerns the many problems that cannot be solved at all with computation. We gave examples in chapter 3, which are either not computational at all, or so complex that they are forever beyond any computing power we can muster. But complexity is not the only barrier. Another is that some problems are inherently outside of science and technology and cannot be solved by scientific and technological methods. A favorite category is「wicked problems」 — especially issues in the interactions of social communities and technologies. They defy solution when factions have enough power to defeat a proposal they dislike but not enough power to form a consensus. Examples are many: Millions of「clean」cars collectively produce unhealthy smog in dense cities. New information technology fosters the growth of income inequality where designers reap much more bounty than users. STEM education struggles to learn how to prepare students to face great uncertainty about the future of work, societal safety nets, technology, and climate change. The solutions to these problems are not scientific, technical, or computational but will emerge from social cooperation among the groups that now offer competing and conflicting approaches. Although computational thinking can help by visualizing the large-scale effects of individual actions, only social consensus and social action can resolve wicked problems.

Computational thinking is a powerful force within science. It emphasizes the「computational way」of doing science and makes its practitioners into skilled computational designers (and thinkers) in their fields of science. It brings forth new information interpretations in a diversity of disciplines. Computational thinkers in sciences spend much of their time modeling physical processes, designing solution methods for those processes, running simulations, and visualizing the results.

## References and Further Reading

Aho, Al. (2011). Computation and computational thinking.

Akera, Atshushi. (2007). Calculating a Natural World: Scientists, Engineers, and Computers During the Rise of U.S. Cold War Research. MIT Press.

Baltimore, David. (2001). How biology became an information science. In The Invisible Future. Peter Denning, ed., pp. 43–46. McGraw-Hill.

Denning, Peter. (2017). Remaining trouble spots with computational thinking. Communications of the ACM 60 (6) (June): 33–39.

Wilson, Ken. (1989). Grand challenges to computational science. In Future Generation Computer Systems, pp. 33–35. Elsevier.

Wolfram, Stephen. (2002). A New Kind of Science. Wolfram Media.