# 0201. Computational Methods

If controversies were to arise, there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in their hands, to sit down to their slates, and to say to each other (with a friend as witness if they liked): Let us calculate.

—— Leibniz, in Russell's translation (1937)

When Peter was age 10, his twinkly-eyed math teacher told him he could read minds.「How could you do that?」Peter asked. The teacher said,「Here, I'll show you. Think of a number. Double it. Add 8. Divide by 2. Subtract your original number. Now concentrate hard on the answer. ... There, I see it. The answer is 4.」Peter was so astonished he insisted that the teacher show him how to do this.「Well,」said the teacher,「it's just math. Let's say X is your number. Then I got you to calculate the expression (2X+8)÷2–X = 4. Your initial number was subtracted out. The answer is always half the number I told you to add.」Peter had many fine moments reading the minds of his family and friends with this method. He also got hooked on mathematics and computing.

This is one of many mathematical methods handed down over many generations. Methods like this are behind a pack of「automatic tricks」used by magicians, where the trickster leads the subject through a series of steps to an answer known to the trickster and believed by the subject to be a secret. The sleights of mind to accomplish this are in the math steps known to the trickster but not the subject. They work for any trickster who follows the directions, even if the trickster has no idea why the directions work.

Many other methods with a more serious purpose were handed down through the ages. One of the earliest methods, taught to many schoolchildren today, comes from the Greek mathematician Euclid around 300 BCE. He gave a method to find the greatest common divisor (GCD) of two numbers, which is the largest integer that divides both numbers. Euclid found a clever reduction rule by noticing that the GCD of two numbers divides their difference. He repeatedly replaced the larger number with their difference, until both were the same. For example, GCD(48,18) = GCD(30,18) = GCD(12,18) = GCD(12,6) = GCD(6,6) = 6. This method was used to reduce fractions. Today it is among the basic methods underlying cryptography.

Another famous method dating back to the Greeks was the Sieve of Eratosthenes, used to find all the prime numbers up to a limit. This method begins with a list of all the integers from 2 to the limit. It then crosses out all the multiples of 2, then all multiples of 3, then of 5, and so on. After each round of elimination, a new prime will be revealed; the next round crosses out all its multiples. This is a very efficient method to find small primes and has been adapted to finding large primes for keys in modern cryptographic systems.

The Greeks were also interested in calculating the areas of shapes. They did so by finding ways to tile the shapes with simple forms such as squares or triangles, and then successively reducing the dimensions of the forms until the original shape is almost completely filled with them. This method, first recorded in the period 400–350 BCE, was a precursor to better methods introduced in the modern calculus two thousand years later.

Many mathematicians used such methods to construct infinite series of simple terms that converged to some limit. Math books are filled with tables of series; mathematicians used them to replace long series with closed forms. One such example is the series , which gives a way to calculate the value of π, with greater precision when more terms are included.

The calculus, proposed independently by Newton and Leibniz around 1680, perfected the idea of approximating objects and curves by calculations over infinite series. The idea was to represent geometric forms and continuous curves with very small components that interacted locally — for example, fill the form with small boxes or model the curve as a series of tiny segments bound to their immediate neighbors with attractive forces. Then find a larger quantity such as area or length by adding the components. When the size of the components was allowed to approach zero, the expressions from these infinite sums would be exact. The rule of local interaction was represented as a derivative and the summation as an integral. Motivated by calculus, mathematicians evaluated functions by dividing time and space into small increments enumerated as a「grid」and iteratively calculated the value of a function at each grid point. This approach was a boon to physics, in which many mathematical models of physical phenomena were expressed as differential equations computable on finite grids.

Another famous mathematical method was the Gaussian elimination for solving systems of linear equations. Gauss worked with this method in the mid-1800s, although Chinese mathematicians knew it in 179 BCE. Very efficient forms of this algorithm are used in modern graphics displays to render three-dimensional objects in real time.

These few examples illustrate the richness and utility of the treasure trove of methods bequeathed to us over many centuries.

We can conclude from examining these methods that many were quite sophisticated. Their purpose was to capture, as a set of steps, what needed to be done in a complex calculation. Initially those steps were carried out by mathematicians, but with enough refinements a method could actually be used by anyone who could follow the directions. An important but subtle point is that the steps of the method had to be unambiguous. The less the ambiguity, the more trustworthy the method was in the hands of non-experts. It was a practice to reduce ambiguity by replacing fuzzy steps with precise chains of arithmetic and logic operations.

Beginning around 1650, some mathematicians started to look for machines to carry out the basic operations of common methods. Some methods were too complex to be easily remembered. Some methods needed to be iterated many times, and it was difficult for easily distracted human beings to complete them without errors. The machines would allow for much faster computation and fewer errors.

To build machines, mathematicians and inventors had to devise methods, such as the positioning of wheels and gears, to represent numbers with physical quantities. They also had to devise representations for logic steps such as a conditional jump or a loop. Today, representations of data and logic steps are important core elements of computational thinking. In the rest of this chapter we describe these aspects in more depth.

## 2.1 The Quest to Eliminate Intuition

The computational methods that evolved in the history of mathematics were intended to help builders, engineers, merchants, and scientists to calculate numbers. Ancient merchants invented number systems, accounting systems, and tools like the abacus to keep track of their businesses. Ancient engineers invented ways to build weapons of war and civilian structures of peace. All sought reliable methods of dealing with calculations involving large quantities of numbers so that their artifacts worked as intended and were dependable.

Their methods were handed down through apprenticeships and were worked mainly by experts. The experts developed rules-of-thumb, heuristics, learned hunches, and other intuitions that enabled them to solve problems that the uninitiated could not solve at all. The modern term「intuition」describes the expert's action of rapidly articulating a solution, based on extensive experience with similar situations. Intuition enables experts to find the essential core of the problem, skip unnecessary steps in solving it, and switch between solution approaches. Intuition is a manifestation of expertise and an enabler of new findings.

It might seem paradoxical then that throughout the ages much work in mathematics and logic has aimed at eliminating intuition from routine calculation and logical inference. Routine computing tasks were required to be as simple and「mechanical」as possible in order to always yield the same results regardless of who did the calculations. Mathematicians throughout history have sought to capture expertise into step-by-step procedures someone could follow with little training. Eliminating intuition from routine jobs did not mean eliminating experts, but rather making their expertise available to a large number of non-expert people.

The modern ideas of symbolic information representation, symbol processing, unambiguous computational steps, basic arithmetic, algorithms, synchronization of computations, and systematic error checking are all inheritances from those many centuries. By showing how mechanization of calculations has been a key feature in numerous developments of computational methods, our aim is to reveal how many computational thinking skills have been an integral part of many other kinds of thinking long before modern computing. Many features and practices of computational thinking support the designing of computations in many fields, not just in today's computer science.

## 2.2 Numerical Representations and Numerical Methods

Computational thinking, like much of modern science, relies on a process of representing things in the world in numbers or other symbols. A representation is an arrangement of symbols in an agreed format; each representation stands for something. We frequently use numbers to represent quantities such as census data on populations, business accounting ledgers, or engineering measurements. But numbers are not the only kinds of representation. Scientists and engineers represent phenomena with equations, such as a linear algebraic matrix for rotating an object or a differential equation for planetary motion. They also represent objects with mechanical artifacts such as models of buildings, wind tunnels, or planetary orreries. Numbers, equations, and models underpin the scientific ideals of measurement, experimentation, and reproducibility of results. In the computing age, representations with non-numeric symbols have become ubiquitous — for example, a Java language program, a bitmap of an image, or a soundtrack of music. Today we use the term digitization for the process of encoding almost any information in the form of data representations that can be processed by computers.

It might seem paradoxical that throughout the ages much work in mathematics and logic has aimed at eliminating intuition from routine calculation and logical inference. Eliminating intuition from routine jobs did not mean eliminating experts, but rather making their expertise available to a large number of non-expert people.

Some skeptics did not trust these computational methods because numerical calculations were too susceptible to tiny errors in the precision of parameters and variables of the computation. This led the designers of methods to find constraints to keep accumulating errors within acceptable bounds. Today's computing machines have the same problems because the machines have limited precision (such as 32 or 64 bits) and round-off errors could accumulate in poorly designed algorithms. The calculus was a breakthrough because it allowed designers systematic ways to limit errors in their finite-difference calculations.

## 2.3 Decomposing Computing Tasks

During the time leading up to World War II, the US Army developed ever more sophisticated artillery that could fire shells over several miles. Gunners needed to know how to aim their artillery given the range, the difference of elevation, and the winds. The Army commissioned teams of human computers to work out firing tables for these guns. The gunners simply looked up the proper angle and direction to aim their guns, given their measurements of range, elevations, and winds.

One of the most well known of these teams comprised women working at Aberdeen Proving Grounds around 1940. They organized into assembly lines, each one doing a different stage of computation, until they compiled the firing tables. For tools they used mechanical calculators that do basic arithmetic (add, subtract, multiply, divide). They followed programs (i.e., sets of procedures) that managers established to divide the work and to govern which intermediate calculations moved from one human computer to the next. As trained mathematicians, the human computers were able to spot errors in their computations and thus keep the firing tables error free.

Today's computational thinking follows a similar pattern learned from those days:

1 Break the entire computation into pieces that could be done by separate, interacting computers.

2 Arrange the computers to optimize their communication and messaging patterns — for example into an assembly line or as a massive parallel fan-out and join.

3 Include error checks into their methods so that recipients could verify that their inputs and outputs were correct.

Modern software designers are familiar with these principles under the following names: distributed computing, parallelization, and error checking. But those practices were not originally developed for machines — they were developed for human computers.

The US Army wanted to perform these computations at much larger scales and much faster than human teams could at Aberdeen, so it commissioned the electronic computer project ENIAC at University of Pennsylvania to do this. The designers of ENIAC faced huge challenges, such as learning how to build reliable electronic circuits to carry out the same computations much faster, and learning how to design the control programs and algorithms to prevent errors from accumulating in the computations. The method of decomposition of the task into unambiguous steps that passed data between them moved from being a management principle at Aberdeen into a design principle for automatic computers.

Concern over errors grew as machines became larger and more complex. Today in computer science we still teach this old wisdom: errors can happen at any stage of the computing process, including describing the problem, preparing the formulas, setting the constants, communicating data, recording and retrieving the data, carrying out the prescribed steps, or displaying the results.

## 2.4 Rules for Reasoning

Designing computations around unambiguous steps is not enough to give confidence that computations are free from errors. The steps must be strung together to follow a plan. At any stage in the computation the plan must tell us unambiguously what the next step is. Deciding what the next step is should be an exercise in logic.

A long-established branch of mathematics and philosophy has been concerned with logic. Can we provide the means to develop long chains of reasoning to solve problems and to verify that a given chain of reasoning is valid? As their counterparts in calculation, logicians sought ways to formalize and automate reasoning. Philosophers such as René Descartes and Gottfried Leibniz in the 1600s sought a language that would completely formalize human inference and reduce misunderstandings. Their goal was to establish a standard way to express concepts and rules of inference to definitively establish the truth or falsity of statements. According to their vision, such a「language of thought」would bring an end to disagreements in all domains, because every debate could be resolved through pure logic.

Progress toward this dream was slow. A breakthrough came in the 1800s. George Boole (1815–1864) was fascinated by how well-formulated, mathematical symbol systems were able to provide results for problems nearly automatically once the correct values were set in the formula. [1] In his book, Laws of Thought (1854), he presented「an algebra of thought」paralleling the algebra of numbers. His logic included variables whose values could be either true or false. He could form logical expressions, which were formulas of variables connected by operators such as and, or, and not. Nearly nine decades later (1937), Claude Shannon showed how Boole's algebra could describe the function of relay circuits in telephone systems and other electrical circuits. Boolean algebra was perfected for electronic circuit design in the 1950s, where it provided a means to find the smallest circuit for a given logic function and a means to design the circuit to avoid race conditions, which are ambiguous outputs caused by signals changing at different speeds in different parts of the circuit. Boolean algebra became a fixture of computer circuit design.

1『这里看到了 2 个熟人。George Boole，布尔，布尔值。Claude Shannon，香农，信息论。（2021-05-24）』

Despite its merits, Boole's algebra of logic had some serious limitations. Sentences that refer to sets, such as「everybody with a gray hair,」while perfectly understandable in natural language, could not be expressed in Boolean logic. There was no way to generate a list of entities satisfying a formula; the concepts of「everybody」and「somebody」had no clear meaning. There were no rules for the important quantifiers all and some.

Gottlob Frege (1848–1925) presented a new system of logical inference,「language for pure thought」(1879), which today is called predicate logic. It included new quantifiers for all and some and closed gaps in Boolean logic. Frege's system also presented mechanical rules for symbol processing that could be followed without appealing to human intuition. Frege's predicate logic resembles a programming language in that it provides an artificial, formal language that presents unambiguous, deterministic, and mechanical rules for symbol processing.

3『戈特洛布·弗雷格 (Gottlob Frege)，著名德国数学家、逻辑学家和哲学家。是数理逻辑和分析哲学的奠基人。（2021-05-24）』

In the early 1900s it looked like the vision for a formal language of thought was about to be fulfilled. The merger of mathematics and logic gave rise to Russell and Whitehead's magnum opus Principia Mathematica (1910) and to logical empiricism in the sciences. But one element was still missing to achieve the dream: a method for definitively deciding whether a statement in predicate logic was true or false. The question for whether such a method exists became known as the「decision problem.」By the 1920s, it was taken as one of the major challenges in mathematics. Most mathematical logicians believed that a method existed, but no one could find it.

## 2.5 Mechanizing Computation

In 1935, a young Cambridge mathematics student was introduced to the decision problem. He became fascinated by the words a lecturer used to pose it: Was there a mechanical process for deciding, in a finite number of steps, whether a proposition in predicate logic is true or false? That student, Alan Turing (1912–1954), decided to develop a thoroughly mechanistic model of computing so that he could investigate the decision problem.

Turing started with the idea that, when calculating numbers, a human computer writes down a series of symbols on the paper. He represented the paper as a linear sequence of boxes each holding a single symbol. In calculating, the person moves attention from the current box to either of its nearest neighbors, possibly changing the symbol in the box. He assumed that the mind of the person doing the calculation was in one of a finite number of states, and that each of these basic moves on the paper was generated by a transition from the current state to a specified next state. This process continues until the calculation is complete. Turing took these basic actions — when in a given state, move left or right one box, read symbol at the current box, change the symbol in the current box, and move to the next state — as the basic mechanics of carrying out a computation. Clearly a machine could do these steps and keep track of the states. He noted that this machine modeled steps in calculating numbers or evaluating logic functions. After demonstrating how such a machine would work, Turing showed that there is one such machine that can simulate all others — implying that the machine model is a universal way to represent all calculations and proofs. He then proved that no machine could solve the decision problem because the very existence of a machine that could do so led to a logical paradox. This tour-de-force eventually made him famous for his「Turing machine」and its implications for computation.

A few years later, the electronic digital computer provided the means to automate calculation and proof — finally realizing, at least to some extent, the visions of mechanizing calculation and reasoning. Automation was the key to all these developments. To emphasize this, Turing called his machines a-machines, with「a」meaning「automatic.」Similarly, the engineers designing the first electronic computers in the 1940s, such as UNIVAC and BINAC, gave them names ending in「-AC」meaning「automatic computer.」Through the 1980s, computer science itself was often characterized as the science of automation. The key aspect of automation demands that a machine do the work without human intervention. The automatic computer is the ultimate realization of the old dream of making calculation available for the masses without requiring them to be experts in doing calculations.

Another key aspect of automation is recognizing that automatic computers cannot perform certain important tasks. Turing showed this when he proved no a-machine could exist to solve the decision problem. His same reasoning shows that problems of practical interest — such as determining whether a given computer program will halt or instead enter an infinite loop, or whether a given program contains a computer virus — cannot be solved by any machine. For this reason, a large segment of CT is concerned with how to provide partial solutions to problems that cannot be solved by automatic computers.

The automatic computer and the understandings of its limitations would not have been implemented without the merger of calculation and logic. It is no wonder many people consider logical thinking an essential element of computational thinking.

## 2.6 Computational Thinking Insights Come from Many Fields

It should be clear from this discussion of the origins of computational thinking that CT is not about how computer scientists think. Modern computer science is the last 1 percent of the historical timeline of computational thinking. Computer scientists inherited and then perfected computational thinking from a long line of mathematicians, natural philosophers, scientists, and engineers all interested in performing large calculations and complex inferences without error. CT is a feature of many fields, not only computing.

Logicians wished to create formal systems where one could start from the premises and, by following chains of substitutions within a formal system of rules, would always arrive at the same conclusions. The logical insights of Boole and Shannon — that a few logical operations can express the truth values of all propositional logic as well as the logical design of digital circuits — were driven by an old quest to banish all human emotion and judgment from logical inference. These insights are counted today as the first principles of computing. Frege's logical insight — predicate logic — presented a more powerful system of inference having many similarities with modern programming languages. Turing's insight into the essential features of automatic processing — that five actions and a finite number of states are enough for any computation — came from mathematical logic.

Other basic insights of computational thinking arose from science and engineering. Among the most important is the realization that most computations in science and technology require unimaginably long calculations that are well beyond the capabilities of a human team. The designers of computational methods to solve practical problems are obsessively concerned with controlling and limiting errors by making the computational steps simple and unambiguous and their connective logic unimpeachable.

The computer of today is the machine many sought throughout the ages to automate calculation and free it from the frailties of humans and the need for their intervention and judgment. Modern computing researchers and professionals embody this long history and excel at automating computations using the best methods available. However, as we will see in the next chapter, the wish of building real systems for very large, error-free computations has been exceedingly difficult to achieve.

The computer of today is the machine many sought throughout the ages to automate calculation and free it from the frailties of humans and the need for their intervention and judgment.