## Epilogue: Lessons Learned

In the research for this book, we learned a few lessons that are worth summarizing here.

### Lesson 1: CT is an addition, not a replacement

Everyone thinks their own field's ways of thinking (and practicing) are valuable and worthy of learning in many other fields. Enthusiasts want to spread the gospel of success to other disciplines. The list of「thinkings」to be spread is long: computational thinking, logical thinking, economic thinking, systems thinking, physics thinking, mathematical thinking, engineering thinking, design thinking, computational thinking, and more.

Our conclusion is that computational thinking is often a welcome addition to other fields, but not a replacement for their ways of thinking and not a meta-skill for all fields.

### Lesson 2: CT is an old, well studied, and diverse topic

The term「computational thinking」(CT) became popular after the US National Science Foundation included it in a funding call in 2007. For many people it was the first time they heard arguments about the value of computing in education. CT seemed to be a new invention, a breakthrough portending a revolution in K–12 education. The truth is, human beings have been doing CT for over 4500 years. It has been advocated for K–12 education since the 1960s.

Some of the first「CT for K–12」curriculum designers attempted to build a「body of knowledge」for CT from scratch without being informed by the long history of computational thinking, including similar attempts to bring computing to schools. They unwittingly created some conceptual errors in their claims about the capabilities and character of CT. We are concerned because inflated expectations and conceptual problems can easily become a part of the CT folklore, and it may take years to dispel them. We urge computing educators to turn to the massive existing body of computing education research to clean this up.

### Lesson 3: The speed of computers is the main enabler of the computing revolution

Most of what software does for us is made possible by the incomprehensible speed gap between computers and humans—billions to trillions times faster. Even though humans can execute computational steps, they could not carry out most of these computations in their lifetimes. The machines can literally do the humanly impossible. While it is true that humans can personally perform algorithms for some information processing tasks, the revolutions of the computer age are not about where people can perform algorithms in their own lives, but about what computers are able to do for them.

### Lesson 4: Advanced CT is domain dependent

For advanced tasks, you need to understand the domain in which you want to figure out how to get a computer to do a job for you. For example, an expert programmer who knows nothing about quantum physics will have little to offer to a team of physicists working on a quantum computer. Similarly, working with the nature's complex algorithms in biology requires considerable understanding of biological processes. Algorithmic models in chemistry require deep familiarity with the corresponding chemical processes. Building an information system for a hospital requires extensive understanding of the institutional, informational, and workflow processes in the hospital context. Much of advanced computational thinking is context-specific and tightly tied to the application domain.

### Lesson 5: CT has changed the tools, methods, and epistemology of science

Computational thinking has fostered a revolution in science. Scientists in all fields have found that CT is a new method of doing science, different from the classic methods of theory and experiment. They came to this discovery in the 1980s when they began using supercomputers to crack scientific「grand challenges.」This was a profound paradigm shift that enabled many new scientific discoveries. Each field developed its own strain of CT that was not imported from computer science. Computer science CT has been enriched by its collaboration with the computational sciences.

### Lesson 6: The public face of CT is that of elementary CT

CT is billed for K–12 curriculum purposes as a set of concepts and rules for programming. But many professionals see CT as a design skill, and many natural scientists see it as an advanced method of scientific interpretation. Like all skills, you can be a beginner, advanced beginner, competent, proficient, expert, or master. Many debates about what CT「really」is seem to collapse different skill levels of CT within the same debate. For example, K–12 teachers argue for curricula that are almost solely aimed at beginners and that contain a small, teachable set of CT insights, practices, and skills. Other advocates argue for CT as advanced, professional skills that require many years of practice and experience. Failing to make the distinction leads to conflicts—for example, the hype about how learning programming opens career paths is silent about what professional computational designers do. Education efforts are important on all levels from K–12 through university and beyond.

### Lesson 7: Beginner and professional CT together comprise a rich tapestry of computational thought

Educators in K–12 schools have developed an impressive「CT for beginners」—insights and methods for teaching computing to newcomers. Professional software systems designers and scientists have developed an impressive「CT for professionals」—advanced methods for designing and building complex software that works reliably and safely, and for conducting scientific investigations. The synergy between these two aspects of computational thinking has propelled the computer revolution.

### Lesson 8: Change is an inseparable part of CT

There has never been a consensus about what computational thinking「really」is. There may never be a full consensus. During every decade in the modern history of computing there would be different answers to questions about the essence of computational thinking. Advances in computing keep computational thinking in constant change. We should embrace the lack of a fixed definition as a sign of the vitality of the field rather than our own failure to understand an eternal truth.

## Glossary

Abstraction

Simplifying complex phenomena by representing only their salient features, while omitting or hiding details.

Algorithm

Description of a method to compute a function, or more broadly, to solve a category of computational problems. All the steps are so precisely specified that a machine can perform them.

Artificial intelligence (AI)

The subfield of computer science that investigates whether computers powered by appropriate software can be intelligent (strong AI), or whether computers can simulate human cognitive tasks with information processes (weak AI).

Automation

Using machines to replace human controllers of physical processes (such as chemical plants or manufacturing lines), to perform knowledge-work processes (such as reviewing documents or processing invoices), or to build a computer to perform a task, replacing humans who formerly performed the task.

Bit and Byte

A bit is the smallest unit of information that distinguishes between something being present (1) or not present (0). A byte is a set of 8 bits, allowing 128 possible combinations of 8 bits. Large enough combinations of bits can stand for anything that can be represented by discrete values, such as numbers, characters, patterns on a display, or colors.

Boolean algebra

The set of expressions that can be formed from logic variables (each representing a single true-false bit) combined with operators such as OR, AND, and NOT. Boolean expressions are used in programming languages to specify conditions under which a statement will be executed. They are also used to describe the functions of logic circuits inside computers.

Central processing unit (CPU)

The hardware component of a computer that fetches and executes elementary instructions such as ADD, SUBTRACT, GO-TO, and COMPARE, and decides on what instructions are executed next. Other hardware components of a computer include the memory (which stores all data and instructions) and the input-output interface (which connects with the outside world).

Cloud, The

A worldwide network of storage systems and processing systems that can be accessed from anywhere just when and as needed. Users who rent data storage and processing time do not know where their data are physically stored and processed.

Compiler

A software program that translates programs written in a high-level programming language meant for humans into binary machine code meant for the processor.

Computational complexity

A subfield of computer science that investigates the intrinsic difficulty of solving problems. Difficulty is measured by the computational steps and memory space needed. Some problems like searching a list for a name are「easy」because they can be computed in time directly proportional to the length of the list. Some problems like finding the shortest tour of a set of cities are「hard」because in the worst case they require enumerating and measuring all the possible tours, the time for which grows exponentially fast as the number of cities and roads grows.

Computational model

The description of an abstract machine that performs algorithms—for example, a conventional computer chip that executes machine instructions one at a time, a neural network that recognizes faces in images, or a quantum computer that cracks cryptographic codes. In science and engineering, it also refers to a mathematical model of a physical process, which can be simulated or evaluated by a computer.

Computer

An entity, human or machine, that can perform calculations and symbol manipulations according to a set of precisely specified rules. From the 1600s to the 1930s,「computer」meant「a person who computes.」The first electronic computers in the 1940s were called「automatic computers.」The adjective「automatic」was dropped by the 1950s.

Data abstraction

A practice that originated with programmers in the 1960s to encapsulate a complicated data structure behind a simple interface. Users could access the data only through the interface; they could not directly access the memory holding the data. The view of the data seen through the interface is much simplified—hence the word abstraction. An example is a file, which looks to a user as a container of a linear string of bits; the interface allows only reading and writing. The actual file may be implemented as a set of blocks scattered around the storage medium, all hidden from the user.

Decision problem

A famous problem from mathematical logic in the early 1900s. Given a logical system consisting of axioms and rules for constructing proofs of propositions, is there an algorithm that will decide whether a given proposition is true? For a long time mathematicians believed there was such a procedure, but could not find it. In the 1930s a number of mathematicians, working independently from each other, formally defined the concept of algorithm and showed that there is no general solution to the decision problem.

Decomposition

Breaking a complex thing down to simpler, smaller parts that are easier to manage. In software, the parts become modules that are plugged together via interfaces.

Digitization

The work of constructing a binary coded representation of an entity. The representation could be processed by a computer. For example, the wave form of speech can be sampled 20,000 times a second, each sample producing a reading of the wave's amplitude and encoding it as a 16-bit value. The digitized speech can then be stored and processed on a computer.

DRUSS objectives

In software engineering, software systems that are dependable, reliable, usable, safe, and secure.

Encapsulation

Using interfaces to hide inner mechanisms and internal information from outside users in order to improve reusability, access restriction, protection of information from user errors, and maintainability.

Fractal

A term coined by mathematician Benoit Mandelbrot for sets that are self-similar at different scales. For example, the coast line of a country looks ragged in a satellite photo; it still looks ragged from a hang glider; and it still looks ragged under an up-close view of a wave rippling over the sand. Fractals have been used in graphics to draw complex objects from simple forms that can be repeated at all scales.

Generalization

Extending a solution to a broader class of similar problems.

Graphics processing unit (GPU)

A chip included in a computer to run the graphical display. Modern GPUs can hold 3D representations of objects and can rotate them to any angle or slide them to any distance computationally, then project the resulting image on to the 2D screen, all in real time.

Heuristics

Procedures for finding approximate solutions to computationally intractable problems. For example, in chess we evaluate proposed moves by a point-counting system for pieces lost; that is much less computing-intensive than enumerating all possible future chessboards. Good heuristics give solutions that are very good most of the time.

if-then-else construct

A form of statement in a programming language that selects between two or more alternative paths in program code. For example,「if sum≥0 then color sum-value black else color sum-value red」is used by accountants to highlight negative numbers in red on their spreadsheets.

Intuition

An aspect of embodied expertise where the expert is able to know immediately how to deal with a situation, based on extensive past experience. The expert may know what to do but cannot explain why.

Logarithm

In mathematics, the logarithm of a given number is the exponent to which a fixed base must be raised to produce that number. Thus, the log-base-2 of 8 is 3 because 23=8. Logarithms are useful for multiplying numbers since the product of two numbers adds their exponents. Take, for example, multiplying 8 by 16. Because 23×24 = 27, we can take the base-2 logs of the two terms (here 3 and 4, respectively), add the logs (yielding 7), and raise the base 2 to the power of the resulting log (here 27). Slide rules multiply by adding the logs of the two multiplicands.

Logic circuits

The basic electronic circuits in a computer. They combine binary signals with operations AND, OR, and NOT and store the results in registers, which are processed by more logic circuits in the next clock cycle.

Machine code

The instructions of an algorithm encoded into binary codes that a computer can recognize and execute.

Neural network

A form of circuit that takes a very large bit pattern as input (such as the 12 megapixels in a photograph) and produces an output (such as faces recognized in the photo). The components of the network are designed to be loosely similar to the neurons in the brain. The network learns by being trained rather than being programmed.

Operating system

The control program that runs a computer system. It allows users to log in and access their data, protects user data from being accessed by others without permission, schedules the resources (CPU, disks, memory) among competing users, and provides an environment in which users can run their programs.

Qubits

The basic elements of a quantum computer. They are the quantum-world analog of bits in a conventional computer, but they have a peculiar property called superposition, which means they can be in the 0 and 1 states simultaneously. Superposition significantly increases their representational and computing power. They are represented by electron spins or magnetic fields.

Race conditions

Many electronic circuits have multiple paths connecting an input to a particular output. If a change of the input travels at different speeds over the different paths, the value of the output can fluctuate randomly depending on the order the signals arrive. That random fluctuation can cause malfunctions in downstream circuits that use the output. Race conditions can also appear in operating systems where two users attempt simultaneous access to a file and the final value of the file depends on which one went last.

Registers

Processor registers are the basic building blocks of storage within a CPU. A register consists of a set of flip-flops, which are small circuits that can store a 0 or 1. Thus, an 8-bit register is made of 8 flip-flops. The CPU instructions combine values in registers and store their results in other registers.

Representation

Computing relies heavily on one thing standing for (representing) something else. Computations require information to be represented in a digital form, such as two values of voltage in circuits or the presence or absence of perturbations on materials. We use 0 and 1 to represent those physical phenomena.

Simulation

Computer simulations rely on computational models of phenomena to track the behavior of those phenomena over time. The elements of a model are theories, variables, equations, parameters, and other known features of the phenomenon in order to faithfully characterize the modeled system. Simulation uses these model elements to see how the system changes from one time unit to the next.

Transfer hypothesis

The hypothesis that learning computational thinking in computer science transfers to problem-solving ability in other fields. The hypothesis would predict that a person who came to be a good problem solver in computer science would be able to solve problems in physics with the same expertise. There is little empirical evidence to support this hypothesis.

Truth values

The two allowed values「true」and「false」of a logic variable. When presented in numbers,「0」is typically interpreted as false and either「1」or any nonzero value as true.

Turing test

A test proposed in 1950 by Alan Turing to settle the question of whether a machine can think. A human observer carries on two text-based conversations, one via a connection to a computer, the other a connection to another human being. The observer does not know which is which. If the observer is unable to definitely identify the human (or machine) over a long period, the machine would be deemed intelligent.

# Notes

Chapter 2



Chapter 4

1. Mahoney (2011);

2. Newell, Perlis, and Simon (1967).

3. Simon (1969).

4. Knuth (1974, 1985).

5. Dijkstra (1974).

6. Forsythe (1968).

7. Knuth (1985).

8. Guzdial (2014)

9. Arden (1980).

10. In his talk A Logical Revolution, Moshe Vardi describes the changing role and perceptions of logic in the field of computing, including the 1980s gloominess over what computers cannot do.

Chapter 5



Chapter 6

1. Forsythe (1966).

2. Grudin (1990).

3. Leveson (1995).

4. Parnas and Denning (2018).

5. Winograd (1983).

6. Denning (2016).

Chapter 7

1. Baltimore (2001).

2. Wilson (1989).

3. Baltimore (2001).

4. For the more mathematically inclined, the Mandelbrot set is the points in the complex plane at which the series of values of a function converges. A complex number is represented as a+bi, where i=sqrt(-1) and i2 = -1. The equation of the series is z(n+1) = z2(n)+c where z(n) and c are complex numbers. Having chosen a value of c, compute a series of z(n)-values starting with z(0)=c. (You may need to go to an algebra refresher for algorithms to multiply complex numbers.) If the z(n) sequence converges (stays within a short radius of c for all n), color the chosen value of c black. If it diverges color c blue or yellow. Now repeat this for all c points on a grid.

Chapter 8

1. Wing (2006)

2. Tedre, Simon, and Malmi (2018).

3. Minsky (1970)

4. Knuth (1974).

5. Bolter (1984)

6. Abelson and Sussman (1996)

7. Guzdial (2015)

8. Denning (2017).

9. See http://csfieldguide.org.nz and http://csunplugged.org.

Chapter 9

1. Denning and Lewis (2017).

2. McGeoch (2014).

3. See Walter Tichy's interview with Catherine McGeoch, Ubiquity July 2017, for a worked example of an Ising equation and its encoding into a form for the D-wave machine to solve, https://ubiquity.acm.org/article.cfm?id=3084688.

4. Adleman (1994).

5. Kurzweil (2006).

6. Wolfram (2002).

7. In April 2016, Scientific American magazine reported on a symposium of physicists and philosophers discussing the whole-world-is-computer hypothesis, giving the impression that they take more delight in entertaining themselves with the hypothesis than in the hypothesis itself. See https://www.scientificamerican.com/article/are-we-living-in-a-computer-simulation/.


Chapter 3

References and Further Reading

Aspray, William, ed. (1990). Computing Before Computers. Iowa State University Press.

Campbell-Kelly, Martin, and William Aspray. (2004). Computer: A History of the Information Machine. 2nd edition. Westview Press.

Ceruzzi, Paul E. (2003). A History of Modern Computing. 2nd edition. MIT Press.

Cortada, J. W. (1993). Before the Computer: IBM, NCR, Burroughs, and Remington Rand and the Industry They Created, 1865–1956. Princeton University Press.

Williams, Michael R. (1997). A History of Computing Technology. 2nd edition. IEEE Computer Society Press.

Chapter 4

References and Further Reading

Arden, Bruce W., ed. (1980). What Can Be Automated? Computer Science and Engineering Research Study. MIT Press.

Daylight, Edgar G. (2012). The Dawn of Software Engineering: From Turing to Dijkstra. Lonely Scholar.

Dijkstra, Edsger. W. (1974). Programming as a discipline of mathematical nature. American Mathematical Monthly 81 (6): 608–612.

Knuth, Donald E. (1974). Computer science and its relation to mathematics. American Mathematical Monthly 81 (April): 323–343.

Knuth, Donald E. (1985). Algorithmic thinking and mathematical thinking. American Mathematical Monthly 92 (March): 170–181.

Mahoney, Michael Sean. (2011). Histories of Computing. Harvard University Press.

Metropolis, N., J. Howlett, and Gian-Carlo Rota, eds. (1980). A History of Computing in the Twentieth Century: A Collection of Essays with Introductory Essay and Indexes. Academic Press.

Newell, Alan, Alan J. Perlis, and Herbert A. Simon. (1967). Computer science. Science 157 (3795): 1373–1374.

Simon, Herbert A. (1969). Sciences of the Artificial. MIT Press.

Smith, Brian C. (1998). On the Origin of Objects. MIT Press.

Chapter 5

Chapter 6

References and Further Reading

Brooks, Frederick P. Jr. (1975). The Mythical Man-Month. (20th anniversary edition, 1995). Addison-Wesley.

Denning, Peter. (2016). Software quality. Communications of ACM 59 (9) (September): 23–25.

Forsythe, George E. (1966). A University's Educational Program in Computer Science. Technical Report No. CS39, May 18, 1966. Stanford University: Computer Science Department, School of Humanities and Sciences.

Grudin, Jonathan. (1990). The computer reaches out: The historical continuity of interface design. In CHI '90: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 261–268. ACM.

Landwehr, Carl, et al. 2017. Software Systems Engineering Programmes: A Capability Approach. Journal of Systems and Software 125: 354–364.

Leveson, Nancy. (1995). SafeWare: System Safety and Computers. Addison-Wesley.

Norman, Donald A. (1993). Things That Make Us Smart. Basic Books.

Norman, Donald A. (2013). The Design of Everyday Things. First edition 1983. Basic Books.

Parnas, Dave, and Peter Denning. (2018). An interview with Dave Parnas. Communications of ACM 61 (6).

Winograd, Terry, and Flores, F. (1987). Understanding Computers and Cognition. Addison-Wesley.

Chapter 7

References and Further Reading

Aho, Al. (2011). Computation and computational thinking.

Akera, Atshushi. (2007). Calculating a Natural World: Scientists, Engineers, and Computers During the Rise of U.S. Cold War Research. MIT Press.

Baltimore, David. (2001). How biology became an information science. In The Invisible Future. Peter Denning, ed., pp. 43–46. McGraw-Hill.

Denning, Peter. (2017). Remaining trouble spots with computational thinking. Communications of the ACM 60 (6) (June): 33–39.

Wilson, Ken. (1989). Grand challenges to computational science. In Future Generation Computer Systems, pp. 33–35. Elsevier.

Wolfram, Stephen. (2002). A New Kind of Science. Wolfram Media.

Chapter 8

References and Further Reading

Abelson, Harold, and Gerald J. Sussman. (1996). Structure and Interpretation of Computer Programs. 2nd edition. MIT Press.

Bolter, J. David. (1984). Turing's Man: Western Culture in the Computer Age. University of North Carolina Press.

Denning, Peter. (2017). Remaining trouble spots with computational thinking. Communications of the ACM 60 (6) (June): 33–39.

Guzdial, Mark. (2015). Learner-Centered Design of Computing Education: Research on Computing for Everyone. Synthesis Lectures on Human-Centered Informatics. Morgan & Claypool.

Kestenbaum, David. (2005). The challenges of IDC: What have we learned from our past? Communications of the ACM 48 (1): 35–38. [A conversation with Seymour Papert, Marvin Minsky, Alan Kay]

Knuth, Donald E. (1974). Computer science and its relation to mathematics. American Mathematical Monthly 81 (April): 323–343.

Lockwood, James, and Aidan Mooney. (2017). Computational Thinking in Education: Where Does It Fit? A Systematic Literary Review. Technical report, National University of Ireland Maynooth.

Minsky, Marvin. (1970). Form and content in computer science. Journal of the ACM 17 (2): 197–215.

Tedre, Matti, Simon, and Lauri Malmi. (2018). Changing aims of computing education: a historical survey. Computer Science Education, June.

Wing, Jeanette M. (2006). Computational thinking. Communications of the ACM 49 (3): 33–35.

Chapter 9

References and Further Reading

Adleman, Leonard M. (1994). Molecular computation of solutions to combinatorial problems. Science 266 (5187): 1021–1024.

Brynjolfsson, E., and McAfee, A. (2014). The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies. W. W. Norton & Company.

Denning, Peter. J., and Ted G. Lewis. (2017). Exponential laws of computing growth. Communications of ACM 60 (1) (January): 54–65.

Friedman, Thomas. (2016). Thank You for Being Late. Farrar, Straus and Giroux.

Kelly, Kevin. (2017). The Inevitable: Understanding the 12 Technological Forces That Will Shape Our Future. Penguin Books.

Kurzweil, Ray. (2006). The Singularity Is Near. Penguin Books.

McGeoch, Catherine. (2014). Adiabatic Quantum Computation and Quantum Annealing. Synthesis Series on Quantum Computing. Morgan & Claypool.

Wolfram, Stephen. (2002). A New Kind of Science. Wolfram Media.
