## Epilogue: Lessons Learned

In the research for this book, we learned a few lessons that are worth summarizing here.

### Lesson 1: CT is an addition, not a replacement

Everyone thinks their own field's ways of thinking (and practicing) are valuable and worthy of learning in many other fields. Enthusiasts want to spread the gospel of success to other disciplines. The list of「thinkings」to be spread is long: computational thinking, logical thinking, economic thinking, systems thinking, physics thinking, mathematical thinking, engineering thinking, design thinking, computational thinking, and more.

Our conclusion is that computational thinking is often a welcome addition to other fields, but not a replacement for their ways of thinking and not a meta-skill for all fields.

### Lesson 2: CT is an old, well studied, and diverse topic

The term「computational thinking」(CT) became popular after the US National Science Foundation included it in a funding call in 2007. For many people it was the first time they heard arguments about the value of computing in education. CT seemed to be a new invention, a breakthrough portending a revolution in K–12 education. The truth is, human beings have been doing CT for over 4500 years. It has been advocated for K–12 education since the 1960s.

Some of the first「CT for K–12」curriculum designers attempted to build a「body of knowledge」for CT from scratch without being informed by the long history of computational thinking, including similar attempts to bring computing to schools. They unwittingly created some conceptual errors in their claims about the capabilities and character of CT. We are concerned because inflated expectations and conceptual problems can easily become a part of the CT folklore, and it may take years to dispel them. We urge computing educators to turn to the massive existing body of computing education research to clean this up.

### Lesson 3: The speed of computers is the main enabler of the computing revolution

Most of what software does for us is made possible by the incomprehensible speed gap between computers and humans—billions to trillions times faster. Even though humans can execute computational steps, they could not carry out most of these computations in their lifetimes. The machines can literally do the humanly impossible. While it is true that humans can personally perform algorithms for some information processing tasks, the revolutions of the computer age are not about where people can perform algorithms in their own lives, but about what computers are able to do for them.

### Lesson 4: Advanced CT is domain dependent

For advanced tasks, you need to understand the domain in which you want to figure out how to get a computer to do a job for you. For example, an expert programmer who knows nothing about quantum physics will have little to offer to a team of physicists working on a quantum computer. Similarly, working with the nature's complex algorithms in biology requires considerable understanding of biological processes. Algorithmic models in chemistry require deep familiarity with the corresponding chemical processes. Building an information system for a hospital requires extensive understanding of the institutional, informational, and workflow processes in the hospital context. Much of advanced computational thinking is context-specific and tightly tied to the application domain.

### Lesson 5: CT has changed the tools, methods, and epistemology of science

Computational thinking has fostered a revolution in science. Scientists in all fields have found that CT is a new method of doing science, different from the classic methods of theory and experiment. They came to this discovery in the 1980s when they began using supercomputers to crack scientific「grand challenges.」This was a profound paradigm shift that enabled many new scientific discoveries. Each field developed its own strain of CT that was not imported from computer science. Computer science CT has been enriched by its collaboration with the computational sciences.

### Lesson 6: The public face of CT is that of elementary CT

CT is billed for K–12 curriculum purposes as a set of concepts and rules for programming. But many professionals see CT as a design skill, and many natural scientists see it as an advanced method of scientific interpretation. Like all skills, you can be a beginner, advanced beginner, competent, proficient, expert, or master. Many debates about what CT「really」is seem to collapse different skill levels of CT within the same debate. For example, K–12 teachers argue for curricula that are almost solely aimed at beginners and that contain a small, teachable set of CT insights, practices, and skills. Other advocates argue for CT as advanced, professional skills that require many years of practice and experience. Failing to make the distinction leads to conflicts—for example, the hype about how learning programming opens career paths is silent about what professional computational designers do. Education efforts are important on all levels from K–12 through university and beyond.

### Lesson 7: Beginner and professional CT together comprise a rich tapestry of computational thought

Educators in K–12 schools have developed an impressive「CT for beginners」—insights and methods for teaching computing to newcomers. Professional software systems designers and scientists have developed an impressive「CT for professionals」—advanced methods for designing and building complex software that works reliably and safely, and for conducting scientific investigations. The synergy between these two aspects of computational thinking has propelled the computer revolution.

### Lesson 8: Change is an inseparable part of CT

There has never been a consensus about what computational thinking「really」is. There may never be a full consensus. During every decade in the modern history of computing there would be different answers to questions about the essence of computational thinking. Advances in computing keep computational thinking in constant change. We should embrace the lack of a fixed definition as a sign of the vitality of the field rather than our own failure to understand an eternal truth.

## Glossary

Abstraction

Simplifying complex phenomena by representing only their salient features, while omitting or hiding details.

Algorithm

Description of a method to compute a function, or more broadly, to solve a category of computational problems. All the steps are so precisely specified that a machine can perform them.

Artificial intelligence (AI)

The subfield of computer science that investigates whether computers powered by appropriate software can be intelligent (strong AI), or whether computers can simulate human cognitive tasks with information processes (weak AI).

Automation

Using machines to replace human controllers of physical processes (such as chemical plants or manufacturing lines), to perform knowledge-work processes (such as reviewing documents or processing invoices), or to build a computer to perform a task, replacing humans who formerly performed the task.

Bit and Byte

A bit is the smallest unit of information that distinguishes between something being present (1) or not present (0). A byte is a set of 8 bits, allowing 128 possible combinations of 8 bits. Large enough combinations of bits can stand for anything that can be represented by discrete values, such as numbers, characters, patterns on a display, or colors.

Boolean algebra

The set of expressions that can be formed from logic variables (each representing a single true-false bit) combined with operators such as OR, AND, and NOT. Boolean expressions are used in programming languages to specify conditions under which a statement will be executed. They are also used to describe the functions of logic circuits inside computers.

Central processing unit (CPU)

The hardware component of a computer that fetches and executes elementary instructions such as ADD, SUBTRACT, GO-TO, and COMPARE, and decides on what instructions are executed next. Other hardware components of a computer include the memory (which stores all data and instructions) and the input-output interface (which connects with the outside world).

Cloud, The

A worldwide network of storage systems and processing systems that can be accessed from anywhere just when and as needed. Users who rent data storage and processing time do not know where their data are physically stored and processed.

Compiler

A software program that translates programs written in a high-level programming language meant for humans into binary machine code meant for the processor.

Computational complexity

A subfield of computer science that investigates the intrinsic difficulty of solving problems. Difficulty is measured by the computational steps and memory space needed. Some problems like searching a list for a name are「easy」because they can be computed in time directly proportional to the length of the list. Some problems like finding the shortest tour of a set of cities are「hard」because in the worst case they require enumerating and measuring all the possible tours, the time for which grows exponentially fast as the number of cities and roads grows.

Computational model

The description of an abstract machine that performs algorithms—for example, a conventional computer chip that executes machine instructions one at a time, a neural network that recognizes faces in images, or a quantum computer that cracks cryptographic codes. In science and engineering, it also refers to a mathematical model of a physical process, which can be simulated or evaluated by a computer.

Computer

An entity, human or machine, that can perform calculations and symbol manipulations according to a set of precisely specified rules. From the 1600s to the 1930s,「computer」meant「a person who computes.」The first electronic computers in the 1940s were called「automatic computers.」The adjective「automatic」was dropped by the 1950s.

Data abstraction

A practice that originated with programmers in the 1960s to encapsulate a complicated data structure behind a simple interface. Users could access the data only through the interface; they could not directly access the memory holding the data. The view of the data seen through the interface is much simplified—hence the word abstraction. An example is a file, which looks to a user as a container of a linear string of bits; the interface allows only reading and writing. The actual file may be implemented as a set of blocks scattered around the storage medium, all hidden from the user.

Decision problem

A famous problem from mathematical logic in the early 1900s. Given a logical system consisting of axioms and rules for constructing proofs of propositions, is there an algorithm that will decide whether a given proposition is true? For a long time mathematicians believed there was such a procedure, but could not find it. In the 1930s a number of mathematicians, working independently from each other, formally defined the concept of algorithm and showed that there is no general solution to the decision problem.

Decomposition

Breaking a complex thing down to simpler, smaller parts that are easier to manage. In software, the parts become modules that are plugged together via interfaces.

Digitization

The work of constructing a binary coded representation of an entity. The representation could be processed by a computer. For example, the wave form of speech can be sampled 20,000 times a second, each sample producing a reading of the wave's amplitude and encoding it as a 16-bit value. The digitized speech can then be stored and processed on a computer.

DRUSS objectives

In software engineering, software systems that are dependable, reliable, usable, safe, and secure.

Encapsulation

Using interfaces to hide inner mechanisms and internal information from outside users in order to improve reusability, access restriction, protection of information from user errors, and maintainability.

Fractal

A term coined by mathematician Benoit Mandelbrot for sets that are self-similar at different scales. For example, the coast line of a country looks ragged in a satellite photo; it still looks ragged from a hang glider; and it still looks ragged under an up-close view of a wave rippling over the sand. Fractals have been used in graphics to draw complex objects from simple forms that can be repeated at all scales.

Generalization

Extending a solution to a broader class of similar problems.

Graphics processing unit (GPU)

A chip included in a computer to run the graphical display. Modern GPUs can hold 3D representations of objects and can rotate them to any angle or slide them to any distance computationally, then project the resulting image on to the 2D screen, all in real time.

Heuristics

Procedures for finding approximate solutions to computationally intractable problems. For example, in chess we evaluate proposed moves by a point-counting system for pieces lost; that is much less computing-intensive than enumerating all possible future chessboards. Good heuristics give solutions that are very good most of the time.

if-then-else construct

A form of statement in a programming language that selects between two or more alternative paths in program code. For example,「if sum≥0 then color sum-value black else color sum-value red」is used by accountants to highlight negative numbers in red on their spreadsheets.

Intuition

An aspect of embodied expertise where the expert is able to know immediately how to deal with a situation, based on extensive past experience. The expert may know what to do but cannot explain why.

Logarithm

In mathematics, the logarithm of a given number is the exponent to which a fixed base must be raised to produce that number. Thus, the log-base-2 of 8 is 3 because 23=8. Logarithms are useful for multiplying numbers since the product of two numbers adds their exponents. Take, for example, multiplying 8 by 16. Because 23×24 = 27, we can take the base-2 logs of the two terms (here 3 and 4, respectively), add the logs (yielding 7), and raise the base 2 to the power of the resulting log (here 27). Slide rules multiply by adding the logs of the two multiplicands.

Logic circuits

The basic electronic circuits in a computer. They combine binary signals with operations AND, OR, and NOT and store the results in registers, which are processed by more logic circuits in the next clock cycle.

Machine code

The instructions of an algorithm encoded into binary codes that a computer can recognize and execute.

Neural network

A form of circuit that takes a very large bit pattern as input (such as the 12 megapixels in a photograph) and produces an output (such as faces recognized in the photo). The components of the network are designed to be loosely similar to the neurons in the brain. The network learns by being trained rather than being programmed.

Operating system

The control program that runs a computer system. It allows users to log in and access their data, protects user data from being accessed by others without permission, schedules the resources (CPU, disks, memory) among competing users, and provides an environment in which users can run their programs.

Qubits

The basic elements of a quantum computer. They are the quantum-world analog of bits in a conventional computer, but they have a peculiar property called superposition, which means they can be in the 0 and 1 states simultaneously. Superposition significantly increases their representational and computing power. They are represented by electron spins or magnetic fields.

Race conditions

Many electronic circuits have multiple paths connecting an input to a particular output. If a change of the input travels at different speeds over the different paths, the value of the output can fluctuate randomly depending on the order the signals arrive. That random fluctuation can cause malfunctions in downstream circuits that use the output. Race conditions can also appear in operating systems where two users attempt simultaneous access to a file and the final value of the file depends on which one went last.

Registers

Processor registers are the basic building blocks of storage within a CPU. A register consists of a set of flip-flops, which are small circuits that can store a 0 or 1. Thus, an 8-bit register is made of 8 flip-flops. The CPU instructions combine values in registers and store their results in other registers.

Representation

Computing relies heavily on one thing standing for (representing) something else. Computations require information to be represented in a digital form, such as two values of voltage in circuits or the presence or absence of perturbations on materials. We use 0 and 1 to represent those physical phenomena.

Simulation

Computer simulations rely on computational models of phenomena to track the behavior of those phenomena over time. The elements of a model are theories, variables, equations, parameters, and other known features of the phenomenon in order to faithfully characterize the modeled system. Simulation uses these model elements to see how the system changes from one time unit to the next.

Transfer hypothesis

The hypothesis that learning computational thinking in computer science transfers to problem-solving ability in other fields. The hypothesis would predict that a person who came to be a good problem solver in computer science would be able to solve problems in physics with the same expertise. There is little empirical evidence to support this hypothesis.

Truth values

The two allowed values「true」and「false」of a logic variable. When presented in numbers,「0」is typically interpreted as false and either「1」or any nonzero value as true.

Turing test

A test proposed in 1950 by Alan Turing to settle the question of whether a machine can think. A human observer carries on two text-based conversations, one via a connection to a computer, the other a connection to another human being. The observer does not know which is which. If the observer is unable to definitely identify the human (or machine) over a long period, the machine would be deemed intelligent.

# Notes

Chapter 2

1. Davis (2012).

Chapter 4

1. Mahoney (2011);

2. Newell, Perlis, and Simon (1967).

3. Simon (1969).

4. Knuth (1974, 1985).

5. Dijkstra (1974).

6. Forsythe (1968).

7. Knuth (1985).

8. Guzdial (2014)

9. Arden (1980).

10. In his talk A Logical Revolution, Moshe Vardi describes the changing role and perceptions of logic in the field of computing, including the 1980s gloominess over what computers cannot do.

Chapter 5

1. Niklaus Wirth, software pioneer and the designer of the popular language Pascal, gives an excellent account of the development of programming practices and their supporting languages (Wirth 2008).

2. Stokes (1997).

3. Wilkes, in Metropolis, Howlett, and Rota (1980).

4. Wirth (2008).

5. Dijkstra (1980).

6. Saltzer and Schroeder (1975).

7. Alexander (1979).

8. Gamma et al. (1994).

9. Lampson (1983).

10. The levels principle was first used by Edsger Dijkstra in 1968 to organize the software of an operating system. It facilitated a correctness proof of the system because each level depended only on its components and the correctness of the lower levels, but not the higher levels. The discipline of designing a system as levels leads to much smaller and more easily verified systems.

Chapter 6

1. Forsythe (1966).

2. Grudin (1990).

3. Leveson (1995).

4. Parnas and Denning (2018).

5. Winograd (1983).

6. Denning (2016).

Chapter 7

1. Baltimore (2001).

2. Wilson (1989).

3. Baltimore (2001).

4. For the more mathematically inclined, the Mandelbrot set is the points in the complex plane at which the series of values of a function converges. A complex number is represented as a+bi, where i=sqrt(-1) and i2 = -1. The equation of the series is z(n+1) = z2(n)+c where z(n) and c are complex numbers. Having chosen a value of c, compute a series of z(n)-values starting with z(0)=c. (You may need to go to an algebra refresher for algorithms to multiply complex numbers.) If the z(n) sequence converges (stays within a short radius of c for all n), color the chosen value of c black. If it diverges color c blue or yellow. Now repeat this for all c points on a grid.

Chapter 8

1. Wing (2006)

2. Tedre, Simon, and Malmi (2018).

3. Minsky (1970)

4. Knuth (1974).

5. Bolter (1984)

6. Abelson and Sussman (1996)

7. Guzdial (2015)

8. Denning (2017).

9. See http://csfieldguide.org.nz and http://csunplugged.org.

Chapter 9

1. Denning and Lewis (2017).

2. McGeoch (2014).

3. See Walter Tichy's interview with Catherine McGeoch, Ubiquity July 2017, for a worked example of an Ising equation and its encoding into a form for the D-wave machine to solve, https://ubiquity.acm.org/article.cfm?id=3084688.

4. Adleman (1994).

5. Kurzweil (2006).

6. Wolfram (2002).

7. In April 2016, Scientific American magazine reported on a symposium of physicists and philosophers discussing the whole-world-is-computer hypothesis, giving the impression that they take more delight in entertaining themselves with the hypothesis than in the hypothesis itself. See https://www.scientificamerican.com/article/are-we-living-in-a-computer-simulation/.

References and Further Reading

Chapter 2

Davis, Martin. (2012). The Universal Computer: The Road from Leibniz to Turing. CRC Press.

Grier, David A. (2005). When Computers Were Human. Princeton University Press.

Hodges, Andrew. (1983). Alan Turing: The Enigma. Vintage Books.

Priestley, Mark. (2011). A Science of Operations: Machines, Logic and the Invention of Programming. Springer-Verlag.

Rapaport, William J. (2018). Philosophy of Computer Science. An online book draft, https://cse.buffalo.edu/~rapaport/Papers/phics.pdf.

Williams, Michael R. (1997). A History of Computing Technology. 2nd edition. IEEE Computer Society Press.

Chapter 3

Aspray, William, ed. (1990). Computing Before Computers. Iowa State University Press.

Campbell-Kelly, Martin, and William Aspray. (2004). Computer: A History of the Information Machine. 2nd edition. Westview Press.

Ceruzzi, Paul E. (2003). A History of Modern Computing. 2nd edition. MIT Press.

Cortada, J. W. (1993). Before the Computer: IBM, NCR, Burroughs, and Remington Rand and the Industry They Created, 1865–1956. Princeton University Press.

Williams, Michael R. (1997). A History of Computing Technology. 2nd edition. IEEE Computer Society Press.

Chapter 4

Arden, Bruce W., ed. (1980). What Can Be Automated? Computer Science and Engineering Research Study. MIT Press.

Daylight, Edgar G. (2012). The Dawn of Software Engineering: From Turing to Dijkstra. Lonely Scholar.

Dijkstra, Edsger. W. (1974). Programming as a discipline of mathematical nature. American Mathematical Monthly 81 (6): 608–612.

Knuth, Donald E. (1974). Computer science and its relation to mathematics. American Mathematical Monthly 81 (April): 323–343.

Knuth, Donald E. (1985). Algorithmic thinking and mathematical thinking. American Mathematical Monthly 92 (March): 170–181.

Mahoney, Michael Sean. (2011). Histories of Computing. Harvard University Press.

Metropolis, N., J. Howlett, and Gian-Carlo Rota, eds. (1980). A History of Computing in the Twentieth Century: A Collection of Essays with Introductory Essay and Indexes. Academic Press.

Newell, Alan, Alan J. Perlis, and Herbert A. Simon. (1967). Computer science. Science 157 (3795): 1373–1374.

Simon, Herbert A. (1969). Sciences of the Artificial. MIT Press.

Smith, Brian C. (1998). On the Origin of Objects. MIT Press.

Chapter 5

Alexander, Christopher. (1979). The Timeless Way of Building. Oxford University Press.

Brooks, Frederick P. Jr. (1975). The Mythical Man-Month. (20th anniversary edition, 1995). Addison-Wesley.

Brooks, Frederick P. Jr. (1987). No silver bullet: Essence and accidents of software engineering. IEEE Computer 20 (4): 10–19.

Campbell-Kelly, Martin. (2003). From Airline Reservations to Sonic the Hedgehog. MIT Press.

Denning, Peter. (2018). Interview with David Parnas. Communications of ACM 61 (6) (June).

Ensmenger, Nathan L. (2010). The Computer Boys Take Over: Computers, Programmers, and the Politics of Technical Expertise. MIT Press.

Gamma, Erich, Richard Helm, Ralph Johnson, and John Vlissides. (1994). Design Patterns: Elements of Reusable Object-Oriented Software. Addison-Wesley.

Koen, Billy V. (2003). Discussion of the Method: Conducting the Engineer's Approach to Problem Solving. Oxford University Press.

Lampson, Butler. (1983). Hints for computer system design. Proc. ACM Symposium on Operating Systems Principles, 33–48.

Metropolis, N., J. Howlett, and Gian-Carlo Rota, eds. (1980). A History of Computing in the Twentieth Century: A Collection of Essays with Introductory Essay and Indexes. Academic Press.

Mitcham, Carl. (1994). Thinking Through Technology: The Path Between Engineering and Philosophy. University of Chicago Press.

Saltzer, Jerome H., and Michael D. Schroeder. (1975). Protection of information computer systems. Proceedings of the IEEE 63 (9) (September): 1278–1308.

Stokes, Donald E. (1997). Pasteur's Quadrant—Basic Science and Technological Innovation. Brookings Institution Press.

Wirth, Niklaus. (2008). A brief history of software engineering. IEEE Annals of the History of Computing, 30 (3): 32–39.

Chapter 6

Brooks, Frederick P. Jr. (1975). The Mythical Man-Month. (20th anniversary edition, 1995). Addison-Wesley.

Denning, Peter. (2016). Software quality. Communications of ACM 59 (9) (September): 23–25.

Forsythe, George E. (1966). A University's Educational Program in Computer Science. Technical Report No. CS39, May 18, 1966. Stanford University: Computer Science Department, School of Humanities and Sciences.

Grudin, Jonathan. (1990). The computer reaches out: The historical continuity of interface design. In CHI '90: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 261–268. ACM.

Landwehr, Carl, et al. 2017. Software Systems Engineering Programmes: A Capability Approach. Journal of Systems and Software 125: 354–364.

Leveson, Nancy. (1995). SafeWare: System Safety and Computers. Addison-Wesley.

Norman, Donald A. (1993). Things That Make Us Smart. Basic Books.

Norman, Donald A. (2013). The Design of Everyday Things. First edition 1983. Basic Books.

Parnas, Dave, and Peter Denning. (2018). An interview with Dave Parnas. Communications of ACM 61 (6).

Winograd, Terry, and Flores, F. (1987). Understanding Computers and Cognition. Addison-Wesley.

Chapter 7

Aho, Al. (2011). Computation and computational thinking.

Akera, Atshushi. (2007). Calculating a Natural World: Scientists, Engineers, and Computers During the Rise of U.S. Cold War Research. MIT Press.

Baltimore, David. (2001). How biology became an information science. In The Invisible Future. Peter Denning, ed., pp. 43–46. McGraw-Hill.

Denning, Peter. (2017). Remaining trouble spots with computational thinking. Communications of the ACM 60 (6) (June): 33–39.

Wilson, Ken. (1989). Grand challenges to computational science. In Future Generation Computer Systems, pp. 33–35. Elsevier.

Wolfram, Stephen. (2002). A New Kind of Science. Wolfram Media.

Chapter 8

Abelson, Harold, and Gerald J. Sussman. (1996). Structure and Interpretation of Computer Programs. 2nd edition. MIT Press.

Bolter, J. David. (1984). Turing's Man: Western Culture in the Computer Age. University of North Carolina Press.

Denning, Peter. (2017). Remaining trouble spots with computational thinking. Communications of the ACM 60 (6) (June): 33–39.

Guzdial, Mark. (2015). Learner-Centered Design of Computing Education: Research on Computing for Everyone. Synthesis Lectures on Human-Centered Informatics. Morgan & Claypool.

Kestenbaum, David. (2005). The challenges of IDC: What have we learned from our past? Communications of the ACM 48 (1): 35–38. [A conversation with Seymour Papert, Marvin Minsky, Alan Kay]

Knuth, Donald E. (1974). Computer science and its relation to mathematics. American Mathematical Monthly 81 (April): 323–343.

Lockwood, James, and Aidan Mooney. (2017). Computational Thinking in Education: Where Does It Fit? A Systematic Literary Review. Technical report, National University of Ireland Maynooth.

Minsky, Marvin. (1970). Form and content in computer science. Journal of the ACM 17 (2): 197–215.

Tedre, Matti, Simon, and Lauri Malmi. (2018). Changing aims of computing education: a historical survey. Computer Science Education, June.

Wing, Jeanette M. (2006). Computational thinking. Communications of the ACM 49 (3): 33–35.

Chapter 9

Adleman, Leonard M. (1994). Molecular computation of solutions to combinatorial problems. Science 266 (5187): 1021–1024.

Brynjolfsson, E., and McAfee, A. (2014). The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies. W. W. Norton & Company.

Denning, Peter. J., and Ted G. Lewis. (2017). Exponential laws of computing growth. Communications of ACM 60 (1) (January): 54–65.

Friedman, Thomas. (2016). Thank You for Being Late. Farrar, Straus and Giroux.

Kelly, Kevin. (2017). The Inevitable: Understanding the 12 Technological Forces That Will Shape Our Future. Penguin Books.

Kurzweil, Ray. (2006). The Singularity Is Near. Penguin Books.

McGeoch, Catherine. (2014). Adiabatic Quantum Computation and Quantum Annealing. Synthesis Series on Quantum Computing. Morgan & Claypool.

Wolfram, Stephen. (2002). A New Kind of Science. Wolfram Media.

Index

Abacus, 28

Aberdeen Proving Ground, 33

Abstraction, 99, 118

data, 105

in engineering, 102

in science, 102

ACM (Association for Computing Machinery), 74, 185, 207

Acoustic delay lines, 55

Address-content distinction, 60

Advanced chess, 201

Advanced placement in computing, 187

Aggregation, hierarchical, 117–119

Agile methods, 134

Aiken, Howard, 52

Aircraft simulation, 167–169

Alexander, Christopher, 115, 127

ALGOL, 82

Algorithm, 1–2, 11

Algorithmic thinking, 79, 83–84

Algorithmizing, 180

AlphaGo game, 172, 201

Analytical engine, 50–51

Apple Mac operating system, 144

Architecture, von Neumann, 54

ARPANET, 81

Artificial intelligence, 90

strong, 93

weak, 93

Assembly language, 81

ATM (automatic teller machine), 142

Automatic programming, 200

Automation, 12, 39, 46, 85–90

of cognitive tasks, 197

Babbage, Charles, 12, 45, 49–52

Bauer, Fritz, 95

Bell, Tim, 191

Bernoulli numbers, 51

Big O-notation, 85

BINAC, 39

Binary code, 54

Bit, origin of term, 55

Boeing 777 aircraft, 150

Boole, George, 36, 56

Boolean algebra, 36, 56

British Computer Society, 185

Brooks, Frederick, 99, 110, 111, 121, 127

Bugs, 108

Bush, Vannevar, 101

Calculus, 26

Call instruction, 60

Chess, 46

advanced, 201

Chomsky, Noam, 77

Church, Alonzo, 77

Clients and servers, 121

Clock, 9

in CPU cycle, 57

Cloud, 171

COBOL, 82

Code optimization, 85

Code.org, 188

Compiler, 82, 85

Computation

as automation, 3, 12

definition, 5

by humans, xiii, 10

intractable, 7

by machines, xiii

mindlessness of, 9

numerical, 5

symbolic, 5

Computational

complexity, 81, 88

Fluid Dynamics, 160, 167

lens, 3

machines, 18

methods, 18

model, 161–162, 194–196

science, 15, 19

science, movement, 75

steps, 9, 27, 29, 41, 57, 58, 64, 88, 194, 196

Computational thinking

by Babylonians, 11

for beginners, xii, 188, 217

definition, ix, 4, 217

as design, 4

by Egyptians, 11

as explanation, 4

in the large, 100, 109–111

origin of term, 14

for professionals, xii, 188, 217

in science, 15

six dimensions, 17–19

as skill set, 6

in the small, 100, 103–107

troublespots, 189–190, 214

two faces of, x

view, strong, 94

weak, 94

Computer

automatic, 39

definition, 1, 5, 6

fluency, 178

general purpose, 49

human, 10

human, 214

literacy, 175, 176

revolution, ix, 1–2

universal, 39

Computer science

departments, 14, 73

for all, 17

principles course, 16

Unplugged, 191

Computer Science Teachers Association (CSTA), 188

Computing at School (CAS), 188

Computing education, 18

Confinement, 63

Constructionism, 183

Control flow, 58

Correctness proofs, 85

COSERS, 87

CRISPR gene editing, 196

CS. See Computer science

CS&E, 74

CT. See Computational thinking

Data abstraction, 105, 118

Data protection, 62

DEC VMS operating system, 144

Decision problem, 37

Decomposition, 34, 118

Deep learning, 172

Derivative. See Calculus

Descartes, René, 35

Design, 4, 19, 20, 197

hints, 115–116

in engineering, 101

in science, 101

patterns, 114–115

patterns, 134

principles, 113–114

way, 146–147

Difference engine, 50

Differential equation models, 154

Difficulty, of problems, 88

Digitization, 30

Dijkstra, Edsger, 83

DNA computing, 195, 196

Doctor's waiting room, 166–167

Domain dependence, 8

Domain dependency, 215

DRUSS objectives, 108, 113, 116, 129, 140

Dynabook, 182

D-wave computer, 195

Eckert, J. Presper, 53

Edifying conversations, 209–212

Encapsulation, 118

Engineering, definition, 101

Engineering-design view, 85, 101–103, 206

ENIAC, 34, 53, 153

Errors

elimination of, 13

in computational thinking, 214

in mathematical tables, 47

representation, 32

round off, 32

Euclid's algorithm, 24

Explainability, 172

of neural networks, 200

Fault tolerance, 62, 124

Fellows, Michael, 191

Fluency, 16, 185

Formal verification, 81

Forsythe, George, 71, 127, 180

FORTRAN, 82

Fractal, 164

Fragility, 126, 172

Frege, Gottlob, 37

Functions, 60

Gauss, Carl, 26

Gaussian elimination 26

General purpose thinking tools, 179–181

Genetic algorithm, 169–171

Grand challenge problems, 15, 150, 154–155, 171–174, 216

Graphics chips for neural networks, 199, 200

Graphics processing unit (GPU), 67

Greatest common divisor, 24

Grid, computational, 26

Halting problem, 40

Hardware, secure, 124

Heuristic, 8, 89

Hewlett-Packard Company, 48

Holland, John, 170

Hollerith, Herman, 12, 49

Hour of code, 188

HP-35 calculator, 48

Human computer team, 33, 201–203

Human computers, 214

IEEE (Institute of Electrical and Electronics Engineers), 74, 185

If-then-else, 58

Industrial Age, x

Infinite series, 25

Inflection point, 204

Informatics, 74

Information hiding, 118

Information interpretation

in science, 159–160

of world, 3

Information object, 118

Information, symbolic, 29

Integral. See Calculus

Interfaces, 118

Intuition, 28

Ising model, 195

ISO software standards, 137, 141

Jacquard loom, 48, 51

Java, 83

Job(s)

definition, 6

impossible, 6, 7

intractable, 7

losses to automation, 2, 6

Kasparov, Garry, 201

Kernel, of operating system, 106, 112, 124

Keuffel and Esser Company, 47

Kleene, Stephen, 77

Knuth, Donald, 1, 83, 181

Kurzweil, Ray, 193, 203–205

Lampson, Butler, 115

Leibniz, Gottfried, 11, 23, 26, 35

LISP, 82

Literacy, 16, 185

Logarithm, 12

Logic, 35

Logic, predicate, 37

Logo (language), 182

Loops, 59

Loops, infinite, 59

Lotus 1-2-3, 144

Lovelace, Ada, 51

Machine, universal, 38

Machine learning, 125, 199–201. See also Neural network

Magician, computational, 24

Malware, 124

Mandelbrot set, 162–164

Mariner I disaster, 95–96

Mark I computer, 53

Mauchly, John, 53

Mechanical steps, 29

Mechanical Turk, 46

Mechanization, 29, 38

Memory partitioning, 63

Messages, inter-process, 106

Microsoft Word, 144

Minsky, Marvin, 181

Modeling and simulation, 161

Modularity, 118

Moore School, 53, 76, 153

Moore's law, 65, 150, 154, 194, 195, 203, 204

Musa al Khwarizmi, Muhammad ibn, 11

Napier, John, 12

NATO software conferences, 99

Neural network, 9, 67, 125, 172, 195

training of, 67

Newell, Allen, 78

Newton, Isaac, 11, 26

Numerical analysis, 153

Objects, class, 119

Operating systems, 80

Orrery, 12, 30

Papert, Seymour, 175, 179, 182–183

Parnas, David, 129

Parsing, 85

Pascal, Blaise, 12, 47

Perlis, Alan, 78, 180

Post, Emil, 77

Prime numbers, 25

Problem solving, xiii

Procedures, 60

Process, 106, 120

homing position, 59

service, 59

Program counter, 58

Programming

functional, 83

imperative, 82

language, 81

in the large, 97

object-oriented, 83, 105

paradigms, 82

in the small, 97

Python (language), 186

Quantum computer, 9, 68, 195

D-wave, 195

Queueing theory, 166

Rabin, Michael, 77

Randell, Brian, 111

Representation, 28, 30, 54

error, 32

round-off error, 32

Revolution in science, 215

Rice, John, 153

RISC computers, 125

RSA cryptosystem, 68

Russell, Bertrand, 37

Saltzer, Jerome, 113

Sandbox, 64

Schroeder, Michael, 113

Science, definition, 101

Science-mathematics view, 85, 101–103, 206

Scott, Dana, 77

S-curve, 204

Series, infinite, 25

Shannon, Claude, 36, 56

Sieve of Eratosthenes, 25

Simon, Herbert, 78

Simulation, 15, 150–152

Singularity, 203–205

Skills, at computational thinking, 216

Slide rule, 12, 47

SmallTalk, 83

Software

as control for computers, 48

crises, 97–100, 107–109

engineering, 18–19, 75, 81

mass production of, 126

mathematical, 153

as product, 100, 107

productivity, 122

quality, 137–146

safety, 126

Speed

importance of, 214

of machines, xiv

STEM education, 173, 186

Steps

algorithmic, 57

computational, 27, 57

Stored-program computer, 52–54

Subprograms, 60

Subroutines, 60

Supercomputers in science, 154

Technology jumping, 203–205

Telephone engineers, 164–166

Texas Instruments Company, 48

Thought

language of, 35, 37

laws of, 36

Time-sharing systems, 78–79

Training, of neural networks, 67, 125, 126, 172, 199, 200, 201, 211

Transfer hypothesis, 183–184

Traveling salesman problem, 7

Trust, 13, 173

Turing, Alan, 38, 59, 61, 87, 119

Turing machine, 38–39

Unambiguity, of computational steps, 27

UNIVAC, 39

Universal machine, 38, 61–62

UNIX operating system, 144

Virtual machines, 119–121

Virtual memory, 106

Virtual world, 136

of games, 136

of software design, 136

VisiCalc, 144

von Ahn, Luis, 202

von Neumann, John, 53, 149, 154

von Neumann architecture, limits, 64

Warranties, software, 109

Whitehead, Alfred, 37

Whitten, Ian, 191

Whole world hypothesis, 205–206

Wicked problems, 173–174

Wilson, Kenneth, 157, 164

Wing, Jeannette, 16, 178, 187

Winograd, Terry, 134, 139

Wirth, Niklaus, 112

Zuse, Konrad, 52

The MIT Press Essential Knowledge Series

Auctions, Timothy P. Hubbard and Harry J. Paarsch

The Book, Amaranth Borsuk,

Carbon Capture, Howard J. Herzog

Cloud Computing, Nayan B. Ruparelia

Computing: A Concise History, Paul E. Ceruzzi

Computational Thinking, Peter J. Denning and Matti Tedre

The Conscious Mind, Zoltan E. Torey

Crowdsourcing, Daren C. Brabham

Data Science, John D. Kelleher and Brendan Tierney

Extremism, J. M. Berger

Free Will, Mark Balaguer

The Future, Nick Montfort

GPS, Paul E. Ceruzzi

Haptics, Lynette A. Jones

Information and Society, Michael Buckland

Information and the Modern Corporation, James W. Cortada

Intellectual Property Strategy, John Palfrey

The Internet of Things, Samuel Greengard

Machine Learning: The New AI, Ethem Alpaydin

Machine Translation, Thierry Poibeau

Memes in Digital Culture, Limor Shifman

Metadata, Jeffrey Pomerantz

The Mind–Body Problem, Jonathan Westphal

MOOCs, Jonathan Haber

Neuroplasticity, Moheb Costandi

Open Access, Peter Suber

Paradox, Margaret Cuonzo

Post-Truth, Lee McIntyre

Robots, John Jordan

School Choice, David R. Garcia

Self-Tracking, Gina Neff and Dawn Nafus

Spaceflight, Michael J. Neufeld

Sustainability, Kent E. Portney

Synesthesia, Richard E. Cytowic

The Technological Singularity, Murray Shanahan

3D Printing, John Jordan

Understanding Beliefs, Nils J. Nilsson

Waves, Frederic Raichlen

Peter J. Denning is Distinguished Professor of Computer Science, Chair of Computer Science Department, Naval Postgraduate School, Monterey, CA.

Matti Tedre is Professor of Computer Science, University of Eastern Finland.