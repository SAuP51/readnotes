# 0102. Basics of Neural Network programming

2.1 二分类 (Binary Classification)

这周我们将学习神经网络的基础知识，其中需要注意的是，当实现一个神经网络的时候，我们需要知道一些非常重要的技术和技巧。例如有一个包含𝑚个样本的训练集，你很可能习 惯于用一个 for 循环来遍历训练集中的每个样本，但是当实现一个神经网络的时候，我们通 常不直接使用 for 循环来遍历整个训练集，所以在这周的课程中你将学会如何处理训练集。

另外在神经网络的计算中，通常先有一个叫做前向暂停 (forward pause) 或叫做前向传播 (foward propagation) 的步骤，接着有一个叫做反向暂停 (backward pause) 或叫做反向传播 (backward propagation) 的步骤。所以这周我也会向你介绍为什么神经网络的训练过程可以分 为前向传播和反向传播两个独立的部分。

在课程中我将使用逻辑回归 (logistic regression) 来传达这些想法，以使大家能够更加容易 地理解这些概念。即使你之前了解过逻辑回归，我认为这里还是有些新的、有趣的东西等着 你去发现和了解，所以现在开始进入正题。

逻辑回归是一个用于二分类 (binary classification) 的算法。首先我们从一个问题开始说起，这里有一个二分类问题的例子，假如你有一张图片作为输入，比如这只猫，如果识别这张图 片为猫，则输出标签 1 作为结果；如果识别出不是猫，那么输出标签 0 作为结果。现在我们 可以用字母𝑦来表示输出的结果标签，如下图所示：

我们来看看一张图片在计算机中是如何表示的，为了保存一张图片，需要保存三个矩阵，它们分别对应图片中的红、绿、蓝三种颜色通道，如果你的图片大小为 64x64 像素，那么你 就有三个规模为 64x64 的矩阵，分别对应图片中红、绿、蓝三种像素的强度值。为了便于表 示，这里我画了三个很小的矩阵，注意它们的规模为 5x4 而不是 64x64，如下图所示：

18 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

为了把这些像素值放到一个特征向量中，我们需要把这些像素值提取出来，然后放入一 个特征向量𝑥。为了把这些像素值转换为特征向量 𝑥，我们需要像下面这样定义一个特征向 量 𝑥 来表示这张图片，我们把所有的像素都取出来，例如 255、231 等等，直到取完所有的 红色像素，接着最后是 255、134、…、255、134 等等，直到得到一个特征向量，把图片中所 有的红、绿、蓝像素值都列出来。如果图片的大小为 64x64 像素，那么向量 𝑥 的总维度，将是 64 乘以 64 乘以 3，这是三个像素矩阵中像素的总量。在这个例子中结果为 12,288。现 在我们用𝑛 𝑥 = 12,288，来表示输入特征向量的维度，有时候为了简洁，我会直接用小写的𝑛 来表示输入特征向量𝑥的维度。所以在二分类问题中，我们的目标就是习得一个分类器，它 以图片的特征向量作为输入，然后预测输出结果𝑦为 1 还是 0，也就是预测图片中是否有猫：

接下来我们说明一些在余下课程中，需要用到的一些符号。符号定义 ： 𝑥：表示一个𝑛 𝑥 维数据，为输入数据，维度为 (𝑛 𝑥 , 1)；

19 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

𝑦：表示输出结果，取值为 (0,1)； (𝑥 (𝑖) , 𝑦 (𝑖) )：表示第𝑖组数据，可能是训练数据，也可能是测试数据，此处默认为训练数 据； 𝑋 = [𝑥 (1) , 𝑥 (2) , . . . , 𝑥 (𝑚) ]：表示所有的训练数据集的输入值，放在一个 𝑛 𝑥 × 𝑚的矩阵中，其中𝑚表示样本数目；

𝑌 = [𝑦 (1) , 𝑦 (2) , . . . , 𝑦 (𝑚) ]：对应表示所有训练数据集的输出值，维度为 1 × 𝑚。

用一对 (𝑥, 𝑦) 来表示一个单独的样本，𝑥代表𝑛 𝑥 维的特征向量，𝑦 表示标签 (输出结果) 只 能为 0 或 1。而训练集将由𝑚个训练样本组成，其中 (𝑥 (1) , 𝑦 (1) ) 表示第一个样本的输入和输 出，(𝑥 (2) , 𝑦 (2) ) 表示第二个样本的输入和输出，直到最后一个样本 (𝑥 (𝑚) , 𝑦 (𝑚) )，然后所有的 这些一起表示整个训练集。有时候为了强调这是训练样本的个数，会写作𝑀 𝑡𝑟𝑎𝑖𝑛 ，当涉及到 测试集的时候，我们会使用𝑀 𝑡𝑒𝑠𝑡 来表示测试集的样本数，所以这是测试集的样本数：

最后为了能把训练集表示得更紧凑一点，我们会定义一个矩阵用大写𝑋的表示，它由输 入向量𝑥 (1) 、𝑥 (2) 等组成，如下图放在矩阵的列中，所以现在我们把𝑥 (1) 作为第一列放在矩阵 中，𝑥 (2) 作为第二列，𝑥 (𝑚) 放到第𝑚列，然后我们就得到了训练集矩阵𝑋。所以这个矩阵有𝑚 列，𝑚是训练集的样本数量，然后这个矩阵的高度记为𝑛 𝑥 ，注意有时候可能因为其他某些原 因，矩阵𝑋会由训练样本按照行堆叠起来而不是列，如下图所示：𝑥 (1) 的转置直到𝑥 (𝑚) 的转 置，但是在实现神经网络的时候，使用左边的这种形式，会让整个实现的过程变得更加简单：

现在来简单温习一下:𝑋是一个规模为𝑛 𝑥 乘以𝑚的矩阵，当你用 Python 实现的时候，你 会看到 X.shape，这是一条 Python 命令，用于显示矩阵的规模，即 X.shape 等于 (𝑛 𝑥 , 𝑚)，

20 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

𝑋是一个规模为𝑛 𝑥 乘以𝑚的矩阵。所以综上所述，这就是如何将训练样本（输入向量𝑋的集 合）表示为一个矩阵。

那么输出标签𝑦呢？同样的道理，为了能更加容易地实现一个神经网络，将标签𝑦放在列 中将会使得后续计算非常方便，所以我们定义大写的𝑌等于𝑦 (1) , 𝑦 (𝑚) , . . . , 𝑦 (𝑚) ，所以在这里 是一个规模为 1 乘以𝑚的矩阵，同样地使用 Python 将表示为 Y.shape 等于 (1, 𝑚)，表示这 是一个规模为 1 乘以𝑚的矩阵。

当你在后面的课程中实现神经网络的时候，你会发现，一个好的符号约定能够将不同训

练样本的数据很好地组织起来。而我所说的数据不仅包括 𝑥 或者 𝑦 还包括之后你会看到 的其他的量。将不同的训练样本的数据提取出来，然后就像刚刚我们对 𝑥 或者 𝑦 所做的那 样，将他们堆叠在矩阵的列中，形成我们之后会在逻辑回归和神经网络上要用到的符号表示。如果有时候你忘了这些符号的意思，比如什么是 𝑚，或者什么是 𝑛，或者忘了其他一些东 西，我们也会在课程的网站上放上符号说明，然后你可以快速地查阅每个具体的符号代表什 么意思，好了，我们接着到下一个视频，在下个视频中，我们将以逻辑回归作为开始。备 注：附录里也写了符号说明。

21 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.2 逻辑回归 (Logistic Regression)

在这个视频中，我们会重温逻辑回归学习算法，该算法适用于二分类问题，本节将主要 介绍逻辑回归的 Hypothesis Function（假设函数）。

对于二元分类问题来讲，给定一个输入特征向量𝑋，它可能对应一张图片，你想识别这 张图片识别看它是否是一只猫或者不是一只猫的图片，你想要一个算法能够输出预测，你只 能称之为𝑦，也就是你对实际值 𝑦 的估计。更正式地来说，你想让 𝑦 表示 𝑦 等于 1 的一 种可能性或者是机会，前提条件是给定了输入特征𝑋。换句话来说，如果𝑋是我们在上个视 频看到的图片，你想让 𝑦 来告诉你这是一只猫的图片的机率有多大。在之前的视频中所说 的，𝑋是一个𝑛 𝑥 维的向量（相当于有𝑛 𝑥 个特征的特征向量）。我们用𝑤来表示逻辑回归的参 数，这也是一个𝑛 𝑥 维向量（因为𝑤实际上是特征权重，维度与特征向量相同），参数里面还 有𝑏，这是一个实数（表示偏差）。所以给出输入𝑥以及参数𝑤和𝑏之后，我们怎样产生输出 预测值𝑦，一件你可以尝试却不可行的事是让𝑦 = 𝑤 𝑇 𝑥 + 𝑏。

这时候我们得到的是一个关于输入𝑥的线性函数，实际上这是你在做线性回归时所用到 的，但是这对于二元分类问题来讲不是一个非常好的算法，因为你想让𝑦表示实际值𝑦等于 1 的机率的话，𝑦 应该在 0 到 1 之间。这是一个需要解决的问题，因为𝑤 𝑇 𝑥 + 𝑏可能比 1 要大 得多，或者甚至为一个负值。对于你想要的在 0 和 1 之间的概率来说它是没有意义的，因此 在逻辑回归中，我们的输出应该是𝑦等于由上面得到的线性函数式子作为自变量的 sigmoid 函数中，公式如上图最下面所示，将线性函数转换为非线性函数。

下图是 sigmoid 函数的图像，如果我把水平轴作为𝑧轴，那么关于𝑧的 sigmoid 函数是这 样的，它是平滑地从 0 走向 1，让我在这里标记纵轴，这是 0，曲线与纵轴相交的截距是 0.5，这就是关于𝑧的 sigmoid 函数的图像。我们通常都使用𝑧来表示𝑤 𝑇 𝑥 + 𝑏的值。

22 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

关于 sigmoid 函数的公式是这样的，𝜎(𝑧) =

1 1+𝑒 

−𝑧 , 在这里𝑧是一个实数，这里要说明一些

要注意的事情，如果𝑧非常大那么𝑒 −𝑧 将会接近于 0，关于𝑧的 sigmoid 函数将会近似等于 1 除 以 1 加上某个非常接近于 0 的项，因为𝑒 的指数如果是个绝对值很大的负数的话，这项将会 接近于 0，所以如果𝑧很大的话那么关于𝑧的 sigmoid 函数会非常接近 1。相反地，如果𝑧非常 小或者说是一个绝对值很大的负数，那么关于𝑒 −𝑧 这项会变成一个很大的数，你可以认为这 是 1 除以 1 加上一个非常非常大的数，所以这个就接近于 0。实际上你看到当𝑧变成一个绝 对值很大的负数，关于𝑧的 sigmoid 函数就会非常接近于 0，因此当你实现逻辑回归时，你的 工作就是去让机器学习参数𝑤以及𝑏这样才使得𝑦成为对𝑦 = 1 这一情况的概率的一个很好的 估计。

在继续进行下一步之前，介绍一种符号惯例，可以让参数𝑤和参数𝑏分开。在符号上要 注意的一点是当我们对神经网络进行编程时经常会让参数𝑤和参数𝑏分开，在这里参数𝑏对应 的是一种偏置。在之前的机器学习课程里，你可能已经见过处理这个问题时的其他符号表示。比如在某些例子里，你定义一个额外的特征称之为𝑥 0 ，并且使它等于 1，那么现在𝑋就是一 个𝑛 𝑥 加 1 维的变量，然后你定义𝑦 = 𝜎(𝜃 𝑇 𝑥) 的 sigmoid 函数。在这个备选的符号惯例里，你 有一个参数向量𝜃0 , 𝜃 1 ,𝜃 2 , . . . , 𝜃 𝑛 𝑥 ，这样𝜃 0 就充当了𝑏，这是一个实数，而剩下的𝜃 1 直到𝜃𝑛 𝑥 充当了𝑤，结果就是当你实现你的神经网络时，有一个比较简单的方法是保持𝑏和𝑤分开。但 是在这节课里我们不会使用任何这类符号惯例，所以不用去担心。现在你已经知道逻辑回 归模型是什么样子了，下一步要做的是训练参数𝑤和参数𝑏，你需要定义一个代价函数，让 我们在下节课里对其进行解释。

23 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.3 逻辑回归的代价函数（Logistic Regression Cost Function）

在上个视频中，我们讲了逻辑回归模型，这个视频里，我们讲逻辑回归的代价函数（也 翻译作成本函数）。为什么需要代价函数： 为了训练逻辑回归模型的参数参数𝑤和参数𝑏我们，需要一个代价函数，通过训练代价 函数来得到参数𝑤和参数𝑏。先看一下逻辑回归的输出函数：

为了让模型通过学习调整参数，你需要给予一个𝑚样本的训练集，这会让你在训练集上 找到参数𝑤和参数𝑏,，来得到你的输出。

对训练集的预测值，我们将它写成𝑦，我们更希望它会接近于训练集中的𝑦值，为了对上 面的公式更详细的介绍，我们需要说明上面的定义是对一个训练样本来说的，这种形式也使 用于每个训练样本，我们使用这些带有圆括号的上标来区分索引和样本，训练样本𝑖所对应 的预测值是𝑦 (𝑖) , 是用训练样本的𝑤 𝑇 𝑥 (𝑖) + 𝑏然后通过 sigmoid 函数来得到，也可以把𝑧定义为 𝑧 (𝑖) = 𝑤 𝑇 𝑥 (𝑖) + 𝑏, 我们将使用这个符号 (𝑖) 注解，上标 (𝑖) 来指明数据表示𝑥或者𝑦或者𝑧或者其 他数据的第𝑖个训练样本，这就是上标 (𝑖) 的含义。

损失函数：

损失函数又叫做误差函数，用来衡量算法的运行情况，Loss function:𝐿(𝑦 , 𝑦).

我们通过这个𝐿称为的损失函数，来衡量预测输出值和实际值有多接近。一般我们用预 测值和实际值的平方差或者它们平方差的一半，但是通常在逻辑回归中我们不这么做，因为 当我们在学习逻辑回归参数的时候，会发现我们的优化目标不是凸优化，只能找到多个局部 最优值，梯度下降法很可能找不到全局最优值，虽然平方差是一个不错的损失函数，但是我 们在逻辑回归模型中会定义另外一个损失函数。

我们在逻辑回归中用到的损失函数是：𝐿(𝑦 , 𝑦) = −𝑦log (𝑦) − (1 − 𝑦) log (1 − 𝑦)

为什么要用这个函数作为逻辑损失函数？当我们使用平方误差作为损失函数的时候，你 会想要让这个误差尽可能地小，对于这个逻辑回归损失函数，我们也想让它尽可能地小，为 了更好地理解这个损失函数怎么起作用，我们举两个例子：

24 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

当𝑦 = 1 时损失函数𝐿 = −log (𝑦)，如果想要损失函数𝐿尽可能得小，那么𝑦就要尽可能大，因为 sigmoid 函数取值 [0,1]，所以𝑦会无限接近于 1。当𝑦 = 0 时损失函数𝐿 = −log (1 − 𝑦)，如果想要损失函数𝐿尽可能得小，那么𝑦就要尽可

能小，因为 sigmoid 函数取值 [0,1]，所以𝑦会无限接近于 0。在这门课中有很多的函数效果和现在这个类似，就是如果𝑦等于 1，我们就尽可能让𝑦变 大，如果𝑦等于 0，我们就尽可能让 𝑦 变小。损失函数是在单个训练样本中定义的，它衡 量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现如何，我们 需要定义一个算法的代价函数，算法的代价函数是对𝑚个样本的损失函数求和然后除以𝑚:

1 1 𝐽(𝑤, 𝑏) = 𝑚 ∑ 𝑖=1 𝑚 𝐿 (𝑦 , 𝑦 (𝑖) ) = 𝑚 ∑ 𝑖=1 𝑚 (−𝑦 (𝑖) log 𝑦 (𝑖) − (1 − 𝑦 (𝑖) )log(1 − 𝑦 (𝑖) )) 

损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价，所以在训练逻 辑回归模型时候，我们需要找到合适的𝑤和𝑏，来让代价函数 𝐽 的总代价降到最低。根据我 们对逻辑回归算法的推导及对单个样本的损失函数的推导和针对算法所选用参数的总代价 函数的推导，结果表明逻辑回归可以看做是一个非常小的神经网络，在下一个视频中，我们 会看到神经网络会做什么。

25 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.4 梯度下降法（Gradient Descent）

梯度下降法可以做什么？ 在你测试集上，通过最小化代价函数（成本函数）𝐽(𝑤, 𝑏) 来训练的参数𝑤和𝑏，

如图，在第二行给出和之前一样的逻辑回归算法的代价函数（成本函数） 梯度下降法的形象化说明

在这个图中，横轴表示你的空间参数𝑤和𝑏，在实践中，𝑤可以是更高的维度，但是为了 更好地绘图，我们定义𝑤和𝑏，都是单一实数，代价函数（成本函数）𝐽(𝑤, 𝑏) 是在水平轴𝑤和 𝑏上的曲面，因此曲面的高度就是𝐽(𝑤, 𝑏) 在某一点的函数值。我们所做的就是找到使得代价

函数（成本函数）𝐽(𝑤, 𝑏) 函数值是最小值，对应的参数𝑤和𝑏。

如图，代价函数（成本函数）𝐽(𝑤, 𝑏) 是一个凸函数 (convex function)，像一个大碗一样。

如图，这就与刚才的图有些相反，因为它是非凸的并且有很多不同的局部最小值。由于

逻辑回归的代价函数（成本函数）𝐽(𝑤, 𝑏) 特性，我们必须定义代价函数（成本函数）𝐽(𝑤, 𝑏)

为凸函数。初始化𝑤和𝑏，

26 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

可以用如图那个小红点来初始化参数𝑤和𝑏，也可以采用随机初始化的方法，对于逻辑 回归几乎所有的初始化方法都有效，因为函数是凸函数，无论在哪里初始化，应该达到同一 点或大致相同的点。

我们以如图的小红点的坐标来初始化参数𝑤和𝑏。2. 朝最陡的下坡方向走一步，不断地迭代

我们朝最陡的下坡方向走一步，如图，走到了如图中第二个小红点处。

我们可能停在这里也有可能继续朝最陡的下坡方向再走一步，如图，经过两次迭代走到 第三个小红点处。3. 直到走到全局最优解或者接近全局最优解的地方 通过以上的三个步骤我们可以找到全局最优解，也就是代价函数（成本函数）𝐽(𝑤, 𝑏) 这 个凸函数的最小值点。梯度下降法的细节化说明（仅有一个参数）

27 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

假定代价函数（成本函数）𝐽(𝑤) 只有一个参数𝑤，即用一维曲线代替多维曲线，这样可 以更好画出图像。

𝑑𝐽(𝑤) 𝑤 ≔ 𝑤 − 𝑎 𝑑𝑤 

迭代就是不断重复做如图的公式:

: = 表示更新参数，𝑎 表示学习率（learning rate），用来控制步长（step），即向下走一步的长度 是函数𝐽(𝑤) 对𝑤 求导（derivative），在代码中我们会使用𝑑𝑤表示这个结果

𝑑𝐽(𝑤) 𝑑𝑤 

就

对于导数更加形象化的理解就是斜率（slope），如图该点的导数就是这个点相切于 𝐽(𝑤) 的小三角形的高除宽。假设我们以如图点为初始化点，该点处的斜率的符号是正的，即

𝑑𝐽(𝑤) 𝑑𝑤 

> 0，所以接下来会向左走一步。

整个梯度下降法的迭代过程就是不断地向左走，直至逼近最小值点。

𝑑𝐽(𝑤) 𝑑𝑤 

假设我们以如图点为初始化点，该点处的斜率的符号是负的，即

< 0，所以接下

28 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

来会向右走一步。

整个梯度下降法的迭代过程就是不断地向右走，即朝着最小值点方向走。梯度下降法的细节化说明（两个参数） 逻辑回归的代价函数（成本函数）𝐽(𝑤, 𝑏) 是含有两个参数的。

𝜕𝐽(𝑤,𝑏) 𝑤 ≔ 𝑤 − 𝑎 𝜕𝑤 

𝜕𝐽(𝑤,𝑏) 𝑏 ≔ 𝑏 − 𝑎 𝜕𝑏 

𝜕 表示求偏导符号，可以读作 round，

𝜕𝐽(𝑤,𝑏) 

𝜕𝐽(𝑤,𝑏) 𝜕𝑤 

就是函数𝐽(𝑤, 𝑏) 对𝑤 求偏导，在代码

中我们会使用𝑑𝑤 表示这个结果，就是函数𝐽(𝑤, 𝑏) 对𝑏 求偏导，在代码中我们会 𝜕𝑏 使用𝑑𝑏 表示这个结果，小写字母𝑑 用在求导数（derivative），即函数只有一个参数，偏 导数符号𝜕 用在求偏导（partial derivative），即函数含有两个以上的参数。

29 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.5 导数（Derivatives）

这个视频我主要是想帮你获得对微积分和导数直观的理解。或许你认为自从大学毕以后 你再也没有接触微积分。这取决于你什么时候毕业，也许有一段时间了，如果你顾虑这点，请不要担心。为了高效应用神经网络和深度学习，你并不需要非常深入理解微积分。因此如 果你观看这个视频或者以后的视频时心想：「哇哦，这些知识、这些运算对我来说很复杂。」我给你的建议是：坚持学习视频，最好下课后做作业，成功的完成编程作业，然后你就可以 使用深度学习了。在第四周之后的学习中，你会看到定义的很多种类的函数，通过微积分他 们能够帮助你把所有的知识结合起来，其中有的叫做前向函数和反向函数，因此你不需要了 解所有你使用的那些微积分中的函数。所以你不用担心他们，除此之外在对深度学习的尝试 中，这周我们要进一步深入了解微积分的细节。所有你只需要直观地认识微积分，用来构建 和成功的应用这些算法。最后，如果你是精通微积分的那一小部分人群，你对微积分非常熟 悉，你可以跳过这部分视频。其他同学让我们开始深入学习导数。

一个函数𝑓(𝑎) = 3𝑎，它是一条直线。下面我们来简单理解下导数。让我们看看函数中 几个点，假定𝑎 = 2，那么𝑓(𝑎) 是𝑎的 3 倍等于 6，也就是说如果𝑎 = 2，那么函数𝑓(𝑎) = 6。假定稍微改变一点点𝑎的值，只增加一点，变为 2.001，这时𝑎将向右做微小的移动。0.001 的 差别实在是太小了，不能在图中显示出来，我们把它右移一点，现在𝑓(𝑎) 等于𝑎的 3 倍是 6.003，画在图里，比例不太符合。请看绿色高亮部分的这个小三角形，如果向右移动 0.001，那么 𝑓(𝑎) 增加 0.003，𝑓(𝑎) 的值增加 3 倍于右移的𝑎，因此我们说函数𝑓(𝑎) 在𝑎 = 2，. 是这个导数 的斜率，或者说，当𝑎 = 2 时，斜率是 3。导数这个概念意味着斜率，导数听起来是一个很可 怕、很令人惊恐的词，但是斜率以一种很友好的方式来描述导数这个概念。所以提到导数，

30 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

我们把它当作函数的斜率就好了。更正式的斜率定义为在上图这个绿色的小三角形中，高除 以宽。即斜率等于 0.003 除以 0.001，等于 3。或者说导数等于 3，这表示当你将𝑎右移 0.001，𝑓(𝑎) 的值增加 3 倍水平方向的量。

现在让我们从不同的角度理解这个函数。

假设𝑎 = 5 ，此时𝑓(𝑎) = 3𝑎 = 15。

把𝑎右移一个很小的幅度，增加到 5.001，𝑓(𝑎) = 15.003。即在𝑎 = 5 时，斜率是 3，𝑑𝑓(𝑎) 这就是表示，当微小改变变量𝑎的值，𝑑𝑎 = 3 。一个等价的导数表达式可以这样写 𝑑 𝑑𝑎 𝑓(𝑎) ，不管你是否将𝑓(𝑎) 放在上面或者放在右边都没有关系。

在这个视频中，我讲解导数讨论的情况是我们将𝑎偏移 0.001，如果你想知道导数的数学 定义，导数是你右移很小的𝑎值（不是 0.001，而是一个非常非常小的值）。通常导数的定义 是你右移𝑎(可度量的值) 一个无限小的值，𝑓(𝑎) 增加 3 倍（增加了一个非常非常小的值）。也 就是这个三角形右边的高度。

那就是导数的正式定义。但是为了直观的认识，我们将探讨右移𝑎 = 0.001 这个值，即 使 0.001 并不是无穷小的可测数据。导数的一个特性是：这个函数任何地方的斜率总是等于 3，不管𝑎 = 2 或 𝑎 = 5，这个函数的斜率总等于 3，也就是说不管𝑎的值如何变化，如果你增 加 0.001，𝑓(𝑎) 的值就增加 3 倍。这个函数在所有地方的斜率都相等。一种证明方式是无论 你将小三角形画在哪里，它的高除以宽总是 3。

我希望带给你一种感觉：什么是斜率？什么是导函数？对于一条直线，在例子中函数的 斜率，在任何地方都是 3。在下一个视频让我们看一个更复杂的例子，这个例子中函数在不 同点的斜率是可变的。

31 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.6 更多的导数例子（More Derivative Examples）

在这个视频中我将给出一个更加复杂的例子，在这个例子中，函数在不同点处的斜率是 不一样的，先来举个例子:

我在这里画一个函数，𝑓(𝑎) = a 2 ，如果𝑎 = 2 的话，那么𝑓(𝑎) = 4。让我们稍稍往右推 进一点点，现在𝑎 = 2.001 ，则𝑓(𝑎) ≈ 4.004 (如果你用计算器算的话，这个准确的值应该 为 4.004。0.001 我只是为了简便起见，省略了后面的部分)，如果你在这儿画，一个小三角 形 你就会发现，如果把𝑎往右移动 0.001，那么𝑓(𝑎) 将增大四倍，即增大 0.004。在微积分中 我们把这个三角形斜边的斜率，称为𝑓(𝑎) 在点𝑎 = 2 处的导数 (即为 4)，或者写成微积分的 𝑑 形式，当𝑎 = 2 的时候，𝑑𝑎 𝑓(𝑎) = 4 由此可知，函数𝑓(𝑎) = 𝑎 2 ，在𝑎取不同值的时候，它 的斜率是不同的，这和上个视频中的例子是不同的。

这里有种直观的方法可以解释，为什么一个点的斜率，在不同位置会不同如果你在曲线 上，的不同位置画一些小小的三角形你就会发现，三角形高和宽的比值，在曲线上不同的地 方，它们是不同的。所以当𝑎 = 2 时，斜率为 4；而当𝑎 = 5 时，斜率为 10 。如果你翻看微 积分的课本，课本会告诉你，函数𝑓(𝑎) = 𝑎 2 的斜率（即导数）为 2𝑎。这意味着任意给定一

点𝑎，如果你稍微将𝑎，增大 0.001，那么你会看到𝑓(𝑎) 将增大 2𝑎，即增大的值为点在𝑎处斜

率或导数，乘以你向右移动的距离。现在有个小细节需要注意，导数增大的值，不是刚好等于导数公式算出来的值，而只是 根据导数算出来的一个估计值。

为了总结这堂课所学的知识，我们再来看看几个例子：

32 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

假设𝑓(𝑎) = 𝑎 3 如果你翻看导数公式表，你会发现这个函数的导数，等于 3𝑎2 。所以这 是什么意思呢，同样地举一个例子：我们再次令𝑎 = 2，所以𝑎 3 = 8 ，如果我们又将𝑎增大 一点点，你会发现𝑓(𝑎) ≈ 8.012，你可以自己检查一遍，如果我们取 8.012，你会发现 2.001 3 ，和 8.012 很接近，事实上当𝑎 = 2 时，导数值为 3 × 22 ，即 3 × 4 = 12。所以导数公式，表明 如果你将𝑎向右移动 0.001 时，𝑓(𝑎) 将会向右移动 12 倍，即 0.012。

来看最后一个例子，假设𝑓(𝑎) = log 𝑒 𝑎，有些可能会写作 ln𝑎，函数 log𝑎 的斜率应该为 1 ，所以我们可以解释如下：如果𝑎取任何值，比如又取𝑎 = 2，然后又把𝑎向右边移动 0.001 𝑎

那么𝑓(𝑎) 将增大 1 × 0.001，如果你借助计算器的话，你会发现当𝑎 = 2 时𝑓(𝑎) ≈ 0.69315 ；

𝑎 

而𝑎 = 2.001 时，𝑓(𝑎) ≈ 0.69365。所以𝑓(𝑎) 增大了 0.0005，如果你查看导数公式，当𝑎 = 2 的时候，导数值 𝑑 𝑓(𝑎) = 1 。这表明如果你把 增大 0.001，𝑓(𝑎) 将只会增大 0.001 的二分之 𝑑𝑎 2 一，即 0.0005。如果你画个小三角形你就会发现，如果𝑥 轴增加了 0.001，那么𝑦 轴上的函 1 数 log𝑎，将增大 0.001 的一半 即 0.0005。所以 𝑎 ，当𝑎 = 2 时这里是 ，就是当𝑎 = 2 时这条

线的斜率。这些就是有关，导数的一些知识。在这个视频中，你只需要记住两点： 第一点，导数就是斜率，而函数的斜率，在不同的点是不同的。在第一个例子中𝑓(𝑎) = 3𝑎 ，这是一条直线，在任何点它的斜率都是相同的，均为 3。但是对于函数𝑓(𝑎) = a 2 ，或 者𝑓(𝑎) = log𝑎，它们的斜率是变化的，所以它们的导数或者斜率，在曲线上不同的点处是不 同的。

33 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

第二点，如果你想知道一个函数的导数，你可参考你的微积分课本或者维基百科，然后 你应该就能找到这些函数的导数公式。

最后我希望，你能通过我生动的讲解，掌握这些有关导数和斜率的知识，下一课我们将 讲解计算图，以及如何用它来求更加复杂的函数的导数。

34 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.7 计算图（Computation Graph）

可以说，一个神经网络的计算，都是按照前向或反向传播过程组织的。首先我们计算出 一个新的网络的输出（前向过程），紧接着进行一个反向传输操作。后者我们用来计算出对 应的梯度或导数。计算图解释了为什么我们用这种方式组织这些计算过程。在这个视频中，我们将举一个例子说明计算图是什么。让我们举一个比逻辑回归更加简单的，或者说不那么 正式的神经网络的例子。

我们尝试计算函数𝐽，𝐽是由三个变量𝑎, 𝑏, 𝑐组成的函数，这个函数是 3 (a + bc) 。计算这 个函数实际上有三个不同的步骤，首先是计算 𝑏 乘以 𝑐，我们把它储存在变量𝑢中，因此 𝑢 = 𝑏𝑐； 然后计算𝑣 = 𝑎 + 𝑢；最后输出𝐽 = 3𝑣，这就是要计算的函数𝐽。我们可以把这三步 画成如下的计算图，我先在这画三个变量𝑎, 𝑏, 𝑐，第一步就是计算𝑢 = 𝑏𝑐，我在这周围放个 矩形框，它的输入是𝑏, 𝑐，接着第二步𝑣 = 𝑎 + 𝑢，最后一步𝐽 = 3𝑣。

举个例子: 𝑎 = 5, 𝑏 = 3, 𝑐 = 2 ，𝑢 = 𝑏𝑐就是 6，就是 5+6=11。𝐽是 3 倍的 ，因此。即 3 × (5 + 3 × 2)。如果你把它算出来，实际上得到 33 就是𝐽的值。当有不同的或者一些特殊 的输出变量时，例如本例中的𝐽和逻辑回归中你想优化的代价函数𝐽，因此计算图用来处理这 些计算会很方便。从这个小例子中我们可以看出，通过一个从左向右的过程，你可以计算出 𝐽的值。为了计算导数，从右到左（红色箭头，和蓝色箭头的过程相反）的过程是用于计算 导数最自然的方式。

概括一下：计算图组织计算的形式是用蓝色箭头从左到右的计算，让我们看看下一个视 频中如何进行反向红色箭头 (也就是从右到左) 的导数计算，让我们继续下一个视频的学习。

35 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.8 使用计算图求导数 （Derivatives with a Computation Graph）

在上一个视频中，我们看了一个例子使用流程计算图来计算函数𝐽。现在我们看看流程 图的描述，看看你如何利用它计算出函数𝐽的导数。下面用到的公式：

𝑑𝐽 𝑑𝑢 

= 

𝑑𝐽 𝑑𝑣 𝑑𝑣 𝑑𝑢 

，

𝑑𝐽 𝑑𝑏 

= 

𝑑𝐽 𝑑𝑢 𝑑𝑢 𝑑𝑏 

，

𝑑𝐽 𝑑𝑎 

= 

𝑑𝐽 𝑑𝑢 𝑑𝑢 𝑑𝑎 

这是一个流程图：

假设你要计算 𝑑𝐽 ，那要怎么算呢？好，比如说，我们要把这个𝑣值拿过来，改变一下，𝑑𝑣 那么𝐽的值会怎么变呢？

所以定义上𝐽 = 3𝑣，现在𝑣 = 11，所以如果你让𝑣增加一点点，比如到 11.001，那么𝐽 = 3𝑣 = 33.003，所以我这里𝑣增加了 0.001，然后最终结果是𝐽上升到原来的 3 倍，所以 𝑑𝐽 𝑑𝑣 = 3，因为对于任何 𝑣 的增量𝐽都会有 3 倍增量，而且这类似于我们在上一个视频中的例子，我们 有𝑓(𝑎) = 3𝑎，然后我们推导出 𝑑𝑓(𝑎) = 3，所以这里我们有𝐽 = 3𝑣，所以 𝑑𝐽 = 3，这里𝐽扮演了 𝑑𝑎 𝑑𝑣 𝑓的角色，在之前的视频里的例子。

在反向传播算法中的术语，我们看到，如果你想计算最后输出变量的导数，使用你最关 心的变量对𝑣的导数，那么我们就做完了一步反向传播，在这个流程图中是一个反向步。

𝑑𝐽 𝑑𝑎 

我们来看另一个例子，

是多少呢？换句话说，如果我们提高𝑎的数值，对𝐽的数值有什

36 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

么影响？ 好，我们看看这个例子。变量𝑎 = 5，我们让它增加到 5.001，那么对𝑣的影响就是𝑎 + 𝑢, 之前𝑣 = 11，现在变成 11.001，我们从上面看到现在𝐽就变成 33.003 了，所以我们看到的是，如果你让𝑎增加 0.001，𝐽增加 0.003。那么增加𝑎，我是说如果你把这个 5 换成某个新值，那

么𝑎的改变量就会传播到流程图的最右，所以𝐽最后是 33.003。所以𝐽的增量是 3 乘以𝑎的增

量，意味着这个导数是 3。

要解释这个计算过程，其中一种方式是：如果你改变了𝑎，那么也会改变𝑣，通过改变𝑣，也会改变𝐽，所以𝐽值的净变化量，当你提升这个值（0.001），当你把𝑎值提高一点点，这就 是𝐽的变化量（0.003）。

首先𝑎增加了，𝑣也会增加，𝑣增加多少呢？这取决于 𝑑𝑣 𝑑𝑎 ，然后𝑣的变化导致𝐽也在增加，所以这在微积分里实际上叫链式法则，如果𝑎影响到𝑣，𝑣影响到𝐽，那么当你让𝑎变大时，𝐽的 变化量就是当你改变𝑎时，𝑣的变化量乘以改变𝑣时𝐽的变化量，在微积分里这叫链式法则。

我们从这个计算中看到，如果你让𝑎增加 0.001，𝑣也会变化相同的大小，所以 𝑑𝑣 = 1。

𝑑𝑎 

37 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

事实上，如果你代入进去，我们之前算过 𝑑𝐽 = 3，𝑑𝑣 = 1，所以这个乘积 3×1，实际上就给出

𝑑𝑣 

𝑑𝑎 

了正确答案，𝑑𝐽 𝑑𝑎 = 3。

这张小图表示了如何计算，𝑑𝐽 就是𝐽对变量𝑣的导数，它可以帮助你计算 𝑑𝐽 ，所以这是另 𝑑𝑣 𝑑𝑎 一步反向传播计算。

现在我想介绍一个新的符号约定，当你编程实现反向传播时，通常会有一个最终输出值 是你要关心的，最终的输出变量，你真正想要关心或者说优化的。在这种情况下最终的输出 变量是𝐽，就是流程图里最后一个符号，所以有很多计算尝试计算输出变量的导数，所以输 出变量对某个变量的导数，我们就用𝑑𝑣𝑎𝑟命名，所以在很多计算中你需要计算最终输出结果 的导数，在这个例子里是𝐽，还有各种中间变量，比如𝑎、𝑏、𝑐、𝑢、𝑣，当你在软件里实现的 时候，变量名叫什么？你可以做的一件事是，在 python 中，你可以写一个很长的变量名，比如𝑑𝐹𝑖𝑛𝑎𝑙𝑂𝑢𝑡𝑝𝑢𝑡𝑣𝑎𝑟_𝑑𝑣𝑎𝑟，但这个变量名有点长，我们就用𝑑𝐽_𝑑𝑣𝑎𝑟，但因为你一直对𝑑𝐽 求导，对这个最终输出变量求导。我这里要介绍一个新符号，在程序里，当你编程的时候，在代码里，我们就使用变量名𝑑𝑣𝑎𝑟，来表示那个量。

好，所以在程序里是𝑑𝑣𝑎𝑟表示导数，你关心的最终变量𝐽的导数，有时最后是𝐿，对代码 中各种中间量的导数，所以代码里这个东西，你用𝑑𝑣表示这个值，所以𝑑𝑣 = 3，你的代码表 示就是𝑑𝑎 = 3。

好，所以我们通过这个流程图完成部分的后向传播算法。我们在下一张幻灯片看看这个 例子剩下的部分。

38 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

我们清理出一张新的流程图，我们回顾一下，到目前为止，我们一直在往回传播，并计 算𝑑𝑣 = 3，再次，𝑑𝑣是代码里的变量名，其真正的定义是 𝑑𝐽 。我发现𝑑𝑎 = 3，再次，𝑑𝑎是代 𝑑𝑣 码里的变量名，其实代表 𝑑𝐽 𝑑𝑎 的值。

大概手算了一下，两条直线怎么计算反向传播。好，我们继续计算导数，我们看看这个值𝑢，那么 𝑑𝐽 𝑑𝑢 是多少呢？通过和之前类似的计算，现在我们从𝑢 = 6 出发，如果你令𝑢增加到 6.001，那么𝑣之前是 11，现在变成 11.001 了，𝐽 就从 33 变成 33.003，所以𝐽 增量是 3 倍，所以 dJ 𝑑𝑢 = 3。对𝑢的分析很类似对 a 的分析，实际 上这计算起来就是 𝑑𝐽 𝑑𝑣 ⋅ 𝑑𝑣 𝑑𝑢 ，有了这个，我们可以算出 𝑑𝐽 𝑑𝑣 = 3，𝑑𝑣 = 1，最终算出结果是 3 × 1 = 𝑑𝑢 3。

所以我们还有一步反向传播，我们最终计算出𝑑𝑢 = 3，这里的𝑑𝑢当然了，就是 𝑑𝐽 𝑑𝑢 。

现在，我们仔细看看最后一个例子，那么 𝑑𝐽 𝑑𝑏 呢？想象一下，如果你改变了𝑏的值，你想 要然后变化一点，让𝐽值到达最大或最小，那么导数是什么呢？这个𝐽函数的斜率，当你稍微 改变𝑏值之后。事实上，使用微积分链式法则，这可以写成两者的乘积，就是 𝑑𝐽 𝑑𝑢 ⋅ 𝑑𝑢 𝑑𝑏 ，理由是，如果你改变𝑏一点点，所以𝑏变化比如说 3.001，它影响 J 的方式是，首先会影响𝑢，它对𝑢的 影响有多大？好，𝑢的定义是𝑏 ⋅ 𝑐，所以𝑏 = 3 时这是 6，现在就变成 6.002 了，对吧，因为 在我们的例子中𝑐 = 2，所以这告诉我们 𝑑𝑢 = 2 当你让𝑏增加 0.001 时，𝑢就增加两倍。所以

𝑑𝑏 

39 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

𝑑𝑢 𝑑𝑏 

= 2，现在我想𝑢的增加量已经是𝑏的两倍，那么 𝑑𝐽 是多少呢？我们已经弄清楚了，这等于 𝑑𝑢 3，所以让这两部分相乘，我们发现 𝑑𝐽 𝑑𝑏 = 6。

好，这就是第二部分的推导，其中我们想知道 𝑢 增加 0.002，会对𝐽有什么影响。实际 上 𝑑𝐽 = 3，这告诉我们 u 增加 0.002 之后，𝐽上升了 3 倍，那么𝐽应该上升 0.006，对吧。这可 𝑑𝑢

以从 𝑑𝐽 = 3 推导出来。𝑑𝑢

如果你仔细看看这些数学内容，你会发现，如果𝑏变成 3.001，那么𝑢就变成 6.002，𝑣变 成 11.002，然后𝐽 = 3𝑣 = 33.006，对吧？这就是如何得到 𝑑𝐽 𝑑𝑏 = 6。

为了填进去，如果我们反向走的话，𝑑𝑏 = 6，而𝑑𝑏其实是 Python 代码中的变量名，表 示 𝑑𝐽 𝑑𝑏 。

我不会很详细地介绍最后一个例子，但事实上，如果你计算 𝑑𝐽 𝑑𝑐 = 果是 9。

𝑑𝐽 𝑑𝑢 

⋅ 𝑑𝑢 𝑑𝑐 = 3 × 3，这个结

我不会详细说明这个例子，在最后一步，我们可以推出𝑑𝑐 = 9。

所以这个视频的要点是，对于那个例子，当计算所有这些导数时，最有效率的办法是从

40 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

右到左计算，跟着这个红色箭头走。特别是当我们第一次计算对𝑣的导数时，之后在计算对 𝑎导数就可以用到。然后对𝑢的导数，比如说这个项和这里这个项：

可以帮助计算对𝑏的导数，然后对𝑐的导数。所以这是一个计算流程图，就是正向或者说从左到右的计算来计算成本函数𝐽，你可能 需要优化的函数，然后反向从右到左计算导数。如果你不熟悉微积分或链式法则，我知道这 里有些细节讲的很快，但如果你没有跟上所有细节，也不用怕。在下一个视频中，我会再过 一遍。在逻辑回归的背景下过一遍，并给你介绍需要做什么才能编写代码，实现逻辑回归模 型中的导数计算。

41 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.9 逻辑回归中的梯度下降（Logistic Regression Gradient Descent）

本节我们讨论怎样通过计算偏导数来实现逻辑回归的梯度下降算法。它的关键点是几个 重要公式，其作用是用来实现逻辑回归中梯度下降算法。但是在本节视频中，我将使用计算 图对梯度下降算法进行计算。我必须要承认的是，使用计算图来计算逻辑回归的梯度下降算 法有点大材小用了。但是，我认为以这个例子作为开始来讲解，可以使你更好的理解背后的 思想。从而在讨论神经网络时，你可以更深刻而全面地理解神经网络。接下来让我们开始学 习逻辑回归的梯度下降算法。

假设样本只有两个特征𝑥 1 和𝑥 2 ，为了计算𝑧，我们需要输入参数𝑤 1 、𝑤 2 和𝑏，除此之外 还有特征值𝑥 1 和𝑥 2 。因此𝑧的计算公式为： 𝑧 = 𝑤 1 𝑥 1 + 𝑤 2 𝑥 2 + 𝑏

1 回想一下逻辑回归的公式定义如下： 𝑦 = 𝑎 = 𝜎(𝑧) 其中𝑧 = 𝑤 𝑇 𝑥 + 𝑏 ，𝜎(𝑧) = 1+𝑒 −𝑧

损失函数： 𝐿(𝑦 , 𝑦 (𝑖) ) = −𝑦 (𝑖) log 𝑦 (𝑖) − (1 − 𝑦 (𝑖) ) log (1 − 𝑦 (𝑖) )

1 代价函数： 𝐽(𝑤, 𝑏) = 𝑚 ∑ 𝑖 𝑚 𝐿(𝑦

, 𝑦 (𝑖) ) 

假设现在只考虑单个样本的情况，单个样本的代价函数定义如下：

𝐿(𝑎, 𝑦) = −(𝑦log(𝑎) + (1 − 𝑦)log(1 − 𝑎)) 

其中𝑎是逻辑回归的输出，𝑦是样本的标签值。现在让我们画出表示这个计算的计算图。这里先复习下梯度下降法，𝑤和𝑏的修正量可以表达如下：

𝑤: = 𝑤 − 𝑎 𝜕𝐽(𝑤,𝑏) ，𝑏: = 𝑏 − 𝑎𝜕𝐽(𝑤,𝑏) 𝜕𝑤 𝜕𝑏 

如图：在这个公式的外侧画上长方形。然后计算： 𝑦 = 𝑎 = 𝜎(𝑧) 也就是计算图的下一 步。最后计算损失函数𝐿(𝑎, 𝑦)。有了计算图，我就不需要再写出公式了。因此，为了使得

42 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

逻辑回归中最小化代价函数𝐿(𝑎, 𝑦)，我们需要做的仅仅是修改参数𝑤和𝑏的值。前面我们已 经讲解了如何在单个训练样本上计算代价函数的前向步骤。现在让我们来讨论通过反向计算 出导数。因为我们想要计算出的代价函数𝐿(𝑎, 𝑦) 的导数，首先我们需要反向计算出代价函

𝑑𝐿(𝑎,𝑦) 数𝐿(𝑎, 𝑦) 关于𝑎的导数，在编写代码时，你只需要用𝑑𝑎 来表示 。𝑑𝑎

𝑑𝐿(𝑎,𝑦) 通过微积分得到： = −𝑦/𝑎 + (1 − 𝑦)/(1 − 𝑎) 𝑑𝑎

如果你不熟悉微积分，也不必太担心，我们会列出本课程涉及的所有求导公式。那么如 果你非常熟悉微积分，我们鼓励你主动推导前面介绍的代价函数的求导公式，使用微积分直 接求出𝐿(𝑎, 𝑦) 关于变量𝑎的导数。如果你不太了解微积分，也不用太担心。现在我们已经计 算出𝑑𝑎，也就是最终输出结果的导数。现在可以再反向一步，在编写 Python 代码时，你只 需要用𝑑𝑧来表示代价函数𝐿关于𝑧 的导数𝑑𝐿 ，也可以写成𝑑𝐿(𝑎,𝑦) ，这两种写法都是正确的。

𝑑𝑧 

𝑑𝑧 

𝑑𝐿 𝑑𝑧 

= 𝑎 − 𝑦 。

因为

𝑑𝐿(𝑎,𝑦) 𝑑𝑧 

= 

𝑑𝐿 𝑑𝑧 

𝑑𝐿 𝑑𝑎 𝑑𝑎 = (𝑑𝑎) ⋅ (𝑑𝑧)，并且 𝑑𝑧 = 𝑎 ⋅ (1 − 𝑎)，而

𝑑𝐿 𝑑𝑎 

= (− 𝑦 𝑎 + (1−𝑦) (1−𝑎) )，

因此将这两项相乘，得到：

𝑑𝐿(𝑎,𝑦) 𝑑𝐿 𝑑𝐿 𝑑𝑎 𝑑𝑧 = 𝑑𝑧 = 𝑑𝑧 = ( 𝑑𝑎 ) ⋅ ( 𝑑𝑧 ) = (− 𝑦 𝑎 + (1−𝑦) (1−𝑎) ) ⋅ 𝑎(1 − 𝑎) = 𝑎 − 𝑦 

视频中为了简化推导过程，假设𝑛 𝑥 这个推导的过程就是我之前提到过的链式法则。如果 你对微积分熟悉，放心地去推导整个求导过程，如果不熟悉微积分，你只需要知道𝑑𝑧 = (𝑎 𝑦) 已经计算好了。

现在进行最后一步反向推导，也就是计算𝑤和𝑏变化对代价函数𝐿的影响，特别地，可以 用:

1 𝑑𝑤 1 = 𝑚 ∑ 𝑖 𝑚 𝑥 1 (𝑖) (𝑎 (𝑖) − 𝑦 (𝑖) ) 

1 𝑑𝑤 2 = 𝑚 ∑ 𝑖 𝑚 𝑥 2 (𝑖) (𝑎 (𝑖) − 𝑦 (𝑖) ) 

1 𝑑𝑏 = 𝑚 ∑ 𝑖 𝑚 (𝑎 (𝑖) − 𝑦 (𝑖) ) 

𝜕𝐿 视频中，𝑑𝑤 1 表示 = 𝑥 1 ⋅ 𝑑𝑧，

𝜕𝑤 1 

𝜕𝐿 𝑑𝑤 2 表示 𝜕𝑤 = 𝑥 2 ⋅ 𝑑𝑧，𝑑𝑏 = 𝑑𝑧。

2 

因此，关于单个样本的梯度下降算法，你所需要做的就是如下的事情：

使用公式𝑑𝑧 = (𝑎 − 𝑦) 计算𝑑𝑧，

使用𝑑𝑤 1 = 𝑥 1 ⋅ 𝑑𝑧 计算𝑑𝑤1 ，𝑑𝑤 2 = 𝑥 2 ⋅ 𝑑𝑧计算𝑑𝑤2 ，𝑑𝑏 = 𝑑𝑧 来计算𝑑𝑏，

43 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

然后：更新𝑤 1 = 𝑤 1 − 𝑎𝑑𝑤1 ，更新𝑤 2 = 𝑤 2 − 𝑎𝑑𝑤 2 ，更新𝑏 = 𝑏 − 𝛼𝑑𝑏。

这就是关于单个样本实例的梯度下降算法中参数更新一次的步骤。

现在你已经知道了怎样计算导数，并且实现针对单个训练样本的逻辑回归的梯度下降算 法。但是，训练逻辑回归模型不仅仅只有一个训练样本，而是有𝑚个训练样本的整个训练集。因此在下一节视频中，我们将这些思想应用到整个训练样本集中，而不仅仅只是单个样本上。

44 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.10 m 个样本的梯度下降 (Gradient Descent on m Examples)

在之前的视频中，你已经看到如何计算导数，以及应用梯度下降在逻辑回归的一个训练 样本上。现在我们想要把它应用在𝑚个训练样本上。

首先，让我们时刻记住有关于损失函数𝐽(𝑤, 𝑏) 的定义。

1 𝐽(𝑤, 𝑏) = ∑ 𝑖=1 𝑚 𝐿(𝑎 (𝑖) , 𝑦 (𝑖) ) 𝑚 

当你的算法输出关于样本𝑦的𝑎 (𝑖) ，𝑎 (𝑖) 是训练样本的预测值，即：𝜎(𝑧 (𝑖) ) = 𝜎(𝑤 𝑇 𝑥 (𝑖) + 𝑏)。所以我们在前面的幻灯中展示的是对于任意单个训练样本，如何计算微分当你只有一 个训练样本。因此𝑑𝑤 1 ，𝑑𝑤 2 和𝑑𝑏 添上上标𝑖表示你求得的相应的值。如果你面对的是我们 在之前的幻灯中演示的那种情况，但只使用了一个训练样本 (𝑥 (𝑖) , 𝑦 (𝑖) )。

现在你知道带有求和的全局代价函数，实际上是 1 到𝑚项各个损失的平均。所以它表 明全局代价函数对𝑤 1 的微分，对𝑤 1 的微分也同样是各项损失对𝑤 1 微分的平均。

45 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

但之前我们已经演示了如何计算这项，即之前幻灯中演示的如何对单个训练样本进行计 算。所以你真正需要做的是计算这些微分，如我们在之前的训练样本上做的。并且求平均，这会给你全局梯度值，你能够把它直接应用到梯度下降算法中。

所以这里有很多细节，但让我们把这些装进一个具体的算法。同时你需要一起应用的就 是逻辑回归和梯度下降。

我们初始化𝐽 = 0, 𝑑𝑤 1 = 0, 𝑑𝑤 2 = 0, 𝑑𝑏 = 0

代码流程： J=0;dw1=0;dw2=0;db=0; for i = 1 to m z (i) = wx (i)+b; a (i) = sigmoid (z (i));

J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i)); 

dz(i) = a(i)-y(i); dw1 += x1(i)dz(i); dw2 += x2(i)dz(i); db += dz(i); J/= m; dw1/= m; dw2/= m; db/= m; 

46 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

w=w-alpha*dw b=b-alpha*db 

幻灯片上只应用了一步梯度下降。因此你需要重复以上内容很多次，以应用多次梯度下 降。看起来这些细节似乎很复杂，但目前不要担心太多。希望你明白，当你继续尝试并应用 这些在编程作业里，所有这些会变的更加清楚。

但这种计算中有两个缺点，也就是说应用此方法在逻辑回归上你需要编写两个 for 循环。第一个 for 循环是一个小循环遍历𝑚个训练样本，第二个 for 循环是一个遍历所有特征的 for 循环。这个例子中我们只有 2 个特征，所以𝑛等于 2 并且𝑛 𝑥 等于 2。但如果你有更多特征，你开始编写你的因此𝑑𝑤 1 ，𝑑𝑤2 ，你有相似的计算从𝑑𝑤 3 一直下去到𝑑𝑤 𝑛 。所以看来你需要一 个 for 循环遍历所有𝑛个特征。

当你应用深度学习算法，你会发现在代码中显式地使用 for 循环使你的算法很低效，同 时在深度学习领域会有越来越大的数据集。所以能够应用你的算法且没有显式的 for 循环会 是重要的，并且会帮助你适用于更大的数据集。所以这里有一些叫做向量化技术，它可以允 许你的代码摆脱这些显式的 for 循环。

我想在先于深度学习的时代，也就是深度学习兴起之前，向量化是很棒的。可以使你有 时候加速你的运算，但有时候也未必能够。但是在深度学习时代向量化，摆脱 for 循环已经 变得相当重要。因为我们越来越多地训练非常大的数据集，因此你真的需要你的代码变得非 常高效。所以在接下来的几个视频中，我们会谈到向量化，以及如何应用向量化而连一个 for 循环都不使用。所以学习了这些，我希望你有关于如何应用逻辑回归，或是用于逻辑回归的 梯度下降，事情会变得更加清晰。当你进行编程练习，但在真正做编程练习之前让我们先谈 谈向量化。然后你可以应用全部这些东西，应用一个梯度下降的迭代而不使用任何 for 循环。

47 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.11 向量化 (Vectorization)

向量化是非常基础的去除代码中 for 循环的艺术，在深度学习安全领域、深度学习实践 中，你会经常发现自己训练大数据集，因为深度学习算法处理大数据集效果很棒，所以你的 代码运行速度非常重要，否则如果在大数据集上，你的代码可能花费很长时间去运行，你将 要等待非常长的时间去得到结果。所以在深度学习领域，运行向量化是一个关键的技巧，让 我们举个栗子说明什么是向量化。

在逻辑回归中你需要去计算𝑧 = 𝑤 𝑇 𝑥 + 𝑏，𝑤、𝑥都是列向量。如果你有很多的特征那么 就会有一个非常大的向量，所以𝑤 ∈ ℝ 𝑛 𝑥 , 𝑥 ∈ ℝ 𝑛 𝑥 ，所以如果你想使用非向量化方法去计 算𝑤 𝑇 𝑥，你需要用如下方式（python） z=0 for i in range (n_x)

z+=w[i]*x[i] z+=b 

这是一个非向量化的实现，你会发现这真的很慢，作为一个对比，向量化实现将会非常 直接计算𝑤 𝑇 𝑥，代码如下：

z=np.dot(w,x)+b 

这是向量化计算𝑤 𝑇 𝑥的方法，你将会发现这个非常快

让我们用一个小例子说明一下，在我的我将会写一些代码（以下为教授在他的 Jupyter notebook 上写的 Python 代码，） import numpy as np #导入 numpy 库

a = np.array ([1,2,3,4]) #创建一个数据 a

48 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

print (a) # [1 2 3 4] import time #导入时间库 a = np.random.rand (1000000) b = np.random.rand (1000000) #通过 round 随机得到两个一百万维度的数组 tic = time.time () #现在测量一下当前时间 #向量化的版本 c = np.dot (a,b) toc = time.time () print (「Vectorized version:」+ str (1000*(toc-tic)) +」ms」) #打印一下向量 化的版本的时间 #继续增加非向量化的版本 c = 0 tic = time.time () for i in range (1000000):

c += a [i]*b [i] toc = time.time () print (c) print (「For loop:」+ str (1000*(toc-tic)) +「ms」)# 打印 for 循环的版本的时 间

返回值见图。

在两个方法中，向量化和非向量化计算了相同的值，如你所见，向量化版本花费了 1.5 毫秒，非向量化版本的 for 循环花费了大约几乎 500 毫秒，非向量化版本多花费了 300 倍时 间。所以在这个例子中，仅仅是向量化你的代码，就会运行 300 倍快。这意味着如果向量化 方法需要花费一分钟去运行的数据，for 循环将会花费 5 个小时去运行。

一句话总结，以上都是再说和 for 循环相比，向量化可以快速得到结果。

你可能听过很多类似如下的话，「大规模的深度学习使用了 GPU 或者图像处理单元实 现」，但是我做的所有的案例都是在 jupyter notebook 上面实现，这里只有 CPU，CPU 和 GPU 都有并行化的指令，他们有时候会叫做 SIMD 指令，这个代表了一个单独指令多维数据，这 个的基础意义是，如果你使用了 built-in 函数，像 np.function 或者并不要求你实现循环的

49 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

函数，它可以让 python 的充分利用并行化计算，这是事实在 GPU 和 CPU 上面计算，GPU 更 加擅长 SIMD 计算，但是 CPU 事实上也不是太差，可能没有 GPU 那么擅长吧。接下来的视 频中，你将看到向量化怎么能够加速你的代码，经验法则是，无论什么时候，避免使用明确 的 for 循环。以下代码及运行结果截图：

50 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

51 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.12 向量化的更多例子（More Examples of Vectorization）

从上节视频中，你知道了怎样通过 numpy 内置函数和避开显式的循环 (loop) 的方式进行 向量化，从而有效提高代码速度。

经验提醒我，当我们在写神经网络程序时，或者在写逻辑 (logistic) 回归，或者其他神经 网络模型时，应该避免写循环 (loop) 语句。虽然有时写循环 (loop) 是不可避免的，但是我们可 以使用比如 numpy 的内置函数或者其他办法去计算。当你这样使用后，程序效率总是快于 循环 (loop)。

让我们看另外一个例子。如果你想计算向量𝑢 = 𝐴𝑣，这时矩阵乘法定义为，矩阵乘法的 定义就是：𝑢 𝑖 = ∑ 𝑗 𝐴 ij 𝑣 𝑖 ，这取决于你怎么定义𝑢 𝑖 值。同样使用非向量化实现，𝑢 = 𝑛𝑝. 𝑧𝑒𝑟𝑜𝑠(𝑛, 1)，并且通过两层循环𝑓𝑜𝑟(𝑖): 𝑓𝑜𝑟(𝑗):，得到𝑢[𝑖] = 𝑢[𝑖] + 𝐴[𝑖][𝑗] ∗ 𝑣[𝑗] 。现在 就有了𝑖 和 𝑗 的两层循环，这就是非向量化。向量化方式就可以用𝑢 = 𝑛𝑝. 𝑑𝑜𝑡(𝐴, 𝑣)，右边 这种向量化实现方式，消除了两层循环使得代码运行速度更快。

下面通过另一个例子继续了解向量化。如果你已经有一个向量𝑣，并且想要对向量𝑣的每 个元素做指数操作，得到向量𝑢等于𝑒的𝑣 1 ，𝑒的𝑣 2 ，一直到𝑒的𝑣 𝑛 次方。这里是非向量化的实 现方式，首先你初始化了向量𝑢 = 𝑛𝑝. 𝑧𝑒𝑟𝑜𝑠(𝑛, 1)，并且通过循环依次计算每个元素。但事实 证明可以通过 python 的 numpy 内置函数，帮助你计算这样的单个函数。所以我会引入 import numpy as np，执行 𝑢 = 𝑛𝑝. 𝑒𝑥𝑝(𝑣) 命令。注意到，在之前有循环的代码中，这 里仅用了一行代码，向量𝑣作为输入，𝑢作为输出。你已经知道为什么需要循环，并且通过右 边代码实现，效率会明显的快于循环方式。

事实上，numpy 库有很多向量函数。比如 u=np.log 是计算对数函数 (𝑙𝑜𝑔)、 np.abs ()

52 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

是 计 算 数 据 的 绝 对 值 、 np.maximum () 计 算 元 素 𝑦 中 的 最 大 值 ，你 也 可 以 1 np.maximum (v,0) 、 𝑣 ∗∗ 2 代表获得元素 𝑦 每个值得平方、 𝑣 获取元素 𝑦 的倒数等

等。所以当你想写循环时候，检查 numpy 是否存在类似的内置函数，从而避免使用循环 (loop) 方式。

那么，将刚才所学到的内容，运用在逻辑回归的梯度下降上，看看我们是否能简化两个 计算过程中的某一步。这是我们逻辑回归的求导代码，有两层循环。在这例子我们有𝑛个特 征值。如果你有超过两个特征时，需要循环 𝑑𝑤 1 、𝑑𝑤 2 、𝑑𝑤 3 等等。所以 𝑗 的实际值是 1、2 和 𝑛 𝑥 ，就是你想要更新的值。所以我们想要消除第二循环，在这一行，这样我们就不 用初始化 𝑑𝑤 1 ，𝑑𝑤 2 都等于 0。去掉这些，而是定义 𝑑𝑤 为一个向量，设置 𝑢 = 𝑛𝑝. 𝑧𝑒𝑟𝑜𝑠(𝑛(𝑥),1)。定义了一个𝑥行的一维向量，从而替代循环。我们仅仅使用了一个向量操 作 𝑑𝑤 = 𝑑𝑤 + 𝑥 (𝑖) 𝑑𝑧 (𝑖) 。最后，我们得到 𝑑𝑤 = 𝑑𝑤/𝑚 。现在我们通过将两层循环转成一 层循环，我们仍然还有这个循环训练样本。

53 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

希望这个视频给了你一点向量化感觉，减少一层循环使你代码更快，但事实证明我们能 做得更好。所以在下个视频，我们将进一步的讲解逻辑回归，你将会看到更好的监督学习结 果。在训练中不需要使用任何 for 循环，你也可以写出代码去运行整个训练集。到此为止一 切都好，让我们看下一个视频。

54 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.13 向量化逻辑回归 (Vectorizing Logistic Regression)

我们已经讨论过向量化是如何显著加速你的代码，在本次视频中我们将讨论如何实现逻 辑回归的向量化计算。这样就能处理整个数据集，甚至不会用一个明确的 for 循环就能实现 对于整个数据集梯度下降算法的优化。我对这项技术感到非常激动，并且当我们后面谈到神 经网络时同样也不会用到一个明确的 for 循环。

让我们开始吧，首先我们回顾一下逻辑回归的前向传播步骤。所以，如果你有 𝑚 个训 练样本，然后对第一个样本进行预测，你需要这样计算。计算 𝑧，我正在使用这个熟悉的公 式 𝑧 (1) = 𝑤 𝑇 𝑥 (1) + 𝑏 。然后计算激活函数 𝑎 (1) = 𝜎(𝑧 (1) ) ，计算第一个样本的预测值 𝑦 。

对第二个样本进行预测，你需要计算 𝑧 (2) = 𝑤 𝑇 𝑥 (2) + 𝑏 ，𝑎 (2) = 𝜎(𝑧 (2) ) 。

对第三个样本进行预测，你需要计算 𝑧 (3) = 𝑤 𝑇 𝑥 (3) + 𝑏 ，𝑎 (3) = 𝜎(𝑧 (3) ) ，依次类推。如果你有 𝑚 个训练样本，你可能需要这样做 𝑚 次，可以看出，为了完成前向传播步骤，即对我们的 𝑚 个样本都计算出预测值。有一个办法可以并且不需要任何一个明确的 for 循 环。让我们来看一下你该怎样做。

首先，回忆一下我们曾经定义了一个矩阵 𝑋 作为你的训练输入，(如下图中蓝色 𝑋) 像 这样在不同的列中堆积在一起。这是一个 𝑛 𝑥 行 𝑚 列的矩阵。我现在将它写为 Python numpy 的形式 (𝑛 𝑥 , 𝑚) ，这只是表示 𝑋 是一个 𝑛 𝑥 乘以 𝑚 的矩阵 𝑅 𝑛 𝑥 ×𝑚 。

现在我首先想做的是告诉你该如何在一个步骤中计算 𝑧1 、 𝑧 2 、𝑧 3 等等。实际上，只 用了一行代码。所以，我打算先构建一个 1 × 𝑚 的矩阵，实际上它是一个行向量，同时我 准备计算 𝑧 (1) ，𝑧 (2) …… 一直到 𝑧 (𝑚) ，所有值都是在同一时间内完成。结果发现它可以表

55 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

达为 𝑤 的转置乘以大写矩阵 𝑥 然后加上向量 [𝑏𝑏. . . 𝑏] ，([𝑧 (1) 𝑧 (2) . . . 𝑧 (𝑚) ] = 𝑤 𝑇 + [𝑏𝑏. . . 𝑏]) 。[𝑏𝑏. . . 𝑏] 是一个 1 × 𝑚 的向量或者 1 × 𝑚 的矩阵或者是一个 𝑚 维的行向量。所以希望你熟悉矩阵乘法，你会发现的 𝑤 转置乘以 𝑥 (1) ，𝑥 (2) 一直到 𝑥 (𝑚) 。所以 𝑤 转置可以是一个行向量。所以第一项 𝑤 𝑇 𝑋 将计算 𝑤 的转置乘以 𝑥 (1) ，𝑤 转置乘以𝑥(2) 等等。然后我们加上第二项 [𝑏𝑏. . . 𝑏] ，你最终将 𝑏 加到了每个元素上。所以你最终得到了 另 一 个 1 × 𝑚 的 向 量 ，[𝑧 (1) 𝑧 (2) .. . 𝑧 (𝑚) ] = 𝑤 𝑇 𝑋 + [𝑏𝑏. . . 𝑏] = [𝑤 𝑇 𝑥 (1) + 𝑏, 𝑤 𝑇 𝑥 (2) + 𝑏. . . 𝑤 𝑇 𝑥 (𝑚) + 𝑏] 。

𝑤 𝑇 𝑥 (1) + 𝑏 这是第一个元素，𝑤 𝑇 𝑥 (2) + 𝑏 这是第二个元素，𝑤 𝑇 𝑥 (𝑚) + 𝑏 这是第 𝑚 个 元素。

如果你参照上面的定义，第一个元素恰好是 𝑧 (1) 的定义，第二个元素恰好是 𝑧 (2) 的定 义，等等。所以，因为𝑋是一次获得的，当你得到你的训练样本，一个一个横向堆积起来，这里我将 [𝑧 (1) 𝑧 (2) . . . 𝑧 (𝑚) ] 定义为大写的 𝑍 ，你用小写 𝑧 表示并将它们横向排在一起。所 以当你将不同训练样本对应的小写 𝑥 横向堆积在一起时得到大写变量 𝑋 并且将小写变量 也用相同方法处理，将它们横向堆积起来，你就得到大写变量 𝑍 。结果发现，为了计算 𝑊 𝑇 𝑋 + [𝑏𝑏. . . 𝑏] ，numpy 命令是𝑍 = 𝑛𝑝. 𝑑𝑜𝑡(𝑤. 𝑇, 𝑋) + 𝑏。这里在 Python 中有一个巧妙的 地方，这里 𝑏 是一个实数，或者你可以说是一个 1 × 1 矩阵，只是一个普通的实数。但是 当你将这个向量加上这个实数时，Python 自动把这个实数 𝑏 扩展成一个 1 × 𝑚 的行向量。所以这种情况下的操作似乎有点不可思议，它在 Python 中被称作广播 (brosdcasting)，目前 你不用对此感到顾虑，我们将在下一个视频中进行进一步的讲解。话说回来它只用一行代码，用这一行代码，你可以计算大写的 𝑍，而大写 𝑍 是一个包含所有小写𝑧 (1) 到 𝑧 (𝑚) 的 1 × 𝑚 的矩阵。这就是 𝑍 的内容，关于变量 𝑎 又是如何呢？

我们接下来要做的就是找到一个同时计算 [𝑎 (1) 𝑎 (2) .. . 𝑎 (𝑚) ] 的方法。就像把小写 𝑥 堆 积起来得到大写 𝑋 和横向堆积小写 𝑧 得到大写 𝑍 一样，堆积小写变量 𝑎 将形成一个新 的变量，我们将它定义为大写 𝐴。在编程作业中，你将看到怎样用一个向量在 sigmoid 函数 中进行计算。所以 sigmoid 函数中输入大写 𝑍 作为变量并且非常高效地输出大写 𝐴。你将 在编程作业中看到它的细节。

总结一下，在这张幻灯片中我们已经看到，不需要 for 循环，利用 𝑚 个训练样本一次 性计算出小写 𝑧 和小写 𝑎，用一行代码即可完成。

Z = np.dot(w.T,X) + b 

这一行代码：𝐴 = [𝑎 (1) 𝑎 (2) . . . 𝑎 (𝑚) ] = 𝜎(𝑍) ，通过恰当地运用𝜎一次性计算所有 𝑎。这

56 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

就是在同一时间内你如何完成一个所有 𝑚 个训练样本的前向传播向量化计算。概括一下，你刚刚看到如何利用向量化在同一时间内高效地计算所有的激活函数的所有 𝑎值。接下来，可以证明，你也可以利用向量化高效地计算反向传播并以此来计算梯度。让 我们在下一个视频中看该如何实现。

57 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.14 向量化 logistic 回归的梯度输出（Vectorizing Logistic Regression's Gradient）

如何向量化计算的同时，对整个训练集预测结果𝑎，这是我们之前已经讨论过的内容。在本次视频中我们将学习如何向量化地计算𝑚个训练数据的梯度，本次视频的重点是如何同 时计算 𝑚 个数据的梯度，并且实现一个非常高效的逻辑回归算法 (Logistic Regression)。

之前我们在讲梯度计算的时候，列举过几个例子，𝑑𝑧 (1) = 𝑎 (1) − 𝑦 (1) ，𝑑𝑧 (2) = 𝑎 (2) 𝑦 (2) …… 等等一系列类似公式。现在，对 𝑚个训练数据做同样的运算，我们可以定义一个新 的变量 𝑑𝑍 = [𝑑𝑧 (1) , 𝑑𝑧 (2) .. . 𝑑𝑧 (𝑚) ] ，所有的 𝑑𝑧 变量横向排列，因此，𝑑𝑍 是一个 1 × 𝑚 的矩阵，或者说，一个 𝑚 维行向量。在之前的幻灯片中，我们已经知道如何计算𝐴，即 [𝑎 (1) , 𝑎 (2) . . . 𝑎 (𝑚) ], 我们需要找到这样的一个行向量 𝑌 = [𝑦 (1) 𝑦 (2) . . . 𝑦 (𝑚) ] ，由此，我们可以 这样计算 𝑑𝑍 = 𝐴 − 𝑌 = [𝑎 (1) − 𝑦 (1) 𝑎 (2) − 𝑦 (2) .. . 𝑎 (𝑚) − 𝑦 (𝑚) ]，不难发现第一个元素就是 𝑑𝑧 (1) ，第二个元素就是 𝑑𝑧 (2) …… 所以我们现在仅需一行代码，就可以同时完成这所有的计 算。

在之前的实现中，我们已经去掉了一个 for 循环，但我们仍有一个遍历训练集的循环，如下所示： 𝑑𝑤 = 0

𝑑𝑤+= 𝑥 (1) ∗ 𝑑𝑧(1) 

𝑑𝑤+= 𝑥 (2) ∗ 𝑑𝑧(2) 

…………. 

𝑑𝑤+= 𝑥 (𝑚) ∗ 𝑑𝑧(𝑚) 

𝑑𝑤 𝑑𝑤 = 𝑚 

𝑑𝑏 = 0 𝑑𝑏+= 𝑑𝑧(1) 𝑑𝑏+= 𝑑𝑧(2) 

…………. 

𝑑𝑏+= 𝑑𝑧(𝑚) 

𝑑𝑏 𝑑𝑏 = 𝑚 

上述（伪）代码就是我们在之前实现中做的，我们已经去掉了一个 for 循环，但用上述

58 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

方法计算 𝑑𝑤 仍然需要一个循环遍历训练集，我们现在要做的就是将其向量化！

1 首先我们来看 𝑑𝑏，不难发现 𝑑𝑏 = 𝑚 ∑ 𝑖=1 𝑚 𝑑 𝑧 (𝑖) ，之前的讲解中，我们知道所有的𝑑𝑧𝑖)

1 已经组成一个行向量 𝑑𝑍了，所以在 Python 中，我们很容易地想到𝑑𝑏 = ∗ 𝑛𝑝. 𝑠𝑢𝑚(𝑑𝑍)；

𝑚 

1 接下来看𝑑𝑤，我们先写出它的公式 𝑑𝑤 = 𝑚 ∗ 𝑋 ∗ 𝑑𝑧 𝑇 其中，𝑋 是一个行向量。因此展开后

1 𝑑𝑤 = 𝑚 ∗ (𝑥 (1) 𝑑𝑧 (1) + 𝑥 (2) 𝑑𝑧 (2) +. . . +𝑥 𝑚 𝑑𝑧 𝑚 ) 。因此我们可以仅用两行代码进行计算：𝑑𝑏 =

1 𝑚 

1 ∗ 𝑛𝑝. 𝑠𝑢𝑚(𝑑𝑍)，𝑑𝑤 = ∗ 𝑋 ∗ 𝑑𝑧 𝑇 。这样，我们就避免了在训练集上使用 for 循环。𝑚

现在，让我们回顾一下，看看我们之前怎么实现的逻辑回归，可以发现，没有向量化是 非常低效的，如下图所示代码：

我们的目标是不使用 for 循环，而是向量，我们可以这么做：

𝑍 = 𝑤 𝑇 𝑋 + 𝑏 = 𝑛𝑝. 𝑑𝑜𝑡(𝑤. 𝑇, 𝑋) + 𝑏 

𝐴 = 𝜎(𝑍) 

𝑑𝑍 = 𝐴 − 𝑌 

1 𝑑𝑤 = ∗ 𝑋 ∗ 𝑑𝑧𝑇 𝑚 

1 𝑑𝑏 = ∗ 𝑛𝑝. 𝑠𝑢𝑚(𝑑𝑍) 𝑚 

𝑤: = 𝑤 − 𝑎 ∗ 𝑑𝑤 

𝑏: = 𝑏 − 𝑎 ∗ 𝑑𝑏 

现在我们利用前五个公式完成了前向和后向传播，也实现了对所有训练样本进行预测和 求导，再利用后两个公式，梯度下降更新参数。我们的目的是不使用 for 循环，所以我们就 通过一次迭代实现一次梯度下降，但如果你希望多次迭代进行梯度下降，那么仍然需要 for

59 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

循环，放在最外层。不过我们还是觉得一次迭代就进行一次梯度下降，避免使用任何循环比 较舒服一些。最后，我们得到了一个高度向量化的、非常高效的逻辑回归的梯度下降算法，我们将在 下次视频中讨论 Python 中的 Broadcasting 技术。

60 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.15 Python 中的广播（Broadcasting in Python）

这是一个不同食物 (每 100g) 中不同营养成分的卡路里含量表格，表格为 3 行 4 列，列表 示不同的食物种类，从左至右依次为苹果，牛肉，鸡蛋，土豆。行表示不同的营养成分，从 上到下依次为碳水化合物，蛋白质，脂肪。

那么，我们现在想要计算不同食物中不同营养成分中的卡路里百分比。现在计算苹果中的碳水化合物卡路里百分比含量，首先计算苹果（100g）中三种营养成 分卡路里总和 56+1.2+1.8 = 59，然后用 56/59 = 94.9% 算出结果。可以看出苹果中的卡路里大部分来自于碳水化合物，而牛肉则不同。对于其他食物，计算方法类似。首先，按列求和，计算每种食物中（100g）三种营养成 分总和，然后分别用不用营养成分的卡路里数量除以总和，计算百分比。那么，能否不使用 for 循环完成这样的一个计算过程呢？ 假设上图的表格是一个 3 行 4 列的矩阵𝐴，记为 𝐴3×4 ，接下来我们要使用 Python 的 numpy 库完成这样的计算。我们打算使用两行代码完成，第一行代码对每一列进行求和，第 二行代码分别计算每种食物每种营养成分的百分比。在 jupyter notebook 中输入如下代码，按 shift+Enter 运行，输出如下。

61 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

下面使用如下代码计算每列的和，可以看到输出是每种食物 (100g) 的卡路里总和。

其中 sum 的参数 axis=0 表示求和运算按列执行，之后会详细解释。接下来计算百分比，这条指令将 3 × 4 的矩阵𝐴除以一个 1 × 4 的矩阵，得到了一个 3 × 4 的结果矩阵，这个结果矩阵就是我们要求的百分比含量。

下面再来解释一下 A.sum (axis = 0) 中的参数 axis。axis 用来指明将要进行的运算 是沿着哪个轴执行，在 numpy 中，0 轴是垂直的，也就是列，而 1 轴是水平的，也就是行。

而第二个 A/cal.reshape (1,4) 指令则调用了 numpy 中的广播机制。这里使用 3 × 4 的矩阵𝐴除以 1 × 4 的矩阵𝑐𝑎𝑙。技术上来讲，其实并不需要再将矩阵𝑐𝑎𝑙 reshape (重塑) 成 1 × 4，因为矩阵𝑐𝑎𝑙本身已经是 1 × 4 了。但是当我们写代码时不确定矩阵维度的时候，通 常会对矩阵进行重塑来确保得到我们想要的列向量或行向量。重塑操作 reshape 是一个常 量时间的操作，时间复杂度是𝑂(1)，它的调用代价极低。

那么一个 3 × 4 的矩阵是怎么和 1 × 4 的矩阵做除法的呢？让我们来看一些更多的广 播的例子。

在 numpy 中，当一个 4 × 1 的列向量与一个常数做加法时，实际上会将常数扩展为一 个 4 × 1 的列向量，然后两者做逐元素加法。结果就是右边的这个向量。这种广播机制对于 行向量和列向量均可以使用。

再看下一个例子。

62 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

用一个 2 × 3 的矩阵和一个 1 × 3 的矩阵相加，其泛化形式是 𝑚 × 𝑛 的矩阵和 1 × 𝑛 的矩阵相加。在执行加法操作时，其实是将 1 × 𝑛 的矩阵复制成为 𝑚 × 𝑛 的矩阵，然后两

者做逐元素加法得到结果。针对这个具体例子，相当于在矩阵的第一列加 100，第二列加 200，第三列加 300。这就是在前一张幻灯片中计算卡路里百分比的广播机制，只不过这里是除法 操作（广播机制与执行的运算种类无关）。下面是最后一个例子

这里相当于是一个 𝑚 × 𝑛 的矩阵加上一个 𝑚 × 1 的矩阵。在进行运算时，会先将 𝑚 × 1 矩阵水平复制 𝑛 次，变成一个 𝑚 × 𝑛 的矩阵，然后再执行逐元素加法。

广播机制的一般原则如下：

这里我先说一下我本人对 numpy 广播机制的理解，再解释上面这张幻灯片。首先是 numpy 广播机制 如果两个数组的后缘维度的轴长度相符或其中一方的轴长度为 1，则认为它们是广播兼 容的。广播会在缺失维度和轴长度为 1 的维度上进行。后缘维度的轴长度：A.shape [-1] 即矩阵维度元组中的最后一个位置的值 对于视频中卡路里计算的例子，矩阵 𝐴 3,4 后缘维度的轴长度是 4，而矩阵 𝑐𝑎𝑙 1,4 的后

63 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

缘维度也是 4，则他们满足后缘维度轴长度相符，可以进行广播。广播会在轴长度为 1 的维 度进行，轴长度为 1 的维度对应 axis=0，即垂直方向，矩阵 cal 1,4 沿 axis=0 (垂直方向) 复制成为 cal_temp 3,4 ，之后两者进行逐元素除法运算。

现在解释上图中的例子

矩阵 𝐴 𝑚,𝑛 和矩阵 𝐵 1,𝑛 进行四则运算，后缘维度轴长度相符，可以广播，广播沿着轴 长度为 1 的轴进行，即 𝐵 1,𝑛 广播成为 𝐵 𝑚,𝑛 ′ ，之后做逐元素四则运算。

矩阵 𝐴 𝑚,𝑛 和矩阵 𝐵 𝑚,1 进行四则运算，后缘维度轴长度不相符，但其中一方轴长度为 1，可以广播，广播沿着轴长度为 1 的轴进行，即 𝐵 𝑚,1 广播成为 𝐵 𝑚,𝑛 ′ ，之后做逐元素四 则运算。

矩阵 𝐴 𝑚,1 和常数𝑅 进行四则运算，后缘维度轴长度不相符，但其中一方轴长度为 1，可以广播，广播沿着缺失维度和轴长度为 1 的轴进行，缺失维度就是 axis=0, 轴长度为 1 的 轴是 axis=1，即𝑅广播成为 𝐵 𝑚,1 ′ ，之后做逐元素四则运算。

最后，对于 Matlab/Octave 有类似功能的函数 bsxfun。

总结一下 broadcasting，可以看看下面的图：

64 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.16 关于 python _ numpy 向量的说明（A note on python or numpy vectors）参考视频：

本节主要讲 Python 中的 numpy 一维数组的特性，以及与行向量或列向量的区别。并介 绍了老师在实际应用中的一些小技巧，去避免在 coding 中由于这些特性而导致的 bug。

Python 的特性允许你使用广播（broadcasting）功能，这是 Python 的 numpy 程序语言 库中最灵活的地方。而我认为这是程序语言的优点，也是缺点。优点的原因在于它们创造出 语言的表达性，Python 语言巨大的灵活性使得你仅仅通过一行代码就能做很多事情。但是 这也是缺点，由于广播巨大的灵活性，有时候你对于广播的特点以及广播的工作原理这些细 节不熟悉的话，你可能会产生很细微或者看起来很奇怪的 bug。例如，如果你将一个列向量 添加到一个行向量中，你会以为它报出维度不匹配或类型错误之类的错误，但是实际上你会 得到一个行向量和列向量的求和。

在 Python 的这些奇怪的影响之中，其实是有一个内在的逻辑关系的。但是如果对 Python 不熟悉的话，我就曾经见过的一些学生非常生硬、非常艰难地去寻找 bug。所以我在这里想 做的就是分享给你们一些技巧，这些技巧对我非常有用，它们能消除或者简化我的代码中所 有看起来很奇怪的 bug。同时我也希望通过这些技巧，你也能更容易地写没有 bug 的 Python 和 numpy 代码。

为了演示 Python-numpy 的一个容易被忽略的效果，特别是怎样在 Python-numpy 中构 造向量，让我来做一个快速示范。首先设置𝑎 = 𝑛𝑝. 𝑟𝑎𝑛𝑑𝑜𝑚. 𝑟𝑎𝑛𝑑𝑛(5)，这样会生成存储在 数组 𝑎 中的 5 个高斯随机数变量。之后输出 𝑎，从屏幕上可以得知，此时 𝑎 的 shape（形 状）是一个 (5,) 的结构。这在 Python 中被称作一个一维数组。它既不是一个行向量也不是 一个列向量，这也导致它有一些不是很直观的效果。举个例子，如果我输出一个转置阵，最 终结果它会和𝑎看起来一样，所以𝑎和𝑎的转置阵最终结果看起来一样。而如果我输出𝑎和𝑎的 转置阵的内积，你可能会想：𝑎乘以𝑎的转置返回给你的可能会是一个矩阵。但是如果我这样 做，你只会得到一个数。

65 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

所以我建议当你编写神经网络时，不要在它的 shape 是 (5,) 还是 (𝑛,) 或者一维数组时使 用数据结构。相反，如果你设置 𝑎 为 (5,1)，那么这就将置于 5 行 1 列向量中。在先前的操 作里 𝑎 和 𝑎 的转置看起来一样，而现在这样的 𝑎 变成一个新的 𝑎 的转置，并且它是一 个行向量。请注意一个细微的差别，在这种数据结构中，当我们输出 𝑎 的转置时有两对方 括号，而之前只有一对方括号，所以这就是 1 行 5 列的矩阵和一维数组的差别。

如果你输出 𝑎 和 𝑎 的转置的乘积，然后会返回给你一个向量的外积，是吧？所以这两 个向量的外积返回给你的是一个矩阵。

66 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

就 我 们 刚 才看 到 的，再进 一 步 说 明。首 先我 们刚 刚 运 行 的命 令 是这 个 (𝑎 = 𝑛𝑝. 𝑟𝑎𝑛𝑑𝑜𝑚. 𝑟𝑎𝑛𝑑𝑛(5))，而且它生成了一个数据结构 (𝑎. 𝑠ℎ𝑎𝑝𝑒)，𝑎. 𝑠ℎ𝑎𝑝𝑒是 (5,)，一个有趣 的东西。这被称作 𝑎 的一维数组，同时这也是一个非常有趣的数据结构。它不像行向量和 列向量那样表现的很一致，这也让它的一些影响不那么明显。所以我建议，当你在编程练习 或者在执行逻辑回归和神经网络时，你不需要使用这些一维数组。

相反，如果你每次创建一个数组，你都得让它成为一个列向量，产生一个 (5,1) 向量或者 你让它成为一个行向量，那么你的向量的行为可能会更容易被理解。所以在这种情况下，𝑎. 𝑠ℎ𝑎𝑝𝑒等同于 (5,1)。这种表现很像 𝑎，但是实际上却是一个列向量。同时这也是为什么当 它是一个列向量的时候，你能认为这是矩阵 (5,1)；同时这里 𝑎. 𝑠ℎ𝑎𝑝𝑒 将要变成 (1,5)，这就 像行向量一样。所以当你需要一个向量时，我会说用这个或那个 (column vector or row vector)，但绝不会是一维数组。

我写代码时还有一件经常做的事，那就是如果我不完全确定一个向量的维度 (dimension)，我经常会扔进一个断言语句 (assertion statement)。像这样，去确保在这种情况下是一个 (5,1) 向量，或者说是一个列向量。这些断言语句实际上是要去执行的，并且它们也会有助于为你 的代码提供信息。所以不论你要做什么，不要犹豫直接插入断言语句。如果你不小心以一维 数组来执行，你也能够重新改变数组维数 𝑎 = 𝑟𝑒𝑠ℎ𝑎𝑝𝑒，表明一个 (5,1) 数组或者一个 (1,5) 数组，以致于它表现更像列向量或行向量。

67 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

我有时候看见学生因为一维数组不直观的影响，难以定位 bug 而告终。通过在原先的代 码里清除一维数组，我的代码变得更加简洁。而且实际上就我在代码中表现的事情而言，我 从来不使用一维数组。因此，要去简化你的代码，而且不要使用一维数组。总是使用 𝑛 × 1 维矩阵（基本上是列向量），或者 1 × 𝑛 维矩阵（基本上是行向量），这样你可以减少很多 assert 语句来节省核矩阵和数组的维数的时间。另外，为了确保你的矩阵或向量所需要的维 数时，不要羞于 reshape 操作。

总之，我希望这些建议能帮助你解决一个 Python 中的 bug，从而使你更容易地完成练 习。

68 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.17 Jupyter/iPython Notebooks 快速入门（Quick tour of Jupyter/iPython Notebooks）

学到现在，你即将要开始处理你的第一个编程作业。但在那之前，让我快速地给你介绍 一下在 Coursera 上的 iPython Notebooks 工具。

这就是 Jupyter iPython Notebooks 的界面，你可以通过它连接到 Coursera。让我快速地 讲解下它的一些特性。关于它的说明已经被写入这个 Notebook 中。

这里有一些空白区域的代码块，你可以在这里编写代码。有时，你也会看到一些函数块。而关于这些的说明都已经在 iPython Notebook 的文本中。在 iPython Notebook 中，在这些 较长的灰色的区域就是代码块。

69 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

有时，你会看到代码块中有像这样的开始代码和结束代码。在进行编程练习时，请确保 你的代码写在开始代码和结束代码之间。

比如，编写打印输出 Hello World 的代码，然后执行这一代码块（你可以按 shift +enter 来执行这一代码块）。最终，它就会输出我们想要的 Hello World。

在运行一个单元格 cell 时，你也可以选择运行其中的一块代码区域。通过点击 Cell 菜单 的 Run Cells 执行这部分代码。

也许，在你的计算机上，运行 cell 的键盘快捷方式可能并非是 shift enter。但是，Mac 应该和我的个人电脑一样，可以使用 shift + enter 来运行 cell。

当你正在阅读指南时，如果不小心双击了它，点中的区域就会变成 markdown 语言形

70 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

式。如果你不小心使其变成了这样的文本框，只要运行下单元格 cell，就可以回到原来的形 式。所以，点击 cell 菜单的 Run Cells 或者使用 shift + enter，就可以使得它变回原样。

这里还有一些其他的小技巧。比如当你执行上面所使用的代码时，它实际上会使用一个 内核在服务器上运行这段代码。如果你正在运行超负荷的进程，或者电脑运行了很长一段时 间，或者在运行中出了错，又或者网络连接失败，这里依然有机会让 Kernel 重新工作。你只 要点击 Kernel，选择 Restart，它会重新运行 Kernel 使程序继续工作。

所以，如果你只是运行相对较小的工作并且才刚刚启动你的 ipad 或笔记本电脑，这种 情况应该是不会发生的。但是，如果你看见错误信息，比如 Kernel 已经中断或者其他信息，你可以试着重启 Kernel。

当我使用 iPython Notebook 时会有多个代码区域块。尽管我并没有在前面的代码块中 添加自己的代码，但还是要确保先执行这块代码。因为在这个例子，它导入了 numpy 包并 另命名为 np 等，并声明了一些你可能需要的变量。为了能顺利地执行下面的代码，就必须 确保先执行上面的代码，即使不要求你去写其他的代码。

71 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

最后，当你完成作业后，可以通过点击右上方蓝色的 Submit Assignment 按钮提交你的 作业。

我发现这种交互式的 shell 命令，在 iPython Notebooks 是非常有用的，能使你快速地实 现代码并且查看输出结果，便于学习。所以我希望这些练习和 Jupyter iPython Notebooks 会 帮助你更快地学习和实践，并且帮助你了解如何去实现这些学习算法。后面一个视频是一个 选学视频，它主要是讲解逻辑回归中的代价函数。你可以选择是否观看。不管怎样，都祝愿 你能通过这两次编程作业。我会在新一周的课程里等待着你。

72 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

2.18 （选修）logistic 损失函数的解释（Explanation of logistic regression cost function）

在前面的视频中，我们已经分析了逻辑回归的损失函数表达式，在这节选修视频中，我 将给出一个简洁的证明来说明逻辑回归的损失函数为什么是这种形式。

回想一下，在逻辑回归中，需要预测的结果𝑦, 可以表示为𝑦 = 𝜎(𝑤 𝑇 𝑥 + 𝑏)，𝜎是我们熟 1 悉的𝑆型函数 𝜎(𝑧) = 𝜎(𝑤 𝑇 𝑥 + 𝑏) = 1+𝑒 −𝑧 。我们约定 𝑦 = 𝑝(𝑦 = 1|𝑥) ，即算法的输出𝑦 是

给定训练样本 𝑥 条件下 𝑦 等于 1 的概率。换句话说，如果𝑦 = 1，在给定训练样本 𝑥 条件下 y = 𝑦； 反过来说，如果𝑦 = 0，在给定训练样本𝑥条件下 (𝑦 = 1 − 𝑦)，因此，如果 𝑦 代表 𝑦 = 1 的概率，那么 1 − 𝑦就是 𝑦 = 0 的概率。

接下来，我们就来分析这两个条件概率公式。

这两个条件概率公式定义形式为 𝑝(𝑦|𝑥) 并且代表了 𝑦 = 0 或者 𝑦 = 1 这两种情况，我们可以将这两个公式合并成一个公式。需要指出的是我们讨论的是二分类问题的损失函

数，因此，𝑦的取值只能是 0 或者 1。上述的两个条件概率公式可以合并成如下公式：

𝑝(𝑦|𝑥) = 𝑦 (1 − 𝑦) 

接下来我会解释为什么可以合并成这种形式的表达式：(1 − 𝑦) 的 (1 − 𝑦) 次方这行表达 式包含了上面的两个条件概率公式，我来解释一下为什么。

73 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

第一种情况，假设 𝑦 = 1，由于𝑦 = 1，那么 (𝑦) 𝑦 = 𝑦 ，因为 𝑦 的 1 次方等于𝑦 ，1 (1 − 𝑦) 的指数项 (1 − 𝑦) 等于 0，由于任何数的 0 次方都是 1，𝑦乘以 1 等于𝑦。因此当

𝑦 = 1 时 𝑝(𝑦|𝑥) = 𝑦（图中绿色部分）。

第二种情况，当 𝑦 = 0 时 𝑝(𝑦|𝑥) 等于多少呢？假设𝑦 = 0，𝑦的𝑦次方就是 𝑦 的 0 次 方，任何数的 0 次方都等于 1，因此 𝑝(𝑦|𝑥) = 1 × (1 − 𝑦) ，前面假设 𝑦 = 0 因此 (1 −

𝑦) 就等于 1，因此 𝑝(𝑦|𝑥) = 1 × (1 − 𝑦)。因此在这里当𝑦 = 0 时，𝑝(𝑦|𝑥) = 1 − 𝑦。这就是这

个公式 (第二个公式，图中紫色字体部分) 的结果。因此，刚才的推导表明 𝑝(𝑦|𝑥) = 𝑦 (1 − 𝑦) ，就是 𝑝(𝑦|𝑥) 的完整定义。由于 log

函数是严格单调递增的函数，最大化 𝑙𝑜𝑔(𝑝(𝑦|𝑥)) 等价于最大化 𝑝(𝑦|𝑥) 并且地计算 𝑝(𝑦|𝑥) 的 log 对数，就是计算 𝑙𝑜𝑔(𝑦 (1 − 𝑦) ) (其实就是将 𝑝(𝑦|𝑥) 代入)，通过对数

函数化简为：

𝑦𝑙𝑜𝑔 𝑦 + (1 − 𝑦)𝑙𝑜𝑔(1 − 𝑦) 

而这就是我们前面提到的损失函数的负数 (−𝐿(𝑦 , 𝑦)) ，前面有一个负号的原因是当你 训练学习算法时需要算法输出值的概率是最大的（以最大的概率预测这个值），然而在逻辑 回归中我们需要最小化损失函数，因此最小化损失函数与最大化条件概率的对数 𝑙𝑜𝑔(𝑝(𝑦|𝑥)) 关联起来了，因此这就是单个训练样本的损失函数表达式。

74 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

在 𝑚个训练样本的整个训练集中又该如何表示呢，让我们一起来探讨一下。让我们一起来探讨一下，整个训练集中标签的概率，更正式地来写一下。假设所有的训 练样本服从同一分布且相互独立，也即独立同分布的，所有这些样本的联合概率就是每个样 本概率的乘积:

𝑃(labels in training set) = ∏ 𝑖=1 𝑚 𝑃(𝑦 (𝑖) |𝑥 (𝑖) )。

如果你想做最大似然估计，需要寻找一组参数，使得给定样本的观测值概率最大，但令 这个概率最大化等价于令其对数最大化，在等式两边取对数：

𝑚 𝑚 𝑚 𝑙𝑜𝑔𝑝(labels in training set) = 𝑙𝑜𝑔 ∏ 𝑃(𝑦 (𝑖) |𝑥 (𝑖) ) = ∑ 𝑙𝑜𝑔𝑃(𝑦 (𝑖) |𝑥 (𝑖) ) = ∑ −𝐿(𝑦 , 𝑦 (𝑖) ) 𝑖=1 𝑖=1 𝑖=1 

在统计学里面，有一个方法叫做最大似然估计，即求出一组参数，使这个式子取最大值，也就是说，使得这个式子取最大值，∑ 𝑖=1 𝑚 −𝐿(𝑦 , 𝑦 (𝑖) )，可以将负号移到求和符号的外面，

75 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第二周：神经网络的编程基础 (Basics of Neural Network programming)

− 

∑𝑖=1 𝑚 

𝐿(𝑦 

, 𝑦 (𝑖) ) ，这样我们就推导出了前面给出的 logistic 回归的成本函数𝐽(𝑤, 𝑏) =

∑𝑖=1 𝑚 

𝐿(𝑦 

, 𝑦 ^ ( 𝑖) )。

由于训练模型时，目标是让成本函数最小化，所以我们不是直接用最大似然概率，要去 掉这里的负号，最后为了方便，可以对成本函数进行适当的缩放，我们就在前面加一个额外 的常数因子 1 𝑚 ，即:

1 𝐽(𝑤, 𝑏) = ∑ 𝑖=1 𝑚 𝐿(𝑦 , 𝑦 (𝑖) )。𝑚 

总结一下，为了最小化成本函数𝐽(𝑤, 𝑏)，我们从 logistic 回归模型的最大似然估计的角 度出发，假设训练集中的样本都是独立同分布的条件下。尽管这节课是选修性质的，但还是 感谢观看本节视频。我希望通过本节课您能更好地明白逻辑回归的损失函数，为什么是那种 形式，明白了损失函数的原理，希望您能继续完成课后的练习，前面课程的练习以及本周的 测验，在课后的小测验和编程练习中，祝您好运。

76 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

第三周：浅层神经网络 (Shallow neural networks)

3.1 神经网络概述（Neural Network Overview）

本周你将学习如何实现一个神经网络。在我们深入学习具体技术之前，我希望快速的带 你预览一下本周你将会学到的东西。如果这个视频中的某些细节你没有看懂你也不用担心，我们将在后面的几个视频中深入讨论技术细节。

现在我们开始快速浏览一下如何实现神经网络。上周我们讨论了逻辑回归，我们了解了 这个模型 (见图 3.1.1) 如何与下面公式 3.1 建立联系。

图 3.1.1 :

公式 3.1：

𝑥 

} ⟹ 𝑧 = 𝑤 𝑇 𝑥 + 𝑏 𝑏 

𝑤 

如上所示，首先你需要输入特征𝑥，参数𝑤和𝑏，通过这些你就可以计算出𝑧，公式 3.2：

接下来使用𝑧就可以计算出𝑎。我们将的符号换为表示输出𝑦 ⟹ 𝑎 = 𝜎(𝑧), 然后可以计算

出 loss function 𝐿(𝑎, 𝑦)

神经网络看起来是如下这个样子（图 3.1.2）。正如我之前已经提到过，你可以把许多 sigmoid 单元堆叠起来形成一个神经网络。对于图 3.1.1 中的节点，它包含了之前讲的计算的 两个步骤：首先通过公式 3.1 计算出值𝑧，然后通过𝜎(𝑧) 计算值𝑎。

77 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

图 3.1.2 在这个神经网络（图 3.1.2）对应的 3 个节点，首先计算第一层网络中的各个节点相关 的数𝑧 [1] ，接着计算𝛼 [1] ，在计算下一层网络同理； 我们会使用符号 [𝑚] 表示第𝑚层网络中 节点相关的数，这些节点的集合被称为第𝑚层网络。这样可以保证 [𝑚] 不会和我们之前用来 表示单个的训练样本的 (𝑖) (即我们使用表示第 i 个训练样本) 混淆； 整个计算过程，公式如 下：公式 3.3：

公式 3.4：

类似逻辑回归，在计算后需要使用计算，接下来你需要使用另外一个线性方程对应的参 数计算𝑧 [2] ，计算𝑎 [2] ，此时𝑎 [2] 就是整个神经网络最终的输出，用 𝑦表示网络的输出。

公式 3.5： 𝑥 𝑑𝑏[1]

𝑑𝑊 [1] } ⟸ 𝑑𝑧 [1] = 𝑑(𝑊 [1] 𝑥 + 𝑏 [1] ) ⟸ 𝑑𝛼 [1] = 𝑑𝜎(𝑧 [1] ) 

公式 3.6：

78 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

我知道这其中有很多细节，其中有一点非常难以理解，即在逻辑回归中，通过直接计算𝑧得 到结果𝑎。而这个神经网络中，我们反复的计算𝑧和𝑎，计算𝑎和𝑧，最后得到了最终的输出 loss function。

你应该记得逻辑回归中，有一些从后向前的计算用来计算导数𝑑𝑎、𝑑𝑧。同样，在神经网 络中我们也有从后向前的计算，看起来就像这样，最后会计算𝑑𝑎 [2] 、𝑑𝑧 [2] ，计算出来之后，然后计算计算𝑑𝑊 [2] 、𝑑𝑏 [2] 等，按公式 3.5、3.6 箭头表示的那样，从右到左反向计算。

现在你大概了解了一下什么是神经网络，基于逻辑回归重复使用了两次该模型得到上述 例子的神经网络。我清楚这里面多了很多新符号和细节，如果没有理解也不用担心，在接下 来的视频中我们会仔细讨论具体细节。

那么，下一个视频讲述神经网络的表示。

79 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

3.2 神经网络的表示（Neural Network Representation）

先回顾一下我在上一个视频画几张神经网络的图片，在这次课中我们将讨论这些图片的 具体含义，也就是我们画的这些神经网络到底代表什么。

我们首先关注一个例子，本例中的神经网络只包含一个隐藏层（图 3.2.1）。这是一张神 经网络的图片，让我们给此图的不同部分取一些名字。

图 3.2.1 我们有输入特征𝑥 1 、𝑥 2 、𝑥 3 ，它们被竖直地堆叠起来，这叫做神经网络的输入层。它包 含了神经网络的输入；然后这里有另外一层我们称之为隐藏层（图 3.2.1 的四个结点）。待 会儿我会回过头来讲解术语 "隐藏" 的意义；在本例中最后一层只由一个结点构成，而这个只 有一个结点的层被称为输出层，它负责产生预测值。解释隐藏层的含义：在一个神经网络中，当你使用监督学习训练它的时候，训练集包含了输入𝑥也包含了目标输出𝑦，所以术语隐藏层 的含义是在训练集中，这些中间结点的准确值我们是不知道到的，也就是说你看不见它们在 训练集中应具有的值。你能看见输入的值，你也能看见输出的值，但是隐藏层中的东西，在 训练集中你是无法看到的。所以这也解释了词语隐藏层，只是表示你无法在训练集中看到他 们。

现在我们再引入几个符号，就像我们之前用向量𝑥表示输入特征。这里有个可代替的记 号𝑎 [0] 可以用来表示输入特征。𝑎表示激活的意思，它意味着网络中不同层的值会传递到它们 后面的层中，输入层将𝑥传递给隐藏层，所以我们将输入层的激活值称为𝑎 [0] ；下一层即隐藏 层也同样会产生一些激活值，那么我将其记作𝑎 [1] ，所以具体地，这里的第一个单元或结点 我们将其表示为𝑎 1 [1] ，第二个结点的值我们记为𝑎 2 [1] 以此类推。所以这里的是一个四维的向量 如果写成 Python 代码，那么它是一个规模为 4x1 的矩阵或一个大小为 4 的列向量，如下公 式，它是四维的，因为在本例中，我们有四个结点或者单元，或者称为四个隐藏层单元； 公

80 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

式 3.7

𝑎1 [1] 

𝑎2 [1] 𝑎 [1] = 𝑎3 [1] 

[ 𝑎 4 [1] ] 

最后输出层将产生某个数值𝑎，它只是一个单独的实数，所以的𝑦值将取为𝑎 。这与逻 辑回归很相似，在逻辑回归中，我们有𝑦直接等于𝑎，在逻辑回归中我们只有一个输出层，所 以我们没有用带方括号的上标。但是在神经网络中，我们将使用这种带上标的形式来明确地 指出这些值来自于哪一层，有趣的是在约定俗成的符号传统中，在这里你所看到的这个例子，只能叫做一个两层的神经网络（图 3.2.2）。原因是当我们计算网络的层数时，输入层是不算 入总层数内，所以隐藏层是第一层，输出层是第二层。第二个惯例是我们将输入层称为第零 层，所以在技术上，这仍然是一个三层的神经网络，因为这里有输入层、隐藏层，还有输出 层。但是在传统的符号使用中，如果你阅读研究论文或者在这门课中，你会看到人们将这个 神经网络称为一个两层的神经网络，因为我们不将输入层看作一个标准的层。

图 3.2.2 最后，我们要看到的隐藏层以及最后的输出层是带有参数的，这里的隐藏层将拥有两个 参数𝑊和𝑏，我将给它们加上上标 [1] (𝑊 [1] ,𝑏 [1] )，表示这些参数是和第一层这个隐藏层有关 系的。之后在这个例子中我们会看到𝑊是一个 4x3 的矩阵，而𝑏是一个 4x1 的向量，第一个 数字 4 源自于我们有四个结点或隐藏层单元，然后数字 3 源自于这里有三个输入特征，我们 之后会更加详细地讨论这些矩阵的维数，到那时你可能就更加清楚了。相似的输出层也有一 些与之关联的参数𝑊 [2] 以及𝑏 [2] 。从维数上来看，它们的规模分别是 1x4 以及 1x1。1x4 是因

81 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

为隐藏层有四个隐藏层单元而输出层只有一个单元，之后我们会对这些矩阵和向量的维度做 出更加深入的解释，所以现在你已经知道一个两层的神经网络什么样的了，即它是一个只有 一个隐藏层的神经网络。在下一个视频中。我们将更深入地了解这个神经网络是如何进行计算的，也就是这个神 经网络是怎么输入𝑥，然后又是怎么得到𝑦。

82 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

3.3 计算一个神经网络的输出（Computing a Neural Network's output）

在上一节的视频中，我们介绍只有一个隐藏层的神经网络的结构与符号表示。在这节的 视频中让我们了解神经网络的输出究竟是如何计算出来的。首先，回顾下只有一个隐藏层的简单两层神经网络结构：

图 3.3.1 其中，𝑥表示输入特征，𝑎表示每个神经元的输出，𝑊表示特征的权重，上标表示神经网 络的层数（隐藏层为 1），下标表示该层的第几个神经元。这是神经网络的符号惯例，下同。

神经网络的计算

关于神经网络是怎么计算的，从我们之前提及的逻辑回归开始，如下图所示。用圆圈表 示神经网络的计算单元，逻辑回归的计算有两个步骤，首先你按步骤计算出𝑧，然后在第二 步中你以 sigmoid 函数为激活函数计算𝑧（得出𝑎），一个神经网络只是这样子做了好多次重 复计算。

83 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

图 3.3.2 回到两层的神经网络，我们从隐藏层的第一个神经元开始计算，如上图第一个最上面的 箭头所指。从上图可以看出，输入与逻辑回归相似，这个神经元的计算与逻辑回归一样分为 两步，小圆圈代表了计算的两个步骤。第一步，计算𝑧 1 [1] , 𝑧 1 [1] = 𝑤 1 [1]𝑇 𝑥 + 𝑏 1 [1] 。第二步，通过激活函数计算𝑎 1 [1] , 𝑎 1 [1] = 𝜎(𝑧 1 [1] )。隐藏层的第二个以及后面两个神经元的计算过程一样，只是注意符号表示不同，最终分

别得到𝑎 2 [1] 、𝑎 3 [1] 、𝑎 4 [1] ，详细结果见下:

𝑧 1 [1] = 𝑤 1 [1]𝑇 𝑥 + 𝑏 1 [1] , 𝑎 1 [1] = 𝜎(𝑧 1 [1] ) 

𝑧 2 [1] = 𝑤 2 [1]𝑇 𝑥 + 𝑏 2 [1] , 𝑎 2 [1] = 𝜎(𝑧 2 [1] ) 

𝑧 3 [1] = 𝑤 3 [1]𝑇 𝑥 + 𝑏 3 [1] , 𝑎 3 [1] = 𝜎(𝑧 3 [1] ) 

𝑧 4 [1] = 𝑤 4 [1]𝑇 𝑥 + 𝑏 4 [1] , 𝑎 4 [1] = 𝜎(𝑧 4 [1] ) 

向量化计算 如果你执行神经网络的程序，用 for 循环来做这些看起来真的很低效。所 以接下来我们要做的就是把这四个等式向量化。向量化的过程是将神经网络中的一层神经元 参数纵向堆积起来，例如隐藏层中的𝑤纵向堆积起来变成一个 (4,3) 的矩阵，用符号𝑊 [1] 表示。另一个看待这个的方法是我们有四个逻辑回归单元，且每一个逻辑回归单元都有相对应的参 数 —— 向量𝑤，把这四个向量堆积在一起，你会得出这 4×3 的矩阵。因此，公式 3.8： 𝑧 [𝑛] =

𝑤 [𝑛] 𝑥 + 𝑏[𝑛] 

公式 3.9：

𝑎 [𝑛] = 𝜎(𝑧 [𝑛] ) 

详细过程见下：公式 3.10：

84 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

𝑎1 [1] 

𝑎2 [1] 𝑎 [1] = = 𝜎(𝑧 [1] ) 𝑎3 [1] 

[ 𝑎 4 [1] ] 

公式 3.11：

𝑊 

[1] 

𝑏 

[1] 

⏞ ⏞ 𝑧 1 [1] . . . 𝑊 [1]𝑇 1 .. . 𝑖𝑛𝑝𝑢𝑡 𝑏1 [1] ⏞ 𝑥1 𝑧 2 [1] . . . 𝑊 [1]𝑇 .. . 𝑏2 [1] 2 = ∗ 𝑥2 𝑧 3 [1] . . . 𝑊 [1]𝑇 .. . 𝑥 3 𝑏3 [1] 3 

[ 

] 

+ 

[ 𝑧 4 [1] ] . . . 𝑊 [1]𝑇 .. . ] 4 

[ 

[ 𝑏 4 [1] ] 

对于神经网络的第一层，给予一个输入𝑥，得到𝑎 [1] ，𝑥可以表示为𝑎 [0] 。通过相似的衍生 你会发现，后一层的表示同样可以写成类似的形式，得到𝑎 [2] ，𝑦 = 𝑎 [2] ，具体过程见公式 3.8、 3.9。

图 3.3.3 如上图左半部分所示为神经网络，把网络左边部分盖住先忽略，那么最后的输出单元就 相当于一个逻辑回归的计算单元。当你有一个包含一层隐藏层的神经网络，你需要去实现以 计算得到输出的是右边的四个等式，并且可以看成是一个向量化的计算过程，计算出隐藏层 的四个逻辑回归单元和整个隐藏层的输出结果，如果编程实现需要的也只是这四行代码。

总结：通过本视频，你能够根据给出的一个单独的输入特征向量，运用四行代码计算出 一个简单神经网络的输出。接下来你将了解的是如何一次能够计算出不止一个样本的神经网 络输出，而是能一次性计算整个训练集的输出。

85 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

3.4 多样本向量化（Vectorizing across multiple examples）

在上一个视频，了解到如何针对于单一的训练样本，在神经网络上计算出预测值。在这个视频，将会了解到如何向量化多个训练样本，并计算出结果。该过程与你在逻辑 回归中所做类似。

逻辑回归是将各个训练样本组合成矩阵，对矩阵的各列进行计算。神经网络是通过对逻 辑回归中的等式简单的变形，让神经网络计算出输出值。这种计算是所有的训练样本同时进 行的，以下是实现它具体的步骤：

图 3.4.1 上一节视频中得到的四个等式。它们给出如何计算出𝑧 [1] ，𝑎 [1] ，𝑧[2] ，𝑎 [2] 。对于一个给定的输入特征向量𝑋，这四个等式可以计算出𝛼 [2] 等于𝑦。这是针对于单一的 训练样本。如果有𝑚个训练样本，那么就需要重复这个过程。用第一个训练样本𝑥 [1] 来计算出预测值𝑦 ，就是第一个训练样本上得出的结果。

然后，用𝑥 [2] 来计算出预测值𝑦 ，循环往复，直至用𝑥 [𝑚] 计算出𝑦 。用激活函数表示法，如上图左下所示，它写成𝑎 [2](1) 、𝑎 [2](2) 和𝑎 [2](𝑚) 。【注】：𝑎 [2](𝑖) ，(𝑖) 是指第𝑖个训练样本而 [2] 是指第二层。如果有一个非向量化形式的实现，而且要计算出它的预测值，对于所有训练样本，需要 让𝑖从 1 到𝑚实现这四个等式：

𝑧 [1](𝑖) = 𝑊 [1](𝑖) 𝑥 (𝑖) + 𝑏[1](𝑖) 

𝑎 [1](𝑖) = 𝜎(𝑧 [1](𝑖) ) 

86 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

𝑧 [2](𝑖) = 𝑊 [2](𝑖) 𝑎 [1](𝑖) + 𝑏[2](𝑖) 

𝑎 [2](𝑖) = 𝜎(𝑧 [2](𝑖) ) 

对于上面的这个方程中的 (𝑖) ，是所有依赖于训练样本的变量，即将 (𝑖) 添加到𝑥，𝑧和𝑎。如果想计算𝑚个训练样本上的所有输出，就应该向量化整个计算，以简化这列。

本课程需要使用很多线性代数的内容，重要的是能够正确地实现这一点，尤其是在深度 学习的错误中。实际上本课程认真地选择了运算符号，这些符号只是针对于这个课程的，并 且能使这些向量化容易一些。

所以，希望通过这个细节可以更快地正确实现这些算法。接下来讲讲如何向量化这些： 公式 3.12： ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮

𝑥 = [ 𝑥 (1) 𝑥 (2) ⋯ 𝑥 (𝑚) ] 

公式 3.13： ⋮ ⋮ ⋮ ⋮

⋮ ⋮ 

⋮ ⋮ 

𝑍 [1] = [ 𝑧 [1](1) 𝑧 [1](2) ⋯ 𝑧 [1](𝑚) ] 

公式 3.14： ⋮ ⋮ ⋮ ⋮

⋮ ⋮ 

⋮ ⋮ 

𝐴 [1] = [ 𝛼 [1](1) 𝛼 [1](2) ⋯ 𝛼 [1](𝑚) ] 

公式 3.15：

𝑧 [1](𝑖) = 𝑊 [1](𝑖) 𝑥 (𝑖) + 𝑏[1] 𝐴 [1] = 𝜎(𝑧 [1] ) 𝛼 [1](𝑖) = 𝜎(𝑧 [1](𝑖) ) ⟹ { 𝑧 [2] = 𝑊 [2] 𝐴 [1] + 𝑏[2] 𝑧 [2](𝑖) = 𝑊 [2](𝑖) 𝛼 [1](𝑖) + 𝑏[2] 𝐴 [2] = 𝜎(𝑧 [2] ) 𝛼 [2](𝑖) = 𝜎(𝑧 [2](𝑖) )} 

前一张幻灯片中的 for 循环是来遍历所有个训练样本。定义矩阵𝑋等于训练样本，将它 们组合成矩阵的各列，形成一个𝑛维或𝑛乘以𝑚维矩阵。接下来计算见公式 3.15： 以此类推，从小写的向量𝑥到这个大写的矩阵𝑋，只是通过组合𝑥向量在矩阵的各列中。同理，𝑧 [1](1) ，𝑧[1](2) 等等都是𝑧 [1](𝑚) 的列向量，将所有𝑚都组合在各列中，就的到矩阵 𝑍 [1] 。同理，𝑎 [1](1) ，𝑎 [1](2) ，……，𝑎 [1](𝑚) 将其组合在矩阵各列中，如同从向量𝑥到矩阵𝑋，以 及从向量𝑧到矩阵𝑍一样，就能得到矩阵𝐴 [1] 。同样的，对于𝑍 [2] 和𝐴 [2] ，也是这样得到。这种符号其中一个作用就是，可以通过训练样本来进行索引。这就是水平索引对应于不

87 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

同的训练样本的原因，这些训练样本是从左到右扫描训练集而得到的。在垂直方向，这个垂直索引对应于神经网络中的不同节点。例如，这个节点，该值位于 矩阵的最左上角对应于激活单元，它是位于第一个训练样本上的第一个隐藏单元。它的下一 个值对应于第二个隐藏单元的激活值。它是位于第一个训练样本上的，以及第一个训练示例 中第三个隐藏单元，等等。

当垂直扫描，是索引到隐藏单位的数字。当水平扫描，将从第一个训练示例中从第一个 隐藏的单元到第二个训练样本，第三个训练样本…… 直到节点对应于第一个隐藏单元的激活 值，且这个隐藏单元是位于这𝑚个训练样本中的最终训练样本。

从水平上看，矩阵𝐴代表了各个训练样本。从竖直上看，矩阵𝐴的不同的索引对应于不 同的隐藏单元。

对于矩阵𝑍，𝑋情况也类似，水平方向上，对应于不同的训练样本；竖直方向上，对应不 同的输入特征，而这就是神经网络输入层中各个节点。

神经网络上通过在多样本情况下的向量化来使用这些等式。

在下一个视频中，将证明为什么这是一种正确向量化的实现。这种证明将会与逻辑回归 中的证明类似。

88 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

3.5 向 量 化 实 现 的 解 释 （ Justification for vectorized implementation）

在上一个视频中，我们学习到如何将多个训练样本横向堆叠成一个矩阵𝑋，然后就可以 推导出神经网络中前向传播（forward propagation）部分的向量化实现。在这个视频中，我们将会继续了解到，为什么上一节中写下的公式就是将多个样本向量 化的正确实现。我们先手动对几个样本计算一下前向传播，看看有什么规律：

公式 3.16： 𝑧 [1](1) = 𝑊 [1] 𝑥 (1) + 𝑏[1]

𝑧 [1](2) = 𝑊 [1] 𝑥 (2) + 𝑏[1] 

𝑧 [1](3) = 𝑊 [1] 𝑥 (3) + 𝑏[1] 

这里，为了描述的简便，我们先忽略掉 𝑏 [1] 后面你将会看到利用 Python 的广播机制，可以很容易的将𝑏 [1] 加进来。

现在 𝑊 [1] 是一个矩阵，𝑥 (1) ,𝑥 (2) , 𝑥 (3) 都是列向量，矩阵乘以列向量得到列向量，下面 将它们用图形直观的表示出来：公式 3.17：

视频中，吴恩达老师很细心的用不同的颜色表示不同的样本向量，及其对应的输出。所以从 图中可以看出，当加入更多样本时，只需向矩阵𝑋中加入更多列。所以从这里我们也可以了解到，为什么之前我们对单个样本的计算要写成 𝑧 [1](𝑖) = 𝑊 [1] 𝑥 (𝑖) + 𝑏 [1] 这种形式，因为当有不同的训练样本时，将它们堆到矩阵𝑋的各列中，那么 它们的输出也就会相应的堆叠到矩阵 𝑍 [1] 的各列中。现在我们就可以直接计算矩阵 𝑍[1] 加上𝑏 [1] ，因为列向量 𝑏 [1] 和矩阵 𝑍 [1] 的列向量有着相同的尺寸，而 Python 的广播机制对 于这种矩阵与向量直接相加的处理方式是，将向量与矩阵的每一列相加。所以这一节只是 说明了为什么公式 𝑍 [1] = 𝑊 [1] 𝑋 + 𝑏 [1] 是前向传播的第一步计算的正确向量化实现，但事实

89 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

证明，类似的分析可以发现，前向传播的其它步也可以使用非常相似的逻辑，即如果将输入 按列向量横向堆叠进矩阵，那么通过公式计算之后，也能得到成列堆叠的输出。

最后，对这一段视频的内容做一个总结:

由公式 3.12、公式 3.13、公式 3.14、公式 3.15 可以看出，使用向量化的方法，可以不 需要显示循环，而直接通过矩阵运算从𝑋就可以计算出 𝐴 [1] ，实际上𝑋可以记为 𝐴 [0] ，使用 同样的方法就可以由神经网络中的每一层的输入 𝐴 [𝑖−1] 计算输出 𝐴 [𝑖] 。其实这些方程有一 定对称性，其中第一个方程也可以写成𝑍 [1] = 𝑊 [1] 𝐴 [0] + 𝑏 [1] ，你看这对方程，还有这对方程 形式其实很类似，只不过这里所有指标加了 1。所以这样就显示出神经网络的不同层次，你 知道大概每一步做的都是一样的，或者只不过同样的计算不断重复而已。这里我们有一个双 层神经网络，我们在下周视频里会讲深得多的神经网络，你看到随着网络的深度变大，基本 上也还是重复这两步运算，只不过是比这里你看到的重复次数更多。在下周的视频中将会讲 解更深层次的神经网络，随着层数的加深，基本上也还是重复同样的运算。

以上就是对神经网络向量化实现的正确性的解释，到目前为止，我们仅使用 sigmoid 函 数作为激活函数，事实上这并非最好的选择，在下一个视频中，将会继续深入的讲解如何使 用更多不同种类的激活函数。

90 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

3.6 激活函数（Activation functions）

使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。到目前为止，之前的视频只用过 sigmoid 激活函数，但是，有时其他的激活函数效果会更好。

在神经网路的前向传播中，𝑎 [1] = 𝜎(𝑧 [1] ) 和𝑎 [2] = 𝜎(𝑧 [2] ) 这两步会使用到 sigmoid 函数。sigmoid 函数在这里被称为激活函数。

1 公式 3.18： 𝑎 = 𝜎(𝑧) = −𝑧 1+𝑒

更通常的情况下，使用不同的函数𝑔(𝑧 [1] )，𝑔可以是除了 sigmoid 函数意外的非线性函 数。tanh 函数或者双曲正切函数是总体上都优于 sigmoid 函数的激活函数。如图，𝑎 = 𝑡𝑎𝑛(𝑧) 的值域是位于 + 1 和 - 1 之间。

𝑧 −𝑒 −𝑧 𝑒 公式 3.19： 𝑎 = 𝑡𝑎𝑛ℎ(𝑧) = 𝑧 −𝑧 𝑒 +𝑒

事实上，tanh 函数是 sigmoid 的向下平移和伸缩后的结果。对它进行了变形后，穿过了 (0,0) 点，并且值域介于 + 1 和 - 1 之间。结果表明，如果在隐藏层上使用函数 公式 3.20： 𝑔(𝑧 [1] ) = 𝑡𝑎𝑛ℎ(𝑧 [1] ) 效果总是优于 sigmoid 函数。因为函数值域在 - 1 和 + 1 的激活函数，其均值是更接近零均值的。在训练一个算法模型时，如果使用 tanh 函数代替 sigmoid 函数中心化数据，使得数据的平均值更接近 0 而不是 0.5.

这会使下一层学习简单一点，在第二门课中会详细讲解。在讨论优化算法时，有一点要说明：我基本已经不用 sigmoid 激活函数了，tanh 函数在 所有场合都优于 sigmoid 函数。但有一个例外：在二分类的问题中，对于输出层，因为𝑦的值是 0 或 1，所以想让𝑦的数 值介于 0 和 1 之间，而不是在 - 1 和 + 1 之间。所以需要使用 sigmoid 激活函数。这里的公式 3.21： 𝑔(𝑧 [2] ) = 𝜎(𝑧 [2] ) 在这个例子里看到的是，对隐藏层使用 tanh 激活函数，输出层使用 sigmoid 函数。所以，在不同的神经网络层中，激活函数可以不同。为了表示不同的激活函数，在不同 的层中，使用方括号上标来指出𝑔上标为 [1] 的激活函数，可能会跟𝑔上标为 [2] 不同。方括号 上标 [1] 代表隐藏层，方括号上标 [2] 表示输出层。sigmoid 函数和 tanh 函数两者共同的缺点是，在𝑧特别大或者特别小的情况下，导数的 梯度或者函数的斜率会变得特别小，最后就会接近于 0，导致降低梯度下降的速度。

91 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

在机器学习另一个很流行的函数是：修正线性单元的函数（ReLu），ReLu 函数图像是如 下图。公式 3.22： 𝑎 = 𝑚𝑎𝑥(0, 𝑧) 所以，只要𝑧是正值的情况下，导数恒等于 1，当𝑧是负 值的时候，导数恒等于 0。从实际上来说，当使用𝑧的导数时，𝑧=0 的导数是没有定义的。但 是当编程实现的时候，𝑧的取值刚好等于 0.0000000，这个值相当小，所以，在实践中，不需 要担心这个值，𝑧是等于 0 的时候，假设一个导数是 1 或者 0 效果都可以。

这有一些选择激活函数的经验法则：

如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单 元都选择 Relu 函数。

这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会 使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当𝑧是负值的 时候，导数等于 0。

这里也有另一个版本的 Relu 被称为 Leaky Relu。

当𝑧是负值时，这个函数的值不是等于 0，而是轻微的倾斜，如图。

这个函数通常比 Relu 激活函数效果要好，尽管在实际中 Leaky ReLu 使用的并不多。

图 3.6.1 两者的优点是： 第一，在𝑧的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于 0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。第二，sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥

92 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

散，而 Relu 和 Leaky ReLu 函数大于 0 部分都为常熟，不会产生梯度弥散现象。(同时应该注 意到的是，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会有这问题)

𝑧在 ReLu 的梯度一半都是 0，但是，有足够的隐藏层使得 z 值大于 0，所以对大多数的 训练数据来说学习过程仍然可以很快。

快速概括一下不同激活函数的过程和结论。

sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。

tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。

ReLu 激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu。

公式 3.23： 𝑎 = 𝑚𝑎𝑥(0.01𝑧, 𝑧)

为什么常数是 0.01？当然，可以为学习算法选择不同的参数。在选择自己神经网络的激活函数时，有一定的直观感受，在深度学习中的经常遇到一个 问题：在编写神经网络的时候，会有很多选择：隐藏层单元的个数、激活函数的选择、初始 化权值…… 这些选择想得到一个对比较好的指导原则是挺困难的。

鉴于以上三个原因，以及在工业界的见闻，提供一种直观的感受，哪一种工业界用的多，哪一种用的少。但是，自己的神经网络的应用，以及其特殊性，是很难提前知道选择哪些效 果更好。所以通常的建议是：如果不确定哪一个激活函数效果更好，可以把它们都试试，然 后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。

为自己的神经网络的应用测试这些不同的选择，会在以后检验自己的神经网络或者评估 算法的时候，看到不同的效果。如果仅仅遵守使用默认的 ReLu 激活函数，而不要用其他的 激励函数，那就可能在近期或者往后，每次解决问题的时候都使用相同的办法。

93 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）

为什么神经网络需要非线性激活函数？事实证明：要让你的神经网络能够计算出有趣的 函数，你必须使用非线性激活函数，证明如下：

这是神经网络正向传播的方程，现在我们去掉函数𝑔，然后令𝑎 [1] = 𝑧 [1] ，或者我们也可 以令𝑔(𝑧) = 𝑧，这个有时被叫做线性激活函数（更学术点的名字是恒等激励函数，因为它们 就是把输入值输出）。为了说明问题我们把𝑎 [2] = 𝑧 [2] ，那么这个模型的输出𝑦或仅仅只是输 入特征𝑥的线性组合。

如果我们改变前面的式子，令：

(1) 𝑎 [1] = 𝑧 [1] = 𝑊 [1] 𝑥 + 𝑏[1] 

(2) 𝑎 [2] = 𝑧 [2] = 𝑊 [2] 𝑎 [1] + 𝑏[2] 

将 式 子 (1) 代 入 式 子 (2) 中 ，则 ：

𝑎 [2] = 𝑧 [2] = 

𝑊 [2] (𝑊 [1] 𝑥 + 𝑏 [1] ) + 𝑏[2] 

(3) 𝑎 [2] = 𝑧 [2] = 𝑊 [2] 𝑊 [1] 𝑥 + 𝑊 [2] 𝑏 [1] + 𝑏[2] 

简化多项式得 𝑎 [2] = 𝑧 [2] = 𝑊 ′ 𝑥 + 𝑏′ 如果你是用线性激活函数或者叫恒等激励函数，那么神经网络只是把输入线性组合再输

出。

我们稍后会谈到深度网络，有很多层的神经网络，很多隐藏层。事实证明，如果你使用 线性激活函数或者没有使用一个激活函数，那么无论你的神经网络有多少层一直在做的只是 计算线性函数，所以不如直接去掉全部隐藏层。在我们的简明案例中，事实证明如果你在隐 藏层用线性激活函数，在输出层用 sigmoid 函数，那么这个模型的复杂度和没有任何隐藏层 的标准 Logistic 回归是一样的，如果你愿意的话，可以证明一下。

在这里线性隐层一点用也没有，因为这两个线性函数的组合本身就是线性函数，所以除 非你引入非线性，否则你无法计算更有趣的函数，即使你的网络层数再多也不行；只有一个 地方可以使用线性激活函数 ------𝑔(𝑧) = 𝑧，就是你在做机器学习中的回归问题。𝑦 是一个实 数，举个例子，比如你想预测房地产价格，𝑦 就不是二分类任务 0 或 1，而是一个实数，从 0 到正无穷。如果𝑦 是个实数，那么在输出层用线性激活函数也许可行，你的输出也是一个 实数，从负无穷到正无穷。

总而言之，不能在隐藏层用线性激活函数，可以用 ReLU 或者 tanh 或者 leaky ReLU 或

94 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

者其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层；除了这种情况，会 在隐层用线性函数的，除了一些特殊情况，比如与压缩有关的，那方面在这里将不深入讨论。在这之外，在隐层使用线性激活函数非常少见。因为房价都是非负数，所以我们也可以在输 出层使用 ReLU 函数这样你的𝑦都大于等于 0。理解为什么使用非线性激活函数对于神经网络十分关键，接下来我们讨论梯度下降，并 在下一个视频中开始讨论梯度下降的基础 —— 激活函数的导数。

95 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

3.8 激活函数的导数（Derivatives of activation functions）

在神经网络中使用反向传播的时候，你真的需要计算激活函数的斜率或者导数。针对以 下四种激活，求其导数如下： 1）sigmoid activation function

图 3.8.1 其具体的求导如下：

𝑑 1 公式 3.25： 𝑔(𝑧) = −𝑧 (1 − 1+𝑒 −𝑧 1) = 𝑔(𝑧)(1 − 𝑔(𝑧)) 𝑑𝑧 1+𝑒

注：

𝑑 当𝑧 = 10 或𝑧 = −10 𝑑𝑧 𝑔(𝑧) ≈ 0

当𝑧= 0

𝑑 𝑑𝑧 

𝑔(𝑧) = g(z)(1 − g(z)) = 1/4 

在神经网络中

𝑎 = 𝑔(𝑧); 

𝑑 𝑔(𝑧) ′ = 𝑔(𝑧) = 𝑎(1 − 𝑎) 𝑑𝑧 

2) Tanh activation function 

图 3.8.2 其具体的求导如下：

𝑧 −𝑒 −𝑧 𝑒 公式 3.26： 𝑔(𝑧) = 𝑡𝑎𝑛ℎ(𝑧) = 𝑧 −𝑧 𝑒 +𝑒

96 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

d 公式 3.27： g (z) = 1 − (tanh ( z)) 2 dz

注：

𝑑 当𝑧 = 10 或𝑧 = −10 𝑔(𝑧) ≈ 0 𝑑𝑧

𝑑 当𝑧 = 0，𝑔(𝑧) = 1 − (0) = 1 𝑑𝑧

在神经网络中；

3）Rectified Linear Unit (ReLU) 

𝑔(𝑧) = 𝑚𝑎𝑥(0,𝑧) 0 

if 

z 

< 

0 

𝑔(𝑧) ′ = { 1 

if z > 0 

𝑢𝑛𝑑𝑒𝑓𝑖𝑛𝑒𝑑 if z = 0 

注：通常在𝑧= 0 的时候给定其导数 1,0；当然𝑧=0 的情况很少

4）Leaky linear unit (Leaky ReLU) 与 ReLU 类似

𝑔(𝑧) = 𝑚𝑎𝑥(0.01𝑧, 𝑧) 

0.01 

if 

z 

< 

0 

𝑔(𝑧) ′ = { 1 

if z > 0 

𝑢𝑛𝑑𝑒𝑓𝑖𝑛𝑒𝑑 if z = 0 

注：通常在𝑧 = 0 的时候给定其导数 1,0.01；当然𝑧 = 0 的情况很少

97 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

3.9 神 经 网 络 的 梯 度 下 降 （ Gradient descent for neural networks）

在这个视频中，我会给你实现反向传播或者说梯度下降算法的方程组，在下一个视频我 们会介绍为什么这几个特定的方程是针对你的神经网络实现梯度下降的正确方程。你的单隐层神经网络会有𝑊 [1] ，𝑏 [1] ，𝑊 [2] ，𝑏 [2] 这些参数，还有个𝑛 𝑥 表示输入特征的个 数，𝑛 [1] 表示隐藏单元个数，𝑛 [2] 表示输出单元个数。在我们的例子中，我们只介绍过的这种情况，那么参数:

矩阵𝑊 [1] 的维度就是 (𝑛 [1] , 𝑛 [0] )，𝑏 [1] 就是𝑛 [1] 维向量，可以写成 (𝑛 [1] , 1)，就是一个的列 向量。矩阵𝑊 [2] 的维度就是 (𝑛 [2] , 𝑛 [1] )，𝑏 [2] 的维度就是 (𝑛 [2] , 1) 维度。你还有一个神经网络的成本函数，假设你在做二分类任务，那么你的成本函数等于： Cost function:

1 公式： 𝐽(𝑊 [1] ∑ 𝑖=1 𝑚 ,𝑏 [1] , 𝑊 [2] , 𝑏 [2] ) = 𝐿 (𝑦 , 𝑦) 𝑚

loss function 和之前做 logistic 回归完全一样。

训练参数需要做梯度下降，在训练神经网络的时候，随机初始化参数很重要，而不是初 始化成全零。当你参数初始化成某些值后，每次梯度下降都会循环计算以下预测值：

𝑦 (𝑖) , (𝑖 = 1,2,… , 𝑚) 

𝑑𝐽 𝑑𝐽 公式 3.28： 𝑑𝑊 [1] = 𝑑𝑊 [1] , 𝑑𝑏 [1] = 𝑑𝑏 [1]

𝑑𝐽 𝑑𝐽 公式 3.29： 𝑑𝑊 [2] = [2] , 𝑑𝑏 [2] = [2] 𝑑𝑊 𝑑𝑏

其中

公式 3.30： 𝑊 [1] ⟹ 𝑊 [1] − 𝑎𝑑𝑊 [1] ,𝑏 [1] ⟹ 𝑏 [1] − 𝑎𝑑𝑏[1]

公式 3.31： 𝑊 [2] ⟹ 𝑊 [2] − 𝑎𝑑𝑊 [2] ,𝑏 [2] ⟹ 𝑏 [2] − 𝑎𝑑𝑏[2]

正向传播方程如下（之前讲过）： forward propagation：

(1) 𝑧 [1] = 𝑊 [1] 𝑥 + 𝑏[1] 

(2) 𝑎 [1] = 𝜎(𝑧 [1] ) 

(3) 𝑧 [2] = 𝑊 [2] 𝑎 [1] + 𝑏[2] 

(4) 𝑎 [2] = 𝑔 [2] (𝑧 [𝑧] ) = 𝜎(𝑧 [2] ) 

98 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

反向传播方程如下: back propagation：

公式 3.32： 𝑑𝑧 [2] = 𝐴 [2] − 𝑌, 𝑌 = [𝑦 [1] 𝑦 [2] ⋯ 𝑦 [𝑚] ]

1 公式 3.33： 𝑑𝑊 [2] = 𝑑𝑧 [2] 𝐴[1]𝑇 𝑚

1 公式 3.34： db [2] = np.sum (dz [2] , axis = 1, keepdims = True) m

公式 3.35：

1 公式 3.36： 𝑑𝑊 [1] = 𝑚 𝑑𝑧 [1] 𝑥𝑇

1 公式 3.37： 𝑑𝑏 [1] = 𝑛𝑝. 𝑠𝑢𝑚(𝑑𝑧 [1] , 𝑎𝑥𝑖𝑠 = 1, 𝑘𝑒𝑒𝑝𝑑𝑖𝑚𝑠 = 𝑇𝑟𝑢𝑒) ⏟ 𝑚

(𝑛 

[1] 

,1) 

上述是反向传播的步骤，注：这些都是针对所有样本进行过向量化，𝑌是 1 × 𝑚的矩阵； 这里 np.sum 是 python 的 numpy 命令，axis=1 表示水平相加求和，keepdims 是防止 python 输出那些古怪的秩数 (𝑛,)，加上这个确保阵矩阵𝑑𝑏 [2] 这个向量输出的维度为 (𝑛, 1) 这 样标准的形式。

目前为止，我们计算的都和 Logistic 回归十分相似，但当你开始计算反向传播时，你需 要计算，是隐藏层函数的导数，输出在使用 sigmoid 函数进行二元分类。这里是进行逐个元 素乘积，因为𝑊 [2]𝑇 𝑑𝑧 [2] 和 (𝑧 [1] ) 这两个都为 (𝑛 [1] , 𝑚) 矩阵；

还有一种防止 python 输出奇怪的秩数，需要显式地调用 reshape 把 np.sum 输出结 果写成矩阵形式。

以上就是正向传播的 4 个方程和反向传播的 6 个方程，这里我是直接给出的，在下个视 频中，我会讲如何导出反向传播的这 6 个式子的。如果你要实现这些算法，你必须正确执行 正向和反向传播运算，你必须能计算所有需要的导数，用梯度下降来学习神经网络的参数； 你也可以许多成功的深度学习从业者一样直接实现这个算法，不去了解其中的知识。

99 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

3.10（选修）直观理解反向传播（Backpropagation intuition）

这个视频主要是推导反向传播。下图是逻辑回归的推导： 回想一下逻辑回归的公式 (参考公式 3.2、公式 3.5、公式 3.6、公式 3.15) 公式 3.38：

𝑥 

} ⟹ 𝑧 = 𝑤 𝑇 𝑥 + 𝑏 ⟹ 𝛼 = 𝜎(𝑧) ⟹ 𝐿(𝑎, 𝑦) 𝑏 

𝑤 

所以回想当时我们讨论逻辑回归的时候，我们有这个正向传播步骤，其中我们计算𝑧，然后𝑎，然后损失函数𝐿。公式 3.39：

𝑥 

𝑤 ⟸ 𝑧 = 𝑤 𝑇 𝑥 + 𝑏 } 𝑏 ⏟ 𝑑𝐿 𝑑𝐿 𝑑𝑎 𝑑 ⏟ 𝑑𝑧=𝑑𝑎⋅𝑔 ′ (𝑧),𝑔(𝑧)=𝜎(𝑧), = ⋅ , 𝑔(𝑧)=𝑔 ′ (𝑧) 𝑑𝑧 𝑑𝑎 𝑑𝑧 𝑑𝑧 𝑑𝑤=𝑑𝑧⋅𝑥,𝑑𝑏=𝑑𝑧 

⟸ 

𝛼 

= 

𝜎(𝑧) 

⟸ 

𝐿(𝑎, 

𝑦) 

⏟ 

𝑑 𝑦 1−𝑦 𝑑𝑎= 𝐿(𝑎,𝑦)=(−𝑦log𝛼−(1−𝑦)log(1−𝑎)) ′ =− + 𝑑𝑎 𝑎 1−𝑎 

神经网络的计算中，与逻辑回归十分类似，但中间会有多层的计算。下图是一个双层神 经网络，有一个输入层，一个隐藏层和一个输出层。前向传播： 计算𝑧 [1] ，𝑎 [1] ，再计算𝑧[2] ，𝑎 [2] ，最后得到 loss function。反向传播： 向后推算出𝑑𝑎 [2] ，然后推算出𝑑𝑧 [2] ，接着推算出𝑑𝑎 [1] ，然后推算出𝑑𝑧 [1] 。我们不需要 对𝑥求导，因为𝑥是固定的，我们也不是想优化𝑥。向后推算出𝑑𝑎 [2] ，然后推算出𝑑𝑧 [2] 的步骤 可以合为一步：

公式 3.40： 𝑑𝑧 [2] = 𝑎 [2] − 𝑦 ，𝑑𝑊 [2] = 𝑑𝑧 [2] 𝑎 [1] 𝑇 (注意：逻辑回归中；为什么𝑎 [1]𝑇 多

了个转置：𝑑𝑤中的𝑊(视频里是𝑊 [2] ) 是一个列向量，而𝑊 [2] 是个行向量，故需要加个转置); 𝑖

公式 3.41： 𝑑𝑏 [2] = 𝑑𝑧[2]

公式 3.42：𝑑𝑧 [1] = 𝑊 [2]𝑇 𝑑𝑧 [2] ∗ 𝑔[1] ′ (𝑧 [1] ) 注意：这里的矩阵：𝑊 [2] 的维度是：(𝑛 [2] ,𝑛 [1] )。𝑧 [2] ，𝑑𝑧 [2] 的维度都是：(𝑛 [2] , 1)，如果是二分类，那维度就是 (1,1)。

100 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

𝑧 [1] ，𝑑𝑧 [1] 的维度都是：(𝑛 [1] ,1)。证明过程： 见公式 3.42，

其中𝑊 [2]𝑇 𝑑𝑧 [2] 维度为：(𝑛 [1] ,𝑛 [2] )、(𝑛 ,1) 相乘得到 (𝑛 [1] ,1)，和𝑧 [1] 维度相同，𝑔[1] ′ (𝑧 [1] )

的维度为 (𝑛 [1] ,1)，这就变成了两个都是 (𝑛 [1] , 1) 向量逐元素乘积。实现后向传播有个技巧，就是要保证矩阵的维度相互匹配。最后得到𝑑𝑊 [1] 和𝑑𝑏 [1] ，公 式 3.43： 𝑑𝑊 [1] = 𝑑𝑧 [1] 𝑥 𝑇 , 𝑑𝑏 [1] = 𝑑𝑧[1]

可以看出𝑑𝑊 [1] 和𝑑𝑊 [2] 非常相似，其中𝑥扮演了𝑎 [0] 的角色，𝑥 𝑇 等同于𝑎 [0]𝑇 。

由： 𝑍 [1] = 𝑊 [1] 𝑥 + 𝑏 [1] , 𝑎 [1] = 𝑔 [1] (𝑍 [1] ) 得到： 𝑍 [1] = 𝑊 [1] 𝑥 + 𝑏 [1] , 𝐴 [1] = 𝑔 [1] (𝑍 [1] )

⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ 注意：大写的𝑍 [1] 表示𝑧 [1](1) 𝑧 [1](2) ,𝑧 [1](3) .. . 𝑧 [1](𝑚) 的列向量堆叠成的矩阵，以下类同。下图写了主要的推导过程：

𝑍 [1] = [ 𝑧 [1](1) 𝑧 [1](2) ⋮ 𝑧 [1](𝑚) ] 

, 

1 公式 3.44： 𝑑𝑍 [2] = 𝐴 [2] − 𝑌 ，𝑑𝑊 [2] = 𝑚 𝑑𝑍 [2] 𝐴[1] 𝑇

1 公式 3.45： 𝐿 = ∑ 𝑖 𝑛 𝐿(𝑦 , 𝑦) 𝑚

1 公式 3.46： 𝑑𝑏 [2] = 𝑚 𝑛𝑝. 𝑠𝑢𝑚(𝑑𝑍 [2] , 𝑎𝑥𝑖𝑠 = 1, 𝑘𝑒𝑒𝑝𝑑𝑖𝑚𝑠 = 𝑇𝑟𝑢𝑒)

公式 3.47： 𝑑𝑍 [1] = 𝑊 [2]𝑇 𝑑𝑍 [2] ∗ 𝑔[1] ′ (𝑍 [1] ) ⏟ ⏟ ⏟

(𝑛 

[1] 

,𝑚) 

(𝑛 

[1] 

,𝑚) 

(𝑛 

[1] 

,𝑚) 

1 公式 3.48： 𝑑𝑊 [1] = 𝑑𝑍 [1] 𝑥𝑇 𝑚

1 公式 3.49： 𝑑𝑏 [1] = 𝑛𝑝. 𝑠𝑢𝑚(𝑑𝑍 [1] , 𝑎𝑥𝑖𝑠 = 1, 𝑘𝑒𝑒𝑝𝑑𝑖𝑚𝑠 = 𝑇𝑟𝑢𝑒) 𝑚

吴恩达老师认为反向传播的推导是机器学习领域最难的数学推导之一，矩阵的导数要用 链式法则来求，如果这章内容掌握不了也没大的关系，只要有这种直觉就可以了。还有一点，就是初始化你的神经网络的权重，不要都是 0，而是随机初始化，下一章将详细介绍原因。

101 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

3.11 随机初始化（Random+Initialization）

当你训练神经网络时，权重随机初始化是很重要的。对于逻辑回归，把权重初始化为 0 当然也是可以的。但是对于一个神经网络，如果你把权重或者参数都初始化为 0，那么梯度 下降将不会起作用。

让我们看看这是为什么。有两个输入特征，𝑛 [0] = 2，2 个隐藏层单元𝑛 [1] 就等于 2。因 此与一个隐藏层相关的矩阵，或者说𝑊 [1] 是 2*2 的矩阵，假设把它初始化为 0 的 2*2 矩阵，𝑏 [1] 也等于 [0 0] 𝑇 ，把偏置项𝑏初始化为 0 是合理的，但是把𝑤初始化为 0 就有问题了。那 这个问题如果按照这样初始化的话，你总是会发现𝑎 1 [1] 和 𝑎 2 [1] 相等，这个激活单元和这个激 活单元就会一样。因为两个隐含单元计算同样的函数，当你做反向传播计算时，这会导致 dz 1 [1] 和 dz 2 [1] 也会一样，对称这些隐含单元会初始化得一样，这样输出的权值也会一模一样，

由此𝑊 [2] 等于 [0 0]；

图 3.11.1 但是如果你这样初始化这个神经网络，那么这两个隐含单元就会完全一样，因此他们完全对称，也就意味着计算同样的函数，并且肯定的是最终经过每次训练的迭代，这两个隐含单元仍然是同一个函数，令人困惑。𝑑𝑊会是一个这样的矩阵，每一行有同样的 值因此我们做权重更新把权重𝑊 [1] ⟹ 𝑊 [1] − 𝑎𝑑𝑊每次迭代后的𝑊 [1] ，第一行等于第二行。

由此可以推导，如果你把权重都初始化为 0，那么由于隐含单元开始计算同一个函数，所有的隐含单元就会对输出单元有同样的影响。一次迭代后同样的表达式结果仍然是相同 的，即隐含单元仍是对称的。通过推导，两次、三次、无论多少次迭代，不管你训练网络多 长时间，隐含单元仍然计算的是同样的函数。因此这种情况下超过 1 个隐含单元也没什么意

102 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第三周：浅层神经网络 (Shallow neural networks)

义，因为他们计算同样的东西。当然更大的网络，比如你有 3 个特征，还有相当多的隐含单 元。如果你要初始化成 0，由于所有的隐含单元都是对称的，无论你运行梯度下降多久，他 们一直计算同样的函数。这没有任何帮助，因为你想要两个不同的隐含单元计算不同的函数，这 个 问 题 的 解 决 方 法 就 是 随 机 初 始 化 参 数 。你 应 该 这 么 做 ： 把 𝑊 [1] 设 为 np.random.randn (2,2)(生成高斯分布)，通常再乘上一个小的数，比如 0.01，这样把它初 始化为很小的随机数。然后𝑏没有这个对称的问题（叫做 symmetry breaking problem），所 以可以把 𝑏 初始化为 0，因为只要随机初始化𝑊你就有不同的隐含单元计算不同的东西，因此不会有 symmetry breaking 问题了。相似的，对于𝑊 [2] 你可以随机初始化，𝑏 [2] 可以初始 化为 0。

𝑊 [1] = 𝑛𝑝. 𝑟𝑎𝑛𝑑𝑜𝑚. 𝑟𝑎𝑛𝑑𝑛(2,2) ∗ 0.01 , 

𝑏 [1] = 𝑛𝑝. 𝑧𝑒𝑟𝑜𝑠((2,1)) 

𝑊 [2] = 𝑛𝑝. 𝑟𝑎𝑛𝑑𝑜𝑚. 𝑟𝑎𝑛𝑑𝑛(2,2) ∗ 0.01 , 𝑏 [2] = 0 

你也许会疑惑，这个常数从哪里来，为什么是 0.01，而不是 100 或者 1000。我们通常 倾向于初始化为很小的随机数。因为如果你用 tanh 或者 sigmoid 激活函数，或者说只在输 出层有一个 Sigmoid，如果（数值）波动太大，当你计算激活值时𝑧 [1] = 𝑊 [1] 𝑥 + 𝑏 [1] , 𝑎 [1] = 𝜎(𝑧 [1] ) = 𝑔 [1] (𝑧 [1] ) 如果𝑊很大，𝑧就会很大。𝑧的一些值𝑎就会很大或者很小，因此这种情况 下你很可能停在 tanh/sigmoid 函数的平坦的地方 (见图 3.8.2)，这些地方梯度很小也就意味着 梯度下降会很慢，因此学习也就很慢。

回顾一下：如果𝑤很大，那么你很可能最终停在（甚至在训练刚刚开始的时候）𝑧很大的 值，这会造成 tanh/Sigmoid 激活函数饱和在龟速的学习上，如果你没有 sigmoid/tanh 激活 函数在你整个的神经网络里，就不成问题。但如果你做二分类并且你的输出单元是 Sigmoid 函数，那么你不会想让初始参数太大，因此这就是为什么乘上 0.01 或者其他一些小数是合 理的尝试。对于𝑤 [2] 一样，就是 np.random.randn ((1,2))，我猜会是乘以 0.01。

事实上有时有比 0.01 更好的常数，当你训练一个只有一层隐藏层的网络时（这是相对 浅的神经网络，没有太多的隐藏层），设为 0.01 可能也可以。但当你训练一个非常非常深 的神经网络，你可能会选择一个不同于的常数而不是 0.01。下一节课我们会讨论怎么并且何 时去选择一个不同于 0.01 的常数，但是无论如何它通常都会是个相对小的数。

好了，这就是这周的视频。你现在已经知道如何建立一个一层的神经网络了，初始化参 数，用前向传播预测，还有计算导数，结合反向传播用在梯度下降中。

103 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

第四周：深层神经网络 (Deep Neural Networks)

4.1 深层神经网络（Deep L-layer neural network）

目前为止我们学习了只有一个单独隐藏层的神经网络的正向传播和反向传播，还有逻辑 回归，并且你还学到了向量化，这在随机初始化权重时是很重要。本周所要做的是把这些理念集合起来，就可以执行你自己的深度神经网络。复习下前三周的课的内容： 1. 逻辑回归，结构如下图左边。一个隐藏层的神经网络，结构下图右边：

注意，神经网络的层数是这么定义的：从左到右，由 0 开始定义，比如上边右图，𝑥 1 、 𝑥 2 、𝑥 3 , 这层是第 0 层，这层左边的隐藏层是第 1 层，由此类推。如下图左边是两个隐藏层 的神经网络，右边是 5 个隐藏层的神经网络。

严格上来说逻辑回归也是一个一层的神经网络，而上边右图一个深得多的模型，浅与深 仅仅是指一种程度。记住以下要点：

有一个隐藏层的神经网络，就是一个两层神经网络。记住当我们算神经网络的层数时，我们不算输入层，我们只算隐藏层和输出层。

但是在过去的几年中，DLI（深度学习学院 deep learning institute）已经意识到有一些函 数，只有非常深的神经网络能学会，而更浅的模型则办不到。尽管对于任何给定的问题很难 去提前预测到底需要多深的神经网络，所以先去尝试逻辑回归，尝试一层然后两层隐含层，然后把隐含层的数量看做是另一个可以自由选择大小的超参数，然后再保留交叉验证数据上 评估，或者用你的开发集来评估。

我们再看下深度学习的符号定义：

104 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

上图是一个四层的神经网络，有三个隐藏层。我们可以看到，第一层（即左边数过去第 二层，因为输入层是第 0 层）有 5 个神经元数目，第二层 5 个，第三层 3 个。我们用 L 表示层数，上图：𝐿 = 4，输入层的索引为「0」，第一个隐藏层𝑛 [1] = 5, 表示有 5 个隐藏神经元，同理𝑛 [2] = 5，𝑛 [3] = 3，𝑛 [4] =𝑛 [𝐿] = 1（输出单元为 1）。而输入层，𝑛 [0] = 𝑛 𝑥 = 3。在不同层所拥有的神经元的数目，对于每层 l 都用𝑎 [𝑙] 来记作 l 层激活后结果，我们会在 后面看到在正向传播时，最终能你会计算出𝑎 [𝑙] 。通过用激活函数 𝑔 计算𝑧 [𝑙] ，激活函数也被索引为层数𝑙，然后我们用𝑤 [𝑙] 来记作在 l 层 计算𝑧 [𝑙] 值的权重。类似的，𝑧 [𝑙] 里的方程𝑏 [𝑙] 也一样。最后总结下符号约定： 输入的特征记作𝑥，但是𝑥同样也是 0 层的激活函数，所以𝑥 = 𝑎 [0] 。最后一层的激活函数，所以𝑎 [𝐿] 是等于这个神经网络所预测的输出结果。但是如果你忘记了某些符号的意义，请看笔记最后的附件：《深度学习符号指南》。

105 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

4.2 前向传播和反向传播（Forward and backward propagation）

之前我们学习了构成深度神经网络的基本模块，比如每一层都有前向传播步骤以及一个 相反的反向传播步骤，这次视频我们讲讲如何实现这些步骤。

先讲前向传播，输入𝑎 [𝑙−1] ，输出是𝑎 [𝑙] ，缓存为𝑧 [𝑙] ；从实现的角度来说我们可以缓存下 𝑤 [𝑙] 和𝑏 [𝑙] ，这样更容易在不同的环节中调用函数。

所以前向传播的步骤可以写成： 𝑧 [𝑙] = 𝑊 [𝑙] ⋅ 𝑎 [𝑙−1] + 𝑏[𝑙]

𝑎 [𝑙] = 𝑔 [𝑙] (𝑧 [𝑙] ) 

向量化实现过程可以写成： 𝑧 [𝑙] = 𝑊 [𝑙] ⋅ 𝐴 [𝑙−1] + 𝑏[𝑙]

𝐴 [𝑙] = 𝑔 [𝑙] (𝑍 [𝑙] ) 

前向传播需要喂入𝐴 [0] 也就是𝑋，来初始化；初始化的是第一层的输入值。𝑎 [0] 对应于一 个训练样本的输入特征，而𝐴 [0] 对应于一整个训练样本的输入特征，所以这就是这条链的第 一个前向函数的输入，重复这个步骤就可以从左到右计算前向传播。下面讲反向传播的步骤： 输入为𝑑𝑎 [𝑙] ，输出为𝑑𝑎 [𝑙−1] ，𝑑𝑤 [𝑙] , 𝑑𝑏[𝑙]

106 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

所以反向传播的步骤可以写成：

（1）𝑑𝑧 [𝑙] = 𝑑𝑎 [𝑙] ∗ 𝑔 [𝑙] ′(𝑧 [𝑙] ) 

（2）𝑑𝑤 [𝑙] = 𝑑𝑧 [𝑙] ⋅ 𝑎[𝑙−1] 

（3）𝑑𝑏 [𝑙] = 𝑑𝑧[𝑙] 

（4）𝑑𝑎 [𝑙−1] = 𝑤 [𝑙]𝑇 ⋅ 𝑑𝑧[𝑙] 

（5）𝑑𝑧 [𝑙] = 𝑤 [𝑙+1]𝑇 𝑑𝑧 [𝑙+1] ⋅ 𝑔 [𝑙] ′(𝑧 [𝑙] ) 

式子（5）由式子（4）带入式子（1）得到，前四个式子就可实现反向函数。向量化实现过程可以写成：

（6）𝑑𝑍 [𝑙] = 𝑑𝐴 [𝑙] ∗ 𝑔 [𝑙] ′(𝑍 [𝑙] ) 

1 （7）𝑑𝑊 [𝑙] = 𝑚 𝑑𝑍 [𝑙] ⋅ 𝐴[𝑙−1]𝑇 

1 （8）𝑑𝑏 [𝑙] = 𝑚 𝑛𝑝. 𝑠𝑢𝑚(𝑑𝑧 [𝑙] , 𝑎𝑥𝑖𝑠 = 1, 𝑘𝑒𝑒𝑝𝑑𝑖𝑚𝑠 = 𝑇𝑟𝑢𝑒) 

（9）𝑑𝐴 [𝑙−1] = 𝑊 [𝑙]𝑇 .𝑑𝑍[𝑙] 

总结一下：

107 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

第一层你可能有一个 ReLU 激活函数，第二层为另一个 ReLU 激活函数，第三层可能是 sigmoid 函数（如果你做二分类的话），输出值为，用来计算损失；这样你就可以向后迭代 进行反向传播求导来求𝑑𝑤[3] ，𝑑𝑏 [3] ，𝑑𝑤 [2] ，𝑑𝑏 [2] ，𝑑𝑤 [1] ，𝑑𝑏 [1] 。在计算的时候，缓存 会把𝑧 [1] 𝑧 [2] 𝑧 [3] 传递过来，然后回传𝑑𝑎 [2] ，𝑑𝑎 [1] ，可以用来计算𝑑𝑎 [0] ，但我们不会使用它，这里讲述了一个三层网络的前向和反向传播，还有一个细节没讲就是前向递归 —— 用输入数 据来初始化，那么反向递归（使用 Logistic 回归做二分类）—— 对𝐴 [𝑙] 求导。

忠告：补补微积分和线性代数，多推导，多实践。

108 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

4.3 深层网络中的前向传播（Forward propagation in a Deep Network）

跟往常一样，我们先来看对其中一个训练样本 x 如何应用前向传播，之后讨论向量化的 版本。

第一层需要计算𝑧 [1] = 𝑤 [1] 𝑥 + 𝑏 [1] ，𝑎 [1] = 𝑔 [1] (𝑧 [1] )（𝑥可以看做𝑎 [0] ）

第二层需要计算𝑧 [2] = 𝑤 [2] 𝑎 [1] + 𝑏 [2] ，𝑎 [2] = 𝑔 [2] (𝑧 [2] )

以此类推，

第四层为𝑧 [4] = 𝑤 [4] 𝑎 [3] + 𝑏 [4] ，𝑎 [4] = 𝑔 [4] (𝑧 [4] )

前向传播可以归纳为多次迭代𝑧 [𝑙] = 𝑤 [𝑙] 𝑎 [𝑙−1] + 𝑏 [𝑙] ，𝑎 [𝑙] = 𝑔 [𝑙] (𝑧 [𝑙] )。

向量化实现过程可以写成：

𝑍 [𝑙] = 𝑊 [𝑙] 𝑎 [𝑙−1] + 𝑏 [𝑙] ，𝐴 [𝑙] = 𝑔 [𝑙] (𝑍 [𝑙] ) (𝐴 [0] = 𝑋) 

这里只能用一个显式 for 循环，𝑙从 1 到𝐿，然后一层接着一层去计算。下一节讲的是避 免代码产生 BUG，我所做的其中一件非常重要的工作。

109 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

4.4 核对矩阵的维数（Getting your matrix dimensions right）

当实现深度神经网络的时候，其中一个我常用的检查代码是否有错的方法就是拿出一张 纸过一遍算法中矩阵的维数。𝑤的维度是（下一层的维数，前一层的维数），即𝑤 [𝑙] : (𝑛 [𝑙] ,𝑛 [𝑙−1] )； 𝑏的维度是（下一层的维数，1），即:

𝑏 [𝑙] : (𝑛 [𝑙] , 1)； 

𝑧 [𝑙] ,𝑎 [𝑙] : (𝑛 [𝑙] , 1); 

𝑑𝑤 [𝑙] 和𝑤 [𝑙] 维度相同，𝑑𝑏 [𝑙] 和𝑏 [𝑙] 维度相同，且𝑤和𝑏向量化维度不变，但𝑧,𝑎以及𝑥的维 度会向量化后发生变化。

向量化后： 𝑍 [𝑙] 可以看成由每一个单独的𝑍 [𝑙] 叠加而得到，𝑍 [𝑙] = (𝑧 [𝑙][1] ，𝑧 [𝑙][2] ，𝑧 [𝑙][3] ，… ，𝑧 [𝑙][𝑚] )，

𝑚为训练集大小，所以𝑍 [𝑙] 的维度不再是 (𝑛 [𝑙] , 1)，而是 (𝑛 [𝑙] , 𝑚)。

𝐴 [𝑙] ：(𝑛 [𝑙] , 𝑚)，𝐴 [0] = 𝑋 = (𝑛 [𝑙] , 𝑚) 

110 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

在你做深度神经网络的反向传播时，一定要确认所有的矩阵维数是前后一致的，可以大 大提高代码通过率。下一节我们讲为什么深层的网络在很多问题上比浅层的好。

111 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

4.5 为什么使用深层表示？（Why deep representations?）

我们都知道深度神经网络能解决好多问题，其实并不需要很大的神经网络，但是得有深 度，得有比较多的隐藏层，这是为什么呢？我们一起来看几个例子来帮助理解，为什么深度 神经网络会很好用。

首先，深度网络在计算什么？

首先，深度网络究竟在计算什么？如果你在建一个人脸识别或是人脸检测系统，深度神 经网络所做的事就是，当你输入一张脸部的照片，然后你可以把深度神经网络的第一层，当 成一个特征探测器或者边缘探测器。在这个例子里，我会建一个大概有 20 个隐藏单元的深 度神经网络，是怎么针对这张图计算的。隐藏单元就是这些图里这些小方块（第一张大图），举个例子，这个小方块（第一行第一列）就是一个隐藏单元，它会去找这张照片里「|」边缘的 方向。那么这个隐藏单元（第四行第四列），可能是在找（「—」）水平向的边缘在哪里。之 后的课程里，我们会讲专门做这种识别的卷积神经网络，到时候会细讲，为什么小单元是这 么表示的。你可以先把神经网络的第一层当作看图，然后去找这张照片的各个边缘。我们可 以把照片里组成边缘的像素们放在一起看，然后它可以把被探测到的边缘组合成面部的不同 部分（第二张大图）。比如说，可能有一个神经元会去找眼睛的部分，另外还有别的在找鼻 子的部分，然后把这许多的边缘结合在一起，就可以开始检测人脸的不同部分。最后再把这 些部分放在一起，比如鼻子眼睛下巴，就可以识别或是探测不同的人脸（第三张大图）。

你可以直觉上把这种神经网络的前几层当作探测简单的函数，比如边缘，之后把它们跟 后几层结合在一起，那么总体上就能学习更多复杂的函数。这些图的意义，我们在学习卷积

112 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

神经网络的时候再深入了解。还有一个技术性的细节需要理解的是，边缘探测器其实相对来 说都是针对照片中非常小块的面积。就像这块（第一行第一列），都是很小的区域。面部探 测器就会针对于大一些的区域，但是主要的概念是，一般你会从比较小的细节入手，比如边 缘，然后再一步步到更大更复杂的区域，比如一只眼睛或是一个鼻子，再把眼睛鼻子装一块 组成更复杂的部分。

这种从简单到复杂的金字塔状表示方法或者组成方法，也可以应用在图像或者人脸识别 以外的其他数据上。比如当你想要建一个语音识别系统的时候，需要解决的就是如何可视化 语音，比如你输入一个音频片段，那么神经网络的第一层可能就会去先开始试着探测比较低 层次的音频波形的一些特征，比如音调是变高了还是低了，分辨白噪音，咝咝咝的声音，或 者音调，可以选择这些相对程度比较低的波形特征，然后把这些波形组合在一起就能去探测 声音的基本单元。在语言学中有个概念叫做音位，比如说单词 ca，c 的发音，「嗑」就是一个 音位，a 的发音「啊」是个音位，t 的发音「特」也是个音位，有了基本的声音单元以后，组合起 来，你就能识别音频当中的单词，单词再组合起来就能识别词组，再到完整的句子。

所以深度神经网络的这许多隐藏层中，较早的前几层能学习一些低层次的简单特征，等 到后几层，就能把简单的特征结合起来，去探测更加复杂的东西。比如你录在音频里的单词、 词组或是句子，然后就能运行语音识别了。同时我们所计算的之前的几层，也就是相对简单 的输入函数，比如图像单元的边缘什么的。到网络中的深层时，你实际上就能做很多复杂的 事，比如探测面部或是探测单词、短语或是句子。

有些人喜欢把深度神经网络和人类大脑做类比，这些神经科学家觉得人的大脑也是先探 测简单的东西，比如你眼睛看得到的边缘，然后组合起来才能探测复杂的物体，比如脸。这 种深度学习和人类大脑的比较，有时候比较危险。但是不可否认的是，我们对大脑运作机制 的认识很有价值，有可能大脑就是先从简单的东西，比如边缘着手，再组合成一个完整的复

113 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

杂物体，这类简单到复杂的过程，同样也是其他一些深度学习的灵感来源，之后的视频我们 也会继续聊聊人类或是生物学理解的大脑。

Small：隐藏单元的数量相对较少

Deep：隐藏层数目比较多

深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的 计算结果则需要指数级增长的单元数量才能达到。

另外一个，关于神经网络为何有效的理论，来源于电路理论，它和你能够用电路元件计 算哪些函数有着分不开的联系。根据不同的基本逻辑门，譬如与门、或门、非门。在非正式 的情况下，这些函数都可以用相对较小，但很深的神经网络来计算，小在这里的意思是隐藏 单元的数量相对比较小，但是如果你用浅一些的神经网络计算同样的函数，也就是说在我们 不能用很多隐藏层时，你会需要成指数增长的单元数量才能达到同样的计算结果。

我再来举个例子，用没那么正式的语言介绍这个概念。假设你想要对输入特征计算异或 或是奇偶性，你可以算𝑥 1 𝑋𝑂𝑅𝑥 2 𝑋𝑂𝑅𝑥 3 𝑋𝑂𝑅 … … 𝑥 𝑛 ，假设你有𝑛或者𝑛 𝑥 个特征，如果你画一 个异或的树图，先要计算𝑥 1 ，𝑥 2 的异或，然后是𝑥 3 和𝑥 4 。技术上来说如果你只用或门，还有 非门的话，你可能会需要几层才能计算异或函数，但是用相对小的电路，你应该就可以计算 异或了。然后你可以继续建这样的一个异或树图（上图左），那么你最后会得到这样的电路 来输出结果𝑦，𝑦 = 𝑦，也就是输入特征的异或，或是奇偶性，要计算异或关系。这种树图对 应网络的深度应该是𝑂(𝑙𝑜𝑔(𝑛))，那么节点的数量和电路部件，或是门的数量并不会很大，你也不需要太多门去计算异或。

但是如果你不能使用多隐层的神经网络的话，在这个例子中隐层数为𝑂(𝑙𝑜𝑔(𝑛))，比如 你被迫只能用单隐藏层来计算的话，这里全部都指向从这些隐藏单元到后面这里，再输出𝑦，那么要计算奇偶性，或者异或关系函数就需要这一隐层（上图右方框部分）的单元数呈指数 增长才行，因为本质上来说你需要列举耗尽 2 𝑛 种可能的配置，或是 2 𝑛 种输入比特的配置。异

114 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

或运算的最终结果是 1 或 0，那么你最终就会需要一个隐藏层，其中单元数目随输入比特指 数上升。精确的说应该是 2𝑛−1 个隐藏单元数，也就是𝑂(2 𝑛)。

我希望这能让你有点概念，意识到有很多数学函数用深度网络计算比浅网络要容易得多，我个人倒是认为这种电路理论，对训练直觉思维没那么有用，但这个结果人们还是经常提到 的，用来解释为什么需要更深层的网络。

除了这些原因，说实话，我认为「深度学习」这个名字挺唬人的，这些概念以前都统称为 有很多隐藏层的神经网络，但是深度学习听起来多高大上，太深奥了，对么？这个词流传出 去以后，这是神经网络的重新包装或是多隐藏层神经网络的重新包装，激发了大众的想象力。抛开这些公关概念重新包装不谈，深度网络确实效果不错，有时候人们还是会按照字面意思 钻牛角尖，非要用很多隐层。但是当我开始解决一个新问题时，我通常会从 logistic 回归开 始，再试试一到两个隐层，把隐藏层数量当作参数、超参数一样去调试，这样去找比较合适 的深度。但是近几年以来，有一些人会趋向于使用非常非常深邃的神经网络，比如好几打的 层数，某些问题中只有这种网络才是最佳模型。

这就是我想讲的，为什么深度学习效果拔群的直觉解释，现在我们来看看除了正向传播 以外，反向传播该怎么具体实现。

115 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

4.6 搭建神经网络块（Building blocks of deep neural networks）

这周的前几个视频和之前几周的视频里，你已经看到过正向反向传播的基础组成部分了，它们也是深度神经网络的重要组成部分，现在我们来用它们建一个深度神经网络。

这是一个层数较少的神经网络，我们选择其中一层（方框部分），从这一层的计算着手。在第𝑙层你有参数𝑊 [𝑙] 和𝑏 [𝑙] ，正向传播里有输入的激活函数，输入是前一层𝑎 [𝑙−1] ，输出是𝑎 [𝑙] ，我们之前讲过𝑧 [𝑙] = 𝑊 [𝑙] 𝑎 [𝑙−1] + 𝑏 [𝑙] ,𝑎 [𝑙] = 𝑔 [𝑙] (𝑧 [𝑙] )，那么这就是你如何从输入𝑎 [𝑙−1] 走到输 出的𝑎 [𝑙] 。之后你就可以把𝑧 [𝑙] 的值缓存起来，我在这里也会把这包括在缓存中，因为缓存的 𝑧 [𝑖] 对以后的正向反向传播的步骤非常有用。

然后是反向步骤或者说反向传播步骤，同样也是第𝑙层的计算，你会需要实现一个函数 输入为𝑑𝑎 [𝑙] ，输出𝑑𝑎 [𝑙−1] 的函数。一个小细节需要注意，输入在这里其实是𝑑𝑎 [𝑙] 以及所缓存 的𝑧 [𝑙] 值，之前计算好的𝑧 [𝑙] 值，除了输出𝑑𝑎 [𝑙−1] 的值以外，也需要输出你需要的梯度𝑑𝑊 [𝑙] 和 𝑑𝑏 [𝑙] ，这是为了实现梯度下降学习。

这就是基本的正向步骤的结构，我把它成为称为正向函数，类似的在反向步骤中会称为 反向函数。总结起来就是，在 l 层，你会有正向函数，输入𝑎 [𝑙−1] 并且输出𝑎 [𝑙] ，为了计算结 果你需要用𝑊 [𝑙] 和𝑏 [𝑙] ，以及输出到缓存的𝑧 [𝑙] 。然后用作反向传播的反向函数，是另一个函 数，输入𝑑𝑎 [𝑙] ，输出𝑑𝑎 [𝑙−1] ，你就会得到对激活函数的导数，也就是希望的导数值𝑑𝑎 [𝑙] 。𝑎[𝑙−1] 是会变的，前一层算出的激活函数导数。在这个方块（第二个）里你需要𝑊 [𝑙] 和𝑏 [𝑙] ，最后你

116 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

要算的是𝑑𝑧 [𝑙] 。然后这个方块（第三个）中，这个反向函数可以计算输出𝑑𝑊 [𝑙] 和𝑑𝑏 [𝑙] 。我 会用红色箭头标注标注反向步骤，如果你们喜欢，我可以把这些箭头涂成红色。

然后如果实现了这两个函数（正向和反向），然后神经网络的计算过程会是这样的：

把输入特征𝑎 [0] ，放入第一层并计算第一层的激活函数，用𝑎 [1] 表示，你需要𝑊 [1] 和𝑏[1] 来计算，之后也缓存𝑧 [𝑙] 值。之后喂到第二层，第二层里，需要用到𝑊 [2] 和𝑏 [2] ，你会需要计 算第二层的激活函数𝑎 [2] 。后面几层以此类推，直到最后你算出了𝑎 [𝐿] ，第𝐿层的最终输出值 𝑦。在这些过程里我们缓存了所有的𝑧值，这就是正向传播的步骤。

对反向传播的步骤而言，我们需要算一系列的反向迭代，就是这样反向计算梯度，你需

117 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

要把𝑑𝑎 [𝐿] 的值放在这里，然后这个方块会给我们𝑑𝑎 [𝐿−1] 的值，以此类推，直到我们得到𝑑𝑎[2] 和𝑑𝑎 [1] ，你还可以计算多一个输出值，就是𝑑𝑎 [0] ，但这其实是你的输入特征的导数，并不重 要，起码对于训练监督学习的权重不算重要，你可以止步于此。反向传播步骤中也会输出 𝑑𝑊 [𝑙] 和𝑑𝑏 [𝑙] ，这会输出𝑑𝑊 [3] 和𝑑𝑏 [3] 等等。目前为止你算好了所有需要的导数，稍微填一下 这个流程图。

神经网络的一步训练包含了，从𝑎 [0] 开始，也就是 𝑥 然后经过一系列正向传播计算得到 𝑦，之后再用输出值计算这个（第二行最后方块），再实现反向传播。现在你就有所有的导 数项了，𝑊也会在每一层被更新为𝑊 = 𝑊 − 𝛼𝑑𝑊，𝑏也一样，𝑏 = 𝑏 − 𝛼𝑑𝑏，反向传播就都 计算完毕，我们有所有的导数值，那么这是神经网络一个梯度下降循环。

继续下去之前再补充一个细节，概念上会非常有帮助，那就是把反向函数计算出来的𝑧 值缓存下来。当你做编程练习的时候去实现它时，你会发现缓存可能很方便，可以迅速得到 𝑊 [𝑙] 和𝑏 [𝑙] 的值，非常方便的一个方法，在编程练习中你缓存了𝑧，还有𝑊和𝑏对吧？从实现角 度上看，我认为是一个很方便的方法，可以将参数复制到你在计算反向传播时所需要的地方。好，这就是实现过程的细节，做编程练习时会用到。

现在你们见过实现深度神经网络的基本元件，在每一层中有一个正向传播步骤，以及对 应的反向传播步骤，以及把信息从一步传递到另一步的缓存。下一个视频我们会讲解这些元 件具体实现过程，我们来看下一个视频吧。

118 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

4.7 参数 VS 超参数（Parameters vs Hyperparameters）

想要你的深度神经网络起很好的效果，你还需要规划好你的参数以及超参数。什么是超参数？ 比如算法中的 learning rate 𝑎（学习率）、iterations (梯度下降法循环的数量)、𝐿（隐藏 层数目）、𝑛[𝑙] （隐藏层单元数目）、choice of activation function（激活函数的选择）都需要

你来设置，这些数字实际上控制了最后的参数𝑊和𝑏的值，所以它们被称作超参数。实际上深度学习有很多不同的超参数，之后我们也会介绍一些其他的超参数，如 momentum、mini batch size、regularization parameters 等等。

如何寻找超参数的最优值？

走 Idea—Code—Experiment—Idea 这个循环，尝试各种不同的参数，实现模型并观察是 否成功，然后再迭代。

119 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

今天的深度学习应用领域，还是很经验性的过程，通常你有个想法，比如你可能大致知 道一个最好的学习率值，可能说𝑎 = 0.01 最好，我会想先试试看，然后你可以实际试一下，训练一下看看效果如何。然后基于尝试的结果你会发现，你觉得学习率设定再提高到 0.05 会 比较好。如果你不确定什么值是最好的，你大可以先试试一个学习率𝑎，再看看损失函数 J 的值有没有下降。然后你可以试一试大一些的值，然后发现损失函数的值增加并发散了。然 后可能试试其他数，看结果是否下降的很快或者收敛到在更高的位置。你可能尝试不同的𝑎 并观察损失函数𝐽这么变了，试试一组值，然后可能损失函数变成这样，这个𝑎值会加快学习 过程，并且收敛在更低的损失函数值上（箭头标识），我就用这个𝑎值了。

在前面几页中，还有很多不同的超参数。然而，当你开始开发新应用时，预先很难确切 知道，究竟超参数的最优值应该是什么。所以通常，你必须尝试很多不同的值，并走这个循 环，试试各种参数。试试看 5 个隐藏层，这个数目的隐藏单元，实现模型并观察是否成功，然后再迭代。这页的标题是，应用深度学习领域，一个很大程度基于经验的过程，凭经验的 过程通俗来说，就是试直到你找到合适的数值。

另一个近来深度学习的影响是它用于解决很多问题，从计算机视觉到语音识别，到自然 语言处理，到很多结构化的数据应用，比如网络广告或是网页搜索或产品推荐等等。我所看 到过的就有很多其中一个领域的研究员，这些领域中的一个，尝试了不同的设置，有时候这 种设置超参数的直觉可以推广，但有时又不会。所以我经常建议人们，特别是刚开始应用于 新问题的人们，去试一定范围的值看看结果如何。然后下一门课程，我们会用更系统的方法，用系统性的尝试各种超参数取值。然后其次，甚至是你已经用了很久的模型，可能你在做网 络广告应用，在你开发途中，很有可能学习率的最优数值或是其他超参数的最优值是会变的，所以即使你每天都在用当前最优的参数调试你的系统，你还是会发现，最优值过一年就会变 化，因为电脑的基础设施，CPU 或是 GPU 可能会变化很大。所以有一条经验规律可能每几 个月就会变。如果你所解决的问题需要很多年时间，只要经常试试不同的超参数，勤于检验 结果，看看有没有更好的超参数数值，相信你慢慢会得到设定超参数的直觉，知道你的问题 最好用什么数值。

这可能的确是深度学习比较让人不满的一部分，也就是你必须尝试很多次不同可能性。但参数设定这个领域，深度学习研究还在进步中，所以可能过段时间就会有更好的方法决定 超参数的值，也很有可能由于 CPU、GPU、网络和数据都在变化，这样的指南可能只会在一 段时间内起作用，只要你不断尝试，并且尝试保留交叉检验或类似的检验方法，然后挑一个 对你的问题效果比较好的数值。

120 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

近来受深度学习影响，很多领域发生了变化，从计算机视觉到语音识别到自然语言处理 到很多结构化的数据应用，比如网络广告、网页搜索、产品推荐等等；有些同一领域设置超 参数的直觉可以推广，但有时又不可以，特别是那些刚开始研究新问题的人们应该去尝试一 定范围内的结果如何，甚至那些用了很久的模型得学习率或是其他超参数的最优值也有可能 会改变。

在下个课程我们会用系统性的方法去尝试各种超参数的取值。有一条经验规律：经常试 试不同的超参数，勤于检查结果，看看有没有更好的超参数取值，你将会得到设定超参数的 直觉。

121 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

4.8 深度学习和大脑的关联性（What does this have to do with the brain?）

深度学习和大脑有什么关联性吗？ 关联不大。那么人们为什么会说深度学习和大脑相关呢？ 当你在实现一个神经网络的时候，那些公式是你在做的东西，你会做前向传播、反向传 播、梯度下降法，其实很难表述这些公式具体做了什么，深度学习像大脑这样的类比其实是 过度简化了我们的大脑具体在做什么，但因为这种形式很简洁，也能让普通人更愿意公开讨 论，也方便新闻报道并且吸引大众眼球，但这个类比是非常不准确的。

一个神经网络的逻辑单元可以看成是对一个生物神经元的过度简化，但迄今为止连神经 科学家都很难解释究竟一个神经元能做什么，它可能是极其复杂的；它的一些功能可能真的 类似 logistic 回归的运算，但单个神经元到底在做什么目前还没有人能够真正可以解释。

深度学习的确是个很好的工具来学习各种很灵活很复杂的函数，学习到从𝑥到𝑦的映射，在监督学习中学到输入到输出的映射。

但这个类比还是很粗略的，这是一个 logistic 回归单元的 sigmoid 激活函数，这里是一 个大脑中的神经元，图中这个生物神经元，也是你大脑中的一个细胞，它能接受来自其他神 经元的电信号，比如𝑥 1 , 𝑥 2 , 𝑥 3 ，或可能来自于其他神经元𝑎 1 , 𝑎 2 , 𝑎 3 。其中有一个简单的临界 计算值，如果这个神经元突然激发了，它会让电脉冲沿着这条长长的轴突，或者说一条导线

122 

第一门课 神经网络和深度学习 (Neural Networks and Deep Learning)- 第四周：深层神经网络 (Deep Neural Networks)

传到另一个神经元。所以这是一个过度简化的对比，把一个神经网络的逻辑单元和右边的生物神经元对比。至今为止其实连神经科学家们都很难解释，究竟一个神经元能做什么。一个小小的神经元其 实却是极其复杂的，以至于我们无法在神经科学的角度描述清楚，它的一些功能，可能真的 是类似 logistic 回归的运算，但单个神经元到底在做什么，目前还没有人能够真正解释，大 脑中的神经元是怎么学习的，至今这仍是一个谜之过程。到底大脑是用类似于后向传播或是 梯度下降的算法，或者人类大脑的学习过程用的是完全不同的原理。

所以虽然深度学习的确是个很好的工具，能学习到各种很灵活很复杂的函数来学到从 x 到 y 的映射。在监督学习中，学到输入到输出的映射，但这种和人类大脑的类比，在这个领 域的早期也许值得一提。但现在这种类比已经逐渐过时了，我自己也在尽量少用这样的说法。

这就是神经网络和大脑的关系，我相信在计算机视觉，或其他的学科都曾受人类大脑启 发，还有其他深度学习的领域也曾受人类大脑启发。但是个人来讲我用这个人类大脑类比的 次数逐渐减少了。

123 

第二门课 改善深层神经网络：超参数调试、正则化以及优化 (Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)- 第一周：深度学习的实践层面 (Practical aspects of Deep Learning)

第二门课 改善深层神经网络：超参数调试、 正 则 化 以 及 优 化 (Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)

第 一 周 ： 深 度 学 习 的 实 践 层 面 (Practical aspects of Deep Learning)

