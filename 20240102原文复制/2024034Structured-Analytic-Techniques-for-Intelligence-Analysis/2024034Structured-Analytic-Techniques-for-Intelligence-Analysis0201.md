## 0201. The Role of Structured Techniques

2.1 Two Types of Thinking

In the last thirty years, important gains have been made in psychological research on human judgment. Dual process theory, positing two systems of decision making called System 1 and System 2, has emerged as the predominant approach. 1 The basic distinction between System 1 and System 2 is intuitive versus analytical thinking.

System 1 Thinking is intuitive, fast, efficient, and often unconscious. It draws naturally on available knowledge, experience, and often a long-established mental model of how people or things work in a specific environment. System 1 Thinking requires little effort; it allows people to solve problems and make judgments quickly and efficiently. Although it is often accurate, intuitive thinking is a common source of cognitive biases and other intuitive mistakes that lead to faulty analysis. Three types of cognitive limitations—cognitive bias, misapplied heuristics, and intuitive traps—are discussed later in this chapter. System 2 Thinking is analytic. It is slow, methodical, and conscious, the result of deliberate reasoning. It includes all types of analysis, such as critical thinking and Structured Analytic Techniques, as well as the whole range of empirical and quantitative methods.

The description of each Structured Analytic Technique in this book includes a discussion of which cognitive biases, misapplied heuristics, and intuitive traps are most effectively avoided, overcome, or at least mitigated by using that technique. The introduction to each family of techniques also identifies how the techniques discussed in that chapter help counter one or more types of cognitive bias and other common intuitive mistakes associated with System 1 Thinking.

Intelligence analysts have largely relied on intuitive judgment—a System 1 process—in constructing their analyses. When done well, intuitive judgment—sometimes referred to as traditional analysis—combines subject-matter expertise with basic thinking skills. Evidentiary reasoning, historical method, case study method, and reasoning by analogy are examples of this category of analysis. 2 The key characteristic that distinguishes intuitive judgment from structured analysis is that intuitive judgment is usually an individual effort in which the reasoning remains largely in the mind of the individual analyst until it is written down in a draft report. Training in this type of analysis is generally acquired through postgraduate education, especially in the social sciences and liberal arts, and often along with some country or language expertise.

This chapter presents a taxonomy that defines the domain of System 2 Thinking. A taxonomy is a classification of all elements of some body of information or knowledge. It defines the domain by identifying, naming, and categorizing all the various objects in a specialized discipline. The objects are organized into related groups based on some factor common to each object in the group.

The word "taxonomy" comes from the Greek taxis , meaning arrangement, division, or order, and nomos , meaning law. Classic examples of a taxonomy are Carolus Linnaeus's hierarchical classification of all living organisms by kingdom, phylum, class, order, family, genus, and species that is widely used in the biological sciences. The periodic table of elements used by chemists is another example. A library catalog is also considered a taxonomy, as it starts with a list of related categories that are then progressively broken down into finer categories.

Development of a taxonomy is an important step in organizing knowledge and furthering the development of any discipline. Rob Johnston developed a taxonomy of variables that influenced intelligence analysis but did not go into depth on analytic techniques or methods. He noted that "a taxonomy differentiates domains by specifying the scope of inquiry, codifying naming conventions, identifying areas of interest, helping to set research priorities, and often leading to new theories. Taxonomies are signposts, indicating what is known and what has yet to be discovered." 3

Robert Clark has described a taxonomy of intelligence sources. 4 He also categorized some analytic methods commonly used in intelligence analysis, but not to the extent of creating a taxonomy. To the best of our knowledge, no one has developed a taxonomy of analytic techniques for intelligence analysis, although taxonomies have been developed to classify research methods used in forecasting, 5 operations research, 6 information systems, 7 visualization tools, 8 electronic commerce, 9 knowledge elicitation, 10 and cognitive task analysis. 11

After examining taxonomies of methods used in other fields, we found that there is no single right way to organize a taxonomy—only different ways that are useful in achieving a specified goal. In this case, our goal is to gain a better understanding of the domain of Structured Analytic Techniques, investigate how these techniques contribute to providing a better analytic product, and consider how they relate to the needs of analysts. The objective has been to identify various techniques that are currently available, identify or develop additional potentially useful techniques, and help analysts compare and select the best technique for solving any specific analytic problem. Standardization of terminology for Structured Analytic Techniques will facilitate collaboration across agency and international boundaries during the use of these techniques.

Description Figure 2.1 System 1 and System 2 Thinking Source: Pherson Associates, LLC, 2019.

The taxonomy presented in Figure 2.1 distinguishes System 1, or intuitive thinking, from the four broad categories of analytic methods used in System 2 Thinking. It describes the nature of these four categories, one of which is structured analysis. The others are critical thinking, empirical analysis, and quasi-quantitative analysis. This chapter describes the rationale for these four broad categories. In the next chapter , we review the six categories or families of Structured Analytic Techniques.

2.2 Developing a Taxonomy of Structured Analytic Techniques

Intelligence analysts employ a wide range of methods to deal with an even wider range of subjects. Although this book focuses on the field of structured analysis, it is appropriate to identify some initial categorization of all the methods to see where structured analysis fits. Many researchers write of only two general approaches to analysis, contrasting qualitative with quantitative, intuitive with empirical, or intuitive with scientific. Others might claim that there are three distinct approaches: intuitive, structured, and scientific. In our taxonomy, we have sought to address this confusion by describing two types of thinking (System 1 and System 2) and defining four categories of System 2 Thinking.

The first step of science is to know one thing from another. This knowledge consists in their specific distinctions; but in order that it may be fixed and permanent, distinct names must be given to different things, and those names must be recorded and remembered.

—Carolus Linnaeus, Systema Naturae (1738)

Whether intelligence analysis is, or should be, an art or science is one of the long-standing debates in the literature on intelligence analysis. As we see it, intelligence analysis has aspects of both spheres. The range of activities that fall under the rubric of intelligence analysis spans the entire range of human cognitive abilities, and it is not possible to divide it into just two categories—art and science—or to say that it is only one or the other. The extent to which any part of intelligence analysis is either art or science is entirely dependent upon how one defines "art" and "science."

The taxonomy described here posits four functionally distinct methodological approaches to intelligence analysis. These approaches are distinguished by the nature of the analytic methods used, the type of quantification if any, the type of data that is available, and the type of training that is expected or required. Although each method is distinct, the borders between them can be blurry.

Critical thinking . Critical thinking, as defined by longtime intelligence methodologist and practitioner Jack Davis, is the application of the processes and values of scientific inquiry to the special circumstances of strategic intelligence. 12 Good critical thinkers will stop and reflect on who is the client, what is the question, where can they find the best information, how can they make a compelling case, and what is required to convey their message effectively. They recognize that this process requires checking key assumptions, looking for disconfirming data, and entertaining multiple explanations. Most students are exposed to critical-thinking techniques at some point in their education—from grade school to university—but few colleges or universities offer specific courses to develop critical thinking and writing skills. Structured analysis . Structured Analytic Techniques involve a step-by-step process that externalizes the analyst's thinking in a manner that makes it readily apparent to others, thereby enabling it to be reviewed, discussed, and critiqued piece by piece. For this reason, structured analysis usually becomes a collaborative effort in which the transparency of the analytic process exposes participating analysts to divergent or conflicting perspectives. We believe this type of analysis helps to mitigate some of the adverse effects of a single analyst's cognitive limitations, an ingrained mindset, and the whole range of cognitive biases, misapplied heuristics, and intuitive traps. Frequently used techniques include Cluster Brainstorming, Foresight analysis, Indicators, Analysis of Competing Hypotheses, and Key Assumptions Check. Structured techniques are taught in undergraduate and graduate school programs as well as many intelligence service training courses and can be used by analysts who do not have a background in statistics, advanced mathematics, or the hard sciences. Empirical analysis . When large stores of quantitative data or social media reporting are available, analysts can engage quantitative methods to study the available information or "Big Data." Quantifiable empirical data are so different from expert-generated data that the methods and types of problems the data are used to analyze are also quite different. Econometric modeling is a common example of this method. With the mushrooming of data obtainable from social media providers and the internet of things, sophisticated algorithms can identify trends and test hypotheses. Empirical data are collected by various types of sensors and are used, for example, in analysis of weapons systems or public response to a new product placement. Training is generally obtained through graduate education in statistics, economics, cyber analysis, or the hard sciences. Quasi-quantitative analysis . When analysts lack the empirical data needed to analyze an intelligence problem, one strategy is to fill the gaps using expert-generated data. Many methods rely on experts to rate key variables as High, Medium, Low, or Not Present, or by assigning a subjective probability judgment. Experts use special procedures to elicit these judgments, and the ratings usually are integrated into a larger model that describes a phenomenon, such as the vulnerability of a civilian leader to a military coup, the level of political instability, or the likely outcome of a legislative debate. This category includes methods such as Bayesian inference, dynamic modeling, and simulation. Training in the use of these methods is provided through graduate education in fields such as mathematics, information science, political science, operations research, or business.

No one of these four methods is better or more effective than another. All are needed in various circumstances to optimize the odds of finding the right answer. The use of multiple methods over the course of a single analytic project should be the norm, not the exception. For example, even a highly quantitative technical analysis may entail assumptions about motivation, intent, or capability that are best handled with critical thinking approaches and/or structured analysis. A brainstorming technique might be used to identify the variables to include in a dynamic model that uses expert-generated data to quantify these variables.

Of these four methods, structured analysis is the "new kid on the block," so it is useful to consider how it relates to System 1 Thinking. System 1 Thinking combines subject-matter expertise and intuitive judgment in an activity that takes place largely in an analyst's head. Although the analyst may gain input from others, the analytic product is frequently perceived as the product of a single analyst, and the analyst tends to feel "ownership" of his or her analytic product. The work of a single analyst is particularly susceptible to the wide range of cognitive pitfalls described in Psychology of Intelligence Analysis, Critical Thinking for Strategic Intelligence, and throughout this book. 13

Structured analysis, which is System 2 Thinking, follows a step-by-step process that can be used by an individual analyst, but we believe a group process provides more benefit. Structured Analytic Techniques guide the dialogue among analysts with common interests as they work step-by-step through an analytic problem. The critical point is that this approach exposes participants with various types and levels of expertise to alternative ideas, evidence, or mental models early in the analytic process and helps even experts avoid some common cognitive pitfalls. The structured group process that identifies and assesses alternative perspectives can also help to avoid Groupthink, the most common problem of small-group processes.

When used by a group or a team, structured techniques can become a mechanism for information sharing and group learning that helps to compensate for gaps or weaknesses in subject-matter expertise. This is especially useful for complex projects that require a synthesis of multiple types of expertise.

2.3 Dealing with Cognitive Limitations

As good as intuitive judgment often is, such judgment is still System 1 activity in the brain and is subject to many different types of cognitive limitations. Potential causes of such biases and mental mistakes include professional experience leading to an ingrained analytic mindset, training or education, the nature of one's upbringing, type of personality, a salient personal experience, or personal equity in a decision.

In this chapter, we distinguish between three types of cognitive limitations (see Figure 2.3 ):

Cognitive biases are inherent thinking errors that people make in processing information. They prevent an analyst from accurately understanding reality even when all the needed data and evidence that would form an accurate view is in hand. Heuristics are experience-based techniques that can give a solution that is not guaranteed to be optimal. The objective of a heuristic is to produce quickly a solution that is good enough to solve the problem at hand. Analysts can err by overrelying on or misapplying heuristics. Heuristics help an analyst generate a quick answer, but sometimes that answer will turn out to be wrong. Intuitive traps are practical manifestations of commonly recognized cognitive biases or heuristics that analysts in the intelligence profession—and many other disciplines—often fall victim to in their day-to-day activities.

There is extensive literature on how cognitive biases and heuristics affect a person's thinking in many fields. Intuitive traps, however, are a new category of bias first identified by Randolph Pherson and his teaching colleagues as they explored the value of using Structured Analytic Techniques to counter the negative impact of cognitive limitations. Additional research is ongoing to refine and revise the list of eighteen intuitive traps.

All cognitive biases, misapplied heuristics, or intuitive traps, except perhaps the personal equity bias, are more frequently the result of fast, unconscious, and intuitive System 1 Thinking and not the result of thoughtful reasoning (System 2). System 1 Thinking—though often correct—is more often influenced by cognitive biases and mindsets as well as insufficient knowledge and the inherent unknowability of the future. Structured Analytic Techniques—a type of System 2 Thinking—help identify and overcome the analytic biases inherent in System 1 Thinking.

Behavioral scientists have studied the impact of cognitive biases on analysis and decision making in many fields, such as psychology, political science, medicine, economics, business, and education ever since Amos Tversky and Daniel Kahneman introduced the concept of cognitive biases in the early 1970s. 14 Richards Heuer's work for the CIA in the late 1970s and the 1980s, subsequently followed by his book Psychology of Intelligence Analysis , first published in 1999, applied Tversky and Kahneman's insights to problems encountered by intelligence analysts. 15 Since the publication of Psychology of Intelligence Analysis , other authors associated with the U.S. Intelligence Community (including Jeffrey Cooper and Rob Johnston) have identified cognitive biases as a major cause of analytic failure at the CIA. 16

Figure 2.3 Glossary of Cognitive Biases, Misapplied Heuristics, and Intuitive Traps

This book is a logical follow-on to Psychology of Intelligence Analysis , which described in detail many of the biases and heuristics that influence intelligence analysis. 17 Since then, hundreds of cognitive biases and heuristics have been described in the academic literature using a wide variety of terms. As Heuer noted many years ago, "Cognitive biases are similar to optical illusions in that the error remains compelling even when one is fully aware of its nature. Awareness of the bias, by itself, does not produce a more accurate perception." 18 This is why cognitive limitations are exceedingly difficult to overcome. For example, Emily Pronin, Daniel Y. Lin, and Lee Ross observed in three different studies that people see the existence and operation of cognitive and motivational biases much more in others than in themselves. 19 This explains why so many analysts believe their own intuitive thinking (System 1) is sufficient.

Analysts in the intelligence profession—and many other disciplines—often fall victim to cognitive biases, misapplied heuristics, and intuitive traps that are manifestations of commonly recognized biases. Structured Analytic Techniques help analysts avoid, overcome, or at least mitigate their impact.

How a person perceives information is strongly influenced by factors such as experience, education, cultural background, and what that person is expected to do with the data. Our brains are trained to process information quickly, which often leads us to process data incorrectly or to not recognize its significance if it does not fit into established patterns. Some heuristics, such as the fight-or-flight instinct or knowing you need to take immediate action when you smell a gas leak, are helpful. Others are nonproductive. Defaulting to "rules of thumb" while problem solving can often lead to inherent thinking errors, because the information is being processed too quickly or incorrectly.

Cognitive biases , such as Confirmation Bias or Hindsight Bias, impede analytic thinking from the very start. 20 Misapplied heuristics , such as Groupthink or Premature Closure, could lead to a correct decision based on a non-rigorous thought process if one is lucky. More often, they impede the analytic process because they prevent us from considering a full range of possibilities. Intuitive traps , such as Projecting Past Experiences or Overinterpreting Small Samples, are mental mistakes practitioners make when conducting their business. A classic example is when a police detective assumes that the next case he or she is working will be like the previous case or a general prepares to fight the last war instead of anticipating that the next war will have to be fought differently.

Unfortunately for analysts, these biases, heuristics, and traps are quick to form and extremely hard to correct. After one's mind has reached closure on an issue, even a substantial accumulation of contradictory evidence is unlikely to force a reappraisal. Analysts often do not see new patterns emerging or fail to detect inconsistent data. An even larger concern is the tendency to ignore or dismiss outlier data as "noise."

Structured Analytic Techniques help analysts avoid, overcome, or at least mitigate these common cognitive limitations. Structured techniques help analysts do the following:

Reduce error rates. Avoid intelligence and other analytic failures. Embrace more collaborative work practices. Ensure accountability. Make the analysis more transparent to other analysts and decision makers.

2.4 Matching Cognitive Limitations to Structured Techniques

Figure 2.4 Matching Cognitive Limitations to the Six Families of Structured Techniques

In this book, we proffer guidance on how to reduce an analyst's vulnerability to cognitive limitations. In the overview of each family of Structured Analytic Techniques, we list two cognitive biases or misapplied heuristics as well as two intuitive traps that the techniques in that family are most effective in countering (see Figure 2.4 ). The descriptions of each of the sixty-six techniques include commentary on which biases, heuristics, and traps that specific technique helps mitigate. In our view, most techniques help counter cognitive limitations with differing degrees of effectiveness, and the matches we selected are only illustrative of what we think works best. Additional research is needed to empirically validate the matches we have identified from our experience teaching the techniques over the past decade and exploring their relationship to key cognitive limitations.

2.5 Combating Digital Disinformation

The growing use of social media platforms to manipulate popular perceptions for partisan political or social purposes has made democratic processes increasingly vulnerable in the United States and across the world. Largely unencumbered by commercial or legal constraints, international standards, or morality, proponents of Digital Disinformation 21 have become increasingly adept at exploiting common cognitive limitations, such as Confirmation Bias, Groupthink, and Judging by Emotion. History may show that we have grossly underestimated how easy it has been to influence popular opinion by leveraging cognitive biases, misapplied heuristics, and intuitive traps.

Digital Disinformation is purposely intended to mislead the reader. Perpetrators of Digital Disinformation compose compelling and seemingly coherent narratives that usually dismiss inconsistent evidence and ignore basic rules of logic. The primary objective of digital deceivers is to provide incorrect information in a seemingly persuasive format that confirms the readers' biases and either hardens mental mindsets or sows apathy or disbelief in the ability to know the truth. 22 Uncritical readers will often believe they have "found the truth" when actually they are functioning as both victims and perpetrators of cognitive bias, misapplied heuristics, and intuitive traps.

Purposeful misinformation, conspiracy theories, deception, and active measures have been used by activists and nation-states to influence people for decades, if not centuries. 23 Such efforts at perception management appear to have had greater impact in recent years because of the following:

The breadth and volume of misinformation has become staggering, owing to the power of social media platforms. The speed of the spread of disinformation is breathtaking as stories can quickly go "viral," spreading to millions of readers. A Massachusetts Institute of Technology study in Science documents that false rumors travel across the internet six times faster than factual stories. 24 People appear to be increasingly seeking simple answers to complex problems. Social network platforms usually present information in simplified form, which makes the message more digestible but far less nuanced—and often inaccurate. 25

The incentives for digital deceivers to leverage social media platforms to manipulate popular perceptions have also increased dramatically because of the following:

Millions of people can be reached almost instantaneously. Few perpetrators are held accountable for their posts. Perpetrators can micro-target their messages to those most easily swayed and open to persuasion.

Another underlying and often overlooked factor explaining the growing impact of Digital Disinformation is the susceptibility of individuals to false messaging. Perpetrators of conspiracy theories know what is most likely to "stick" in the minds of their audiences. This "stickiness" is usually attributable to the exploitation of human vulnerabilities that are manifestations of omnipresent, and well-ingrained, cognitive biases, misapplied heuristics, and intuitive traps.

Perpetrators of Digital Disinformation know that the best way to manipulate popular perceptions is to exploit well-ingrained cognitive limitations. They can anticipate when a person is likely to fall victim to a cognitive bias or to misapply a heuristic, and they leverage this knowledge to increase the impact of their messaging. Experts in false messaging, for example, are aware that people's perceptions of data are strongly influenced by their past experiences, education, cultural values, and how they identify themselves. People with different backgrounds will perceive information differently.

Moreover, knowledge of someone's social media profile greatly facilitates the process of identifying how best to package misinformation to reinforce that person's thinking. With the explosive growth in the use of social media platforms and databases, the use of such micro-targeting strategies has proven increasingly effective in product marketing and more recently in political campaigns.

Two of the most powerful biases that perpetrators of misinformation exploit are Confirmation Bias—seeking only information that confirms your viewpoint—and Vividness Bias—focusing attention only on the most vivid possibility. 26 , 27 Digital deceivers have also become masters of exploiting misapplied heuristics, such as the Anchoring Effect, Groupthink, and Satisficing. Intuitive traps that create vulnerabilities include Judging by Emotion, Presuming Patterns, and Overinterpreting Small Samples.

Recognizing one's vulnerability to Digital Disinformation is insufficient for mitigating the threat. A more productive strategy is needed—one that involves the use of critical thinking strategies and Structured Analytic Techniques. People are less likely to be deceived if they make it a habit to evaluate the quality of the evidence used to support a claim and ask what other credible, alternative narratives could explain what has occurred. Four Structured Analytic Techniques that are particularly effective in helping counter the impact of Digital Disinformation are as follows: 28

Key Assumptions Check . Making explicit and questioning the assumptions that guide an analyst's interpretation of evidence and the reasoning underlying a judgment or conclusion. Analysis of Competing Hypotheses . The evaluation of information against a set of alternative hypotheses to determine the consistency/inconsistency of each piece of data against each hypothesis and the rejection of hypotheses with much inconsistent data. Premortem Analysis and Structured Self-Critique . A systematic process using brainstorming and checklist procedures to identify critical weaknesses in an argument and assess how a key analytic judgment could be spectacularly wrong.

Notes

1. For further information on dual process theory, see the research by Jonathan Evans and Keith Frankish, In Two Minds: Dual Processes and Beyond (Oxford, UK: Oxford University Press, 2009); and Pat Croskerry, "A Universal Model of Diagnostic Reasoning," Academic Medicine 84, no. 8 (August 2009).

2. Reasoning by analogy can also be a structured technique called Structured Analogies, as described in chapter 8 .

3. Rob Johnston, Analytic Culture in the U.S. Intelligence Community (Washington, DC: CIA Center for the Study of Intelligence, 2005), 34.

4. Robert M. Clark, Intelligence Analysis: A Target-Centric Approach , 2nd ed. (Washington, DC: CQ Press, 2007), 84.

5. Forecasting Principles website, last accessed November 6, 2019, www.forecastingprinciples.com/files/pdf/methodsselectionchart.pdf

6. Russell W. Frenske, "A Taxonomy for Operations Research," Operations Research 19, no. 1 (January–February 1971).

7. Kai R. T. Larson, "A Taxonomy of Antecedents of Information Systems Success: Variable Analysis Studies," Journal of Management Information Systems 20, no. 2 (Fall 2003).

8. Ralph Lengler and Martin J. Epler, "A Periodic Table of Visualization Methods," n.d., www.visual-literacy.org/periodic_table/periodic_table.html

9. Roger Clarke, Appropriate Research Methods for Electronic Commerce (Canberra, Australia: Xanax Consultancy Pty Ltd., 2000), www.forecastingprinciples.com/files/pdf/methodsselectionchart.pdf

10. Robert R. Hoffman, Nigel R. Shadbolt, A. Mike Burton, and Gary Klein, "Eliciting Knowledge from Experts," Organizational Behavior and Human Decision Processes 62 (May 1995): 129–158.

11. Robert R. Hoffman and Laura G. Militello, Perspectives on Cognitive Task Analysis: Historical Origins and Modern Communities of Practice (Boca Raton, FL: CRC Press/Taylor and Francis, 2008); Beth Crandall, Gary Klein, and Robert R. Hoffman, Working Minds: A Practitioner's Guide to Cognitive Task Analysis (Cambridge, MA: MIT Press, 2006).

12. See Katherine Hibbs Pherson and Randolph H. Pherson, Critical Thinking for Strategic Intelligence, 2nd ed. (Washington, DC: CQ Press/SAGE, 2017), xxii.

13. Richards J. Heuer Jr., Psychology of Intelligence Analysis (Washington, DC: CIA Center for the Study of Intelligence, 1999; reprinted by Pherson Associates, LLC, Reston, VA, 2007).

14. Amos Tversky and Daniel Kahneman, "Judgment under Uncertainty: Heuristics and Biases," Science 185, no. 4157 (1974): 1124–1131.

15. Psychology of Intelligence Analysis was republished by Pherson Associates, LLC, in 2007, and can be purchased on its website at shop.globalytica.com.

16. Jeffrey R. Cooper, Curing Analytic Pathologies: Pathways to Improved Intelligence Analysis (Washington, DC: CIA Center for the Study of Intelligence, 2005); Rob Johnston, Analytic Culture in the U.S. Intelligence Community: An Ethnographic Study (Washington, DC: CIA Center for the Study of Intelligence, 2005).

17. Heuer, Psychology of Intelligence Analysis.

18. Ibid., 112.

19. Emily Pronin, Daniel Y. Lin, and Lee L. Ross, "The Bias Blind Spot: Perceptions of Bias in Self versus Others," Personality and Social Psychology Bulletin 28, no. 3 (2002): 369–381.

20. Definitions of these and other cognitive biases, misapplied heuristics, and intuitive traps mentioned later in this chapter are provided in Figure 2.3 on pages 24–25 .

21. Efforts to purposefully mislead or misinform have also been described as "Fake News," "False News," or "Agenda-Driven News." The phrase most often used in the public domain is Fake News, but the inaccurate use of this term to describe any critical news reporting has undermined its usefulness.

22. Rob Brotherton, "Five Myths about Conspiracy Theories," Washington Post , January 17, 2019, https://www.washingtonpost.com/outlook/five-myths/five-myths-about-conspiracy-theories/2019/01/17/0ef1b840-1818-11e9-88fe-f9f77a3bcb6c_story

23. The term "active measures" refers to actions taken by the Soviet Union, and later Russia, beginning in the 1920s to influence popular perceptions through propaganda, false documentation, penetration of institutions, persecution of political activists, and political violence, including assassinations. For more information, see the testimony of Gen. (ret.) Keith B. Alexander, Disinformation: A Primer in Russian Active Measures and Influence Campaigns, United States Senate Select Committee on Intelligence , March 30, 2017, https://www.intelligence.senate.gov/sites/default/files/documents/os-kalexander-033017.pdf .

24. Soroush Vosoughi, Deb Roy, and Sinan Aral, "The Spread of True and False News Online," Science 359, no. 6380 (2018): 1146–1151.

25. Elisa Shearer and Jeffrey Gottfried, News Use across Social Media Platforms 2017 , (Washington, DC: Pew Research Center, September 7, 2017), http://www.journalism.org/2017/09/07/news-use-across-social-media-platforms-2017/

26. A fuller discussion of this issue can be found in Randolph H. Pherson and Penelope Mort Ranta, "Cognitive Bias, Digital Disinformation, and Structured Analytic Techniques," Revista Romănă de studii de intelligence ( Romanian Journal of Intelligence Studies ), Vol. 21, 2019). The article was inspired in large part by observations made during the U.S. presidential election in 2016. Similar dynamics, however, have been observed in subsequent elections in France, Germany, and several other European states as well as the Brexit campaign in the United Kingdom.

27. A review of the cognitive biases and misapplied heuristics most often experienced by intelligence analysts can be found in Katherine Hibbs Pherson and Randolph H. Pherson, Critical Thinking for Strategic Intelligence , 2nd ed. (Washington, DC: CQ Press/SAGE, 2017), 55.

28. Randolph H. Pherson, Handbook of Analytic Tools and Techniques , 5th ed. (Tysons, VA: Pherson Associates, LLC, 2019), 5, 9, 19, 31, 43, 53.

Descriptions of Images and Figures

Back to Figure

In system 1 thinking, the judgment is intuitive. System 2 thinking involves critical thinking, structured analysis, quasi-quantitative analysis, and empirical analysis. Critical thinking is qualitative with known data and includes getting started, source validation, argumentation, and presentation. Structured analysis is qualitative with known and unknown data, and includes exploration, diagnostic, reframing, and foresight. Quasi-Quantitative Analysis is quantitative with known and unknown data, and includes computer-based tools using expert-generated data. Empirical analysis is quantitative with known data, and includes data-based computer tools and visualization techniques.
