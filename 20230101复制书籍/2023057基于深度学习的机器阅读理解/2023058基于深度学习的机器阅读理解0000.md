参考：张鑫.(2021).基于深度学习的机器阅读理解.科学出版社 => xxxx

## 附录一：机器学习基础概念

深度学习是机器学习的一个重要分支，从传统机器学习中继承了很多概念和方法，比如损失函数、优化算法，等等。尽管深度学习大行其道，但无论是从学术体系完整性，还是从算法模型继承性的角度，都有必要对机器学习的基础概念通常，可按学习过程中的监督程度将机器学习划分为有监督、无监督和弱监做一个简要介绍。

督学习。近年来，为了使用大规模无标记样本对深度神经网络这类含参量巨大的模型进行训练，研究人员引入了「自监督学习技术」，本质上是通过任务变换自动从无标记样本生成有标记样本，进而通过有监督学习实现模型参数训练。

接下来并未完全依照上述分类方式来组织内容，而是从有监督学习说起，然后介绍从不同角度拓展有监督学习技术而得到的弱监督学习、集成学习、强化学习、增量学习、迁移学习和最近几年比较常用的自监督学习，最后概述无监督学习技术。

一、有监督学习

有监督学习是指利用一批已知标签的样本进行学习的过程。这种机制可以类比为现实世界中学生通过练习若干有参考答案（对应于样本标签）的习题（对应于输入样本）来学习知识的过程。根据模型的预测输出连续与否，可以将有监督学习模型分为分类模型和回归模型。当输入为序列形式时，又衍生出序列分类和序列标注模型。下面分别加以简介。

1、分类模型。

通常，分类模型的解空间集合（或者说预测输出集合）大小是固定的，包含固定数量的类别标签值，表示类别空间中的一些离散点。在机器阅读理解任务中，分类模型可以用来确定问题和候选文档所属的领域或类型，比如区分问题是「5W1H」（who，what、where、when、why 与 how）中的哪一类，为后续的文章精细化筛选（找到包含答案的文章）以及答案片段定位做铺垫；也可以用在多文档机器阅读理解任务中，判断输入问题是否可以（用给定的文章集合）回答。

根据分类原理可以将分类模型分为概率模型和非概率模型。常见的概率模型为贝叶斯概率模型，根据已掌握的先验知识，利用条件概率理论分别估算当前样本隶属每种类别的概率值，选择概率最大的类别作为估计结果。有时根据先验知识的可获取程度，需要将简单贝叶斯模型拓展为朴素贝叶斯或半朴素贝叶斯模型。常见的非概率模型包括支持向量机、决策树等，其中，支持向量机是通过最大化不同类别样本与分割超平面（Separating Hyperplane）之间的间隔（Margin）来实现最优超平面定位的，进而通过判别输入样本与分割超平面间的关系来实现分类。决策树则基于树形结构进行分类，每个节点根据不同的特征进行分叉，各分叉方向对应不同的特征表现值，同时也对应着一个类别（若分叉后到了叶子节点）或者若干类别的集合（若分叉后还有分叉）。给定一个样本，由树根开始逐层分流判断，直至分支叶节点得到分类结果。

2、回归模型。

与分类模型不同，回归模型的预测输出是连续分布的，不可计数。根据模型假设的输入与输出之间的关系可将回归模型分为线性和非线性两类。在线性回归模型中，通过对各属性值做线性组合（或者说加权求和）来计算预测值，训练过程实际就是学习这些属性对应回归系数（或者说权重）的过程。对于任意一个样本，回归模型的预测结果都对应连续解空间中的某一点。非线性回归模型的解分布较线性模型复杂，求解也更为困难，能使模型达到（局部）最优的回归系数组往往不唯一。

有时，我们也利用回归模型来解求解类问题，如：逻辑回归（LogisticRegression）通过 sigmoid 运算可将输入的线性变换映射至 0-1 区间内，实现概率计算，进一步通过判断概率大小来实现对输入样本的二分类。

在机器阅读理解任务中，我们可以利用回归模型来对文章与问题之间的相关性进行排序，进而筛选出关联度最高的 N 篇文档，供进一步从中定位答案。

3、序列分类与序列标注。

序列分类和序列标注任务的输入都是序列，但二者的输出有很大差异：前者输出的是对输入序列整体的分类结果，后者输出的是对输入序列中的单元逐一进行分类得到的标签序列。序列标注任务是分类任务的拓展和延伸，特别是，序列标注过程中往往需要考虑单元间的先后关系（如词语在句子中的顺序），所以在对各单元进行分类时常常不应完全独立进行（比如句子中各词语的词性标签之间往序列分类与标注任务是在考虑序列特征的基础上，结合传统的分类模型对序往相互依赖）。

列中各词语的类别标签进行预测。一般的，通常将序列任务建模为图模型来刻画基于深度学习的机器阅读理解各词语之间的复杂关系。常用的序列模型包括隐马尔可夫模型（Hidden Markov Model，HMM）、条件随机场（Conditional Random Field，CRF）等，HMM 在时序性数据建模方面有着突出的表现，其构建的是有向图模型，而 CRF 则是通过构建无向图模型对相邻词语进行关系建模。

4、推荐阅读

由于上述几类概念（或者说任务）相互之间关联比较紧密，所以在文献中往往也一起介绍，而很难找到专门介绍其中某一类概念或任务的文献。因此，本节没有区分每类模型或任务来单独推荐文献，而是推荐如下一些综合性书籍或文章，供感兴趣的读者体系化学习和了解相关概念。

《机器学习》［70］

《机器学习从公理到算法》［162］

《统计学习方法》［71］

《Logistic Regression and Artificial Neural Network Classification Models: A Methodology Review》［163］

《Bidirectional LSTM-CRF Models for Sequence Tagging》［164］

《Overview of Supervised Learning》［165］

《Comparisons of Sequence Labeling Algorithms and Extensions》［166］

《A Brief Survey on Sequence Classification》［167］

二、有监督学习拓展

随着任务复杂性的不断增加及人工智能技术的快速发展，同时也受到种种现实约束（如标记数据获取障碍）的驱动，以有监督学习为核心发展起来一些其他学习方式，包括弱监督学习、集成学习、强化学习、增量学习、迁移学习以及自监督学习。下面我们对这些学习方式逐一进行简要的介绍和说明。

1、弱监督学习。

如前所述，有监督学习中每个训练样本均带标签且标签正确，所以有时也叫「强监督学习」。与此不同，弱监督学习所使用的训练样本则存在不同程度的标记出错或无标记的现象。

通常，弱监督学习可以分为三种不同类型；不完全监督学习、不确切监督学习和不精确监督学习。其中，不完全监督学习是指用于训练的样本集合里，一部分样本有标签，余下样本没有标签。针对这种情况，一般通过为无标签样本生成对应标签来加以解决，人工干涉（主动学习）方法和自动标记（半监督学习）方法均附录一机器学习基础概念可实现。不确切监督学习是指用于训练的样本集合里并不是「一个样本 / 一个标签」的分布，而是「一批样本 / 一个标签」的分布。在这种情况下，我们无法确定一批样本的具体数量，也无法确定这批样本中符合对应标签的样本分布，往往采用多

示例学习（Multiple Instance Learning，MIL）解决。不精确监督学习则指用于训练的样本集合里，有的样本被正确标记，有的样本被错误标记，处理这种情况的方

法称为带噪学习。感兴趣的读者可进一步查阅以下文献：

《A Brief Introduction to Weakly Supervised Learning》［168］

《Large-scale Interactive Object Segmentation with Human Annotators》［169］

《learning with Label Noise》［170］

《Not-so-supervised: A Survey of Semi-supervised，Multi-instance，and Transfer Learning in Medical Image Analysis》［171］

《Semi-Supervised Learning》［172］

2、集成学习。

集成学习是指通过将针对同一任务训练的多个不同模型有机结合起来，达到比任意单个模型更优的效果。一般的，这些模型的差异性越大，互补性就越强，集成模型的效果也就越好。现有的集成学习方法可分为两类：串行式方法、并行式方法。前者中，各分类器的训练通常按照一定顺序进行，前一分类器训练的误差可用来辅助后一分类器训练的调整，最后将各分类器集成，常用的串行式方法包括 Boosting 及其变形。并行式方法强调对任务数据集进行合理采样，得到若干个子数据集，各分类器在各自拥有的子数据集上进行训练。分类器训练完成后，再按照一定策略将其结合为集成模型，常用方法包括 Bagging 和随机森林。

集成学习方法的实现细节这里不再赘述，谨提供以下文献供参考学习：

《Ensemble Learning》［173］

《An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging，Boosting，and Randomization》［174］

《Ensemble Learning: A Survey》175］

《Pruning Adaptive Boosting》［176］

《Ensemble Learning for Hidden Markov Models》［177］

《Random Forests》［178］

3、强化学习。

强化学习指一个智能体通过感知周围环境状态作出决策进行状态转移，状态转移的结果作为奖励回馈给智能体，使其不断得到学习更新的过程。不同于有监督学习，强化学习不要求预先给定训练样本，而是通过接受环境对动作（引发的状基于深度学习的机器阅读理解态转移）的反馈（奖励或惩罚）来获得学习素材并更新模型参数。

强化学习中有四个重要的要素，即状态空间、行为空间、状态转移策略、奖励策略，它们支撑了整个强化学习过程。根据这四个要素是否已知可以将强化学习分为有模式学习和无模式学习。有模式学习过程中四个要素均已知，无模式学习中状态转移策略和奖励策略一般不可知。此外，根据智能体决策是否固定可以将强化学习分为主动学习和被动学习。主动学习是指状态转移策略不固定，智能体通过尽可能感知当前状态来确定状态转移策略，环境状态与行为并不唯一对应；被动学习指智能体的状态转移策略固定，确定的环境对应唯一的状态转移策略。

常见的强化学习方法包括动态规划、蒙特卡罗方法、时序差分学习方法等。为了帮助感兴趣读者深入了解强化学习技术，我们推荐以下文献供参考：

《Reinforcement Learning: A Survey》 ［179]

《Bayesian Reinforcement Learning: A Survey》［180］

4、增量学习。

增量学习是一种渐进性的、持续性的学习模式，模型训练好之后并非一成不变，而是随着新数据样本的到来不断优化更新。这种学习方式有两个显著优势：

其一，避免一次存入海量数据对存储设备带来的压力，通过在线学习的方式实时处理到来的数据流，不必保存历史数据；其二，对于新到来的数据，模型并不是从零开始训练，而是以历史数据训练好的模型参数作为基础，利用新来的数据实现模型微调，在良好基础上进行再训练，极大地节省了训练时间。常见的增量学习方法包括自组织增量学习神经网络（Self-Organizing Incremental Neural Network，SOINN）和情景记忆马尔可夫决策过程（Episodic Memory Markov Decision Process，EM-MDP）。如需进一步了解可参考下面几篇文献：

《Methods for Incremental Learning: A Survey》［181］

《Incremental On-line Learning: A Review and Comparison of State of the Art Algorithms》［182］

《Learn++: An Incremental Learning Algorithm for Supervised Neural Networks》［183］

《PANFIS: A Novel Incremental Learning Machine》［184］

《Broad Learning System: An Effective and Efficient Incremental Learning System without the Need for Deep Architecture》［185］

《Large Scale Incremental Learning》［186］

5、迁移学习。

迁移学习是指通过一定学习过程，将从其他领域（通常称为「源域」）获取的知识，迁移到新的感兴趣领域（通常称为「目标域」）。在自然语言处理中，经常需要从数据资源充足的领域迁移知识到资源相对贫乏的领域。

迁移学习可分为直推式迁移、归纳式迁移和无监督迁移三类。其中，直推式迁移是指应用领域相关、任务性质相同的迁移模式；归纳式迁移指应用领域和任务性质均不同，但它们之间有一定相关性的迁移模式；无监督迁移也指应用领域和任务性质均不同但有一定相关性的迁移模式，不过与归纳式迁移不同，迁移所涉及的源数据和目标数据均无标签。相反，归纳式迁移的目标数据一定是有标签的，根据源数据是否有标签又可以进一步分为多任务学习和自我学习。

迁移学习和增量学习有一定相似性，二者均涉及将训练好的模型放在新的数据集上进行再训练。但需要注意，增量学习的目的是利用新的数据样本（通常是同一领域的）来优化和提升原来的模型，使其能「与时俱进」；而迁移学习则更强调对新任务的解决，不太关注新训练（或者说迁移后）的模型是否能很好解决原任务，所以再训练使用的「新」样本是新领域（也即目标域）中的样本。

近年开发的 BERT 等预训练语言模型为机器阅读理解等任务中的迁移学习提供了便利 —— 利用归纳式迁移方法，将在大规模语料上预训练好的 BERT 模型外接目标任务的任务头，然后利用针对目标任务的小规模标记语料进行模型微调，所获模型即可很好完成目标任务。这个过程中，由于源语料数据量庞大，模型训练足够充分，学到了尽可能多的通识性知识，所以只需在目标任务上使用少量、甚至极少量的训练样本进行模型微调，便可学到目标任务特需的知识。

下面我们推荐几篇文献供大家进一步了解相关内容：

《A Survey on Transfer Learning》［187］

《Self-taught Learning: Transfer Learning from Unlabeled Data》［188］

《Deep Transfer Learning with Joint Adaptation Networks》［189］

《A Survey on Deep Transfer Learning》［190］

《Multi-task Transfer Learning for Biomedical Machine Reading Comprehension》［191］

《Multi-Task Learning for Machine Reading Comprehension》［192］

6、自监督学习。

自监督学习是指通过引入一些辅助任务来从大规模无标签数据中自动生成样本标签，进而利用这些自动生成的带标签样本对辅助任务进行有监督学习，来训练能够从大规模数据中挖掘相关特征的网络模型的过程。其核心是如何合理定义辅助任务，进而自动为数据产生标签。

近两年，随着基于上下文的表示学习方法的提出，自监督学习模式得到广泛应用。以 BERT 为例，在其预训练阶段，引入了掩码语言模型（Masked Language Model）和下一句预测（Next Sentence Prediction，NSP）作为辅助任务，通过遮掩上下文的一部分、甚至下一句来自动为大规模无标签语料生成标签，进而对包含大量参数的多层 Transformer 网络进行充分预训练，并保存模型参数作为实体抽取、阅读理解等下游任务的训练基础。在微调阶段，将原模型的任务头替换为目标任务头（如片段抽取任务常用的头指针和尾指针位置预测任务头），并在目标任务的有标签数据集上进行监督训练，来获得针对目标任务的模型。这种学习模式解决了无监督学习模式（见下文）下训练目标不够明确的问题，节省训练时间的同时提升了模型准确性，还避免了为每个目标任务标记大规模数据的繁杂工程。这种学习模式在机器阅读理解任务中应用十分广泛，因为阅读理解任务需要模型能尽可能得到充分训练以学习到更多的语言特征，这就需要大规模的语料作为支撑，而构建大规模的阅读理解数据集往往需要消耗很多人力物力且非常耗时。

同样推荐几篇参考文献，供感兴趣的读者进一步学习：

《A Survey on Self-supervised Pre-training for Sequential Transfer Learning in Neural Networks》［193］

《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》［5］

《ALBERT: A Lite BERT for Self-supervised Learning of Language Represen- tations》［194］

《Automatic Shortcut Removal for Self-Supervised Representation Learning»［195］ «Self-supervised Representation Learning from Multi-domain Data》［196］

《Self-supervised Relational Reasoning for Representation Learning》［197］

三、无监督学习。

无监督学习是指在所有样本无标记的情况下，完全依赖模型自身从中发现规律或模式的学习过程。这就好比，老师给班里学生布置了若干作业且都不提供标准答案，要求学生自主完成并自行批阅，老师并不参与批阅过程。所以无监督学习有时也叫「无师学习」。除样本无标签外，无监督学习过程应包含对一定的目标（函数）进行优化的过程。因此，尽管计算若干样本的统计直方图时也不需要样本有标签，但它却不属于无监督学习。

无监督学习有两个典型的应用场景，分别是聚类和降维。由于在机器阅读理解任务中较少涉及无监督学习，所以下面仅对聚类、降维的相关概念进行一个简要说明。

1、聚类。

聚类是指在没有标签的前提下根据样本之间的一些共性特征将相似性较高的样本归为一类（每一类称为一个簇）的过程，是一种典型的数据分组过程。常见的聚类算法包括 K 均值算法、学习向量量化算法、高斯混合聚类算法，等等。感兴趣的读者可以阅读以下参考文献进一步学习了解：

《K-center Clustering under Perturbation Resilience》［198］

《A Comprehensive Survey of Clustering Algorithm》［199］

《A Survey of Clustering Algorithms for Big Data: Taxonomy and Empirical Analysis》［200］

2、降维。

在解决实际问题的过程中，往往会碰到需要考虑的因素众多，或者对象具有大量相关属性的情况。这时，就涉及高维（往往也很稀疏）向量的计算和存储，使问题变得复杂、建模求解困难。因此，研究人员提出在保留绝大多数数据特征（或者说主要信息）的前提下对数据进行降维处理。降维是指利用一定算法对高维空间中的样本点进行空间变换，投影到更低维的空间中，并保留原样本点的主要特征，忽略次要特征。常用的降维算法包括主成分分析法、局部线性嵌入法、拉普拉斯特征映射法等。

感兴趣的读者可以进一步查阅以下参考文献：

《A Survey of Dimension Reduction Techniques》［201］

《Principal Component Analysis》［202］

《Penalized Discriminant Analysis》［203］