第 14 章　可迭代的对象、迭代器和生成器

当我在自己的程序中发现用到了模式，我觉得这就表明某个地方出错了。程序的形式应该仅仅反映它所要解决的问题。代码中其他任何外加的形式都是一个信号，（至少对我来说）表明我对问题的抽象还不够深 —— 这通常意味着自己正在手动完成的事情，本应该通过写代码来让宏的扩展自动实现。1

——Paul Graham2

Lisp 黑客和风险投资人

1 摘自一篇博客文章，「Revenge of the Nerds」（「书呆子的复仇」）。

2Paul Graham 的文集《黑客与画家：来自计算机时代的高见》已由人民邮电出版社出版，书号：978-7-115-32656-0。—— 编者注

迭代是数据处理的基石。扫描内存中放不下的数据集时，我们要找到一种惰性获取数据项的方式，即按需一次获取一个数据项。这就是迭代器模式（Iterator pattern）。本章说明 Python 语言是如何内置迭代器模式的，这样就避免了自己手动去实现。

与 Lisp（Paul Graham 最喜欢的语言）不同，Python 没有宏，因此为了抽象出迭代器模式，需要改动语言本身。为此，Python 2.2（2001 年）加入了 yield 关键字。3 这个关键字用于构建生成器（generator），其作用与迭代器一样。

3Python 2.2 的用户可以使用 from __future__ import generators 指令获取 yield 关键字；在 Python 2.3 中，yield 关键字默认可用。

所有生成器都是迭代器，因为生成器完全实现了迭代器接口。不过，根据《设计模式：可复用面向对象软件的基础》一书的定义，迭代器用于从集合中取出元素；而生成器用于「凭空」生成元素。通过斐波纳契数列能很好地说明二者之间的区别：斐波纳契数列中的数有无穷个，在一个集合里放不下。不过要知道，在 Python 社区中，大多数时候都把迭代器和生成器视作同一概念。

在 Python 3 中，生成器有广泛的用途。现在，即使是内置的 range () 函数也返回一个类似生成器的对象，而以前则返回完整的列表。如果一定要让 range () 函数返回列表，那么必须明确指明（例如，list (range (100))）。

在 Python 中，所有集合都可以迭代。在 Python 语言内部，迭代器用于支持：

for 循环

构建和扩展集合类型

逐行遍历文本文件

列表推导、字典推导和集合推导

元组拆包

调用函数时，使用 * 拆包实参

本章涵盖以下话题：

语言内部使用 iter (...) 内置函数处理可迭代对象的方式

如何使用 Python 实现经典的迭代器模式

详细说明生成器函数的工作原理

如何使用生成器函数或生成器表达式代替经典的迭代器

如何使用标准库中通用的生成器函数

如何使用 yield from 语句合并生成器

案例分析：在一个数据库转换工具中使用生成器函数处理大型数据集

为什么生成器和协程看似相同，实则差别很大，不能混淆

首先来研究 iter (...) 函数如何把序列变得可以迭代。

14.1　Sentence 类第 1 版：单词序列

我们要实现一个 Sentence 类，以此打开探索可迭代对象的旅程。我们向这个类的构造方法传入包含一些文本的字符串，然后可以逐个单词迭代。第 1 版要实现序列协议，这个类的对象可以迭代，因为所有序列都可以迭代 —— 这一点前面已经说过，不过现在要说明真正的原因。

示例 14-1 定义了一个 Sentence 类，通过索引从文本中提取单词。

示例 14-1　sentence.py：把句子划分为单词序列

import re import reprlib RE_WORD = re.compile('\w+') class Sentence: def __init__(self, text): self.text = text self.words = RE_WORD.findall(text) ➊ def __getitem__(self, index): return self.words[index] ➋ def __len__(self): ➌ return len(self.words) def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) ➍

❶ re.findall 函数返回一个字符串列表，里面的元素是正则表达式的全部非重叠匹配。

❷ self.words 中保存的是 .findall 函数返回的结果，因此直接返回指定索引位上的单词。

❸ 为了完善序列协议，我们实现了 __len__ 方法；不过，为了让对象可以迭代，没必要实现这个方法。

❹ reprlib.repr 这个实用函数用于生成大型数据结构的简略字符串表示形式。4

4 首次使用 reprlib 模块是在 10.2 节。

默认情况下，reprlib.repr 函数生成的字符串最多有 30 个字符。Sentence 类的用法参见示例 14-2 中的控制台会话。

示例 14-2　测试 Sentence 实例能否迭代

>>> s = Sentence('"The time has come," the Walrus said,') # ➊ >>> s Sentence('"The time ha... Walrus said,') # ➋ >>> for word in s: # ➌ ... print(word) The time has come the Walrus said >>> list(s) # ➍ ['The', 'time', 'has', 'come', 'the', 'Walrus', 'said']

❶ 传入一个字符串，创建一个 Sentence 实例。

❷ 注意，__repr__ 方法的输出中包含 reprlib.repr 方法生成的 ...。

❸ Sentence 实例可以迭代，稍后说明原因。

❹ 因为可以迭代，所以 Sentence 对象可以用于构建列表和其他可迭代的类型。

在接下来的几页中，我们还要开发其他 Sentence 类，而且都能通过示例 14-2 中的测试。不过，示例 14-1 中的实现与其他实现都不同，因为这一版 Sentence 类也是序列，可以按索引获取单词：

>>> s[0] 'The' >>> s[5] 'Walrus' >>> s[-1] 'said'

所有 Python 程序员都知道，序列可以迭代。下面说明具体的原因。

序列可以迭代的原因：iter 函数

解释器需要迭代对象 x 时，会自动调用 iter (x)。

内置的 iter 函数有以下作用。

(1) 检查对象是否实现了 __iter__ 方法，如果实现了就调用它，获取一个迭代器。

(2) 如果没有实现 __iter__ 方法，但是实现了 __getitem__ 方法，Python 会创建一个迭代器，尝试按顺序（从索引 0 开始）获取元素。

(3) 如果尝试失败，Python 抛出 TypeError 异常，通常会提示「C object is not iterable」（C 对象不可迭代），其中 C 是目标对象所属的类。

任何 Python 序列都可迭代的原因是，它们都实现了 __getitem__ 方法。其实，标准的序列也都实现了 __iter__ 方法，因此你也应该这么做。之所以对 __getitem__ 方法做特殊处理，是为了向后兼容，而未来可能不会再这么做（不过，写作本书时还未弃用）。

11.2 节提到过，这是鸭子类型（duck typing）的极端形式：不仅要实现特殊的 __iter__ 方法，还要实现 __getitem__ 方法，而且 __getitem__ 方法的参数是从 0 开始的整数（int），这样才认为对象是可迭代的。

在白鹅类型（goose-typing）理论中，可迭代对象的定义简单一些，不过没那么灵活：如果实现了 __iter__ 方法，那么就认为对象是可迭代的。此时，不需要创建子类，也不用注册，因为 abc.Iterable 类实现了 __subclasshook__ 方法，如 11.10 节所述。下面举个例子：

>>> class Foo: ... def __iter__(self): ... pass ... >>> from collections import abc >>> issubclass(Foo, abc.Iterable) True >>> f = Foo() >>> isinstance(f, abc.Iterable) True

不过要注意，虽然前面定义的 Sentence 类是可以迭代的，但却无法通过 issubclass (Sentence, abc.Iterable) 测试。

从 Python 3.4 开始，检查对象 x 能否迭代，最准确的方法是：调用 iter (x) 函数，如果不可迭代，再处理 TypeError 异常。这比使用 isinstance (x, abc.Iterable) 更准确，因为 iter (x) 函数会考虑到遗留的 __getitem__ 方法，而 abc.Iterable 类则不考虑。

迭代对象之前显式检查对象是否可迭代或许没必要，毕竟尝试迭代不可迭代的对象时，Python 抛出的异常信息很明确：TypeError: 'C' object is not iterable。如果除了抛出 TypeError 异常之外还要做进一步的处理，可以使用 try/except 块，而无需显式检查。如果要保存对象，等以后再迭代，或许可以显式检查，因为这种情况可能需要尽早捕获错误。

下一节详述可迭代的对象和迭代器之间的关系。

14.2　可迭代的对象与迭代器的对比

从 14.1.1 节的解说可以推知下述定义。

可迭代的对象

使用 iter 内置函数可以获取迭代器的对象。如果对象实现了能返回迭代器的 __iter__ 方法，那么对象就是可迭代的。序列都可以迭代；实现了 __getitem__ 方法，而且其参数是从零开始的索引，这种对象也可以迭代。

我们要明确可迭代的对象和迭代器之间的关系：Python 从可迭代的对象中获取迭代器。

下面是一个简单的 for 循环，迭代一个字符串。这里，字符串 'ABC' 是可迭代的对象。背后是有迭代器的，只不过我们看不到：

>>> s = 'ABC' >>> for char in s: ... print(char) ... A B C

如果没有 for 语句，不得不使用 while 循环模拟，要像下面这样写：

>>> s = 'ABC' >>> it = iter(s) # ➊ >>> while True: ... try: ... print(next(it)) # ➋ ... except StopIteration: # ➌ ... del it # ➍ ... break # ➎ ... A B C

❶ 使用可迭代的对象构建迭代器 it。

❷ 不断在迭代器上调用 next 函数，获取下一个字符。

❸ 如果没有字符了，迭代器会抛出 StopIteration 异常。

❹ 释放对 it 的引用，即废弃迭代器对象。

❺ 退出循环。

StopIteration 异常表明迭代器到头了。Python 语言内部会处理 for 循环和其他迭代上下文（如列表推导、元组拆包，等等）中的 StopIteration 异常。

标准的迭代器接口有两个方法。

__next__

返回下一个可用的元素，如果没有元素了，抛出 StopIteration 异常。

__iter__

返回 self，以便在应该使用可迭代对象的地方使用迭代器，例如在 for 循环中。

这个接口在 collections.abc.Iterator 抽象基类中制定。这个类定义了 __next__ 抽象方法，而且继承自 Iterable 类；__iter__ 抽象方法则在 Iterable 类中定义。如图 14-1 所示。

图 14-1：Iterable 和 Iterator 抽象基类。以斜体显示的是抽象方法。具体的 Iterable.__iter__ 方法应该返回一个 Iterator 实例。具体的 Iterator 类必须实现 __next__ 方法。Iterator.__iter__ 方法直接返回实例本身

Iterator 抽象基类实现 __iter__ 方法的方式是返回实例本身（return self）。这样，在需要可迭代对象的地方可以使用迭代器。示例 14-3 是 abc.Iterator 类的源码。

示例 14-3　abc.Iterator 类，摘自 Lib/_collections_abc.py

class Iterator(Iterable): __slots__ = () @abstractmethod def __next__(self): 'Return the next item from the iterator. When exhausted, raise StopIteration' raise StopIteration def __iter__(self): return self @classmethod def __subclasshook__(cls, C): if cls is Iterator: if (any("__next__" in B.__dict__ for B in C.__mro__) and any("__iter__" in B.__dict__ for B in C.__mro__)): return True return NotImplemented

在 Python 3 中，Iterator 抽象基类定义的抽象方法是 it.__next__()，而在 Python 2 中是 it.next ()。一如既往，我们应该避免直接调用特殊方法，使用 next (it) 即可，这个内置的函数在 Python 2 和 Python 3 中都能使用。

在 Python 3.4 中，Lib/types.py 模块的源码里有下面这段注释：

# Iterators in Python aren't a matter of type but of protocol. A large # and changing number of builtin types implement *some* flavor of # iterator. Don't check the type! Use hasattr to check for both # "__iter__" and "__next__" attributes instead.

其实，这就是 abc.Iterator 抽象基类中 __subclasshook__ 方法的作用（参见示例 14-3）。

考虑到 Lib/types.py 中的建议，以及 Lib/_collections_abc.py 中的实现逻辑，检查对象 x 是否为迭代器最好的方式是调用 isinstance (x, abc.Iterator)。得益于 Iterator.__subclasshook__ 方法，即使对象 x 所属的类不是 Iterator 类的真实子类或虚拟子类，也能这样检查。

再看示例 14-1 中定义的 Sentence 类，在 Python 控制台中能清楚地看出如何使用 iter (...) 函数构建迭代器，以及如何使用 next (...) 函数使用迭代器：

>>> s3 = Sentence('Pig and Pepper') # ➊ >>> it = iter(s3) # ➋ >>> it # doctest: +ELLIPSIS <iterator object at 0x...> >>> next(it) # ➌ 'Pig' >>> next(it) 'and' >>> next(it) 'Pepper' >>> next(it) # ➍ Traceback (most recent call last): ... StopIteration >>> list(it) # ➎ [] >>> list(iter(s3)) # ➏ ['Pig', 'and', 'Pepper']

❶ 创建一个 Sentence 实例 s3，包含 3 个单词。

❷ 从 s3 中获取迭代器。

❸ 调用 next (it)，获取下一个单词。

❹ 没有单词了，因此迭代器抛出 StopIteration 异常。

❺ 到头后，迭代器没用了。

❻ 如果想再次迭代，要重新构建迭代器。

因为迭代器只需 __next__ 和 __iter__ 两个方法，所以除了调用 next () 方法，以及捕获 StopIteration 异常之外，没有办法检查是否还有遗留的元素。此外，也没有办法「还原」迭代器。如果想再次迭代，那就要调用 iter (...)，传入之前构建迭代器的可迭代对象。传入迭代器本身没用，因为前面说过 Iterator.__iter__ 方法的实现方式是返回实例本身，所以传入迭代器无法还原已经耗尽的迭代器。

根据本节的内容，可以得出迭代器的定义如下。

迭代器

迭代器是这样的对象：实现了无参数的 __next__ 方法，返回序列中的下一个元素；如果没有元素了，那么抛出 StopIteration 异常。Python 中的迭代器还实现了 __iter__ 方法，因此迭代器也可以迭代。

因为内置的 iter (...) 函数会对序列做特殊处理，所以第 1 版 Sentence 类可以迭代。接下来要实现标准的可迭代协议。

14.3　Sentence 类第 2 版：典型的迭代器

第 2 版 Sentence 类根据《设计模式：可复用面向对象软件的基础》一书给出的模型，实现典型的迭代器设计模式。注意，这不符合 Python 的习惯做法，后面重构时会说明原因。不过，通过这一版能明确可迭代的集合和迭代器对象之间的关系。

示例 14-4 中定义的 Sentence 类可以迭代，因为它实现了特殊的 __iter__ 方法，构建并返回一个 SentenceIterator 实例。《设计模式：可复用面向对象软件的基础》一书就是这样描述迭代器设计模式的。

这里之所以这么做，是为了清楚地说明可迭代的对象和迭代器之间的重要区别，以及二者之间的联系。

示例 14-4　sentence_iter.py：使用迭代器模式实现 Sentence 类

import re import reprlib RE_WORD = re.compile('\w+') class Sentence: def __init__(self, text): self.text = text self.words = RE_WORD.findall(text) def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): ➊ return SentenceIterator(self.words) ➋ class SentenceIterator: def __init__(self, words): self.words = words ➌ self.index = 0 ➍ def __next__(self): try: word = self.words[self.index] ➎ except IndexError: raise StopIteration() ➏ self.index += 1 ➐ return word ➑ def __iter__(self): ➒ return self

❶ 与前一版相比，这里只多了一个 __iter__ 方法。这一版没有 __getitem__ 方法，为的是明确表明这个类可以迭代，因为实现了 __iter__ 方法。

❷ 根据可迭代协议，__iter__ 方法实例化并返回一个迭代器。

❸ SentenceIterator 实例引用单词列表。

❹ self.index 用于确定下一个要获取的单词。

❺ 获取 self.index 索引位上的单词。

❻ 如果 self.index 索引位上没有单词，那么抛出 StopIteration 异常。

❼ 递增 self.index 的值。

❽ 返回单词。

❾ 实现 self.__iter__ 方法。

示例 14-4 中的代码能通过示例 14-2 中的测试。

注意，对这个示例来说，其实没必要在 SentenceIterator 类中实现 __iter__ 方法，不过这么做是对的，因为迭代器应该实现 __next__ 和 __iter__ 两个方法，而且这么做能让迭代器通过 issubclass (SentenceIterator, abc.Iterator) 测试。如果让 SentenceIterator 类继承 abc.Iterator 类，那么它会继承 abc.Iterator.__iter__ 这个具体方法。

这一版的工作量很大（对懒惰的 Python 程序员来说确实如此）。注意，SentenceIterator 类的大多数代码都在处理迭代器的内部状态。稍后会说明如何简化。不过，在此之前我们先稍微离题，讨论一个看似合理实则错误的实现捷径。

把 Sentence 变成迭代器：坏主意

构建可迭代的对象和迭代器时经常会出现错误，原因是混淆了二者。要知道，可迭代的对象有个 __iter__ 方法，每次都实例化一个新的迭代器；而迭代器要实现 __next__ 方法，返回单个元素，此外还要实现 __iter__ 方法，返回迭代器本身。

因此，迭代器可以迭代，但是可迭代的对象不是迭代器。

除了 __iter__ 方法之外，你可能还想在 Sentence 类中实现 __next__ 方法，让 Sentence 实例既是可迭代的对象，也是自身的迭代器。可是，这种想法非常糟糕。根据有大量 Python 代码审查经验的 Alex Martelli 所说，这也是常见的反模式。

《设计模式：可复用面向对象软件的基础》一书讲解迭代器设计模式时，在「适用性」一节中说：5

5《设计模式：可复用面向对象软件的基础》第 172 页。

迭代器模式可用来：

访问一个聚合对象的内容而无需暴露它的内部表示

支持对聚合对象的多种遍历

为遍历不同的聚合结构提供一个统一的接口（即支持多态迭代）

为了「支持多种遍历」，必须能从同一个可迭代的实例中获取多个独立的迭代器，而且各个迭代器要能维护自身的内部状态，因此这一模式正确的实现方式是，每次调用 iter (my_iterable) 都新建一个独立的迭代器。这就是为什么这个示例需要定义 SentenceIterator 类。

可迭代的对象一定不能是自身的迭代器。也就是说，可迭代的对象必须实现 __iter__ 方法，但不能实现 __next__ 方法。

另一方面，迭代器应该一直可以迭代。迭代器的 __iter__ 方法应该返回自身。

至此，我们演示了如何正确地实现典型的迭代器模式。本节至此告一段落，下一节展示如何使用更符合 Python 习惯的方式实现 Sentence 类。

14.4　Sentence 类第 3 版：生成器函数

实现相同功能，但却符合 Python 习惯的方式是，用生成器函数代替 SentenceIterator 类。先看示例 14-5，然后详细说明生成器函数。

示例 14-5　sentence_gen.py：使用生成器函数实现 Sentence 类

import re import reprlib RE_WORD = re.compile ('\w+') class Sentence: def __init__(self, text): self.text = text self.words = RE_WORD.findall (text) def __repr__(self): return 'Sentence (% s)' % reprlib.repr (self.text) def __iter__(self): for word in self.words: ➊ yield word ➋ return ➌ # 完成！ ➍

❶ 迭代 self.words。

❷ 产出当前的 word。

❸ 这个 return 语句不是必要的；这个函数可以直接「落空」，自动返回。不管有没有 return 语句，生成器函数都不会抛出 StopIteration 异常，而是在生成完全部值之后会直接退出。6

6Alex Martelli 审查这段代码时建议简化这个方法的定义体，直接使用 return iter (self.words)。当然，他是对的，毕竟调用 __iter__ 方法得到的就是迭代器。不过，这里我用的是 for 循环，而且用到了 yield 关键字，这样做是为了介绍生成器函数的句法。下一节会详细说明。

❹ 不用再单独定义一个迭代器类！

我们又使用一种不同的方式实现了 Sentence 类，而且也能通过示例 14-2 中的测试。

在示例 14-4 定义的 Sentence 类中，__iter__ 方法调用 SentenceIterator 类的构造方法创建一个迭代器并将其返回。而在示例 14-5 中，迭代器其实是生成器对象，每次调用 __iter__ 方法都会自动创建，因为这里的 __iter__ 方法是生成器函数。

下面全面说明生成器函数。

生成器函数的工作原理

只要 Python 函数的定义体中有 yield 关键字，该函数就是生成器函数。调用生成器函数时，会返回一个生成器对象。也就是说，生成器函数是生成器工厂。

普通的函数与生成器函数在句法上唯一的区别是，在后者的定义体中有 yield 关键字。有些人认为定义生成器函数应该使用一个新的关键字，例如 gen，而不该使用 def，但是 Guido 不同意。他的理由参见「PEP 255—Simple Generators」。7

7 有时，我会在生成器函数的名称中加上 gen 前缀或后缀，不过这不是习惯做法。显然，如果实现的是迭代器，那就不能这么做，因为所需的特殊方法必须命名为 __iter__。

下面以一个特别简单的函数说明生成器的行为：8

8 感谢 David Kwast 建议使用这个示例。

>>> def gen_123(): # ➊ ... yield 1 # ➋ ... yield 2 ... yield 3 ... >>> gen_123 # doctest: +ELLIPSIS <function gen_123 at 0x...> # ➌ >>> gen_123() # doctest: +ELLIPSIS <generator object gen_123 at 0x...> # ➍ >>> for i in gen_123(): # ➎ ... print(i) 1 2 3 >>> g = gen_123() # ➏ >>> next(g) # ➐ 1 >>> next(g) 2 >>> next(g) 3 >>> next(g) # ➑ Traceback (most recent call last): ... StopIteration

❶ 只要 Python 函数中包含关键字 yield，该函数就是生成器函数。

❷ 生成器函数的定义体中通常都有循环，不过这不是必要条件；这里我重复使用 3 次 yield。

❸ 仔细看，gen_123 是函数对象。

❹ 但是调用时，gen_123 () 返回一个生成器对象。

❺ 生成器是迭代器，会生成传给 yield 关键字的表达式的值。

❻ 为了仔细检查，我们把生成器对象赋值给 g。

❼ 因为 g 是迭代器，所以调用 next (g) 会获取 yield 生成的下一个元素。

❽ 生成器函数的定义体执行完毕后，生成器对象会抛出 StopIteration 异常。

生成器函数会创建一个生成器对象，包装生成器函数的定义体。把生成器传给 next (...) 函数时，生成器函数会向前，执行函数定义体中的下一个 yield 语句，返回产出的值，并在函数定义体的当前位置暂停。最终，函数的定义体返回时，外层的生成器对象会抛出 StopIteration 异常 —— 这一点与迭代器协议一致。

我觉得，使用准确的词语描述从生成器中获取结果的过程，有助于理解生成器。注意，我说的是产出或生成值。如果说生成器「返回」值，就会让人难以理解。函数返回值；调用生成器函数返回生成器；生成器产出或生成值。生成器不会以常规的方式「返回」值：生成器函数定义体中的 return 语句会触发生成器对象抛出 StopIteration 异常。9

9 在 Python 3.3 之前，如果生成器函数中的 return 语句有返回值，那么会报错。现在可以这么做，不过 return 语句仍会导致 StopIteration 异常抛出。调用方可以从异常对象中获取返回值。可是，只有把生成器函数当成协程使用时，这么做才有意义，详情参见 16.6 节。

示例 14-6 使用 for 循环更清楚地说明了生成器函数定义体的执行过程。

示例 14-6　运行时打印消息的生成器函数

>>> def gen_AB(): # ➊ ... print('start') ... yield 'A' # ➋ ... print('continue') ... yield 'B' # ➌ ... print('end.') # ➍ ... >>> for c in gen_AB(): # ➎ ... print('-->', c) # ➏ ... start ➐ --> A ➑ continue ➒ --> B ➓ end. ⓫ >>> ⓬

❶ 定义生成器函数的方式与普通的函数无异，只不过要使用 yield 关键字。

❷ 在 for 循环中第一次隐式调用 next () 函数时（序号➎），会打印'start'，然后停在第一个 yield 语句，生成值 'A'。

❸ 在 for 循环中第二次隐式调用 next () 函数时，会打印 'continue'，然后停在第二个 yield 语句，生成值 'B'。

❹ 第三次调用 next () 函数时，会打印 'end.'，然后到达函数定义体的末尾，导致生成器对象抛出 StopIteration 异常。

❺ 迭代时，for 机制的作用与 g = iter (gen_AB ()) 一样，用于获取生成器对象，然后每次迭代时调用 next (g)。

❻ 循环块打印 --> 和 next (g) 返回的值。但是，生成器函数中的 print 函数输出结果之后才会看到这个输出。

❼ 'start' 是生成器函数定义体中 print ('start') 输出的结果。

❽ 生成器函数定义体中的 yield 'A' 语句会生成值 A，提供给 for 循环使用，而 A 会赋值给变量 c，最终输出 --> A。

❾ 第二次调用 next (g)，继续迭代，生成器函数定义体中的代码由 yield 'A' 前进到 yield 'B'。文本 continue 是由生成器函数定义体中的第二个 print 函数输出的。

❿ yield 'B' 语句生成值 B，提供给 for 循环使用，而 B 会赋值给变量 c，所以循环打印出 --> B。

⓫ 第三次调用 next (it)，继续迭代，前进到生成器函数的末尾。文本 end. 是由生成器函数定义体中的第三个 print 函数输出的。到达生成器函数定义体的末尾时，生成器对象抛出 StopIteration 异常。for 机制会捕获异常，因此循环终止时没有报错。

⓬ 现在，希望你已经知道示例 14-5 中 Sentence.__iter__ 方法的作用了：__iter__ 方法是生成器函数，调用时会构建一个实现了迭代器接口的生成器对象，因此不用再定义 SentenceIterator 类了。

这一版 Sentence 类比前一版简短多了，但是还不够懒惰。如今，人们认为惰性是好的特质，至少在编程语言和 API 中是如此。惰性实现是指尽可能延后生成值。这样做能节省内存，而且或许还可以避免做无用的处理。

下一节以这种惰性方式定义 Sentence 类。

14.5　Sentence 类第 4 版：惰性实现

设计 Iterator 接口时考虑到了惰性：next (my_iterator) 一次生成一个元素。懒惰的反义词是急迫，其实，惰性求值（lazy evaluation）和及早求值（eager evaluation）是编程语言理论方面的技术术语。

目前实现的几版 Sentence 类都不具有惰性，因为 __init__ 方法急迫地构建好了文本中的单词列表，然后将其绑定到 self.words 属性上。这样就得处理整个文本，列表使用的内存量可能与文本本身一样多（或许更多，这取决于文本中有多少非单词字符）。如果只需迭代前几个单词，大多数工作都是白费力气。

只要使用的是 Python 3，思索着做某件事有没有懒惰的方式，答案通常都是肯定的。

re.finditer 函数是 re.findall 函数的惰性版本，返回的不是列表，而是一个生成器，按需生成 re.MatchObject 实例。如果有很多匹配，re.finditer 函数能节省大量内存。我们要使用这个函数让第 4 版 Sentence 类变得懒惰，即只在需要时才生成下一个单词。代码如示例 14-7 所示。

示例 14-7　sentence_gen2.py： 在生成器函数中调用 re.finditer 生成器函数，实现 Sentence 类

import re import reprlib RE_WORD = re.compile('\w+') class Sentence: def __init__(self, text): self.text = text ➊ def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): for match in RE_WORD.finditer(self.text): ➋ yield match.group() ➌

❶ 不再需要 words 列表。

❷ finditer 函数构建一个迭代器，包含 self.text 中匹配 RE_WORD 的单词，产出 MatchObject 实例。

❸ match.group () 方法从 MatchObject 实例中提取匹配正则表达式的具体文本。

生成器函数已经极大地简化了代码，但是使用生成器表达式甚至能把代码变得更简短。

14.6　Sentence 类第 5 版：生成器表达式

简单的生成器函数，如前面的 Sentence 类中使用的那个（见示例 14-7），可以替换成生成器表达式。

生成器表达式可以理解为列表推导的惰性版本：不会迫切地构建列表，而是返回一个生成器，按需惰性生成元素。也就是说，如果列表推导是制造列表的工厂，那么生成器表达式就是制造生成器的工厂。

示例 14-8 演示了一个简单的生成器表达式，并且与列表推导做了对比。

示例 14-8　先在列表推导中使用 gen_AB 生成器函数，然后在生成器表达式中使用

>>> def gen_AB(): # ➊ ... print('start') ... yield 'A' ... print('continue') ... yield 'B' ... print('end.') ... >>> res1 = [x*3 for x in gen_AB()] # ➋ start continue end. >>> for i in res1: # ➌ ... print('-->', i) ... --> AAA --> BBB >>> res2 = (x*3 for x in gen_AB()) # ➍ >>> res2 # ➎ <generator object <genexpr> at 0x10063c240> >>> for i in res2: # ➏ ... print('-->', i) ... start --> AAA continue --> BBB end.

❶ gen_AB 函数与示例 14-6 中的一样。

❷ 列表推导迫切地迭代 gen_AB () 函数生成的生成器对象产出的元素：'A' 和 'B'。注意，下面的输出是 start、continue 和 end.。

❸ 这个 for 循环迭代列表推导生成的 res1 列表。

❹ 把生成器表达式返回的值赋值给 res2。只需调用 gen_AB () 函数，虽然调用时会返回一个生成器，但是这里并不使用。

❺ res2 是一个生成器对象。

❻ 只有 for 循环迭代 res2 时，gen_AB 函数的定义体才会真正执行。for 循环每次迭代时会隐式调用 next (res2)，前进到 gen_AB 函数中的下一个 yield 语句。注意，gen_AB 函数的输出与 for 循环中 print 函数的输出夹杂在一起。

可以看出，生成器表达式会产出生成器，因此可以使用生成器表达式进一步减少 Sentence 类的代码，如示例 14-9 所示。

示例 14-9　sentence_genexp.py：使用生成器表达式实现 Sentence 类

import re import reprlib RE_WORD = re.compile('\w+') class Sentence: def __init__(self, text): self.text = text def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): return (match.group() for match in RE_WORD.finditer(self.text))

与示例 14-7 唯一的区别是 __iter__ 方法，这里不是生成器函数了（没有 yield），而是使用生成器表达式构建生成器，然后将其返回。不过，最终的效果一样：调用 __iter__ 方法会得到一个生成器对象。

生成器表达式是语法糖：完全可以替换成生成器函数，不过有时使用生成器表达式更便利。下一节说明生成器表达式的用途。

14.7　何时使用生成器表达式

在示例 10-16 中，为了实现 Vector 类，我用了几个生成器表达式，__eq__、__hash__、__abs__、angle、angles、format、__add__ 和 __mul__ 方法中各有一个生成器表达式。在这些方法中使用列表推导也行，不过立即返回的列表要使用更多的内存。

通过示例 14-9 可知，生成器表达式是创建生成器的简洁句法，这样无需先定义函数再调用。不过，生成器函数灵活得多，可以使用多个语句实现复杂的逻辑，也可以作为协程使用（参见第 16 章）。

遇到简单的情况时，可以使用生成器表达式，因为这样扫一眼就知道代码的作用，如 Vector 类的示例所示。

根据我的经验，选择使用哪种句法很容易判断：如果生成器表达式要分成多行写，我倾向于定义生成器函数，以便提高可读性。此外，生成器函数有名称，因此可以重用。

句法提示

如果函数或构造方法只有一个参数，传入生成器表达式时不用写一对调用函数的括号，再写一对括号围住生成器表达式，只写一对括号就行了，如示例 10-16 中 __mul__ 方法对 Vector 构造方法的调用，转摘如下。然而，如果生成器表达式后面还有其他参数，那么必须使用括号围住，否则会抛出 SyntaxError 异常：

def __mul__(self, scalar): if isinstance(scalar, numbers.Real): return Vector(n * scalar for n in self) else: return NotImplemented

目前所见的 Sentence 类示例说明了如何把生成器当作典型的迭代器使用，即从集合中获取元素。不过，生成器也可用于生成不受数据源限制的值。下一节会举例说明。

14.8　另一个示例：等差数列生成器

典型的迭代器模式作用很简单 —— 遍历数据结构。不过，即便不是从集合中获取元素，而是获取序列中即时生成的下一个值时，也用得到这种基于方法的标准接口。例如，内置的 range 函数用于生成有穷整数等差数列（Arithmetic Progression，AP），itertools.count 函数用于生成无穷等差数列。

下一节会说明 itertools.count 函数，本节探讨如何生成不同数字类型的有穷等差数列。

下面我们在控制台中对稍后实现的 ArithmeticProgression 类做一些测试，如示例 14-10 所示。这里，构造方法的签名是 ArithmeticProgression (begin, step [, end])。range () 函数与这个 ArithmeticProgression 类的作用类似，不过签名是 range (start, stop [, step])。我选择使用不同的签名是因为，创建等差数列时必须指定公差（step），而末项（end）是可选的。我还把参数的名称由 start/stop 改成了 begin/end，以明确表明签名不同。在示例 14-10 里的每个测试中，我都调用了 list () 函数，用于查看生成的值。

示例 14-10　演示 ArithmeticProgression 类的用法

>>> ap = ArithmeticProgression(0, 1, 3) >>> list(ap) [0, 1, 2] >>> ap = ArithmeticProgression(1, .5, 3) >>> list(ap) [1.0, 1.5, 2.0, 2.5] >>> ap = ArithmeticProgression(0, 1/3, 1) >>> list(ap) [0.0, 0.3333333333333333, 0.6666666666666666] >>> from fractions import Fraction >>> ap = ArithmeticProgression(0, Fraction(1, 3), 1) >>> list(ap) [Fraction(0, 1), Fraction(1, 3), Fraction(2, 3)] >>> from decimal import Decimal >>> ap = ArithmeticProgression(0, Decimal('.1'), .3) >>> list(ap) [Decimal('0.0'), Decimal('0.1'), Decimal('0.2')]

注意，在得到的等差数列中，数字的类型与 begin 或 step 的类型一致。如果需要，会根据 Python 算术运算的规则强制转换类型。在示例 14-10 中，有 int、float、Fraction 和 Decimal 数字组成的列表。

示例 14-11 列出的是 ArithmeticProgression 类的实现。

示例 14-11　ArithmeticProgression 类

class ArithmeticProgression: def __init__(self, begin, step, end=None): ➊ self.begin = begin self.step = step self.end = end # None -> 无穷数列 def __iter__(self): result = type (self.begin + self.step)(self.begin) ➋ forever = self.end is None ➌ index = 0 while forever or result < self.end: ➍ yield result ➎ index += 1 result = self.begin + self.step * index ➏

❶ __init__ 方法需要两个参数：begin 和 step。end 是可选的，如果值是 None，那么生成的是无穷数列。

❷ 这一行把 self.begin 赋值给 result，不过会先强制转换成前面的加法算式得到的类型。10

10Python 2 内置了 coerce () 函数，不过 Python 3 没有内置。开发者觉得没必要内置，因为算术运算符会隐式应用数值强制转换规则。所以，为了让数列的首项与其他项的类型一样，我能想到最好的方式是，先做加法运算，然后使用计算结果的类型强制转换生成的结果。我在 Python 邮件列表中问了这个问题，Steven D'Aprano 给出了妙极的答复。

❸ 为了提高可读性，我们创建了 forever 变量，如果 self.end 属性的值是 None，那么 forever 的值是 True，因此生成的是无穷数列。

❹ 这个循环要么一直执行下去，要么当 result 大于或等于 self.end 时结束。如果循环退出了，那么这个函数也随之退出。

❺ 生成当前的 result 值。

❻ 计算可能存在的下一个结果。这个值可能永远不会产出，因为 while 循环可能会终止。

在示例 14-11 中的最后一行，我没有直接使用 self.step 不断地增加 result，而是选择使用 index 变量，把 self.begin 与 self.step 和 index 的乘积相加，计算 result 的各个值，以此降低处理浮点数时累积效应致错的风险。

示例 14-11 中定义的 ArithmeticProgression 类能按预期那样使用。这是个简单的示例，说明了如何使用生成器函数实现特殊的 __iter__ 方法。然而，如果一个类只是为了构建生成器而去实现 __iter__ 方法，那还不如使用生成器函数。毕竟，生成器函数是制造生成器的工厂。

示例 14-12 中定义了一个名为 aritprog_gen 的生成器函数，作用与 ArithmeticProgression 类一样，只不过代码量更少。如果把 ArithmeticProgression 类换成 aritprog_gen 函数，示例 14-10 中的测试也都能通过。11

11 本书源码仓库中的 14-it-generator/ 目录里包含 doctest，以及一个 aritprog_runner.py 脚本，用于测试 aritprog*.py 脚本的所有版本。

示例 14-12　aritprog\_gen 生成器函数

def aritprog_gen(begin, step, end=None): result = type(begin + step)(begin) forever = end is None index = 0 while forever or result < end: yield result index += 1 result = begin + step * index

示例 14-12 很棒，不过始终要记住，标准库中有许多现成的生成器。下一节会使用 itertools 模块实现，那个版本更棒。

使用 itertools 模块生成等差数列

Python 3.4 中的 itertools 模块提供了 19 个生成器函数，结合起来使用能实现很多有趣的用法。

例如，itertools.count 函数返回的生成器能生成多个数。如果不传入参数，itertools.count 函数会生成从零开始的整数数列。不过，我们可以提供可选的 start 和 step 值，这样实现的作用与 aritprog_gen 函数十分相似：

>>> import itertools >>> gen = itertools.count(1, .5) >>> next(gen) 1 >>> next(gen) 1.5 >>> next(gen) 2.0 >>> next(gen) 2.5

然而，itertools.count 函数从不停止，因此，如果调用 list (count ())，Python 会创建一个特别大的列表，超出可用内存，在调用失败之前，电脑会疯狂地运转。

不过，itertools.takewhile 函数则不同，它会生成一个使用另一个生成器的生成器，在指定的条件计算结果为 False 时停止。因此，可以把这两个函数结合在一起使用，编写下述代码：

>>> gen = itertools.takewhile(lambda n: n < 3, itertools.count(1, .5)) >>> list(gen) [1, 1.5, 2.0, 2.5]

示例 14-13 利用 takewhile 和 count 函数，写出的代码流畅而简短。

示例 14-13　aritprog_v3.py：与前面的 aritprog_gen 函数作用相同

import itertools def aritprog_gen(begin, step, end=None): first = type(begin + step)(begin) ap_gen = itertools.count(first, step) if end is not None: ap_gen = itertools.takewhile(lambda n: n < end, ap_gen) return ap_gen

注意，示例 14-13 中的 aritprog_gen 不是生成器函数，因为定义体中没有 yield 关键字。但是它会返回一个生成器，因此它与其他生成器函数一样，也是生成器工厂函数。

示例 14-13 想表达的观点是，实现生成器时要知道标准库中有什么可用，否则很可能会重新发明轮子。鉴于此，下一节会介绍一些现成的生成器函数。

14.9　标准库中的生成器函数

标准库提供了很多生成器，有用于逐行迭代纯文本文件的对象，还有出色的 os.walk 函数。这个函数在遍历目录树的过程中产出文件名，因此递归搜索文件系统像 for 循环那样简单。

os.walk 生成器函数的作用令人赞叹，不过本节专注于通用的函数：参数为任意的可迭代对象，返回值是生成器，用于生成选中的、计算出的和重新排列的元素。在下述几个表格中，我会概述其中的 24 个，有些是内置的，有些在 itertools 和 functools 模块中。为了方便，我按照函数的高阶功能分组，而不管函数是在哪里定义的。

你可能知道本节所述的全部函数，但是某些函数没有得到充分利用，因此快速概览一遍能让你知道有什么函数可用。

第一组是用于过滤的生成器函数：从输入的可迭代对象中产出元素的子集，而且不修改元素本身。本章前面的 14.8.1 节用过 itertools.takewhile 函数。与 takewhile 函数一样，表 14-1 中的大多数函数都接受一个断言参数（predicate）。这个参数是个布尔函数，有一个参数，会应用到输入中的每个元素上，用于判断元素是否包含在输出中。

表 14-1：用于过滤的生成器函数

模块

函数

说明

itertools

compress(it, selector_it)

并行处理两个可迭代的对象；如果 selector_it 中的元素是真值，产出 it 中对应的元素

itertools

dropwhile(predicate, it)

处理 it，跳过 predicate 的计算结果为真值的元素，然后产出剩下的各个元素（不再进一步检查）

（内置）

filter(predicate, it)

把 it 中的各个元素传给 predicate，如果 predicate (item) 返回真值，那么产出对应的元素；如果 predicate 是 None，那么只产出真值元素

itertools

filterfalse(predicate, it)

与 filter 函数的作用类似，不过 predicate 的逻辑是相反的：predicate 返回假值时产出对应的元素

itertools

islice (it, stop) 或 islice (it, start, stop, step=1)

产出 it 的切片，作用类似于 s [:stop] 或 s [start:stop:step]，不过 it 可以是任何可迭代的对象，而且这个函数实现的是惰性操作

itertools

takewhile(predicate, it)

predicate 返回真值时产出对应的元素，然后立即停止，不再继续检查

示例 14-14 在控制台中演示表 14-1 中各个函数的用法。

示例 14-14　演示用于过滤的生成器函数

>>> def vowel(c): ... return c.lower() in 'aeiou' ... >>> list(filter(vowel, 'Aardvark')) ['A', 'a', 'a'] >>> import itertools >>> list(itertools.filterfalse(vowel, 'Aardvark')) ['r', 'd', 'v', 'r', 'k'] >>> list(itertools.dropwhile(vowel, 'Aardvark')) ['r', 'd', 'v', 'a', 'r', 'k'] >>> list(itertools.takewhile(vowel, 'Aardvark')) ['A', 'a'] >>> list(itertools.compress('Aardvark', (1,0,1,1,0,1))) ['A', 'r', 'd', 'a'] >>> list(itertools.islice('Aardvark', 4)) ['A', 'a', 'r', 'd'] >>> list(itertools.islice('Aardvark', 4, 7)) ['v', 'a', 'r'] >>> list(itertools.islice('Aardvark', 1, 7, 2)) ['a', 'd', 'a']

下一组是用于映射的生成器函数：在输入的单个可迭代对象（map 和 starmap 函数处理多个可迭代的对象）中的各个元素上做计算，然后返回结果。12 表 14-2 中的生成器函数会从输入的可迭代对象中的各个元素中产出一个元素。如果输入来自多个可迭代的对象，第一个可迭代的对象到头后就停止输出。

12 这里所说的「映射」与字典没有关系，而与内置的 map 函数有关。

表 14-2：用于映射的生成器函数

模块

函数

说明

itertools

accumulate(it, [func])

产出累积的总和；如果提供了 func，那么把前两个元素传给它，然后把计算结果和下一个元素传给它，以此类推，最后产出结果

（内置）

enumerate(iterable, start=0)

产出由两个元素组成的元组，结构是 (index, item)，其中 index 从 start 开始计数，item 则从 iterable 中获取

（内置）

map(func, it1, [it2, ..., itN])

把 it 中的各个元素传给 func，产出结果；如果传入 N 个可迭代的对象，那么 func 必须能接受 N 个参数，而且要并行处理各个可迭代的对象

itertools

starmap(func, it)

把 it 中的各个元素传给 func，产出结果；输入的可迭代对象应该产出可迭代的元素 iit，然后以 func (*iit) 这种形式调用 func

示例 14-15 演示 itertools.accumulate 函数的几个用法。

示例 14-15　演示 itertools.accumulate 生成器函数

>>> sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1] >>> import itertools >>> list(itertools.accumulate(sample)) # ➊ [5, 9, 11, 19, 26, 32, 35, 35, 44, 45] >>> list(itertools.accumulate(sample, min)) # ➋ [5, 4, 2, 2, 2, 2, 2, 0, 0, 0] >>> list(itertools.accumulate(sample, max)) # ➌ [5, 5, 5, 8, 8, 8, 8, 8, 9, 9] >>> import operator >>> list(itertools.accumulate(sample, operator.mul)) # ➍ [5, 20, 40, 320, 2240, 13440, 40320, 0, 0, 0] >>> list(itertools.accumulate(range(1, 11), operator.mul)) [1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800] # ➎

❶ 计算总和。

❷ 计算最小值。

❸ 计算最大值。

❹ 计算乘积。

❺ 从 1! 到 10!，计算各个数的阶乘。

表 14-2 中剩余函数的演示如示例 14-16 所示。

示例 14-16　演示用于映射的生成器函数

>>> list(enumerate('albatroz', 1)) # ➊ [(1, 'a'), (2, 'l'), (3, 'b'), (4, 'a'), (5, 't'), (6, 'r'), (7, 'o'), (8, 'z')] >>> import operator >>> list(map(operator.mul, range(11), range(11))) # ➋ [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100] >>> list(map(operator.mul, range(11), [2, 4, 8])) # ➌ [0, 4, 16] >>> list(map(lambda a, b: (a, b), range(11), [2, 4, 8])) # ➍ [(0, 2), (1, 4), (2, 8)] >>> import itertools >>> list(itertools.starmap(operator.mul, enumerate('albatroz', 1))) # ➎ ['a', 'll', 'bbb', 'aaaa', 'ttttt', 'rrrrrr', 'ooooooo', 'zzzzzzzz'] >>> sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1] >>> list(itertools.starmap(lambda a, b: b/a, ... enumerate(itertools.accumulate(sample), 1))) # ➏ [5.0, 4.5, 3.6666666666666665, 4.75, 5.2, 5.333333333333333, 5.0, 4.375, 4.888888888888889, 4.5]

❶ 从 1 开始，为单词中的字母编号。

❷ 从 0 到 10，计算各个整数的平方。

❸ 计算两个可迭代对象中对应位置上的两个元素之积，元素最少的那个可迭代对象到头后就停止。

❹ 作用等同于内置的 zip 函数。

❺ 从 1 开始，根据字母所在的位置，把字母重复相应的次数。

❻ 计算平均值。

接下来这一组是用于合并的生成器函数，这些函数都从输入的多个可迭代对象中产出元素。chain 和 chain.from_iterable 按顺序（一个接一个）处理输入的可迭代对象，而 product、zip 和 zip_longest 并行处理输入的各个可迭代对象。如表 14-3 所示。

表 14-3：合并多个可迭代对象的生成器函数

模块

函数

说明

itertools

chain(it1, ..., itN)

先产出 it1 中的所有元素，然后产出 it2 中的所有元素，以此类推，无缝连接在一起

itertools

chain.from_iterable(it)

产出 it 生成的各个可迭代对象中的元素，一个接一个，无缝连接在一起；it 应该产出可迭代的元素，例如可迭代的对象列表

itertools

product(it1, ..., itN, repeat=1)

计算笛卡儿积：从输入的各个可迭代对象中获取元素，合并成由 N 个元素组成的元组，与嵌套的 for 循环效果一样；repeat 指明重复处理多少次输入的可迭代对象

（内置）

zip(it1, ..., itN)

并行从输入的各个可迭代对象中获取元素，产出由 N 个元素组成的元组，只要有一个可迭代的对象到头了，就默默地停止

itertools

zip_longest(it1, ..., itN, fillvalue=None)

并行从输入的各个可迭代对象中获取元素，产出由 N 个元素组成的元组，等到最长的可迭代对象到头后才停止，空缺的值使用 fillvalue 填充

示例 14-17 展示 itertools.chain 和 zip 生成器函数及其同胞的用法。再次提醒，zip 函数的名称出自 zip fastener 或 zipper（拉链，与 ZIP 压缩没有关系）。「出色的 zip 函数」附注栏介绍过 zip 和 itertools.zip_longest 函数。

示例 14-17　演示用于合并的生成器函数

>>> list(itertools.chain('ABC', range(2))) # ➊ ['A', 'B', 'C', 0, 1] >>> list(itertools.chain(enumerate('ABC'))) # ➋ [(0, 'A'), (1, 'B'), (2, 'C')] >>> list(itertools.chain.from_iterable(enumerate('ABC'))) # ➌ [0, 'A', 1, 'B', 2, 'C'] >>> list(zip('ABC', range(5))) # ➍ [('A', 0), ('B', 1), ('C', 2)] >>> list(zip('ABC', range(5), [10, 20, 30, 40])) # ➎ [('A', 0, 10), ('B', 1, 20), ('C', 2, 30)] >>> list(itertools.zip_longest('ABC', range(5))) # ➏ [('A', 0), ('B', 1), ('C', 2), (None, 3), (None, 4)] >>> list(itertools.zip_longest('ABC', range(5), fillvalue='?')) # ➐ [('A', 0), ('B', 1), ('C', 2), ('?', 3), ('?', 4)]

❶ 调用 chain 函数时通常传入两个或更多个可迭代对象。

❷ 如果只传入一个可迭代的对象，那么 chain 函数没什么用。

❸ 但是 chain.from_iterable 函数从可迭代的对象中获取每个元素，然后按顺序把元素连接起来，前提是各个元素本身也是可迭代的对象。

❹ zip 常用于把两个可迭代的对象合并成一系列由两个元素组成的元组。

❺ zip 可以并行处理任意数量个可迭代的对象，不过只要有一个可迭代的对象到头了，生成器就停止。

❻ itertools.zip_longest 函数的作用与 zip 类似，不过输入的所有可迭代对象都会处理到头，如果需要会填充 None。

❼ fillvalue 关键字参数用于指定填充的值。

itertools.product 生成器是计算笛卡儿积的惰性方式；在 2.2.3 节，我们在多个 for 子句中使用列表推导计算过笛卡儿积。此外，也可以使用包含多个 for 子句的生成器表达式，以惰性方式计算笛卡儿积。示例 14-18 演示 itertools.product 函数的用法。

示例 14-18　演示 itertools.product 生成器函数

>>> list(itertools.product('ABC', range(2))) # ➊ [('A', 0), ('A', 1), ('B', 0), ('B', 1), ('C', 0), ('C', 1)] >>> suits = 'spades hearts diamonds clubs'.split() >>> list(itertools.product('AK', suits)) # ➋ [('A', 'spades'), ('A', 'hearts'), ('A', 'diamonds'), ('A', 'clubs'), ('K', 'spades'), ('K', 'hearts'), ('K', 'diamonds'), ('K', 'clubs')] >>> list(itertools.product('ABC')) # ➌ [('A',), ('B',), ('C',)] >>> list(itertools.product('ABC', repeat=2)) # ➍ [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')] >>> list(itertools.product(range(2), repeat=3)) [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)] >>> rows = itertools.product('AB', range(2), repeat=2) >>> for row in rows: print(row) ... ('A', 0, 'A', 0) ('A', 0, 'A', 1) ('A', 0, 'B', 0) ('A', 0, 'B', 1) ('A', 1, 'A', 0) ('A', 1, 'A', 1) ('A', 1, 'B', 0) ('A', 1, 'B', 1) ('B', 0, 'A', 0) ('B', 0, 'A', 1) ('B', 0, 'B', 0) ('B', 0, 'B', 1) ('B', 1, 'A', 0) ('B', 1, 'A', 1) ('B', 1, 'B', 0) ('B', 1, 'B', 1)

❶ 三个字符的字符串与两个整数的值域得到的笛卡儿积是六个元组（因为 3 * 2 等于 6）。

❷ 两张牌（'AK'）与四种花色得到的笛卡儿积是八个元组。

❸ 如果传入一个可迭代的对象，product 函数产出的是一系列只有一个元素的元组，不是特别有用。

❹ repeat=N 关键字参数告诉 product 函数重复 N 次处理输入的各个可迭代对象。

有些生成器函数会从一个元素中产出多个值，扩展输入的可迭代对象，如表 14-4 所示。

表 14-4：把输入的各个元素扩展成多个输出元素的生成器函数

模块

函数

说明

itertools

combinations(it, out_len)

把 it 产出的 out_len 个元素组合在一起，然后产出

itertools

combinations_with_replacement(it, out_len)

把 it 产出的 out_len 个元素组合在一起，然后产出，包含相同元素的组合

itertools

count(start=0, step=1)

从 start 开始不断产出数字，按 step 指定的步幅增加

itertools

cycle(it)

从 it 中产出各个元素，存储各个元素的副本，然后按顺序重复不断地产出各个元素

itertools

permutations(it, out_len=None)

把 out_len 个 it 产出的元素排列在一起，然后产出这些排列；out_len 的默认值等于 len (list (it))

itertools

repeat(item, [times])

重复不断地产出指定的元素，除非提供 times，指定次数

itertools 模块中的 count 和 repeat 函数返回的生成器「无中生有」：这两个函数都不接受可迭代的对象作为输入。14.8.1 节见过 itertools.count 函数。cycle 生成器会备份输入的可迭代对象，然后重复产出对象中的元素。示例 14-19 演示 count、repeat 和 cycle 的用法。

示例 14-19　演示 count、repeat 和 cycle 的用法

>>> ct = itertools.count() # ➊ >>> next(ct) # ➋ 0 >>> next(ct), next(ct), next(ct) # ➌ (1, 2, 3) >>> list(itertools.islice(itertools.count(1, .3), 3)) # ➍ [1, 1.3, 1.6] >>> cy = itertools.cycle('ABC') # ➎ >>> next(cy) 'A' >>> list(itertools.islice(cy, 7)) # ➏ ['B', 'C', 'A', 'B', 'C', 'A', 'B'] >>> rp = itertools.repeat(7) # ➐ >>> next(rp), next(rp) (7, 7) >>> list(itertools.repeat(8, 4)) # ➑ [8, 8, 8, 8] >>> list(map(operator.mul, range(11), itertools.repeat(5))) # ➒ [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]

❶ 使用 count 函数构建 ct 生成器。

❷ 获取 ct 中的第一个元素。

❸ 不能使用 ct 构建列表，因为 ct 是无穷的，所以我获取接下来的 3 个元素。

❹ 如果使用 islice 或 takewhile 函数做了限制，可以从 count 生成器中构建列表。

❺ 使用 'ABC' 构建一个 cycle 生成器，然后获取第一个元素 ——'A'。

❻ 只有受到 islice 函数的限制，才能构建列表；这里获取接下来的 7 个元素。

❼ 构建一个 repeat 生成器，始终产出数字 7。

❽ 传入 times 参数可以限制 repeat 生成器生成的元素数量：这里会生成 4 次数字 8。

❾ repeat 函数的常见用途：为 map 函数提供固定参数，这里提供的是乘数 5。

在 itertools 模块的文档中，combinations、combinations_with_replacement 和 permutations 生成器函数，连同 product 函数，称为组合学生成器（combinatoric generator）。itertools.product 函数和其余的组合学函数有紧密的联系，如示例 14-20 所示。

示例 14-20　组合学生成器函数会从输入的各个元素中产出多个值

>>> list(itertools.combinations('ABC', 2)) # ➊ [('A', 'B'), ('A', 'C'), ('B', 'C')] >>> list(itertools.combinations_with_replacement('ABC', 2)) # ➋ [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')] >>> list(itertools.permutations('ABC', 2)) # ➌ [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')] >>> list(itertools.product('ABC', repeat=2)) # ➍ [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')]

❶ 'ABC' 中每两个元素（len ()==2）的各种组合；在生成的元组中，元素的顺序无关紧要（可以视作集合）。

❷ 'ABC' 中每两个元素（len ()==2）的各种组合，包括相同元素的组合。

❸ 'ABC' 中每两个元素（len ()==2）的各种排列；在生成的元组中，元素的顺序有重要意义。

❹ 'ABC' 和 'ABC'（repeat=2 的效果）的笛卡儿积。

本节要讲的最后一组生成器函数用于产出输入的可迭代对象中的全部元素，不过会以某种方式重新排列。其中有两个函数会返回多个生成器，分别是 itertools.groupby 和 itertools.tee。这一组里的另一个生成器函数，内置的 reversed 函数，是本节所述的函数中唯一一个不接受可迭代的对象，而只接受序列为参数的函数。这在情理之中，因为 reversed 函数从后向前产出元素，而只有序列的长度已知时才能工作。不过，这个函数会按需产出各个元素，因此无需创建反转的副本。我把 itertools.product 函数划分为用于合并的生成器，列在表 14-3 中，因为那一组函数都处理多个可迭代的对象，而表 14-5 中的生成器最多只能接受一个可迭代的对象。

表 14-5：用于重新排列元素的生成器函数

模块

函数

说明

itertools

groupby(it,key=None)

产出由两个元素组成的元素，形式为 (key, group)，其中 key 是分组标准，group 是生成器，用于产出分组里的元素

（内置）

reversed(seq)

从后向前，倒序产出 seq 中的元素；seq 必须是序列，或者是实现了 __reversed__ 特殊方法的对象

itertools

tee(it, n=2)

产出一个由 n 个生成器组成的元组，每个生成器用于单独产出输入的可迭代对象中的元素

示例 14-21 演示 itertools.groupby 函数和内置的 reversed 函数的用法。注意，itertools.groupby 假定输入的可迭代对象要使用分组标准排序；即使不排序，至少也要使用指定的标准分组各个元素。

示例 14-21　itertools.groupby 函数的用法

>>> list(itertools.groupby('LLLLAAGGG')) # ➊ [('L', <itertools._grouper object at 0x102227cc0>), ('A', <itertools._grouper object at 0x102227b38>), ('G', <itertools._grouper object at 0x102227b70>)] >>> for char, group in itertools.groupby('LLLLAAAGG'): # ➋ ... print(char, '->', list(group)) ... L -> ['L', 'L', 'L', 'L'] A -> ['A', 'A',] G -> ['G', 'G', 'G'] >>> animals = ['duck', 'eagle', 'rat', 'giraffe', 'bear', ... 'bat', 'dolphin', 'shark', 'lion'] >>> animals.sort(key=len) # ➌ >>> animals ['rat', 'bat', 'duck', 'bear', 'lion', 'eagle', 'shark', 'giraffe', 'dolphin'] >>> for length, group in itertools.groupby(animals, len): # ➍ ... print(length, '->', list(group)) ... 3 -> ['rat', 'bat'] 4 -> ['duck', 'bear', 'lion'] 5 -> ['eagle', 'shark'] 7 -> ['giraffe', 'dolphin'] >>> for length, group in itertools.groupby(reversed(animals), len): # ➎ ... print(length, '->', list(group)) ... 7 -> ['dolphin', 'giraffe'] 5 -> ['shark', 'eagle'] 4 -> ['lion', 'bear', 'duck'] 3 -> ['bat', 'rat'] >>>

❶ groupby 函数产出 (key, group_generator) 这种形式的元组。

❷ 处理 groupby 函数返回的生成器要嵌套迭代：这里在外层使用 for 循环，内层使用列表推导。

❸ 为了使用 groupby 函数，要排序输入；这里按照单词的长度排序。

❹ 再次遍历 key 和 group 值对，把 key 显示出来，并把 group 扩展成列表。

❺ 这里使用 reverse 生成器从右向左迭代 animals。

这一组里的最后一个生成器函数是 iterator.tee，这个函数只有一个作用：从输入的一个可迭代对象中产出多个生成器，每个生成器都可以产出输入的各个元素。产出的生成器可以单独使用，如示例 14-22 所示。

示例 14-22　itertools.tee 函数产出多个生成器，每个生成器都可以产出输入的各个元素

>>> list(itertools.tee('ABC')) [<itertools._tee object at 0x10222abc8>, <itertools._tee object at 0x10222ac08>] >>> g1, g2 = itertools.tee('ABC') >>> next(g1) 'A' >>> next(g2) 'A' >>> next(g2) 'B' >>> list(g1) ['B', 'C'] >>> list(g2) ['C'] >>> list(zip(*itertools.tee('ABC'))) [('A', 'A'), ('B', 'B'), ('C', 'C')]

注意，这一节的示例多次把不同的生成器函数组合在一起使用。这是这些函数的优秀特性：这些函数的参数都是生成器，而返回的结果也是生成器，因此能以很多不同的方式结合在一起使用。

既然讲到了这个话题，那就介绍一下 Python 3.3 中新出现的 yield from 语句。这个语句的作用就是把不同的生成器结合在一起使用。

14.10　Python 3.3 中新出现的句法：yield from

如果生成器函数需要产出另一个生成器生成的值，传统的解决方法是使用嵌套的 for 循环。

例如，下面是我们自己实现的 chain 生成器：13

13 标准库中的 itertools.chain 函数是使用 C 语言编写的。

>>> def chain(*iterables): ... for it in iterables: ... for i in it: ... yield i ... >>> s = 'ABC' >>> t = tuple(range(3)) >>> list(chain(s, t)) ['A', 'B', 'C', 0, 1, 2]

chain 生成器函数把操作依次交给接收到的各个可迭代对象处理。为此，「PEP 380 — Syntax for Delegating to a Subgenerator」引入了一个新句法，如下述控制台中的代码清单所示：

>>> def chain(*iterables): ... for i in iterables: ... yield from i ... >>> list(chain(s, t)) ['A', 'B', 'C', 0, 1, 2]

可以看出，yield from i 完全代替了内层的 for 循环。在这个示例中使用 yield from 是对的，而且代码读起来更顺畅，不过感觉更像是语法糖。除了代替循环之外，yield from 还会创建通道，把内层生成器直接与外层生成器的客户端联系起来。把生成器当成协程使用时，这个通道特别重要，不仅能为客户端代码生成值，还能使用客户端代码提供的值。第 16 章会深入讲解协程，其中有几页会说明为什么 yield from 不只是语法糖而已。

一瞥 yield from 之后，我们回过头继续复习标准库中善于处理可迭代对象的函数。

14.11　可迭代的归约函数

表 14-6 中的函数都接受一个可迭代的对象，然后返回单个结果。这些函数叫「归约」函数、「合拢」函数或「累加」函数。其实，这里列出的每个内置函数都可以使用 functools.reduce 函数实现，内置是因为使用它们便于解决常见的问题。此外，对 all 和 any 函数来说，有一项重要的优化措施是 reduce 函数做不到的：这两个函数会短路（即一旦确定了结果就立即停止使用迭代器）。参见示例 14-23 中 any 函数的最后一个测试。

表 14-6：读取迭代器，返回单个值的内置函数

模块

函数

说明

（内置）

all(it)

it 中的所有元素都为真值时返回 True，否则返回 False；all ([]) 返回 True

（内置）

any(it)

只要 it 中有元素为真值就返回 True，否则返回 False；any ([]) 返回 False

（内置）

max(it, [key=,] [default=])

返回 it 中值最大的元素；*key 是排序函数，与 sorted 函数中的一样；如果可迭代的对象为空，返回 default

（内置）

min(it, [key=,] [default=])

返回 it 中值最小的元素；#key 是排序函数，与 sorted 函数中的一样；如果可迭代的对象为空，返回 default

functools

reduce(func, it, [initial])

把前两个元素传给 func，然后把计算结果和第三个元素传给 func，以此类推，返回最后的结果；如果提供了 initial，把它当作第一个元素传入

（内置）

sum(it, start=0)

it 中所有元素的总和，如果提供可选的 start，会把它加上（计算浮点数的加法时，可以使用 math.fsum 函数提高精度）

* 也可以像这样调用：max (arg1, arg2, ..., [key=?])，此时返回参数中的最大值。

# 也可以像这样调用：min (arg1, arg2, ..., [key=?])，此时返回参数中的最小值。

all 和 any 函数的操作演示如示例 14-23 所示。

示例 14-23　把几个序列传给 all 和 any 函数后得到的结果

>>> all([1, 2, 3]) True >>> all([1, 0, 3]) False >>> all([]) True >>> any([1, 2, 3]) True >>> any([1, 0, 3]) True >>> any([0, 0.0]) False >>> any([]) False >>> g = (n for n in [0, 0.0, 7, 8]) >>> any(g) True >>> next(g) 8

10.6 节更为深入地解释过 functools.reduce 函数。

还有一个内置的函数接受一个可迭代的对象，返回不同的值 ——sorted。reversed 是生成器函数，与此不同，sorted 会构建并返回真正的列表。毕竟，要读取输入的可迭代对象中的每一个元素才能排序，而且排序的对象是列表，因此 sorted 操作完成后返回排序后的列表。我在这里提到 sorted，是因为它可以处理任意的可迭代对象。

当然，sorted 和这些归约函数只能处理最终会停止的可迭代对象。否则，这些函数会一直收集元素，永远无法返回结果。

下面，我们回过头来分析内置的 iter () 函数，它还有一个鲜为人知的特性没有介绍。

14.12　深入分析 iter 函数

如前所述，在 Python 中迭代对象 x 时会调用 iter (x)。

可是，iter 函数还有一个鲜为人知的用法：传入两个参数，使用常规的函数或任何可调用的对象创建迭代器。这样使用时，第一个参数必须是可调用的对象，用于不断调用（没有参数），产出各个值；第二个值是哨符，这是个标记值，当可调用的对象返回这个值时，触发迭代器抛出 StopIteration 异常，而不产出哨符。

下述示例展示如何使用 iter 函数掷骰子，直到掷出 1 点为止：14

14 需要在这个示例的最前面添加一句：from random import randint。—— 编者注

>>> def d6(): ... return randint(1, 6) ... >>> d6_iter = iter(d6, 1) >>> d6_iter <callable_iterator object at 0x00000000029BE6A0> >>> for roll in d6_iter: ... print(roll) ... 4 3 6 3

注意，这里的 iter 函数返回一个 callable_iterator 对象。示例中的 for 循环可能运行特别长的时间，不过肯定不会打印 1，因为 1 是哨符。与常规的迭代器一样，这个示例中的 d6_iter 对象一旦耗尽就没用了。如果想重新开始，必须再次调用 iter (...)，重新构建迭代器。

内置函数 iter 的文档中有个实用的例子。这段代码逐行读取文件，直到遇到空行或者到达文件末尾为止：

with open('mydata.txt') as fp: for line in iter(fp.readline, '\n'): process_line(line)

结束本章之前，我要举个实用的例子，说明如何使用生成器高效处理大量数据。

14.13　案例分析：在数据库转换工具中使用生成器

几年前，我在 BIREME 工作，这是 PAHO/WHO（Pan-American Health Organization/World Health Organization，泛美卫生组织 / 世界卫生组织）在圣保罗运营的一家数字图书馆。BIREME 制作的众多书目数据集中包含 LILACS（Latin American and Caribbean Health Sciences index，拉美和加勒比地区健康科学索引）和 SciELO（Scientific Electronic Library Online，电子科学在线图书馆），这两个数据库完整索引了这一地区发布的科学和技术作品。

从 20 世纪 80 年代后期开始，管理 LILACS 的数据库系统是 CDS/ISIS。这是 UNESCO 开发的非关系型文档数据库，后来为了在 GNU/Linux 服务器上运行，BIREME 使用 C 语言重写了。我的工作之一是探索替代方案，把 LILACS 移植到现代的开源文档数据库（最终还要移植大得多的 SciELO），例如 CouchDB 或 MongoDB。

在探索的过程中，我编写了一个 Python 脚本 ——isis2json.py，把 CDS/ISIS 文件转换成适合导入 CouchDB 或 MongoDB 的 JSON 文件。起初，这个脚本读取文件的是 CDS/ISIS 导出的 ISO-2709 格式。读写过程必须采用渐进方式，因为完整的数据集比主内存大得多。解决方法很简单：主 for 循环每次迭代时从 .iso 文件中读取一个记录，转换后将其写入 .json 文件。

然而，在实际操作中有必要让 isis2json.py 支持 CDS/ISIS 的另一种数据格式 ——BIREME 在生产环境中使用的二进制 .mst 文件，避免导出为 ISO-2709 格式时消耗过多资源。

现在我遇到一个问题：用来读取 ISO-2709 和 .mst 文件的库提供的 API 差别很大。而输出 JSON 格式的循环已经很复杂了，因为这个脚本要接受多个命令行选项，每次输出时调整记录的结构。在同一个 for 循环中使用两个不同的 API，同时还要生成 JSON，这样太难以管理了。

解决方法是隔离读取逻辑，写进两个生成器函数中：一个函数支持一种输入格式。最终，我把 isis2json.py 脚本分成了四个函数。使用 Python 2 编写的主脚本如示例 A-5，带依赖的完整源码在 GitHub 中的 fluentpython/isis2json 仓库里。

下面概览这个脚本的结构。

main

main 函数使用 argparse 模块读取命令行选项，用于配置输出记录的结构。根据输入文件的扩展名，main 函数会选择一个合适的生成器函数，逐个读取数据，然后产出记录。

iter_iso_records

这个生成器函数用于读取 .iso 文件（假设是 ISO-2709 格式），有两个参数：一个是文件名；另一个是 isis_json_type，即一个与记录结构有关的选项。在这个函数的 for 循环中，每次迭代读取一个记录，然后创建一个空字典，把数据填充进字段之后产出字典。

iter_mst_records

这也是一个生成器函数，用于读取 .mst 文件。15 阅读 isis2json.py 脚本的源码后你会发现，这个函数没有 iter_iso_records 函数简单，不过接口和整体结构是相同的：参数是文件名和 isis_json_type，for 循环每次迭代时构建并产出一个字典，表示一个记录。

15 用来读取复杂的 .mst 二进制文件的库其实是用 Java 编写的，因此只有使用 Jython 解释器 2.5 或以上版本执行 isis2json.py 脚本才能使用这个功能。详情参见仓库里的 README.rst 文件。因为依赖在需要使用的生成器函数中导入，所以即便只有一个外部依赖可用，这个脚本仍能运行。

write_json

这个函数把记录输出为 JSON 格式，而且一次输出一个记录。它的参数很多，其中第一个参数（input_gen）是对某个生成器函数的引用：iter_iso_records 或 iter_mst_records。write_json 函数的主 for 循环迭代 input_gen 引用的生成器产出的字典，根据命令行选项设定的方式处理，然后把 JSON 格式的记录附加到输出文件里。

我利用生成器函数解耦了读逻辑和写逻辑。当然，解耦二者最简单的方式是，把所有记录读进内存，然后写入硬盘。可是这样并不可行，因为数据集很大。而使用生成器的话，可以交叉读写，因此这个脚本可以处理任意大小的文件。

现在，如果 isis2json.py 脚本需要再支持一种输入格式，比如说美国国会图书馆用于表示 ISO-2709 格式数据的 MARCXML 文档格式，只需再添加一个生成器函数，实现读逻辑，而复杂的 write_json 函数无需任何改动。

这不是什么尖端科技，可是通过这个实例我们看到了生成器的灵活性。使用生成器处理数据库时，我们把记录看成数据流，这样消耗的内存量最低，而且不管数据有多大都能处理。只要管理着大型数据集，都有可能在实践中找到机会使用生成器。

下一节讨论暂时要跳过的一个生成器特性。为什么要跳过呢？原因如下。

14.14　把生成器当成协程

Python 2.2 引入了 yield 关键字实现的生成器函数，大约五年后，Python 2.5 实现了「PEP 342 — Coroutines via Enhanced Generators」。这个提案为生成器对象添加了额外的方法和功能，其中最值得关注的是 .send () 方法。

与 .__next__() 方法一样，.send () 方法致使生成器前进到下一个 yield 语句。不过，.send () 方法还允许使用生成器的客户把数据发给自己，即不管传给 .send () 方法什么参数，那个参数都会成为生成器函数定义体中对应的 yield 表达式的值。也就是说，.send () 方法允许在客户代码和生成器之间双向交换数据。而 .__next__() 方法只允许客户从生成器中获取数据。

这是一项重要的「改进」，甚至改变了生成器的本性：像这样使用的话，生成器就变身为协程。在 PyCon US 2009 期间举办的一场著名的课程中，David Beazley（可能是 Python 社区中在协程方面最多产的作者和演讲者）提醒道：

生成器用于生成供迭代的数据

协程是数据的消费者

为了避免脑袋炸裂，不能把这两个概念混为一谈

协程与迭代无关

注意，虽然在协程中会使用 yield 产出值，但这与迭代无关 16

——David Beazley

「A Curious Course on Coroutines and Concurrency」

16 摘自「A Curious Course on Coroutines and Concurrency」的第 33 张幻灯片，题为「Keeping It Straight」。

我会遵从他的建议，至此结束本章（因为本章真正讨论的是迭代技术），而不涉及把生成器当成协程使用的 send 方法和其他特性。第 16 章会讨论协程。

14.15　本章小结

Python 语言对迭代的支持如此深入，因此我经常说，Python 已经融合（grok）了迭代器。17Python 从语义上集成迭代器模式是个很好的例证，说明设计模式在各种编程语言中使用的方式并不相同。在 Python 中，自己动手实现的典型迭代器（如示例 14-4 所示）没有实际用途，只能用作教学示例。

17 根据新黑客字典（Jargon file），grok 的意思不仅是学会了新知识，还要充分吸收知识，做到「人剑合一」。

本章中编写了一个类的几个版本，用于读取内容可能很多的文件，并迭代里面的单词。因为用了生成器，所以在重构的过程中，Sentence 类越来越简短，越来越易于阅读。最终，我们知道了生成器的工作原理。

后来，我们编写了一个用于生成等差数列的生成器，还说明了如何利用 itertools 模块做简化。随后，概览了标准库中 24 个通用的生成器函数。

接着，我们分析了内置的 iter 函数：首先说明，以 iter (o) 的形式调用时返回的是迭代器；之后分析，以 iter (func, sentinel) 的形式调用时，能使用任何函数构建迭代器。

分析实例时，我说明了一个数据库转换工具的实现方式，指明如何使用生成器函数解耦读写逻辑，如何高效处理大型数据集，以及如何轻易支持多种数据输入格式。

本章还提到了 Python 3.3 中新出现的 yield from 句法，还有协程。这里只对二者做了简单介绍，本书后面会更为深入地讨论。

14.16　延伸阅读

在 Python 语言参考手册中，「6.2.9. Yield expressions」从技术层面深入说明了生成器。定义生成器函数的 PEP 是「PEP 255—Simple Generators」。

itertools 模块的文档写得很棒，包含大量示例。虽然那个模块里的函数是使用 C 语言实现的，不过文档展示了如何使用 Python 实现部分函数，这通常要利用模块里的其他函数。用法示例也很好，例如，有一个代码片段说明如何使用 accumulate 函数计算带利息的分期还款，得出每次要还多少。文档中还有一节是「Itertools Recipes」，说明如何使用 itertools 模块中的现有函数实现额外的高性能函数。

在 David Beazley 与 Brian K. Jones 的《Python Cookbook（第 3 版）中文版》一书中，第 4 章有 16 个诀窍涵盖了这个话题，虽然角度不同，但都关注实际应用。

「What's New in Python 3.3」（参见「PEP 380: Syntax for Delegating to a Subgenerator」）通过示例说明了 yield from 句法。本书 16.7 节和 16.8 节还会讨论这个句法。

如果你对文档数据库感兴趣，想进一步了解 14.13 节的背景，可以阅读我发布在 Code4Lib Journal（涵盖图书馆与技术交集）上的论文，题为「From ISIS to CouchDB: Databases and Data Models for Bibliographic Records」，其中有一节对 isis2json.py 脚本做了说明。这篇论文的剩余内容说明文档数据库（如 CouchDB 和 MongoDB）实现半结构化数据模型的方式，以及为什么这种模型比关系模型更适合用于收集书目数据。

杂谈

生成器函数的语法糖多一些更好

在设计不同目的的控制和显示设备时，设计师需要确认它们之间具有明显差异。

——Donald Norman

《设计心理学》

在编程语言中，源码是「控制和显示设备」。我觉得 Python 设计得特别好，源码的可读性通常很高，好像伪代码一样。可是，没有什么是完美的。Guido van Rossum 应该遵从 Donald Norman 的建议（如上述引文），引入新的关键字，用于定义生成器函数，而不该继续使用 def。其实，「PEP 255 — Simple Generators」中的「BDFL Pronouncements」一节已经提议：

深藏于定义体中的「yield」语句不足以提醒语义发生了重大变化。

可是，Guido 讨厌引入新关键字，而且觉得这项提议没有说服力，因此我们只好被迫接受 def。

沿用函数句法定义生成器会导致几个不好的后果。在 Politz 等人发布的试验成果论文「Python, the Full Monty: A Tested Semantics for the Python Programming Language」18 中，有个简单的生成器函数示例（这篇论文的 4.1 节）：

def f(): x=0 while True: x += 1 yield x

然后，论文的作者指出，我们无法通过函数调用抽象产出这个过程（如示例 14-24 所示）。

示例 14-24　「（这样）似乎能简单地抽象产出这个过程」（Politz 等人）

def f(): def do_yield(n): yield n x = 0 while True: x += 1 do_yield(x)

如果调用示例 14-24 中的 f ()，会得到一个无限循环，而不是生成器，因为 yield 关键字只能把最近的外层函数变成生成器函数。虽然生成器函数看起来像函数，可是我们不能通过简单的函数调用把职责委托给另一个生成器函数。与此相比，Lua 语言就没有强加这一限制。在 Lua 中，协程可以调用其他函数，而且其中任何一个函数都能把职责交给原来的调用方。

Python 新引入的 yield from 句法允许生成器或协程把工作委托给第三方完成，这样就无需嵌套 for 循环作为变通了。在函数调用前面加上 yield from 能「解决」示例 14-24 中的问题，如示例 14-25 所示。

示例 14-25　这样才能简单地抽象产出这个过程

def f(): def do_yield(n): yield n x = 0 while True: x += 1 yield from do_yield(x)

沿用 def 声明生成器犯了可用性方面的错误，而 Python 2.5 引入的协程（也写成包含 yield 关键字的函数）把这个问题进一步恶化了。在协程中，yield 碰巧（通常）出现在赋值语句的右手边，因为 yield 用于接收客户传给 .send () 方法的参数。正如 David Beazley 所说的：

尽管有一些相同之处，但是生成器和协程基本上是两个不同的概念。19

我觉得协程也应该有专用的关键字。读到后文你会发现，协程经常会用到特殊的装饰器，这样就能与其他的函数区分开。可是，生成器函数不常使用装饰器，因此我们不得不扫描函数的定义体，看有没有 yield 关键字，以此判断它究竟是普通的函数，还是完全不同的洪水猛兽。

也许有人会说，这么做是为了在不增加句法的前提下支持这些特性，即便添加额外的句法，也只是「语法糖」。可是，如果能让不同的特性看起来也不同，那么我更喜欢语法糖。Lisp 代码难以阅读的主要原因就是缺少语法糖，这也导致 Lisp 语言中的所有结构看起来都像是函数调用。

生成器与迭代器的语义对比

思考迭代器与生成器之间的关系时，至少可以从三方面入手。

第一方面是接口。Python 的迭代器协议定义了两个方法：__next__ 和 __iter__。生成器对象实现了这两个方法，因此从这方面来看，所有生成器都是迭代器。由此可以得知，内置的 enumerate () 函数创建的对象是迭代器：

>>> from collections import abc >>> e = enumerate('ABC') >>> isinstance(e, abc.Iterator) True

第二方面是实现方式。从这个角度来看，生成器这种 Python 语言结构可以使用两种方式编写：含有 yield 关键字的函数，或者生成器表达式。调用生成器函数或者执行生成器表达式得到的生成器对象属于语言内部的 GeneratorType 类型（https://docs.python.org/3/library/types.html#types.GeneratorType）。从这方面来看，所有生成器都是迭代器，因为 GeneratorType 类型的实例实现了迭代器接口。不过，我们可以编写不是生成器的迭代器，方法是实现经典的迭代器模式，如示例 14-4 所示，或者使用 C 语言编写扩展。从这方面来看，enumerate 对象不是生成器：

>>> import types >>> e = enumerate('ABC') >>> isinstance(e, types.GeneratorType) False

这是因为 types.GeneratorType 类型（https://docs.python.org/3/library/types.html#types.GeneratorType）是这样定义的：「生成器－迭代器对象的类型，调用生成器函数时生成。」

第三方面是概念。根据《设计模式：可复用面向对象软件的基础》一书的定义，在典型的迭代器设计模式中，迭代器用于遍历集合，从中产出元素。迭代器可能相当复杂，例如，遍历树状数据结构。但是，不管典型的迭代器中有多少逻辑，都是从现有的数据源中读取值；而且，调用 next (it) 时，迭代器不能修改从数据源中读取的值，只能原封不动地产出值。

而生成器可能无需遍历集合就能生成值，例如 range 函数。即便依附了集合，生成器不仅能产出集合中的元素，还可能会产出派生自元素的其他值。enumerate 函数是很好的例子。根据迭代器设计模式的原始定义，enumerate 函数返回的生成器不是迭代器，因为创建的是生成器产出的元组。

从概念方面来看，实现方式无关紧要。不使用 Python 生成器对象也能编写生成器。为了表明这一点，我写了一个斐波纳契数列生成器，如示例 14-26 所示。

示例 14-26　fibo_by_hand.py：不使用 GeneratorType 实例实现斐波纳契数列生成器

class Fibonacci: 　 def __iter__(self): return FibonacciGenerator() 　 　 class FibonacciGenerator: 　 def __init__(self): self.a = 0 self.b = 1 　 def __next__(self): result = self.a self.a, self.b = self.b, self.a + self.b return result

示例 14-26 虽然可行，但只是一个愚蠢的示例。符合 Python 风格的斐波纳契数列生成器如下所示：

def fibonacci(): a, b = 0, 1 while True: yield a a, b = b, a + b

当然，始终可以使用生成器这个语言结构履行迭代器的基本职责：遍历集合，并从中产出元素。

事实上，Python 程序员不会严格区分二者，即便在官方文档中也把生成器称作迭代器。Python 词汇表对迭代器下的权威定义比较笼统，涵盖了迭代器和生成器。

迭代器：表示数据流的对象……

建议你读一下 Python 词汇表中对迭代器的完整定义。而在生成器的定义中，迭代器和生成器是同义词，「生成器」指代生成器函数，以及生成器函数构建的生成器对象。因此，在 Python 社区的行话中，迭代器和生成器在一定程度上是同义词。

Python 中最简的迭代器接口

《设计模式：可复用面向对象软件的基础》一书讲解迭代器模式时，在「实现」一节中说道：20

迭代器的最小接口由 First、Next、IsDone 和 CurrentItem 操作组成。

不过，这句话有个脚注：

甚至可以将 Next、IsDone 和 CurrentItem 并入到一个操作中，该操作前进到下一个对象并返回这个对象，如果遍历结束，那么这个操作返回一个特定的值（例如，0）标志该迭代结束。这样我们就使这个接口变得更小了。

这与 Python 的做法接近：只用一个 __next__ 方法完成这项工作。不过，为了表明迭代结束，这个方法没有使用哨符，因为哨符可能不小心被忽略，而是使用 StopIteration 异常。简单且正确，这正是 Python 之道。

18Joe Gibbs Politz, Alejandro Martinez, Matthew Milano, Sumner Warren, Daniel Patterson, Junsong Li, Anand Chitipothu, and Shriram Krishnamurthi,「Python: The Full Monty,」SIGPLAN Not. 48, 10 (October 2013), 217-232.

19「A Curious Course on Coroutines and Concurrency」，第 31 张幻灯片。

20《设计模式：可复用面向对象软件的基础》第 174 页。

第 15 章　上下文管理器和 else 块

最终，上下文管理器可能几乎与子程序（subroutine）本身一样重要。目前，我们只了解了上下文管理器的皮毛……Basic 语言有 with 语句，而且很多语言都有。但是，在各种语言中 with 语句的作用不同，而且做的都是简单的事，虽然可以避免不断使用点号查找属性，但是不会做事前准备和事后清理。不要觉得名字一样，就意味着作用也一样。with 语句是非常了不起的特性。1

——Raymond Hettinger

雄辩的 Python 布道者

1 节选自 PyCon US 2013 主题演讲「What Makes Python Awesome」；关于 with 的部分从 23:00 开始，到 26:15 结束。

本章讨论其他语言中不常见的一些流程控制特性，正因如此，Python 用户往往会忽视或没有充分使用这些特性。下面要讨论的特性有：

with 语句和上下文管理器

for、while 和 try 语句的 else 子句

with 语句会设置一个临时的上下文，交给上下文管理器对象控制，并且负责清理上下文。这么做能避免错误并减少样板代码，因此 API 更安全，而且更易于使用。除了自动关闭文件之外，with 块还有很多用途。

else 子句与 with 语句完全没有关系。可是已经写到第五部分了，我找不到其他地方介绍 else，又不能单写只有一页内容的一章，因此就在这一章讨论了。

下面从这个较小的话题开始，进入本章的实质内容。

15.1　先做这个，再做那个：if 语句之外的 else 块

这个语言特性不是什么秘密，但却没有得到重视：else 子句不仅能在 if 语句中使用，还能在 for、while 和 try 语句中使用。

for/else、while/else 和 try/else 的语义关系紧密，不过与 if/else 差别很大。起初，else 这个单词的意思阻碍了我对这些特性的理解，但是最终我习惯了。

else 子句的行为如下。

for

仅当 for 循环运行完毕时（即 for 循环没有被 break 语句中止）才运行 else 块。

while

仅当 while 循环因为条件为假值而退出时（即 while 循环没有被 break 语句中止）才运行 else 块。

try

仅当 try 块中没有异常抛出时才运行 else 块。官方文档还指出：「else 子句抛出的异常不会由前面的 except 子句处理。」

在所有情况下，如果异常或者 return、break 或 continue 语句导致控制权跳到了复合语句的主块之外，else 子句也会被跳过。

我觉得除了 if 语句之外，其他语句选择使用 else 关键字是个错误。else 蕴含着「排他性」这层意思，例如「要么运行这个循环，要么做那件事」。可是，在循环中，else 的语义恰好相反：「运行这个循环，然后做那件事。」因此，使用 then 关键字更好。then 在 try 语句的上下文中也说得通：「尝试运行这个，然后做那件事。」可是，添加新关键字属于语言的重大变化，而 Guido 唯恐避之不及。

在这些语句中使用 else 子句通常能让代码更易于阅读，而且能省去一些麻烦，不用设置控制标志或者添加额外的 if 语句。

在循环中使用 else 子句的方式如下述代码片段所示：

for item in my_list: if item.flavor == 'banana': break else: raise ValueError('No banana flavor found!')

一开始，你可能觉得没必要在 try/except 块中使用 else 子句。毕竟，在下述代码片段中，只有 dangerous_call () 不抛出异常，after_call () 才会执行，对吧？

try: dangerous_call() after_call() except OSError: log('OSError...')

然而，after_call () 不应该放在 try 块中。为了清晰和准确，try 块中应该只抛出预期异常的语句。因此，像下面这样写更好：

try: dangerous_call() except OSError: log('OSError...') else: after_call()

现在很明确，try 块防守的是 dangerous_call () 可能出现的错误，而不是 after_call ()。而且很明显，只有 try 块不抛出异常，才会执行 after_call ()。

在 Python 中，try/except 不仅用于处理错误，还常用于控制流程。为此，Python 官方词汇表还定义了一个缩略词（口号）。

EAFP

取得原谅比获得许可容易（easier to ask for forgiveness than permission）。这是一种常见的 Python 编程风格，先假定存在有效的键或属性，如果假定不成立，那么捕获异常。这种风格简单明快，特点是代码中有很多 try 和 except 语句。与其他很多语言一样（如 C 语言），这种风格的对立面是 LBYL 风格。

接下来，词汇表定义了 LBYL。

LBYL

三思而后行（look before you leap）。这种编程风格在调用函数或查找属性或键之前显式测试前提条件。与 EAFP 风格相反，这种风格的特点是代码中有很多 if 语句。在多线程环境中，LBYL 风格可能会在「检查」和「行事」的空当引入条件竞争。例如，对 if key in mapping: return mapping [key] 这段代码来说，如果在测试之后，但在查找之前，另一个线程从映射中删除了那个键，那么这段代码就会失败。这个问题可以使用锁或者 EAFP 风格解决。

如果选择使用 EAFP 风格，那就要更深入地了解 else 子句，并在 try/except 语句中合理使用。

下面探讨本章的主要话题：强大的 with 语句。

15.2　上下文管理器和 with 块

上下文管理器对象存在的目的是管理 with 语句，就像迭代器的存在是为了管理 for 语句一样。

with 语句的目的是简化 try/finally 模式。这种模式用于保证一段代码运行完毕后执行某项操作，即便那段代码由于异常、return 语句或 sys.exit () 调用而中止，也会执行指定的操作。finally 子句中的代码通常用于释放重要的资源，或者还原临时变更的状态。

上下文管理器协议包含 __enter__ 和 __exit__ 两个方法。with 语句开始运行时，会在上下文管理器对象上调用 __enter__ 方法。with 语句运行结束后，会在上下文管理器对象上调用 __exit__ 方法，以此扮演 finally 子句的角色。

最常见的例子是确保关闭文件对象。使用 with 语句关闭文件的详细说明参见示例 15-1。

示例 15-1　演示把文件对象当成上下文管理器使用

>>> with open('mirror.py') as fp: # ➊ ... src = fp.read(60) # ➋ ... >>> len(src) 60 >>> fp # ➌ <_io.TextIOWrapper name='mirror.py' mode='r' encoding='UTF-8'> >>> fp.closed, fp.encoding # ➍ (True, 'UTF-8') >>> fp.read(60) # ➎ Traceback (most recent call last): File "<stdin>", line 1, in <module> ValueError: I/O operation on closed file.

❶ fp 绑定到打开的文件上，因为文件的 __enter__ 方法返回 self。

❷ 从 fp 中读取一些数据。

❸ fp 变量仍然可用。2

2 与函数和模块不同，with 块没有定义新的作用域。

❹ 可以读取 fp 对象的属性。

❺ 但是不能在 fp 上执行 I/O 操作，因为在 with 块的末尾，调用 TextIOWrapper.__exit__ 方法把文件关闭了。

示例 15-1 中标注❶的那行代码道出了不易察觉但很重要的一点：执行 with 后面的表达式得到的结果是上下文管理器对象，不过，把值绑定到目标变量上（as 子句）是在上下文管理器对象上调用 __enter__ 方法的结果。

碰巧，示例 15-1 中的 open () 函数返回 TextIOWrapper 类的实例，而该实例的 __enter__ 方法返回 self。不过，__enter__ 方法除了返回上下文管理器之外，还可能返回其他对象。

不管控制流程以哪种方式退出 with 块，都会在上下文管理器对象上调用 __exit__ 方法，而不是在 __enter__ 方法返回的对象上调用。

with 语句的 as 子句是可选的。对 open 函数来说，必须加上 as 子句，以便获取文件的引用。不过，有些上下文管理器会返回 None，因为没什么有用的对象能提供给用户。

示例 15-2 使用一个精心制作的上下文管理器执行操作，以此强调上下文管理器与 __enter__ 方法返回的对象之间的区别。

示例 15-2　测试 LookingGlass 上下文管理器类

>>> from mirror import LookingGlass >>> with LookingGlass() as what: ➊ ... print('Alice, Kitty and Snowdrop') ➋ ... print(what) ... pordwonS dna yttiK ,ecilA ➌ YKCOWREBBAJ >>> what ➍ 'JABBERWOCKY' >>> print('Back to normal.') ➎ Back to normal.

❶ 上下文管理器是 LookingGlass 类的实例；Python 在上下文管理器上调用 __enter__ 方法，把返回结果绑定到 what 上。

❷ 打印一个字符串，然后打印 what 变量的值。

❸ 打印出的内容是反向的。

❹ 现在，with 块已经执行完毕。可以看出，__enter__ 方法返回的值 —— 即存储在 what 变量中的值 —— 是字符串 'JABBERWOCKY'。

❺ 输出不再是反向的了。

示例 15-3 是 LookingGlass 类的实现。

示例 15-3　mirror.py：LookingGlass 上下文管理器类的代码

class LookingGlass: def __enter__(self): ➊ import sys self.original_write = sys.stdout.write ➋ sys.stdout.write = self.reverse_write ➌ return 'JABBERWOCKY' ➍ def reverse_write(self, text): ➎ self.original_write(text[::-1]) def __exit__(self, exc_type, exc_value, traceback): ➏ import sys ➐ sys.stdout.write = self.original_write ➑ if exc_type is ZeroDivisionError: ➒ print('Please DO NOT divide by zero!') return True ➓ ⓫

❶ 除了 self 之外，Python 调用 __enter__ 方法时不传入其他参数。

❷ 把原来的 sys.stdout.write 方法保存在一个实例属性中，供后面使用。

❸ 为 sys.stdout.write 打猴子补丁，替换成自己编写的方法。

❹ 返回 'JABBERWOCKY' 字符串，这样才有内容存入目标变量 what。

❺ 这是用于取代 sys.stdout.write 的方法，把 text 参数的内容反转，然后调用原来的实现。

❻ 如果一切正常，Python 调用 __exit__ 方法时传入的参数是 None, None, None；如果抛出了异常，这三个参数是异常数据，如下所述。

❼ 重复导入模块不会消耗很多资源，因为 Python 会缓存导入的模块。

❽ 还原成原来的 sys.stdout.write 方法。

❾ 如果有异常，而且是 ZeroDivisionError 类型，打印一个消息……

❿ …… 然后返回 True，告诉解释器，异常已经处理了。

⓫ 如果 __exit__ 方法返回 None，或者 True 之外的值，with 块中的任何异常都会向上冒泡。

在实际使用中，如果应用程序接管了标准输出，可能会暂时把 sys.stdout 换成类似文件的其他对象，然后再切换成原来的版本。contextlib.redirect_stdout 上下文管理器就是这么做的：只需传入类似文件的对象，用于替代 sys.stdout。

解释器调用 __enter__ 方法时，除了隐式的 self 之外，不会传入任何参数。传给 __exit__ 方法的三个参数列举如下。

exc_type

异常类（例如 ZeroDivisionError）。

exc_value

异常实例。有时会有参数传给异常构造方法，例如错误消息，这些参数可以使用 exc_value.args 获取。

traceback

traceback 对象。3

3 在 try/finally 语句的 finally 块中调用 sys.exc_info ()（https://docs.python.org/3/library/sys.html#sys.exc_info），得到的就是 __exit__ 接收的这三个参数。鉴于 with 语句是为了取代大多数 try/finally 语句，而且通常需要调用 sys.exc_info () 来判断做什么清理操作，这种行为是合理的。

上下文管理器的具体工作方式参见示例 15-4。在这个示例中，我们在 with 块之外使用 LookingGlass 类，因此可以手动调用 __enter__ 和 __exit__ 方法。

示例 15-4　在 with 块之外使用 LookingGlass 类

>>> from mirror import LookingGlass >>> manager = LookingGlass() ➊ >>> manager <mirror.LookingGlass object at 0x2a578ac> >>> monster = manager.__enter__() ➋ >>> monster == 'JABBERWOCKY' ➌ eurT >>> monster 'YKCOWREBBAJ' >>> manager >ca875a2x0 ta tcejbo ssalGgnikooL.rorrim< >>> manager.__exit__(None, None, None) ➍ >>> monster 'JABBERWOCKY'

❶ 实例化并审查 manager 实例。

❷ 在上下文管理器上调用 __enter__() 方法，把结果存储在 monster 中。

❸ monster 的值是字符串 'JABBERWOCKY'。打印出的 True 标识符是反向的，因为 stdout 的所有输出都经过 __enter__ 方法中打补丁的 write 方法处理。

❹ 调用 manager.__exit__，还原成之前的 stdout.write。

上下文管理器是相当新颖的特性，Python 社区肯定还在不断寻找新的创意用法。标准库中有一些示例。

在 sqlite3 模块中用于管理事务，参见「12.6.7.3. Using the connection as a context manager」。4

在 threading 模块中用于维护锁、条件和信号，参见「17.1.10. Using locks, conditions, and semaphores in the with statement」。

为 Decimal 对象的算术运算设置环境，参见 decimal.localcontext 函数的文档。

为了测试临时给对象打补丁，参见 unittest.mock.patch 函数的文档。

4 在 Python 3.5 文档中是「12.6.8.3」。—— 编者注

标准库中还有个 contextlib 模块，提供一些实用工具，参见下一节。

15.3　contextlib 模块中的实用工具

自己定义上下文管理器类之前，先看一下 Python 标准库文档中的「29.6 contextlib — Utilities for with-statement contexts」。除了前面提到的 redirect_stdout 函数，contextlib 模块中还有一些类和其他函数，使用范围更广。

closing

如果对象提供了 close () 方法，但没有实现 __enter__/__exit__ 协议，那么可以使用这个函数构建上下文管理器。

suppress

构建临时忽略指定异常的上下文管理器。

@contextmanager

这个装饰器把简单的生成器函数变成上下文管理器，这样就不用创建类去实现管理器协议了。

ContextDecorator

这是个基类，用于定义基于类的上下文管理器。这种上下文管理器也能用于装饰函数，在受管理的上下文中运行整个函数。

ExitStack

这个上下文管理器能进入多个上下文管理器。with 块结束时，ExitStack 按照后进先出的顺序调用栈中各个上下文管理器的 __exit__ 方法。如果事先不知道 with 块要进入多少个上下文管理器，可以使用这个类。例如，同时打开任意一个文件列表中的所有文件。

显然，在这些实用工具中，使用最广泛的是 @contextmanager 装饰器，因此要格外留心。这个装饰器也有迷惑人的一面，因为它与迭代无关，却要使用 yield 语句。由此可以引出协程，这是下一章的主题。

15.4　使用 @contextmanager

@contextmanager 装饰器能减少创建上下文管理器的样板代码量，因为不用编写一个完整的类，定义 __enter__ 和 __exit__ 方法，而只需实现有一个 yield 语句的生成器，生成想让 __enter__ 方法返回的值。

在使用 @contextmanager 装饰的生成器中，yield 语句的作用是把函数的定义体分成两部分：yield 语句前面的所有代码在 with 块开始时（即解释器调用 __enter__ 方法时）执行，yield 语句后面的代码在 with 块结束时（即调用 __exit__ 方法时）执行。

下面举个例子。示例 15-5 使用一个生成器函数代替示例 15-3 中定义的 LookingGlass 类。

示例 15-5　mirror_gen.py：使用生成器实现的上下文管理器

import contextlib @contextlib.contextmanager ➊ def looking_glass(): import sys original_write = sys.stdout.write ➋ def reverse_write(text): ➌ original_write(text[::-1]) sys.stdout.write = reverse_write ➍ yield 'JABBERWOCKY' ➎ sys.stdout.write = original_write ➏

❶ 应用 contextmanager 装饰器。

❷ 贮存原来的 sys.stdout.write 方法。

❸ 定义自定义的 reverse_write 函数；在闭包中可以访问 original_write。

❹ 把 sys.stdout.write 替换成 reverse_write。

❺ 产出一个值，这个值会绑定到 with 语句中 as 子句的目标变量上。执行 with 块中的代码时，这个函数会在这一点暂停。

❻ 控制权一旦跳出 with 块，继续执行 yield 语句之后的代码；这里是恢复成原来的 sys. stdout.write 方法。

示例 15-6 是使用 looking_glass 函数的例子。

示例 15-6　测试 looking_glass 上下文管理器函数

>>> from mirror_gen import looking_glass >>> with looking_glass() as what: ➊ ... print('Alice, Kitty and Snowdrop') ... print(what) ... pordwonS dna yttiK ,ecilA YKCOWREBBAJ >>> what 'JABBERWOCKY'

➊ 与示例 15-2 唯一的区别是上下文管理器的名字：LookingGlass 变成了 looking_glass。

其实，contextlib.contextmanager 装饰器会把函数包装成实现 __enter__ 和 __exit__ 方法的类。5

5 类的名称是 _GeneratorContextManager。如果想了解具体的工作方式，可以阅读 Python 3.4 发行版中 Lib/contextlib.py 文件里的源码。

这个类的 __enter__ 方法有如下作用。

(1) 调用生成器函数，保存生成器对象（这里把它称为 gen）。

(2) 调用 next (gen)，执行到 yield 关键字所在的位置。

(3) 返回 next (gen) 产出的值，以便把产出的值绑定到 with/as 语句中的目标变量上。

with 块终止时，__exit__ 方法会做以下几件事。

(1) 检查有没有把异常传给 exc_type；如果有，调用 gen.throw (exception)，在生成器函数定义体中包含 yield 关键字的那一行抛出异常。

(2) 否则，调用 next (gen)，继续执行生成器函数定义体中 yield 语句之后的代码。

示例 15-5 有一个严重的错误：如果在 with 块中抛出了异常，Python 解释器会将其捕获，然后在 looking_glass 函数的 yield 表达式里再次抛出。但是，那里没有处理错误的代码，因此 looking_glass 函数会中止，永远无法恢复成原来的 sys.stdout.write 方法，导致系统处于无效状态。

示例 15-7 添加了一些代码，特别用于处理 ZeroDivisionError 异常；这样，在功能上它就与示例 15-3 中基于类的实现等效了。

示例 15-7　mirror_gen_exc.py：基于生成器的上下文管理器，而且实现了异常处理 —— 从外部看，行为与示例 15-3 一样

import contextlib @contextlib.contextmanager def looking_glass(): import sys original_write = sys.stdout.write def reverse_write(text): original_write(text[::-1]) sys.stdout.write = reverse_write msg = '' ➊ try: yield 'JABBERWOCKY' except ZeroDivisionError: ➋ msg = 'Please DO NOT divide by zero!' finally: sys.stdout.write = original_write ➌ if msg: print(msg) ➍

❶ 创建一个变量，用于保存可能出现的错误消息；与示例 15-5 相比，这是第一处改动。

❷ 处理 ZeroDivisionError 异常，设置一个错误消息。

❸ 撤销对 sys.stdout.write 方法所做的猴子补丁。

❹ 如果设置了错误消息，把它打印出来。

前面说过，为了告诉解释器异常已经处理了，__exit__ 方法会返回 True，此时解释器会压制异常。如果 __exit__ 方法没有显式返回一个值，那么解释器得到的是 None，然后向上冒泡异常。使用 @contextmanager 装饰器时，默认的行为是相反的：装饰器提供的 __exit__ 方法假定发给生成器的所有异常都得到处理了，因此应该压制异常。6 如果不想让 @contextmanager 压制异常，必须在被装饰的函数中显式重新抛出异常。7

6 把异常发给生成器的方式是使用 throw 方法，参见 16.5 节。

7 这样约定的原因是，创建上下文管理器时，生成器无法返回值，只能产出值。不过，现在可以返回值了，如 16.6 节所述。届时你会看到，如果在生成器中返回值，那么会抛出异常。

使用 @contextmanager 装饰器时，要把 yield 语句放在 try/finally 语句中（或者放在 with 语句中），这是无法避免的，因为我们永远不知道上下文管理器的用户会在 with 块中做什么。8

8 这条提示直接引用 Leonardo Rochael 的评论，他是本书的技术审校之一。说得好，Leo ！

除了标准库中举的例子之外，Martijn Pieters 实现的原地文件重写上下文管理器是 @contextmanager 不错的使用实例。用法如示例 15-8 所示。

示例 15-8　用于原地重写文件的上下文管理器

import csv with inplace(csvfilename, 'r', newline='') as (infh, outfh): reader = csv.reader(infh) writer = csv.writer(outfh) for row in reader: row += ['new', 'columns'] writer.writerow(row)

inplace 函数是个上下文管理器，为同一个文件提供了两个句柄（这个示例中的 infh 和 outfh），以便同时读写同一个文件。这比标准库中的 fileinput.input 函数；顺便说一下，这个函数也提供了一个上下文管理器）易于使用。

如果想学习 Martijn 实现 inplace 的源码（列在这篇文章中），找到 yield 关键字，在此之前的所有代码都用于设置上下文：先创建备份文件，然后打开并产出 __enter__ 方法返回的可读和可写文件句柄的引用。yield 关键字之后的 __exit__ 处理过程把文件句柄关闭；如果什么地方出错了，那么从备份中恢复文件。

注意，在 @contextmanager 装饰器装饰的生成器中，yield 与迭代没有任何关系。在本节所举的示例中，生成器函数的作用更像是协程：执行到某一点时暂停，让客户代码运行，直到客户让协程继续做事。第 16 章会全面讨论协程。

15.5　本章小结

本章从简单的话题入手，先讨论了 for、while 和 try 语句的 else 子句。当你习惯 else 子句在这些语句中的奇怪意思之后，我相信 else 能阐明你的意图。

然后，本章讨论了上下文管理器和 with 语句的作用。很快我们就知道，除了自动关闭打开的文件之外，with 语句还有很多用途。我们自己动手实现了一个上下文管理器 —— 含有 __enter__/__exit__ 方法的 LookingGlass 类，说明了如何在 __exit__ 方法中处理异常。Raymond Hettinger 在 PyCon US 2013 上所做的主题演讲传达了一个重要的观点：with 不仅能管理资源，还能用于去掉常规的设置和清理代码，或者在另一个过程前后执行的操作（「What Makes Python Awesome?」，第 21 张幻灯片）。

最后，我们分析了标准库中 contextlib 模块里的函数。其中，@contextmanager 装饰器能把包含一个 yield 语句的简单生成器变成上下文管理器 —— 这比定义一个至少包含两个方法的类要更简洁。我们使用 looking_glass 生成器函数实现了 LookingGlass 类，还讨论了使用 @contextmanager 时如何处理异常。

@contextmanager 装饰器优雅且实用，把三个不同的 Python 特性结合到了一起：函数装饰器、生成器和 with 语句。

15.6　延伸阅读

Python 语言参考手册中的「8. Compound statements」一章全面说明了 if、for、while 和 try 语句的 else 子句。关于 try/except 语句（有 else 子句，或者没有）是否符合 Python 风格，Raymond Hettinger 在 Stack Overflow 中对「Is it a good practice to use try-except-else in Python?」这一问题做了精彩的回答。在 Alex Martelli 写的《Python 技术手册（第 2 版）》一书中，有一章是关于异常的，那一章极好地讨论了 EAFP 风格。Alex 认为「取得原谅比获得许可容易」是由计算领域的先驱 Grace Hopper 首先提出的。

在 Python 标准库文档中，「4. Built-in Types」一章中有一节专门说明了上下文管理器的类型。Python 语言参考手册中还有 __enter__/__exit__ 两个特殊方法的文档，在「3.3.8. With Statement Context Managers」一节中。上下文管理器在「PEP 343—The‘with’Statement」中引入。这份 PEP 不易读懂，因为大量篇幅都在讲极端情况，以及反对其他提案。这就是 PEP 的特点。

在 PyCon US 2013 的主题演讲中，Raymond Hettinger 强调，with 语句是「这门语言的一项迷人特性」。在这次大会上的「Transforming Code into Beautiful, Idiomatic Python」演讲中，他还展示了上下文管理器的几个有趣应用。

Jeff Preshing 写的一篇博客文章很有趣，题为「The Python with Statement by Example」，他举例说明了 pycairo 图形库中的上下文管理器。

Beazley 与 Jones 在他们的《Python Cookbook（第 3 版）中文版》一书中，发明了上下文管理器的独特用途。「8.3 让对象支持上下文管理协议」一节实现了一个 LazyConnection 类，它的实例是上下文管理器，在 with 块中能自动打开和关闭网络连接。「9.22 以简单的方式定义上下文管理器」一节编写了一个用于统计代码运行时间的上下文管理器，还编写了一个使用事务修改 list 对象的上下文管理器：在 with 块中创建 list 实例的副本，所有改动都针对那个副本；仅当 with 块没有抛出异常，正常执行完毕之后，才用副本替代原来的列表。这样做简单又巧妙。

杂谈

取出面包

在 PyCon US 2013 的主题演讲「What Makes Python Awesome」中，Raymond Hettinger 说他第一次看到 with 语句的提案时，觉得「有点晦涩难懂」。这和我一开始的反应类似。PEP 通常难以阅读，PEP 343 尤其如此。

然后，Hettinger 告诉我们，他认识到在计算机语言的发展历程中，子程序是最重要的发明。如果有一系列操作，如 A-B-C 和 P-B-Q，那么可以把 B 拿出来，变成子程序。这就好比把三明治的馅儿取出来，这样我们就能使用金枪鱼搭配不同的面包。可是，如果我们想把面包取出来，使用小麦面包夹不同的馅儿呢？这就是 with 语句实现的功能。with 语句是子程序的补充。Hettinger 接着说道：

with 语句是非常了不起的特性。我建议你在实践中深挖这个特性的用途。使用 with 语句或许能做意义深远的事情。with 语句最好的用法还未被发掘出来。我预料，如果有好的用法，其他语言以及未来的语言会借鉴这个特性。或许，你正在参与的事情几乎与子程序的发明一样意义深远。

Hettinger 承认，他夸大了 with 语句的作用。尽管如此，with 语句仍是一个十分有用的特性。他用三明治类比，道出 with 语句是子程序的补充；那一刻，我的脑海中浮现了许多可能性。

如果你想让任何人信服 Python 是出色的语言，一定要观看 Hettinger 的主题演讲。关于上下文管理器的部分从 23:00 开始，到 26:15 结束。不过，整个主题演讲都很精彩。

第 16 章　协程

如果 Python 书籍有一定的指导作用，那么（协程就是）文档最匮乏、最鲜为人知的 Python 特性，因此表面上看是最无用的特性。

——David Beazley

Python 图书作者

字典为动词「to yield」给出了两个释义：产出和让步。对于 Python 生成器中的 yield 来说，这两个含义都成立。yield item 这行代码会产出一个值，提供给 next (...) 的调用方；此外，还会作出让步，暂停执行生成器，让调用方继续工作，直到需要使用另一个值时再调用 next ()。调用方会从生成器中拉取值。

从句法上看，协程与生成器类似，都是定义体中包含 yield 关键字的函数。可是，在协程中，yield 通常出现在表达式的右边（例如，datum = yield），可以产出值，也可以不产出 —— 如果 yield 关键字后面没有表达式，那么生成器产出 None。协程可能会从调用方接收数据，不过调用方把数据提供给协程使用的是 .send (datum) 方法，而不是 next (...) 函数。通常，调用方会把值推送给协程。

yield 关键字甚至还可以不接收或传出数据。不管数据如何流动，yield 都是一种流程控制工具，使用它可以实现协作式多任务：协程可以把控制器让步给中心调度程序，从而激活其他的协程。

从根本上把 yield 视作控制流程的方式，这样就好理解协程了。

本书前面介绍的生成器函数作用不大，但是进行一系列功能改进之后，得到了 Python 协程。了解 Python 协程的进化过程有助于理解各个阶段改进的功能和复杂度。

本章首先要简单介绍生成器如何变成协程，然后再进入核心内容。本章涵盖以下话题：

生成器作为协程使用时的行为和状态

使用装饰器自动预激协程

调用方如何使用生成器对象的 .close () 和 .throw (...) 方法控制协程

协程终止时如何返回值

yield from 新句法的用途和语义

使用案例 —— 使用协程管理仿真系统中的并发活动

16.1　生成器如何进化成协程

协程的底层架构在「PEP 342—Coroutines via Enhanced Generators」中定义，并在 Python 2.5（2006 年）实现了。自此之后，yield 关键字可以在表达式中使用，而且生成器 API 中增加了 .send (value) 方法。生成器的调用方可以使用 .send (...) 方法发送数据，发送的数据会成为生成器函数中 yield 表达式的值。因此，生成器可以作为协程使用。协程是指一个过程，这个过程与调用方协作，产出由调用方提供的值。

除了 .send (...) 方法，PEP 342 还添加了 .throw (...) 和 .close () 方法：前者的作用是让调用方抛出异常，在生成器中处理；后者的作用是终止生成器。下一节和 16.5 节会说明这些方法。

协程最近的演进来自 Python 3.3（2012 年）实现的「PEP 380—Syntax for Delegating to a Subgenerator」。PEP 380 对生成器函数的句法做了两处改动，以便更好地作为协程使用。

现在，生成器可以返回一个值；以前，如果在生成器中给 return 语句提供值，会抛出 SyntaxError 异常。

新引入了 yield from 句法，使用它可以把复杂的生成器重构成小型的嵌套生成器，省去了之前把生成器的工作委托给子生成器所需的大量样板代码。

这两个最新的改动分别在 16.6 节和 16.7 节讨论。

按照本书的惯例，我们先从基本概念和示例入手，然后再深入越来越难以理解的特性。

16.2　用作协程的生成器的基本行为

示例 16-1 展示了协程的行为。

示例 16-1　可能是协程最简单的使用演示

>>> def simple_coroutine(): # ➊ ... print('-> coroutine started') ... x = yield # ➋ ... print('-> coroutine received:', x) ... >>> my_coro = simple_coroutine() >>> my_coro # ➌ <generator object simple_coroutine at 0x100c2be10> >>> next(my_coro) # ➍ -> coroutine started >>> my_coro.send(42) # ➎ -> coroutine received: 42 Traceback (most recent call last): # ➏ ... StopIteration

❶ 协程使用生成器函数定义：定义体中有 yield 关键字。

❷ yield 在表达式中使用；如果协程只需从客户那里接收数据，那么产出的值是 None—— 这个值是隐式指定的，因为 yield 关键字右边没有表达式。

❸ 与创建生成器的方式一样，调用函数得到生成器对象。

❹ 首先要调用 next (...) 函数，因为生成器还没启动，没在 yield 语句处暂停，所以一开始无法发送数据。

❺ 调用这个方法后，协程定义体中的 yield 表达式会计算出 42；现在，协程会恢复，一直运行到下一个 yield 表达式，或者终止。

❻ 这里，控制权流动到协程定义体的末尾，导致生成器像往常一样抛出 StopIteration 异常。

协程可以身处四个状态中的一个。当前状态可以使用 inspect.getgeneratorstate (...) 函数确定，该函数会返回下述字符串中的一个。

'GEN_CREATED'

等待开始执行。

'GEN_RUNNING'

解释器正在执行。1

1 只有在多线程应用中才能看到这个状态。此外，生成器对象在自己身上调用 getgeneratorstate 函数也行，不过这样做没什么用。

'GEN_SUSPENDED'

在 yield 表达式处暂停。

'GEN_CLOSED'

执行结束。

因为 send 方法的参数会成为暂停的 yield 表达式的值，所以，仅当协程处于暂停状态时才能调用 send 方法，例如 my_coro.send (42)。不过，如果协程还没激活（即，状态是 'GEN_CREATED'），情况就不同了。因此，始终要调用 next (my_coro) 激活协程 —— 也可以调用 my_coro.send (None)，效果一样。

如果创建协程对象后立即把 None 之外的值发给它，会出现下述错误：

>>> my_coro = simple_coroutine() >>> my_coro.send(1729) Traceback (most recent call last): File "<stdin>", line 1, in <module> TypeError: can't send non-None value to a just-started generator

注意错误消息，它表述得相当清楚。

最先调用 next (my_coro) 函数这一步通常称为「预激」（prime）协程（即，让协程向前执行到第一个 yield 表达式，准备好作为活跃的协程使用）。

下面举个产出多个值的例子，以便更好地理解协程的行为，如示例 16-2 所示。

示例 16-2　产出两个值的协程

>>> def simple_coro2(a): ... print('-> Started: a =', a) ... b = yield a ... print('-> Received: b =', b) ... c = yield a + b ... print('-> Received: c =', c) ... >>> my_coro2 = simple_coro2(14) >>> from inspect import getgeneratorstate >>> getgeneratorstate(my_coro2) ➊ 'GEN_CREATED' >>> next(my_coro2) ➋ -> Started: a = 14 14 >>> getgeneratorstate(my_coro2) ➌ 'GEN_SUSPENDED' >>> my_coro2.send(28) ➍ -> Received: b = 28 42 >>> my_coro2.send(99) ➎ -> Received: c = 99 Traceback (most recent call last): File "<stdin>", line 1, in <module> StopIteration >>> getgeneratorstate(my_coro2) ➏ 'GEN_CLOSED'

❶ inspect.getgeneratorstate 函数指明，处于 GEN_CREATED 状态（即协程未启动）。

❷ 向前执行协程到第一个 yield 表达式，打印 -> Started: a = 14 消息，然后产出 a 的值，并且暂停，等待为 b 赋值。

❸ getgeneratorstate 函数指明，处于 GEN_SUSPENDED 状态（即协程在 yield 表达式处暂停）。

❹ 把数字 28 发给暂停的协程；计算 yield 表达式，得到 28，然后把那个数绑定给 b。打印 -> Received: b = 28 消息，产出 a + b 的值（42），然后协程暂停，等待为 c 赋值。

❺ 把数字 99 发给暂停的协程；计算 yield 表达式，得到 99，然后把那个数绑定给 c。打印 -> Received: c = 99 消息，然后协程终止，导致生成器对象抛出 StopIteration 异常。

❻ getgeneratorstate 函数指明，处于 GEN_CLOSED 状态（即协程执行结束）。

关键的一点是，协程在 yield 关键字所在的位置暂停执行。前面说过，在赋值语句中，= 右边的代码在赋值之前执行。因此，对于 b = yield a 这行代码来说，等到客户端代码再激活协程时才会设定 b 的值。这种行为要花点时间才能习惯，不过一定要理解，这样才能弄懂异步编程中 yield 的作用（后文探讨）。

simple_coro2 协程的执行过程分为 3 个阶段，如图 16-1 所示。

(1) 调用 next (my_coro2)，打印第一个消息，然后执行 yield a，产出数字 14。

(2) 调用 my_coro2.send (28)，把 28 赋值给 b，打印第二个消息，然后执行 yield a + b，产出数字 42。

(3) 调用 my_coro2.send (99)，把 99 赋值给 c，打印第三个消息，协程终止。

图 16-1：执行 simple_coro2 协程的 3 个阶段（注意，各个阶段都在 yield 表达式中结束，而且下一个阶段都从那一行代码开始，然后再把 yield 表达式的值赋给变量）

下面来看一个稍微复杂的协程示例。

16.3　示例：使用协程计算移动平均值

第 7 章讨论闭包时，我们分析了如何使用对象计算移动平均值：示例 7-8 定义的是一个简单的类；示例 7-14 定义的是一个高阶函数，用于生成一个闭包，在多次调用之间跟踪 total 和 count 变量的值。示例 16-3 展示如何使用协程实现相同的功能。2

2 这个示例的灵感来自 Jacob Holm 在 Python-ideas 邮件列表中发布的一个代码片段，他发布的消息题为「Yield-From: Finalization guarantees」。在那个消息的后续回复中，那段代码有几个变体。Holm 在 003912 号消息中进一步说明了自己的想法。

示例 16-3　coroaverager0.py：定义一个计算移动平均值的协程

def averager(): total = 0.0 count = 0 average = None while True: ➊ term = yield average ➋ total += term count += 1 average = total/count

➊ 这个无限循环表明，只要调用方不断把值发给这个协程，它就会一直接收值，然后生成结果。仅当调用方在协程上调用 .close () 方法，或者没有对协程的引用而被垃圾回收程序回收时，这个协程才会终止。

➋ 这里的 yield 表达式用于暂停执行协程，把结果发给调用方；还用于接收调用方后面发给协程的值，恢复无限循环。

使用协程的好处是，total 和 count 声明为局部变量即可，无需使用实例属性或闭包在多次调用之间保持上下文。示例 16-4 是使用 averager 协程的 doctest。

示例 16-4　coroaverager0.py：示例 16-3 中定义的移动平均值协程的 doctest

>>> coro_avg = averager() ➊ >>> next(coro_avg) ➋ >>> coro_avg.send(10) ➌ 10.0 >>> coro_avg.send(30) 20.0 >>> coro_avg.send(5) 15.0

❶ 创建协程对象。

❷ 调用 next 函数，预激协程。

❸ 计算移动平均值：多次调用 .send (...) 方法，产出当前的平均值。

在上述 doctest 中（示例 16-4），调用 next (coro_avg) 函数后，协程会向前执行到 yield 表达式，产出 average 变量的初始值 ——None，因此不会出现在控制台中。此时，协程在 yield 表达式处暂停，等到调用方发送值。coro_avg.send (10) 那一行发送一个值，激活协程，把发送的值赋给 term，并更新 total、count 和 average 三个变量的值，然后开始 while 循环的下一次迭代，产出 average 变量的值，等待下一次为 term 变量赋值。

细心的读者可能迫切地想知道如何终止执行 averager 实例（如 coro_avg），因为定义体中有个无限循环。16.5 节会讨论这个话题。

讨论如何终止协程之前，我们要先谈谈如何启动协程。使用协程之前必须预激，可是这一步容易忘记。为了避免忘记，可以在协程上使用一个特殊的装饰器。接下来介绍这样一个装饰器。

16.4　预激协程的装饰器

如果不预激，那么协程没什么用。调用 my_coro.send (x) 之前，记住一定要调用 next (my_coro)。为了简化协程的用法，有时会使用一个预激装饰器。示例 16-5 中的 coroutine 装饰器是一例。3

3 网上有多个类似的装饰器。这个改自 ActiveState 中的一个诀窍 ——「Pipeline made of coroutines」，作者是 Chaobin Tang，而他是受到了 David Beazley 的启发。

示例 16-5　coroutil.py：预激协程的装饰器

from functools import wraps def coroutine (func): """装饰器：向前执行到第一个`yield`表达式，预激`func`""" @wraps (func) def primer (*args,**kwargs): ➊ gen = func (*args,**kwargs) ➋ next (gen) ➌ return gen ➍ return primer

❶ 把被装饰的生成器函数替换成这里的 primer 函数；调用 primer 函数时，返回预激后的生成器。

❷ 调用被装饰的函数，获取生成器对象。

❸ 预激生成器。

❹ 返回生成器。

示例 16-6 展示 @coroutine 装饰器的用法。请与示例 16-3 对比。

示例 16-6　coroaverager1.py：使用示例 16-5 中定义的 @coroutine 装饰器定义并测试计算移动平均值的协程

"""用于计算移动平均值的协程 >>> coro_avg = averager () ➊>>> from inspect import getgeneratorstate >>> getgeneratorstate (coro_avg) ➋ 'GEN_SUSPENDED' >>> coro_avg.send (10) ➌ 10.0 >>> coro_avg.send (30) 20.0 >>> coro_avg.send (5) 15.0""" from coroutil import coroutine ➍ @coroutine ➎ def averager (): ➏ total = 0.0 count = 0 average = None while True: term = yield average total += term count += 1 average = total/count

❶ 调用 averager () 函数创建一个生成器对象，在 coroutine 装饰器的 primer 函数中已经预激了这个生成器。

❷ getgeneratorstate 函数指明，处于 GEN_SUSPENDED 状态，因此这个协程已经准备好，可以接收值了。

❸ 可以立即开始把值发给 coro_avg—— 这正是 coroutine 装饰器的目的。

❹ 导入 coroutine 装饰器。

❺ 把装饰器应用到 averager 函数上。

❻ 函数的定义体与示例 16-3 完全一样。

很多框架都提供了处理协程的特殊装饰器，不过不是所有装饰器都用于预激协程，有些会提供其他服务，例如勾入事件循环。比如说，异步网络库 Tornado 提供了 tornado.gen 装饰器。

使用 yield from 句法（参见 16.7 节）调用协程时，会自动预激，因此与示例 16-5 中的 @coroutine 等装饰器不兼容。Python 3.4 标准库里的 asyncio.coroutine 装饰器（第 18 章介绍）不会预激协程，因此能兼容 yield from 句法。

接下来探讨协程的重要特性 —— 用于终止协程，以及在协程中抛出异常的方法。

16.5　终止协程和异常处理

协程中未处理的异常会向上冒泡，传给 next 函数或 send 方法的调用方（即触发协程的对象）。示例 16-7 举例说明如何使用示例 16-6 中由装饰器定义的 averager 协程。

示例 16-7　未处理的异常会导致协程终止

>>> from coroaverager1 import averager >>> coro_avg = averager() >>> coro_avg.send(40) # ➊ 40.0 >>> coro_avg.send(50) 45.0 >>> coro_avg.send('spam') # ➋ Traceback (most recent call last): ... TypeError: unsupported operand type(s) for +=: 'float' and 'str' >>> coro_avg.send(60) # ➌ Traceback (most recent call last): File "<stdin>", line 1, in <module> StopIteration

❶ 使用 @coroutine 装饰器装饰的 averager 协程，可以立即开始发送值。

❷ 发送的值不是数字，导致协程内部有异常抛出。

❸ 由于在协程内没有处理异常，协程会终止。如果试图重新激活协程，会抛出 StopIteration 异常。

出错的原因是，发送给协程的'spam' 值不能加到 total 变量上。

示例 16-7 暗示了终止协程的一种方式：发送某个哨符值，让协程退出。内置的 None 和 Ellipsis 等常量经常用作哨符值。Ellipsis 的优点是，数据流中不太常有这个值。我还见过有人把 StopIteration 类（类本身，而不是实例，也不抛出）作为哨符值；也就是说，是像这样使用的：my_coro.send (StopIteration)。

从 Python 2.5 开始，客户代码可以在生成器对象上调用两个方法，显式地把异常发给协程。

这两个方法是 throw 和 close。

generator.throw(exc_type[, exc_value[, traceback]])

致使生成器在暂停的 yield 表达式处抛出指定的异常。如果生成器处理了抛出的异常，代码会向前执行到下一个 yield 表达式，而产出的值会成为调用 generator.throw 方法得到的返回值。如果生成器没有处理抛出的异常，异常会向上冒泡，传到调用方的上下文中。

generator.close()

致使生成器在暂停的 yield 表达式处抛出 GeneratorExit 异常。如果生成器没有处理这个异常，或者抛出了 StopIteration 异常（通常是指运行到结尾），调用方不会报错。如果收到 GeneratorExit 异常，生成器一定不能产出值，否则解释器会抛出 RuntimeError 异常。生成器抛出的其他异常会向上冒泡，传给调用方。

生成器对象方法的官方文档深藏在 Python 语言参考手册中，参见「6.2.9.1.Generator-iterator methods」。

下面举例说明如何使用 close 和 throw 方法控制协程。示例 16-8 列出的是接下来的例子使用的 demo_exc_handling 函数。

示例 16-8　coro_exc_demo.py：学习在协程中处理异常的测试代码

class DemoException (Exception): """为这次演示定义的异常类型。""" def demo_exc_handling (): print ('-> coroutine started') while True: try: x = yield except DemoException: ➊ print ('*** DemoException handled. Continuing...') else: ➋ print ('-> coroutine received: {!r}'.format (x)) rai se RuntimeError ('This line should never run.') ➌

❶ 特别处理 DemoException 异常。

❷ 如果没有异常，那么显示接收到的值。

❸ 这一行永远不会执行。

示例 16-8 中的最后一行代码不会执行，因为只有未处理的异常才会中止那个无限循环，而一旦出现未处理的异常，协程会立即终止。

demo_exc_handling 函数的常规用法如示例 16-9 所示。

示例 16-9　激活和关闭 demo_exc_handling，没有异常

>>> exc_coro = demo_exc_handling() >>> next(exc_coro) -> coroutine started >>> exc_coro.send(11) -> coroutine received: 11 >>> exc_coro.send(22) -> coroutine received: 22 >>> exc_coro.close() >>> from inspect import getgeneratorstate >>> getgeneratorstate(exc_coro) 'GEN_CLOSED'

如果把 DemoException 异常传入 demo_exc_handling 协程，它会处理，然后继续运行，如示例 16-10 所示。

示例 16-10　把 DemoException 异常传入 demo_exc_handling 不会导致协程中止

>>> exc_coro = demo_exc_handling() >>> next(exc_coro) -> coroutine started >>> exc_coro.send(11) -> coroutine received: 11 >>> exc_coro.throw(DemoException) *** DemoException handled. Continuing... >>> getgeneratorstate(exc_coro) 'GEN_SUSPENDED'

但是，如果传入协程的异常没有处理，协程会停止，即状态变成 'GEN_CLOSED'。示例 16-11 演示了这种情况。

示例 16-11　如果无法处理传入的异常，协程会终止

>>> exc_coro = demo_exc_handling() >>> next(exc_coro) -> coroutine started >>> exc_coro.send(11) -> coroutine received: 11 >>> exc_coro.throw(ZeroDivisionError) Traceback (most recent call last): ... ZeroDivisionError >>> getgeneratorstate(exc_coro) 'GEN_CLOSED'

如果不管协程如何结束都想做些清理工作，要把协程定义体中相关的代码放入 try/finally 块中，如示例 16-12。

示例 16-12　coro_finally_demo.py：使用 try/finally 块在协程终止时执行操作

class DemoException (Exception): """为这次演示定义的异常类型。""" def demo_finally (): print ('-> coroutine started') try: while True: try: x = yield except DemoException: print ('*** DemoException handled. Continuing...') else: print ('-> coroutine received: {!r}'.format (x)) finally: print ('-> coroutine ending')

Python 3.3 引入 yield from 结构的主要原因之一与把异常传入嵌套的协程有关。另一个原因是让协程更方便地返回值。请继续往下读，了解详情。

16.6　让协程返回值

示例 16-13 是 averager 协程的不同版本，这一版会返回结果。为了说明如何返回值，每次激活协程时不会产出移动平均值。这么做是为了强调某些协程不会产出值，而是在最后返回一个值（通常是某种累计值）。

示例 16-13 中的 averager 协程返回的结果是一个 namedtuple，两个字段分别是项数（count）和平均值（average）。我本可以只返回平均值，但是返回一个元组可以获得累积数据的另一个重要信息 —— 项数。

示例 16-13　coroaverager2.py：定义一个求平均值的协程，让它返回一个结果

from collections import namedtuple Result = namedtuple('Result', 'count average') def averager(): total = 0.0 count = 0 average = None while True: term = yield if term is None: break ➊ total += term count += 1 average = total/count return Result(count, average) ➋

➊ 为了返回值，协程必须正常终止；因此，这一版 averager 中有个条件判断，以便退出累计循环。

➋ 返回一个 namedtuple，包含 count 和 average 两个字段。在 Python 3.3 之前，如果生成器返回值，解释器会报句法错误。

下面在控制台中说明如何使用新版 averager，如示例 16-14 所示。

示例 16-14　coroaverager2.py：说明 averager 行为的 doctest

>>> coro_avg = averager() >>> next(coro_avg) >>> coro_avg.send(10) ➊ >>> coro_avg.send(30) >>> coro_avg.send(6.5) >>> coro_avg.send(None) ➋ Traceback (most recent call last): ... StopIteration: Result(count=3, average=15.5)

❶ 这一版不产出值。

❷ 发送 None 会终止循环，导致协程结束，返回结果。一如既往，生成器对象会抛出 StopIteration 异常。异常对象的 value 属性保存着返回的值。

注意，return 表达式的值会偷偷传给调用方，赋值给 StopIteration 异常的一个属性。这样做有点不合常理，但是能保留生成器对象的常规行为 —— 耗尽时抛出 StopIteration 异常。

示例 16-15 展示如何获取协程返回的值。

示例 16-15　捕获 StopIteration 异常，获取 averager 返回的值

>>> coro_avg = averager() >>> next(coro_avg) >>> coro_avg.send(10) >>> coro_avg.send(30) >>> coro_avg.send(6.5) >>> try: ... coro_avg.send(None) ... except StopIteration as exc: ... result = exc.value ... >>> result Result(count=3, average=15.5)

获取协程的返回值虽然要绕个圈子，但这是 PEP 380 定义的方式，当我们意识到这一点之后就说得通了：yield from 结构会在内部自动捕获 StopIteration 异常。这种处理方式与 for 循环处理 StopIteration 异常的方式一样：循环机制使用用户易于理解的方式处理异常。对 yield from 结构来说，解释器不仅会捕获 StopIteration 异常，还会把 value 属性的值变成 yield from 表达式的值。可惜，我们无法在控制台中使用交互的方式测试这种行为，因为在函数外部使用 yield from（以及 yield）会导致句法出错。4

4iPython 有个扩展 ——ipython-yf，安装这个扩展后可以在 iPython 控制台中直接执行 yield from。这个扩展用于测试异步代码，可以结合 asyncio 模块使用。这个扩展已经提交为 Python 3.5 的补丁，但是没有被接受。参见 Python 缺陷追踪系统中的 22412 号工单： Towards an asyncio-enabled command line。

下一节会举例说明如何使用 yield from 结构按照 PEP 380 定义的方式获取 averager 协程返回的值。下面讨论 yield from 结构。

16.7　使用 yield from

首先要知道，yield from 是全新的语言结构。它的作用比 yield 多很多，因此人们认为继续使用那个关键字多少会引起误解。在其他语言中，类似的结构使用 await 关键字，这个名称好多了，因为它传达了至关重要的一点：在生成器 gen 中使用 yield from subgen () 时，subgen 会获得控制权，把产出的值传给 gen 的调用方，即调用方可以直接控制 subgen。与此同时，gen 会阻塞，等待 subgen 终止。5

5 写作本书时，有个 PEP 正在讨论中，提议增加 await 和 async 关键字：PEP 492—Coroutines with async and await syntax。

第 14 章说过，yield from 可用于简化 for 循环中的 yield 表达式。例如：

>>> def gen(): ... for c in 'AB': ... yield c ... for i in range(1, 3): ... yield i ... >>> list(gen()) ['A', 'B', 1, 2]

可以改写为：

>>> def gen(): ... yield from 'AB' ... yield from range(1, 3) ... >>> list(gen()) ['A', 'B', 1, 2]

14.10 节首次提到 yield from 时举了一个例子，演示这个结构的用法，如示例 16-16 所示。6

6 示例 16-16 仅供教学使用。itertools 模块提供了优化版 chain 函数，使用 C 语言编写。

示例 16-16　使用 yield from 链接可迭代的对象

>>> def chain(*iterables): ... for it in iterables: ... yield from it ... >>> s = 'ABC' >>> t = tuple(range(3)) >>> list(chain(s, t)) ['A', 'B', 'C', 0, 1, 2]

在 Beazley 与 Jones 的《Python Cookbook（第 3 版）中文版》一书中，「4.14 扁平化处理嵌套型的序列」一节有个稍微复杂（不过更有用）的 yield from 示例（源码在 GitHub 中）。

yield from x 表达式对 x 对象所做的第一件事是，调用 iter (x)，从中获取迭代器。因此，x 可以是任何可迭代的对象。

可是，如果 yield from 结构唯一的作用是替代产出值的嵌套 for 循环，这个结构很有可能不会添加到 Python 语言中。yield from 结构的本质作用无法通过简单的可迭代对象说明，而要发散思维，使用嵌套的生成器。因此，引入 yield from 结构的 PEP 380 才起了「Syntax for Delegating to a Subgenerator」（「把职责委托给子生成器的句法」）这个标题。

yield from 的主要功能是打开双向通道，把最外层的调用方与最内层的子生成器连接起来，这样二者可以直接发送和产出值，还可以直接传入异常，而不用在位于中间的协程中添加大量处理异常的样板代码。有了这个结构，协程可以通过以前不可能的方式委托职责。

若想使用 yield from 结构，就要大幅改动代码。为了说明需要改动的部分，PEP 380 使用了一些专门的术语。

委派生成器

包含 yield from <iterable> 表达式的生成器函数。

子生成器

从 yield from 表达式中 <iterable> 部分获取的生成器。这就是 PEP 380 的标题（「Syntax for Delegating to a Subgenerator」）中所说的「子生成器」（subgenerator）。

调用方

PEP 380 使用「调用方」这个术语指代调用委派生成器的客户端代码。在不同的语境中，我会使用「客户端」代替「调用方」，以此与委派生成器（也是调用方，因为它调用了子生成器）区分开。

PEP 380 经常使用「迭代器」这个词指代子生成器。这样会让人误解，因为委派生成器也是迭代器。因此，我选择使用「子生成器」这个术语，与 PEP 380 的标题（「Syntax for Delegating to a Subgenerator」）保持一致。然而，子生成器可能是简单的迭代器，只实现了 __next__ 方法；但是，yield from 也能处理这种子生成器。不过，引入 yield from 结构的目的是为了支持实现了 __next__、send、close 和 throw 方法的生成器。

示例 16-17 能更好地说明 yield from 结构的用法。图 16-2 把该示例中各个相关的部分标识出来了。7

7 图 16-2 的灵感来自 Paul Sokolovsky 绘制的示意图。

图 16-2：委派生成器在 yield from 表达式处暂停时，调用方可以直接把数据发给子生成器，子生成器再把产出的值发给调用方。子生成器返回之后，解释器会抛出 StopIteration 异常，并把返回值附加到异常对象上，此时委派生成器会恢复

coroaverager3.py 脚本从一个字典中读取虚构的七年级男女学生的体重和身高。例如，'boys;m' 键对应于 9 个男学生的身高（单位是米），'girls;kg' 键对应于 10 个女学生的体重（单位是千克）。这个脚本把各组数据传给前面定义的 averager 协程，然后生成一个报告，如下所示：

$ python3 coroaverager3.py 9 boys averaging 40.42kg 9 boys averaging 1.39m 10 girls averaging 42.04kg 10 girls averaging 1.43m

示例 16-17 中列出的代码显然不是解决这个问题最简单的方案，但是通过实例说明了 yield from 结构的用法。这个示例的灵感来自「What's New in Python 3.3」一文给出的例子。

示例 16-17　coroaverager3.py：使用 yield from 计算平均值并输出统计报告

from collections import namedtuple Result = namedtuple ('Result', 'count average') # 子生成器 def averager (): ➊ total = 0.0 count = 0 average = None while True: term = yield ➋ if term is None: ➌ break total += term count += 1 average = total/count return Result (count, average) ➍ # 委派生成器 def grouper (results, key): ➎ while True: ➏ results [key] = yield from averager () ➐ # 客户端代码，即调用方 def main (data): ➑ results = {} for key, values in data.items (): group = grouper (results, key) ➒ next (group) ➓ for value in values: group.send (value) ⓫ group.send (None) # 重要！ ⓬ # print (results) # 如果要调试，去掉注释 report (results) # 输出报告 def report (results): for key, result in sorted (results.items ()): group, unit = key.split (';') print ('{:2} {:5} averaging {:.2f}{}'.format ( result.count, group, result.average, unit)) data = {'girls;kg': [40.9, 38.5, 44.3, 42.2, 45.2, 41.7, 44.5, 38.0, 40.6, 44.5], 'girls;m': [1.6, 1.51, 1.4, 1.3, 1.41, 1.39, 1.33, 1.46, 1.45, 1.43], 'boys;kg': [39.0, 40.8, 43.2, 40.8, 43.1, 38.6, 41.4, 40.6, 36.3], 'boys;m': [1.38, 1.5, 1.32, 1.25, 1.37, 1.48, 1.25, 1.49, 1.46], } if __name__ == '__main__': main (data)

❶ 与示例 16-13 中的 averager 协程一样。这里作为子生成器使用。

❷ main 函数中的客户代码发送的各个值绑定到这里的 term 变量上。

❸ 至关重要的终止条件。如果不这么做，使用 yield from 调用这个协程的生成器会永远阻塞。

❹ 返回的 Result 会成为 grouper 函数中 yield from 表达式的值。

❺ grouper 是委派生成器。

❻ 这个循环每次迭代时会新建一个 averager 实例；每个实例都是作为协程使用的生成器对象。

❼ grouper 发送的每个值都会经由 yield from 处理，通过管道传给 averager 实例。grouper 会在 yield from 表达式处暂停，等待 averager 实例处理客户端发来的值。averager 实例运行完毕后，返回的值绑定到 results [key] 上。while 循环会不断创建 averager 实例，处理更多的值。

❽ main 函数是客户端代码，用 PEP 380 定义的术语来说，是「调用方」。这是驱动一切的函数。

❾ group 是调用 grouper 函数得到的生成器对象，传给 grouper 函数的第一个参数是 results，用于收集结果；第二个参数是某个键。group 作为协程使用。

❿ 预激 group 协程。

⓫ 把各个 value 传给 grouper。传入的值最终到达 averager 函数中 term = yield 那一行；grouper 永远不知道传入的值是什么。

⓬ 把 None 传入 grouper，导致当前的 averager 实例终止，也让 grouper 继续运行，再创建一个 averager 实例，处理下一组值。

示例 16-17 中最后一个标号前面有个注释 ——「重要！」，强调这行代码（group.send (None)）至关重要：终止当前的 averager 实例，开始执行下一个。如果注释掉那一行，这个脚本不会输出任何报告。此时，把 main 函数靠近末尾的 print (results) 那行的注释去掉，你会发现，results 字典是空的。

研究为何没有收集到数据，能检验自己有没有理解 yield from 结构的运作方式。本书的代码仓库中有 coroaverager3.py 脚本的代码。原因说明如下。

下面简要说明示例 16-17 的运作方式，还会说明把 main 函数中调用 group.send (None) 那一行代码（带有「重要！」注释的那一行）去掉会发生什么事。

外层 for 循环每次迭代会新建一个 grouper 实例，赋值给 group 变量；group 是委派生成器。

调用 next (group)，预激委派生成器 grouper，此时进入 while True 循环，调用子生成器 averager 后，在 yield from 表达式处暂停。

内层 for 循环调用 group.send (value)，直接把值传给子生成器 averager。同时，当前的 grouper 实例（group）在 yield from 表达式处暂停。

内层循环结束后，group 实例依旧在 yield from 表达式处暂停，因此，grouper 函数定义体中为 results [key] 赋值的语句还没有执行。

如果外层 for 循环的末尾没有 group.send (None)，那么 averager 子生成器永远不会终止，委派生成器 group 永远不会再次激活，因此永远不会为 results [key] 赋值。

外层 for 循环重新迭代时会新建一个 grouper 实例，然后绑定到 group 变量上。前一个 grouper 实例（以及它创建的尚未终止的 averager 子生成器实例）被垃圾回收程序回收。

这个试验想表明的关键一点是，如果子生成器不终止，委派生成器会在 yield from 表达式处永远暂停。如果是这样，程序不会向前执行，因为 yield from（与 yield 一样）把控制权转交给客户代码（即，委派生成器的调用方）了。显然，肯定有任务无法完成。

示例 16-17 展示了 yield from 结构最简单的用法，只有一个委派生成器和一个子生成器。因为委派生成器相当于管道，所以可以把任意数量个委派生成器连接在一起：一个委派生成器使用 yield from 调用一个子生成器，而那个子生成器本身也是委派生成器，使用 yield from 调用另一个子生成器，以此类推。最终，这个链条要以一个只使用 yield 表达式的简单生成器结束；不过，也能以任何可迭代的对象结束，如示例 16-16 所示。

任何 yield from 链条都必须由客户驱动，在最外层委派生成器上调用 next (...) 函数或 .send (...) 方法。可以隐式调用，例如使用 for 循环。

下面综述 PEP 380 对 yield from 结构的正式说明。

16.8　yield from 的意义

制定 PEP 380 时，有人质疑作者 Greg Ewing 提议的语义过于复杂了。他的回应之一是：「对人类来说，几乎所有最重要的信息都在靠近顶部的某个段落里。」他还引述了 PEP 380 草稿中的一段话，当时那段话是这样的：

「把迭代器当作生成器使用，相当于把子生成器的定义体内联在 yield from 表达式中。此外，子生成器可以执行 return 语句，返回一个值，而返回的值会成为 yield from 表达式的值。」8

8 摘自 Python-Dev 邮件列表中的一个消息：「PEP 380 (yield from a subgenerator) comments」（发布于 2009 年 3 月 21 日）。

PEP 380 中已经没有这段宽慰人心的话，因为没有涵盖所有极端情况。不过，一开始可以这样粗略地说。

批准后的 PEP 380 在「Proposal」一节分六点说明了 yield from 的行为。这里，我几乎原封不动地引述，不过把有歧义的「迭代器」一词都换成了「子生成器」，还做了进一步说明。示例 16-17 阐明了下述四点。

子生成器产出的值都直接传给委派生成器的调用方（即客户端代码）。

使用 send () 方法发给委派生成器的值都直接传给子生成器。如果发送的值是 None，那么会调用子生成器的 __next__() 方法。如果发送的值不是 None，那么会调用子生成器的 send () 方法。如果调用的方法抛出 StopIteration 异常，那么委派生成器恢复运行。任何其他异常都会向上冒泡，传给委派生成器。

生成器退出时，生成器（或子生成器）中的 return expr 表达式会触发 StopIteration (expr) 异常抛出。

yield from 表达式的值是子生成器终止时传给 StopIteration 异常的第一个参数。

yield from 结构的另外两个特性与异常和终止有关。

传入委派生成器的异常，除了 GeneratorExit 之外都传给子生成器的 throw () 方法。如果调用 throw () 方法时抛出 StopIteration 异常，委派生成器恢复运行。StopIteration 之外的异常会向上冒泡，传给委派生成器。

如果把 GeneratorExit 异常传入委派生成器，或者在委派生成器上调用 close () 方法，那么在子生成器上调用 close () 方法，如果它有的话。如果调用 close () 方法导致异常抛出，那么异常会向上冒泡，传给委派生成器；否则，委派生成器抛出 GeneratorExit 异常。

yield from 的具体语义很难理解，尤其是处理异常的那两点。Greg Ewing 做得很好，在 PEP 380 中使用英语阐述了 yield from 的语义。

Ewing 还使用伪代码（使用 Python 句法）演示了 yield from 的行为。我个人认为值得花时间研究 PEP 380 中的伪代码。不过，那段伪代码长达 40 行，看一遍很难理解。

若想研究那段伪代码，最好将其简化，只涵盖 yield from 最基本且最常见的用法。

假设 yield from 出现在委派生成器中。客户端代码驱动着委派生成器，而委派生成器驱动着子生成器。那么，为了简化涉及到的逻辑，我们假设客户端没有在委派生成器上调用 .throw (...) 或 .close () 方法。此外，我们还假设子生成器不会抛出异常，而是一直运行到终止，让解释器抛出 StopIteration 异常。

示例 16-17 中的脚本就做了这些简化逻辑的假设。其实，在真实的代码中，委派生成器应该运行到结束。下面来看一下在这个简化的美满世界中，yield from 是如何运作的。

请看示例 16-18，那里列出的代码是委派生成器的定义体中下面这一行代码的扩充：

RESULT = yield from EXPR

自己试着理解示例 16-18 中的逻辑。

示例 16-18　简化的伪代码，等效于委派生成器中的 RESULT = yield from EXPR 语句（这里针对的是最简单的情况：不支持 .throw (...) 和 .close () 方法，而且只处理 StopIteration 异常）

_i = iter(EXPR) ➊ try: _y = next(_i) ➋ except StopIteration as _e: _r = _e.value ➌ else: while 1: ➍ _s = yield _y ➎ try: _y = _i.send(_s) ➏ except StopIteration as _e: ➐ _r = _e.value break RESULT = _r ➑

❶ EXPR 可以是任何可迭代的对象，因为获取迭代器 _i（这是子生成器）使用的是 iter () 函数。

❷ 预激子生成器；结果保存在 _y 中，作为产出的第一个值。

❸ 如果抛出 StopIteration 异常，获取异常对象的 value 属性，赋值给 _r—— 这是最简单情况下的返回值（RESULT）。

❹ 运行这个循环时，委派生成器会阻塞，只作为调用方和子生成器之间的通道。

❺ 产出子生成器当前产出的元素；等待调用方发送 _s 中保存的值。注意，这个代码清单中只有这一个 yield 表达式。

❻ 尝试让子生成器向前执行，转发调用方发送的 _s。

❼ 如果子生成器抛出 StopIteration 异常，获取 value 属性的值，赋值给 _r，然后退出循环，让委派生成器恢复运行。

❽ 返回的结果（RESULT）是 _r，即整个 yield from 表达式的值。

在这段简化的伪代码中，我保留了 PEP 380 中那段伪代码使用的变量名称。这些变量是：

_i（迭代器）

子生成器

_y（产出的值）

子生成器产出的值

_r（结果）

最终的结果（即子生成器运行结束后 yield from 表达式的值）

_s（发送的值）

调用方发给委派生成器的值，这个值会转发给子生成器

_e（异常）

异常对象（在这段简化的伪代码中始终是 StopIteration 实例）

除了没有处理 .throw (...) 和 .close () 方法之外，这段简化的伪代码还在子生成器上调用 .send (...) 方法，以此达到客户调用 next () 函数或 .send (...) 方法的目的。首次阅读时不要担心这些细微的差别。前面说过，即使 yield from 结构只做示例 16-18 中展示的事情，示例 16-17 也依旧能正常运行。

但是，现实情况要复杂一些，因为要处理客户对 .throw (...) 和 .close () 方法的调用，而这两个方法执行的操作必须传入子生成器。此外，子生成器可能只是纯粹的迭代器，不支持 .throw (...) 和 .close () 方法，因此 yield from 结构的逻辑必须处理这种情况。如果子生成器实现了这两个方法，而在子生成器内部，这两个方法都会触发异常抛出，这种情况也必须由 yield from 机制处理。调用方可能会无缘无故地让子生成器自己抛出异常，实现 yield from 结构时也必须处理这种情况。最后，为了优化，如果调用方调用 next (...) 函数或 .send (None) 方法，都要转交职责，在子生成器上调用 next (...) 函数；仅当调用方发送的值不是 None 时，才使用子生成器的 .send (...) 方法。

为了方便对比，下面列出 PEP 380 中扩充 yield from 表达式的完整伪代码，而且加上了带标号的注解。示例 16-19 中的代码是一字不差复制过来的，只有标注是我自己加的。

再次说明，示例 16-19 中的代码是委派生成器的定义体中下面这一个语句的扩充：

RESULT = yield from EXPR

示例 16-19　伪代码，等效于委派生成器中的 RESULT = yield from EXPR 语句

_i = iter(EXPR) ➊ try: _y = next(_i) ➋ except StopIteration as _e: _r = _e.value ➌ else: while 1: ➍ try: _s = yield _y ➎ except GeneratorExit as _e: ➏ try: _m = _i.close except AttributeError: pass else: _m() raise _e except BaseException as _e: ➐ _x = sys.exc_info() try: _m = _i.throw except AttributeError: raise _e else: ➑ try: _y = _m(*_x) except StopIteration as _e: _r = _e.value break else: ➒ try: ➓ if _s is None: ⓫ _y = next(_i) else: _y = _i.send(_s) except StopIteration as _e: ⓬ _r = _e.value break RESULT = _r ⓭

❶ EXPR 可以是任何可迭代的对象，因为获取迭代器 _i（这是子生成器）使用的是 iter () 函数。

❷ 预激子生成器；结果保存在 _y 中，作为产出的第一个值。

❸ 如果抛出 StopIteration 异常，获取异常对象的 value 属性，赋值给 _r—— 这是最简单情况下的返回值（RESULT）。

❹ 运行这个循环时，委派生成器会阻塞，只作为调用方和子生成器之间的通道。

❺ 产出子生成器当前产出的元素；等待调用方发送 _s 中保存的值。这个代码清单中只有这一个 yield 表达式。

❻ 这一部分用于关闭委派生成器和子生成器。因为子生成器可以是任何可迭代的对象，所以可能没有 close 方法。

❼ 这一部分处理调用方通过 .throw (...) 方法传入的异常。同样，子生成器可以是迭代器，从而没有 throw 方法可调用 —— 这种情况会导致委派生成器抛出异常。

❽ 如果子生成器有 throw 方法，调用它并传入调用方发来的异常。子生成器可能会处理传入的异常（然后继续循环）；可能抛出 StopIteration 异常（从中获取结果，赋值给 _r，循环结束）；还可能不处理，而是抛出相同的或不同的异常，向上冒泡，传给委派生成器。

❾ 如果产出值时没有异常……

❿ 尝试让子生成器向前执行……

⓫ 如果调用方最后发送的值是 None，在子生成器上调用 next 函数，否则调用 send 方法。

⓬ 如果子生成器抛出 StopIteration 异常，获取 value 属性的值，赋值给 _r，然后退出循环，让委派生成器恢复运行。

⓭ 返回的结果（RESULT）是 _r，即整个 yield from 表达式的值。

这段 yield from 伪代码的大多数逻辑通过六个 try/except 块实现，而且嵌套了四层，因此有点难以阅读。此外，用到的其他流程控制关键字有一个 while、一个 if 和一个 yield。找到 while 循环、yield 表达式以及 next (...) 函数和 .send (...) 方法调用，这些代码有助于对 yield from 结构的运作方式有个整体的了解。

就在示例 16-19 所列伪代码的顶部，有行代码（标号❷）揭示了一个重要的细节：要预激子生成器。9 这表明，用于自动预激的装饰器（如 16.4 节定义的那个）与 yield from 结构不兼容。

9Nick Coghlan 于 2009 年 4 月 5 日在 Python-ideas 邮件列表中发布的一个消息中质疑，yield from 结构隐式预激是不是好主意。

在本节开头引用的那个消息中，关于扩充 yield from 结构的伪代码，Greg Ewing 说：

我不是让你通过扩充的伪代码学习这个结构，那段伪代码是为了让语言专家弄明白细节。

仔细研究扩充的伪代码可能没什么用 —— 这与你的学习方式有关。显然，分析真正使用 yield from 结构的代码要比深入研究实现这一结构的伪代码更有好处。不过，我见过的 yield from 示例几乎都使用 asyncio 模块做异步编程，因此要有有效的事件循环才能运行。第 18 章会多次用到 yield from 结构。16.11 节中有几个链接，指向使用 yield from 结构的一些有趣代码，而且无需事件循环。

下面分析一个使用协程的经典案例：仿真编程。这个案例没有展示 yield from 结构的用法，但是揭示了如何使用协程在单个线程中管理并发活动。

16.9　使用案例：使用协程做离散事件仿真

协程能自然地表述很多算法，例如仿真、游戏、异步 I/O，以及其他事件驱动型编程形式或协作式多任务。10

——Guido van Rossum 和 Phillip J. Eby

PEP 342—Coroutines via Enhanced Generators

10PEP 342 中「Motivation」一节开头的第一句话。

本节我会说明如何只使用协程和标准库中的对象实现一个特别简单的仿真系统。在计算机科学领域，仿真是协程的经典应用。第一门面向对象的语言 Simula 引入了协程这个概念，目的就是为了支持仿真。

下述仿真示例不是为了做学术研究。协程是 asyncio 包的基础构建。通过仿真系统能说明如何使用协程代替线程实现并发的活动，而且对理解第 18 章讨论的 asyncio 包有极大的帮助。

分析示例之前，先简单介绍一下仿真。

16.9.1　离散事件仿真简介

离散事件仿真（Discrete Event Simulation，DES）是一种把系统建模成一系列事件的仿真类型。在离散事件仿真中，仿真「钟」向前推进的量不是固定的，而是直接推进到下一个事件模型的模拟时间。假如我们抽象模拟出租车的运营过程，其中一个事件是乘客上车，下一个事件则是乘客下车。不管乘客坐了 5 分钟还是 50 分钟，一旦乘客下车，仿真钟就会更新，指向此次运营的结束时间。使用离散事件仿真可以在不到一秒钟的时间内模拟一年的出租车运营过程。这与连续仿真不同，连续仿真的仿真钟以固定的量（通常很小）不断向前推进。

显然，回合制游戏就是离散事件仿真的例子：游戏的状态只在玩家操作时变化，而且一旦玩家决定下一步怎么走了，仿真钟就会冻结。而实时游戏则是连续仿真，仿真钟一直在运行，游戏的状态在一秒钟之内更新很多次，因此反应慢的玩家特别吃亏。

这两种仿真类型都能使用多线程或在单个线程中使用面向事件的编程技术（例如事件循环驱动的回调或协程）实现。可以说，为了实现连续仿真，在多个线程中处理实时并行的操作更自然。而协程恰好为实现离散事件仿真提供了合理的抽象。SimPy11 是一个实现离散事件仿真的 Python 包，通过一个协程表示离散事件仿真系统中的各个进程。

11 参见 SimPy 的官方文档。不要和著名的 SymPy 混淆了。SymPy 是一个符号数学库，与 DES 无关。

在仿真领域，进程这个术语指代模型中某个实体的活动，与操作系统中的进程无关。仿真系统中的一个进程可以使用操作系统中的一个进程实现，但是通常会使用一个线程或一个协程实现。

如果对仿真感兴趣，值得研究一下 SimPy。不过，在这一节我会说明如何只使用标准库提供的功能实现一个特别简单的离散事件仿真系统。我的目的是增进你对使用协程管理并发操作的感性认知。若想理解下一节所讲的内容，要仔细研究，不过这一付出能得到很大回报，让我们洞悉 asyncio、Twisted 和 Tornado 等库是如何在单个线程中管理多个并发活动的。

16.9.2　出租车队运营仿真

仿真程序 taxi_sim.py 会创建几辆出租车，每辆车会拉几个乘客，然后回家。出租车首先驶离车库，四处徘徊，寻找乘客；拉到乘客后，行程开始；乘客下车后，继续四处徘徊。

四处徘徊和行程所用的时间使用指数分布生成。为了让显示的信息更加整洁，时间使用取整的分钟数，不过这个仿真程序也能使用浮点数表示耗时。12 每辆出租车每次的状态变化都是一个事件。图 16-3 是运行这个程序的输出示例。

12 我不是运营出租车队的行家，因此别太在意显示的时间。离散事件仿真经常使用指数分布。你会看到一些非常短的行程，你就假设那是一个雨天，一些乘客坐出租车只走了一个街区。在理想的城市中，即使下雨也有出租车。

图 16-3：运行 taxi_sim.py 创建 3 辆出租车的输出示例。-s 3 参数设置随机数生成器的种子，这样在调试和演示时可以重复运行程序，输出相同的结果。不同颜色的箭头表示不同出租车的行程 13

13 图 16-3 的彩色图片可从本书页面的「随书下载」部分获取。—— 编者注

图 16-3 中最值得注意的一件事是，3 辆出租车的行程是交叉进行的。那些箭头是我加上的，为的是让你看清各辆出租车的行程：箭头从乘客上车时开始，到乘客下车后结束。有了箭头，能直观地看出如何使用协程管理并发的活动。

图 16-3 中还有几件事值得注意。

出租车每隔 5 分钟从车库中出发。

0 号出租车 2 分钟后拉到乘客（time=2），1 号出租车 3 分钟后拉到乘客（time=8），2 号出租车 5 分钟后拉到乘客（time=15）。

0 号出租车拉了两个乘客（紫色箭头）：第一个乘客从 time=2 时上车，到 time=18 时下车；第二个乘客从 time=28 时上车，到 time=65 时下车 —— 这是此次仿真中最长的行程。

1 号出租车拉了四个乘客（绿色箭头），在 time=110 时回家。

2 号出租车拉了六个乘客（红色箭头），在 time=109 时回家。这辆车最后一次行程从 time=97 时开始，只持续了一分钟。14

1 号出租车的第一次行程从 time=8 时开始，在这个过程中 2 号出租车离开了车库（time=10），而且完成了两次行程（那两个短的红色箭头）。

在此次运行示例中，所有排定的事件都在默认的仿真时间内（180 分钟）完成；最后一次事件发生在 time=110 时。

14 乘客是我，我发现忘了带钱包。

仿真结束时可能还有未完成的事件。如果是这种情况，最后一条消息会是下面这样：

*** end of simulation time: 3 events pending ***

taxi_sim.py 脚本的完整代码在示例 A-6 中，本章只会列出与协程相关的部分。真正重要的函数只有两个：taxi_process（一个协程），以及执行仿真主循环的 Simulator.run 方法。

示例 16-20 是 taxi_process 函数的代码。这个协程用到了别处定义的两个对象：compute_delay 函数，返回单位为分钟的时间间隔；Event 类，一个 namedtuple，定义方式如下：

Event = collections.namedtuple('Event', 'time proc action')

在 Event 实例中，time 字段是事件发生时的仿真时间，proc 字段是出租车进程实例的编号，action 字段是描述活动的字符串。

下面逐行分析示例 16-20 中的 taxi_process 函数。

示例 16-20　taxi_sim.py：taxi_process 协程，实现各辆出租车的活动

def taxi_process (ident, trips, start_time=0): ➊ """每次改变状态时创建事件，把控制权让给仿真器""" time = yield Event (start_time, ident, 'leave garage') ➋ for i in range (trips): ➌ time = yield Event (time, ident, 'pick up passenger') ➍ time = yield Event (time, ident, 'drop off passenger') ➎ yield Event (time, ident, 'going home') ➏ # 出租车进程结束 ➐

❶ 每辆出租车调用一次 taxi_process 函数，创建一个生成器对象，表示各辆出租车的运营过程。ident 是出租车的编号（如上述运行示例中的 0、1、2）；trips 是出租车回家之前的行程数量；start_time 是出租车离开车库的时间。

❷ 产出的第一个 Event 是 'leave garage'。执行到这一行时，协程会暂停，让仿真主循环着手处理排定的下一个事件。需要重新激活这个进程时，主循环会发送（使用 send 方法）当前的仿真时间，赋值给 time。

❸ 每次行程都会执行一遍这个代码块。

❹ 产出一个 Event 实例，表示拉到乘客了。协程在这里暂停。需要重新激活这个协程时，主循环会发送（使用 send 方法）当前的时间。

❺ 产出一个 Event 实例，表示乘客下车了。协程在这里暂停，等待主循环发送时间，然后重新激活。

❻ 指定的行程数量完成后，for 循环结束，最后产出 'going home' 事件。此时，协程最后一次暂停。仿真主循环发送时间后，协程重新激活；不过，这里没有把产出的值赋值给变量，因为用不到了。

❼ 协程执行到最后时，生成器对象抛出 StopIteration 异常。

你可以在 Python 控制台中调用 taxi_process 函数，自己「驾驶」（drive）一辆出租车 15，如示例 16-21 所示。

15 描述协程的操作时经常使用「drive」这个动词，例如：客户代码把值发给协程，驱动协程。在示例 16-21 中，客户代码是你在控制台中输入的代码。（drive 一词有不同的含义，因此在不同的语境中有不同的译法，例如这个脚注所在的那句话中译为「驾驶」。—— 译者注）

示例 16-21　驱动 taxi_process 协程

>>> from taxi_sim import taxi_process >>> taxi = taxi_process(ident=13, trips=2, start_time=0) ➊ >>> next(taxi) ➋ Event(time=0, proc=13, action='leave garage') >>> taxi.send(_.time + 7) ➌ Event(time=7, proc=13, action='pick up passenger') ➍ >>> taxi.send(_.time + 23) ➎ Event(time=30, proc=13, action='drop off passenger') >>> taxi.send(_.time + 5) ➏ Event(time=35, proc=13, action='pick up passenger') >>> taxi.send(_.time + 48) ➐ Event(time=83, proc=13, action='drop off passenger') >>> taxi.send(_.time + 1) Event(time=84, proc=13, action='going home') ➑ >>> taxi.send(_.time + 10) ➒ Traceback (most recent call last): File "<stdin>", line 1, in <module> StopIteration

❶ 创建一个生成器对象，表示一辆出租车。这辆出租车的编号是 13（ident=13），从 t=0 时开始工作，有两次行程。

❷ 预激协程；产出第一个事件。

❸ 现在可以发送当前时间。在控制台中，_ 变量绑定的是前一个结果；这里我在时间上加 7，意思是这辆出租车 7 分钟后找到第一个乘客。

❹ 这个事件由 for 循环在第一个行程的开头产出。

❺ 发送 _.time + 23，表示第一个乘客的行程持续了 23 分钟。

❻ 然后，这辆出租车会徘徊 5 分钟。

❼ 最后一次行程持续 48 分钟。

❽ 两次行程完成后，for 循环结束，产出 'going home' 事件。

❾ 如果尝试再把值发给协程，会执行到协程的末尾。协程返回后，解释器会抛出 StopIteration 异常。

注意，在示例 16-21 中，我使用控制台模拟仿真主循环。我从 taxi 协程产出的 Event 实例中获取 .time 属性，随意与一个数相加，然后调用 taxi.send 方法发送两数之和，重新激活协程。在这个仿真系统中，各个出租车协程由 Simulator.run 方法中的主循环驱动。仿真「钟」保存在 sim_time 变量中，每次产出事件时都会更新仿真钟。

为了实例化 Simulator 类，taxi_sim.py 脚本的 main 函数构建了一个 taxis 字典，如下所示：

taxis = {i: taxi_process(i, (i + 1) * 2, i * DEPARTURE_INTERVAL) for i in range(num_taxis)} sim = Simulator(taxis)

DEPARTURE_INTERVAL 的值是 5；如果 num_taxis 的值与前面的运行示例一样也是 3，这三行代码的作用与下述代码一样：

taxis = {0: taxi_process(ident=0, trips=2, start_time=0), 1: taxi_process(ident=1, trips=4, start_time=5), 2: taxi_process(ident=2, trips=6, start_time=10)} sim = Simulator(taxis)

因此，taxis 字典的值是三个参数不同的生成器对象。例如，1 号出租车从 start_time=5 时开始，寻找四个乘客。构建 Simulator 实例只需这个字典参数。

Simulator.__init__ 方法如示例 16-22 所示。Simulator 类的主要数据结构如下。

self.events

PriorityQueue 对象，保存 Event 实例。元素可以放进（使用 put 方法）PriorityQueue 对象中，然后按 item [0]（即 Event 对象的 time 属性）依序取出（使用 get 方法）。

self.procs

一个字典，把出租车的编号映射到仿真过程中激活的进程（表示出租车的生成器对象）。这个属性会绑定前面所示的 taxis 字典副本。

示例 16-22　taxi_sim.py：Simulator 类的初始化方法

class Simulator: def __init__(self, procs_map): self.events = queue.PriorityQueue() ➊ self.procs = dict(procs_map) ➋

❶ 保存排定事件的 PriorityQueue 对象，按时间正向排序。

❷ 获取的 procs_map 参数是一个字典（或其他映射），可是又从中构建一个字典，创建本地副本，因为在仿真过程中，出租车回家后会从 self.procs 属性中移除，而我们不想修改用户传入的对象。

优先队列是离散事件仿真系统的基础构件：创建事件的顺序不定，放入这种队列之后，可以按照各个事件排定的时间顺序取出。例如，可能会把下面两个事件放入优先队列：

Event(time=14, proc=0, action='pick up passenger') Event(time=11, proc=1, action='pick up passenger')

这两个事件的意思是，0 号出租车 14 分钟后拉到第一个乘客，而 1 号出租车（time=10 时出发）1 分钟后（time=11）拉到乘客。如果这两个事件在队列中，主循环从优先队列中获取的第一个事件将是 Event (time=11, proc=1, action='pick up passenger')。

下面分析这个仿真系统的主算法 ——Simulator.run 方法。在 main 函数中，实例化 Simulator 类之后立即就调用了这个方法，如下所示：

sim = Simulator(taxis) sim.run(end_time)

Simulator 类带有注解的代码清单在示例 16-23 中，下面先概述 Simulator.run 方法实现的算法。

(1) 迭代表示各辆出租车的进程。

a. 在各辆出租车上调用 next () 函数，预激协程。这样会产出各辆出租车的第一个事件。

b. 把各个事件放入 Simulator 类的 self.events 属性（队列）中。

(2) 满足 sim_time < end_time 条件时，运行仿真系统的主循环。

a. 检查 self.events 属性是否为空；如果为空，跳出循环。

b. 从 self.events 中获取当前事件（current_event），即 PriorityQueue 对象中时间值最小的 Event 对象。

c. 显示获取的 Event 对象。

d. 获取 current_event 的 time 属性，更新仿真时间。

e. 把时间发给 current_event 的 proc 属性标识的协程，产出下一个事件（next_event）。

f. 把 next_event 添加到 self.events 队列中，排定 next_event。

Simulator 类完整的代码如示例 16-23 所示。

示例 16-23　taxi_sim.py：Simulator，一个简单的离散事件仿真类；关注的重点是 run 方法

class Simulator: def __init__(self, procs_map): self.events = queue.PriorityQueue () self.procs = dict (procs_map) def run (self, end_time): ➊ """排定并显示事件，直到时间结束""" # 排定各辆出租车的第一个事件 for _, proc in sorted (self.procs.items ()): ➋ first_event = next (proc) ➌ self.events.put (first_event) ➍ # 这个仿真系统的主循环 sim_time = 0 ➎ while sim_time <end_time: ➏ if self.events.empty (): ➐ print ('*** end of events ***') break current_event = self.events.get () ➑ sim_time, proc_id, previous_action = current_event ➒ print ('taxi:', proc_id, proc_id * ' ', current_event) ➓ active_proc = self.procs [proc_id] ⓫ next_time = sim_time + compute_duration (previous_action) ⓬ try: next_event = active_proc.send (next_time) ⓭ except StopIteration: del self.procs [proc_id] ⓮ else: self.events.put (next_event) ⓯ else: ⓰ msg = '*** end of simulation time: {} events pending ***' print (msg.format (self.events.qsize ()))

❶ run 方法只需要仿真结束时间（end_time）这一个参数。

❷ 使用 sorted 函数获取 self.procs 中按键排序的元素；用不到键，因此赋值给 _。

❸ 调用 next (proc) 预激各个协程，向前执行到第一个 yield 表达式，做好接收数据的准备。产出一个 Event 对象。

❹ 把各个事件添加到 self.events 属性表示的 PriorityQueue 对象中。如示例 16-20 中的运行示例，各辆出租车的第一个事件是 'leave garage'。

❺ 把 sim_time 变量（仿真钟）归零。

❻ 这个仿真系统的主循环：sim_time 小于 end_time 时运行。

❼ 如果队列中没有未完成的事件，退出主循环。

❽ 获取优先队列中 time 属性最小的 Event 对象；这是当前事件（current_event）。

❾ 拆包 Event 对象中的数据。这一行代码会更新仿真钟 sim_time，对应于事件发生时的时间。16

16 这通常是离散事件仿真：每次循环时仿真钟不会以固定的量推进，而是根据各个事件持续的时间推进。

❿ 显示 Event 对象，指明是哪辆出租车，并根据出租车的编号缩进。

⓫ 从 self.procs 字典中获取表示当前活动的出租车的协程。

⓬ 调用 compute_duration (...) 函数，传入前一个动作（例如，'pick up passenger'、'drop off passenger' 等），把结果加到 sim_time 上，计算出下一次活动的时间。

⓭ 把计算得到的时间发给出租车协程。协程会产出下一个事件（next_event），或者抛出 StopIteration 异常（完成时）。

⓮ 如果抛出了 StopIteration 异常，从 self.procs 字典中删除那个协程。

⓯ 否则，把 next_event 放入队列中。

⓰ 如果循环由于仿真时间到了而退出，显示待完成的事件数量（有时可能碰巧是零）。

注意，示例 16-23 中的 Simulator.run 方法有两处用到了第 15 章介绍的 else 块，而且都不在 if 语句中。

主 while 循环有一个 else 语句，报告仿真系统由于到达结束时间而结束，而不是由于没有事件要处理而结束。

* 靠近主 while 循环底部那个 try 语句把 next_time 发给当前的出租车进程，尝试获取下一个事件（next_event），如果成功，执行 else 块，把 next_event 放入 self.events 队列中。

我觉得，如果没有这两个 else 块，Simulator.run 方法的代码会有点难以阅读。

这个示例的要旨是说明如何在一个主循环中处理事件，以及如何通过发送数据驱动协程。这是 asyncio 包底层的基本思想，我们在第 18 章会学习这个包。

16.10　本章小结

Guido van Rossum 写道，生成器有三种不同的代码编写风格：

有传统的「拉取式」（迭代器）、「推送式」（例如计算平均值那个示例），还有「任务式」（读过 Dave Beazley 写的协程教程了吗……）。17

17 摘自对 Python-ideas 邮件列表中「Yield-From: Finalization guarantees」消息的回复。Guido 所说的 David Beazley 写的教程是「A Curious Course on Coroutines and Concurrency」。

第 14 章专门介绍了迭代器，本章则介绍了「推送式」协程，还介绍了特别简单的「任务式」—— 仿真示例中的出租车进程。第 18 章会在并发编程中使用这两种技术实现异步任务。

计算移动平均值的示例展示了协程的常见用途：累加器，处理接收到的值。我们知道，可以在协程上应用装饰器，预激协程；在某些情况下，这么做更方便。不过要记住，预激装饰器与协程的某些用法不兼容。尤其是 yield from subgenerator ()，这个结构假定 subgenerator 没有预激，然后自动预激。

每次调用 send 方法时，作为累加器使用的协程可以获取部分结果，不过能返回值的协程更有用。这个特性在 PEP 380 中定义，于 Python 3.3 引入。我们知道，现在生成器中的 return the_result 语句会抛出 StopIteration (the_result) 异常，这样调用方可以从异常的 value 属性中获取 the_result。这样获取协程的结果还是很麻烦，不过 PEP 380 引入的 yield from 句法能自动处理。

探讨 yield from 结构时，我们首先从使用简单的迭代器的示例入手，然后又举了一个例子，重点说明 yield from 结构的三个主要组件：委派生成器（在定义体中使用 yield from），yield from 激活的子生成器，以及通过委派生成器中 yield from 表达式架设起来的通道把值发给子生成器，从而驱动整个过程的客户代码。最后，那一节参照 PEP 380 中使用的英语和类似 Python 的伪代码分析了 yield from 结构的正式定义。

本章最后举了一个离散事件仿真示例，说明如何使用生成器代替线程和回调，实现并发。那个出租车仿真系统虽然简单，但是首次一窥了事件驱动型框架（如 Tornado 和 asyncio）的运作方式：在单个线程中使用一个主循环驱动协程执行并发活动。使用协程做面向事件编程时，协程会不断把控制权让步给主循环，激活并向前运行其他协程，从而执行各个并发活动。这是一种协作式多任务：协程显式自主地把控制权让步给中央调度程序。而多线程实现的是抢占式多任务。调度程序可以在任何时刻暂停线程（即使在执行一个语句的过程中），把控制权让给其他线程。

最后要说明一点，本章对协程的定义是宽泛的、不正式的，即：通过客户调用 .send (...) 方法发送数据或使用 yield from 结构驱动的生成器函数。写作本书时，「PEP 342— Coroutines via Enhanced Generators」和现有的大多数 Python 书籍都使用这个宽泛的定义。第 18 章介绍的 asyncio 库建构在协程之上，不过采用的协程定义更为严格：在 asyncio 库中，协程（通常）使用 @asyncio.coroutine 装饰器装饰，而且始终使用 yield from 结构驱动，而不通过直接在协程上调用 .send (...) 方法驱动。当然，在 asyncio 库的底层，协程使用 next (...) 函数和 .send (...) 方法驱动，不过在用户代码中只使用 yield from 结构驱动协程运行。

16.11　延伸阅读

David Beazley 是 Python 生成器和协程的终极权威。他与 Brian Jones 合著的《Python Cookbook（第 3 版）中文版》一书中有很多使用协程编写的诀窍。Beazley 在 PyCon 期间开设的课程兼有深度和广度，因此享有盛名。首先是 PyCon US 2008 期间的「Generator Tricks for Systems Programmers」课程，在 PyCon US 2009 期间又开设了声名远播的「A Curious Course on Coroutines and Concurrency」课程（三个部分的全部视频链接很难找到：第一部分；第二部分；第三部分）。他最新的课程在蒙特利尔 PyCon 2014 期间开设，题为「Generators: The Final Frontier」。在这个课程中，他举了更多并发的例子，因此与本书第 18 章的话题联系更大。他根本不担心学员的大脑会爆炸，因此在「The Final Frontier」课程的最后一部分用协程代替了经典的访问者模式，用于计算算术表达式。

使用协程能以多种新方式组织代码，不过与递归和多态（动态调度）一样，要花点时间才能习惯。James Powell 写了一篇文章，题为「Greedy algorithm with coroutines」。他在这篇文章中使用协程重写了经典的算法。你可能还想浏览 ActiveState Code 诀窍数据库中标记为协程的流行诀窍。

Paul Sokolovsky 为 Damien George 开发的超级精简的 MicroPython（针对微控制器）解释器实现了 yield from 结构。在研究这个特性的过程中，他制作了非常详细的示意图，解说 yield from 结构的工作原理，并在 python-tulip 邮件列表中分享。Sokolovsky 很友好，允许我把那个 PDF 文件复制到本书的网站中，那个文件的固定链接是 http://flupy.org/resources/yield-from.pdf。

写作本书时，只有 asyncio 库本身和使用这个库的代码大量使用 yield from。我花了很多时间，想找到不依赖 asyncio 库的 yield from 示例。Greg Ewing（PEP 380 的作者，为 CPython 实现了 yield from）发表了一些 yield from 的使用示例：BinaryTree 类、一个简单的 XML 解析器和一个任务调度程序。

Brett Slatkin 写的《Effective Python：编写高质量 Python 代码的 59 个有效方法》一书中的第 40 条短小精辟，题为「考虑用协程来并发地运行多个函数」（网上有免费的英文版样章）。这一节中使用 yield from 驱动生成器的示例是我见过最棒的：那个示例实现了 John Conway 发明的「生命游戏」，使用协程管理游戏运行过程中各个细胞的状态。该书的随书代码在一个 GitHub 仓库中。我重构了那个「生命游戏」示例 —— 把 Slatkin 书中的函数和类与测试代码分开（原来的代码）。我还编写了 doctest 形式的测试，因此不用运行脚本就能看到各个协程和类的输出。重构后的示例发布在 GitHub Gist 网站上。

还有几个有趣的示例没用 asyncio 库，只用了 yield from：Peter Otten 在 Python Tutor 邮件列表中发布的消息，「Comparing two CSV files using Python」；Ian Ward 以 iPython Notebook 形式发布的「Iterables, Iterators, and Generators」教程，实现的是剪刀石头布游戏。

Guido van Rossum 在 python-tulip Google Group 中发表了一篇内容很长的消息，题为「The difference between yield and yield-from」，值得一读。2009 年 3 月 21 日，Nick Coghlan 在 Python-Dev 邮件列表中发布了带有大量注释的 yield from 扩充实现（https://mail.python.org/pipermail/python-dev/2009-March/087382.html）。在那篇消息中，他写道：

不管人们是否觉得使用 yield from 结构的代码难以理解，也不管人们能否领会协作式多线程相关的概念，yield from 结构底层的精巧处理能实现真正的嵌套生成器。

Yury Selivanov 撰写的「PEP 492—Coroutines with async and await syntax」提议为 Python 增加两个关键字：async 和 await。async 与其他现有的关键字结合使用，用于定义新的语言结构。例如，async def 用于定义协程，async for 用于使用异步迭代器（实现 __aiter__ 和 __anext__ 方法，这是协程版的 __iter__ 和 __next__ 方法）迭代可迭代的异步对象。为了避免与即将引入的 async 关键字冲突，asyncio.async () 函数将在 Python 3.4.4 中重命名为 asyncio.ensure_future ()。await 关键字的作用与 yield from 结构类似，不过只能在以 async def 定义的协程（禁止使用 yield 和 yield from）中使用。PEP 492 使用新句法把发展成类似协程对象的生成器与全新的原生协程对象明确地区分开了。得益于 async 和 await 关键字，以及几个特殊的新方法，Python 语言将对原生的协程对象提供更好的支持。协程已经做好准备，会成为 Python 未来特别重要的特性，因此 Python 语言应该更好地集成协程。

使用离散事件仿真系统做试验是熟悉协作式多任务的好方法。维基百科中的「Discrete event simulation」一文是不错的入门资料。18Ashish Gupta 写的短篇教程「Writing a Discrete Event Simulation: Ten Easy Lessons」说明了如何自己动手（不使用特别的库）编写离散事件仿真系统。那篇教程中的代码使用 Java 编写，因此是基于类的，而且没使用协程，不过可以轻松地移植到 Python。除了代码之外，那篇简短的教程还介绍了离散事件仿真的术语和组件。把 Gupta 教程中的示例转换成 Python 类，然后再转换成利用协程的类，是个很好的练习。

18 如今，即使终身教授也同意，维基百科几乎是学习任何计算机科学知识的入门首选。对其他知识而言虽然不是如此，但是在计算机科学这方面，维基百科特别棒。

如果想使用现成的 Python 协程库，可以使用 SimPy。这个库的在线文档中说道：

SimPy 是使用标准的 Python 开发的基于进程的离散事件仿真框架，事件调度程序基于 Python 的生成器实现，因此还可用于异步网络或实现多智能体系统（即可模拟，也可真正通信）。

协程不是特别新的 Python 特性，但是得到异步编程框架支持（Tornado 最先支持）之前，只在较窄的应用领域内使用。Python 3.3 引入的 yield from 结构和 Python 3.4 添加的 asyncio 包可能会提升协程（和 Python 3.4 本身）的使用量。但写作本书时，Python 3.4 发布还不到一年，因此观看 David Beazley 的课程，阅读涉及这个话题的经典实例时，不会有太多内容深入探讨 Python 协程编程。不过，这只是暂时的。

杂谈

raise from lambda

对编程语言来说，关键字的作用是建立控制流程和表达式计算的基本规则。

语言的关键字像是棋盘游戏中的棋子。对国际象棋来说，关键字是♔、♕、、、和；对围棋来说，关键字是●。

国际象棋的棋手实现计划时，有六种类型的棋子可用；而围棋的棋手看起来只有一种类型的棋子可用。可是，在围棋的玩法中，相邻的棋子能构成更大更稳定的棋子，形状各异，不受束缚。围棋棋子的某些排列是不可摧毁的。围棋的表现力比国际象棋强。围棋的开局走法有 361 种，大约有 1e+170 个合规的位置；而国际象棋的开局走法有 20 种，有 1e+50 个位置。

如果为国际象棋添加一个新棋子，将带来颠覆性的改变；为编程语言添加一个新的关键字也是如此。因此，语言的设计者谨慎考虑引入新关键字是合理的。

表 16-1：不同编程语言中的关键字数量

关键字数量

语言

备注

5

Smalltalk-80

以句法极简而著称

25

Go

编程语言，而不是围棋 *

32

C

指 ANSI C。C99 有 37 个关键字，C11 有 44 个

33

Python

Python 2.7 有 31 个关键字，Python 1.5 有 28 个

41

Ruby

关键字可以作为标识符使用（例如，class 也是一个方法的名称）

49

Java

与 C 语言一样，基本类型的名称（char、float 等）是保留字

60

JavaScript

包含 Java 1.0 的所有关键字，很多都没用（http://mzl.la/1JIr8fM）

65

PHP

PHP 5.3 之后引入了七个关键字，如 goto、trait 和 yield

85

C++

据 cppreference.com 网站给出的信息，C++11 在现有的 75 个关键字的基础上添加了 10 个

555

COBOL

这不是我捏造的。参见 IBM ILE COBOL 手册

∞

Scheme

任何人都能定义新关键字

* 围棋的英文是 Go，因此作者备注这里说的是 Go 语言。—— 译者注

Python 3 添加了 nonlocal 关键字，把 None、True 和 False 提升为关键字，废弃了 print 和 exec。在语言的发展过程中，弃用关键字十分罕见。表 16-1 列出了几门语言，按照关键字的数量排序。

Scheme 继承了 Lisp 的宏，允许任何人创建特殊的形式，为语言添加新的控制结构和计算规则。用户定义的这种标识符叫作「句法关键字」。Scheme R5RS 标准声称，「这门语言没有保留的标识符」（标准的第 45 页，但是 MIT/GNU Scheme 这种特殊的实现预定义了 34 个句法关键字，例如 if、lambda 和 define-syntax（用于创建新关键字的关键字）。19

Python 像国际象棋，而 Scheme 像围棋。

现在，回到 Python 句法。我觉得 Guido 对关键字的态度过于保守了。关键字的数量应该少，添加新关键字可能会破坏大量代码，但是在循环中使用 else 揭示了一个递归问题：在更适合使用新关键字的地方重用现有的关键字。在 for、while 和 try 的上下文中，应该使用 then 关键字，而不该妄用 else。

在这个问题上，最严重的一点是重用 def。现在，这个关键字用于定义函数、生成器和协程，而这些对象之间的差异很大，不应该使用相同的句法声明。20

引入 yield from 句法尤其让人失望。再次声明，我觉得真的应该为 Python 使用者提供新的关键字。更糟的是，这开启了新的趋势：把现有的关键字串起来，创建新的句法，而不添加描述性的合理关键字。恐怕有一天我们要苦苦思索 raise from lambda 是什么意思。

突发新闻

完成本书的技术审校之后，Yury Selivanov 提交的「PEP 492 — Coroutines with async and await syntax」好像要被接受了，将在 Python 3.5 中实现。21Guido van Rossum 和 Victor Stinner 都支持这个 PEP，前者是 Python 语言的创造者，后者是 asyncio 库的主要维护者，而 asyncio 库将是新句法的主要使用案例。回应 Selivanov 在 Python-ideas 邮件列表中发布的消息时，Guido 甚至暗示，为了实现这个 PEP，可能会延迟发布 Python 3.5。

当然，这会平息前一节所述的大部分抱怨。

19「The Value Of Syntax?」一文对可扩展的句法和编程语言的可用性做了有趣的探讨。Lambda the Ultimate 讨论组是编程语言极客的度假胜地。

20JavaScript、Python 和其他语言都有这样的问题。推荐阅读 Bob Nystrom 写的「What Color Is Your Function?」一文。

21Python 3.5 已经接受了 PEP 492，增加了两个关键字：async 和 await。—— 编者注

第 17 章　使用期物处理并发

抨击线程的往往是系统程序员，他们考虑的使用场景对一般的应用程序员来说，也许一生都不会遇到…… 应用程序员遇到的使用场景，99% 的情况下只需知道如何派生一堆独立的线程，然后用队列收集结果。1

——Michele Simionato

深度思考 Python 的人

1 摘自 Michele Simionato 发表的文章「Threads, processes and concurrency in Python: some thoughts」，副标题为「Removing the hype around the multicore (non) revolution and some (hopefully) sensible comment about threads and other forms of concurrency」。

本章主要讨论 Python 3.2 引入的 concurrent.futures 模块，从 PyPI 中安装 futures 包之后，也能在 Python 2.5 及以上版本中使用这个库。这个库封装了前面的引文中 Michele Simionato 所述的模式，特别易于使用。

这一章还会介绍「期物」（future）2 的概念。期物指一种对象，表示异步执行的操作。这个概念的作用很大，是 concurrent.futures 模块和 asyncio 包（第 18 章讨论）的基础。

2「期物」是我自创的词，其中的「物」是指「物件」（object，也就是对象）。起初读者可能不明其意，可与期货、期权和期房对比理解。—— 译者注

下面举个示例，作为引子。

17.1　示例：网络下载的三种风格

为了高效处理网络 I/O，需要使用并发，因为网络有很高的延迟，所以为了不浪费 CPU 周期去等待，最好在收到网络响应之前做些其他的事。

为了通过代码说明这一点，我写了三个示例程序，从网上下载 20 个国家的国旗图像。第一个示例程序 flags.py 是依序下载的：下载完一个图像，并将其保存在硬盘中之后，才请求下一个图像。另外两个脚本是并发下载的：几乎同时请求所有图像，每下载完一个文件就保存一个文件。flags_threadpool.py 脚本使用 concurrent.futures 模块，而 flags_asyncio.py 脚本使用 asyncio 包。

示例 17-1 是运行这三个脚本得到的结果，每个脚本都运行三次。我还在 YouTube 上发布了一个 73 秒的视频，让你观看这些脚本的运行情况，你会看到一个 OS X Finder 窗口，显示运行过程中保存的国旗图像文件。这些脚本从 flupy.org 下载图像，而这个网站架设在 CDN 之后，因此第一次运行时可能要等很久才能看到结果。示例 17-1 中显示的结果是运行几次之后收集的，因此 CDN 中已经有了缓存。

示例 17-1　运行 flags.py、flags_threadpool.py 和 flags_asyncio.py 脚本得到的结果

$ python3 flags.py BD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN ➊ 20 flags downloaded in 7.26s ➋ $ python3 flags.py BD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN 20 flags downloaded in 7.20s $ python3 flags.py BD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN 20 flags downloaded in 7.09s $ python3 flags_threadpool.py DE BD CN JP ID EG NG BR RU CD IR MX US PH FR PK VN IN ET TR 20 flags downloaded in 1.37s ➌ $ python3 flags_threadpool.py EG BR FR IN BD JP DE RU PK PH CD MX ID US NG TR CN VN ET IR 20 flags downloaded in 1.60s $ python3 flags_threadpool.py BD DE EG CN ID RU IN VN ET MX FR CD NG US JP TR PK BR IR PH 20 flags downloaded in 1.22s $ python3 flags_asyncio.py ➍ BD BR IN ID TR DE CN US IR PK PH FR RU NG VN ET MX EG JP CD 20 flags downloaded in 1.36s $ python3 flags_asyncio.py RU CN BR IN FR BD TR EG VN IR PH CD ET ID NG DE JP PK MX US 20 flags downloaded in 1.27s $ python3 flags_asyncio.py RU IN ID DE BR VN PK MX US IR ET EG NG BD FR CN JP PH CD TR ➎ 20 flags downloaded in 1.42s

❶ 每次运行脚本后，首先显示下载过程中下载完毕的国家代码，最后显示一个消息，说明耗时。

❷ flags.py 脚本下载 20 个图像平均用时 7.18 秒。

❸ flags_threadpool.py 脚本平均用时 1.40 秒。

❹ flags_asyncio.py 脚本平均用时 1.35 秒。

❺ 注意国家代码的顺序：对并发下载的脚本来说，每次下载的顺序都不同。

两个并发下载的脚本之间性能差异不大，不过都比依序下载的脚本快 5 倍多。这只是一个特别小的任务，如果把下载的文件数量增加到几百个，并发下载的脚本能比依序下载的脚本快 20 倍或更多。

在公网中测试 HTTP 并发客户端可能不小心变成拒绝服务（Denial-of-Service，DoS）攻击，或者有这么做的嫌疑。我们可以像示例 17-1 那样做，因为那三个脚本被硬编码，限制只发起 20 个请求。如果想大规模测试 HTTP 服务器，应该自己架设测试服务器。在本书的 GitHub 仓库中，17-futures/countries/README.rst 文件说明了如何在本地架设 Nginx 服务器。

下面我们来分析示例 17-1 测试的两个脚本 ——flags.py 和 flags_threadpool.py，看看它们的实现方式。第三个脚本 flags_asyncio.py 留到第 18 章再分析。将这三个脚本一起演示是为了表明一个观点：在 I/O 密集型应用中，如果代码写得正确，那么不管使用哪种并发策略（使用线程或 asyncio 包），吞吐量都比依序执行的代码高很多。

下面分析代码。

17.1.1　依序下载的脚本

示例 17-2 不太有吸引力，不过实现并发下载的脚本时会重用其中的大部分代码和设置，因此值得分析一下。

为了清楚起见，示例 17-2 没有处理异常。稍后会处理异常，这里我们想集中说明代码的基本结构，以便和并发下载的脚本进行对比。

示例 17-2　flags.py：依序下载的脚本；另外两个脚本会重用其中几个函数

import os import time import sys import requests ➊ POP20_CC = ('CN IN US ID BR PK NG BD RU JP ' 'MX PH VN ET EG DE IR TR CD FR').split() ➋ BASE_URL = 'http://flupy.org/data/flags' ➌ DEST_DIR = 'downloads/' ➍ def save_flag(img, filename): ➎ path = os.path.join(DEST_DIR, filename) with open(path, 'wb') as fp: fp.write(img) def get_flag(cc): ➏ url = '{}/{cc}/{cc}.gif'.format(BASE_URL, cc=cc.lower()) resp = requests.get(url) return resp.content def show(text): ➐ print(text, end=' ') sys.stdout.flush() def download_many(cc_list): ➑ for cc in sorted(cc_list): ➒ image = get_flag(cc) show(cc) save_flag(image, cc.lower() + '.gif') return len(cc_list) def main(download_many): ➓ t0 = time.time() count = download_many(POP20_CC) elapsed = time.time() - t0 msg = '\n{} flags downloaded in {:.2f}s' print(msg.format(count, elapsed)) if __name__ == '__main__': main(download_many) ⓫

❶ 导入 requests 库。这个库不在标准库中，因此依照惯例，在导入标准库中的模块（os、time 和 sys）之后导入，而且使用一个空行分隔开。3

3 可以使用 pip install requests 命令安装 requests 库。—— 编者注

❷ 列出人口最多的 20 个国家的 ISO 3166 国家代码，按照人口数量降序排列。

❸ 获取国旗图像的网站。4

4 国旗图像出自 CIA 世界概况，由美国政府发布，属公共领域。我把这些图像复制到了自己的网站，以此避免向 CIA.gov 发起 DoS 攻击的嫌疑。

❹ 保存图像的本地目录。

❺ 把 img（字节序列）保存到 DEST_DIR 目录中，命名为 filename。

❻ 指定国家代码，构建 URL，然后下载图像，返回响应中的二进制内容。

❼ 显示一个字符串，然后刷新 sys.stdout，这样能在一行消息中看到进度。在 Python 中得这么做，因为正常情况下，遇到换行才会刷新 stdout 缓冲。

❽ download_many 是与并发实现比较的关键函数。

❾ 按字母表顺序迭代国家代码列表，明确表明输出的顺序与输入一致。返回下载的国旗数量。

❿ main 函数记录并报告运行 download_many 函数之后的耗时。

⓫ main 函数必须调用执行下载的函数；我们把 download_many 函数当作参数传给 main 函数，这样 main 函数可以用作库函数，在后面的示例中接收 download_many 函数的其他实现。

Kenneth Reitz 开发的 requests 库可通过 PyPI 安装，比 Python 3 标准库中的 urllib.request 模块更易于使用。其实，requests 库提供的 API 更符合 Python 的习惯用法，而且与 Python 2.6 及以上版本兼容。因为 Python 2 中删除了 urllib2，Python 3 又使用了其他名称，所以不管使用哪一版 Python，使用 requests 库都更方便。

flags.py 脚本中没有什么新知识，只是与其他脚本对比的基准，而且我把它作为一个库使用，避免实现其他脚本时重复编写代码。下面分析使用 concurrent.futures 模块重新实现的版本。

17.1.2　使用 concurrent.futures 模块下载

concurrent.futures 模块的主要特色是 ThreadPoolExecutor 和 ProcessPoolExecutor 类，这两个类实现的接口能分别在不同的线程或进程中执行可调用的对象。这两个类在内部维护着一个工作线程或进程池，以及要执行的任务队列。不过，这个接口抽象的层级很高，像下载国旗这种简单的案例，无需关心任何实现细节。

示例 17-3 展示如何使用 ThreadPoolExecutor.map 方法，以最简单的方式实现并发下载。

示例 17-3　flags_threadpool.py：使用 futures.ThreadPoolExecutor 类实现多线程下载的脚本

from concurrent import futures from flags import save_flag, get_flag, show, main ➊ MAX_WORKERS = 20 ➋ def download_one(cc): ➌ image = get_flag(cc) show(cc) save_flag(image, cc.lower() + '.gif') return cc def download_many(cc_list): workers = min(MAX_WORKERS, len(cc_list)) ➍ with futures.ThreadPoolExecutor(workers) as executor: ➎ res = executor.map(download_one, sorted(cc_list)) ➏ return len(list(res)) ➐ if __name__ == '__main__': main(download_many) ➑

❶ 重用 flags 模块（见示例 17-2）中的几个函数。

❷ 设定 ThreadPoolExecutor 类最多使用几个线程。

❸ 下载一个图像的函数；这是在各个线程中执行的函数。

❹ 设定工作的线程数量：使用允许的最大值（MAX_WORKERS）与要处理的数量之间较小的那个值，以免创建多余的线程。

❺ 使用工作的线程数实例化 ThreadPoolExecutor 类；executor.__exit__ 方法会调用 executor.shutdown (wait=True) 方法，它会在所有线程都执行完毕前阻塞线程。

❻ map 方法的作用与内置的 map 函数类似，不过 download_one 函数会在多个线程中并发调用；map 方法返回一个生成器，因此可以迭代，获取各个函数返回的值。

❼ 返回获取的结果数量；如果有线程抛出异常，异常会在这里抛出，这与隐式调用 next () 函数从迭代器中获取相应的返回值一样。

❽ 调用 flags 模块中的 main 函数，传入 download_many 函数的增强版。

注意，示例 17-3 中的 download_one 函数其实是示例 17-2 中 download_many 函数的 for 循环体。编写并发代码时经常这样重构：把依序执行的 for 循环体改成函数，以便并发调用。

我们用的库叫 concurrent.futures，可是在示例 17-3 中没有见到期物，因此你可能想知道期物在哪里。下一节会解答这个问题。

17.1.3　期物在哪里

期物是 concurrent.futures 模块和 asyncio 包的重要组件，可是，作为这两个库的用户，我们有时却见不到期物。示例 17-3 在背后用到了期物，但是我编写的代码没有直接使用。这一节概述期物，还会举一个例子，展示用法。

从 Python 3.4 起，标准库中有两个名为 Future 的类：concurrent.futures.Future 和 asyncio.Future。这两个类的作用相同：两个 Future 类的实例都表示可能已经完成或者尚未完成的延迟计算。这与 Twisted 引擎中的 Deferred 类、Tornado 框架中的 Future 类，以及多个 JavaScript 库中的 Promise 对象类似。

期物封装待完成的操作，可以放入队列，完成的状态可以查询，得到结果（或抛出异常）后可以获取结果（或异常）。

我们要记住一件事：通常情况下自己不应该创建期物，而只能由并发框架（concurrent.futures 或 asyncio）实例化。原因很简单：期物表示终将发生的事情，而确定某件事会发生的唯一方式是执行的时间已经排定。因此，只有排定把某件事交给 concurrent.futures.Executor 子类处理时，才会创建 concurrent.futures.Future 实例。例如，Executor.submit () 方法的参数是一个可调用的对象，调用这个方法后会为传入的可调用对象排期，并返回一个期物。

客户端代码不应该改变期物的状态，并发框架在期物表示的延迟计算结束后会改变期物的状态，而我们无法控制计算何时结束。

这两种期物都有 .done () 方法，这个方法不阻塞，返回值是布尔值，指明期物链接的可调用对象是否已经执行。客户端代码通常不会询问期物是否运行结束，而是会等待通知。因此，两个 Future 类都有 .add_done_callback () 方法：这个方法只有一个参数，类型是可调用的对象，期物运行结束后会调用指定的可调用对象。

此外，还有 .result () 方法。在期物运行结束后调用的话，这个方法在两个 Future 类中的作用相同：返回可调用对象的结果，或者重新抛出执行可调用的对象时抛出的异常。可是，如果期物没有运行结束，result 方法在两个 Future 类中的行为相差很大。对 concurrency.futures.Future 实例来说，调用 f.result () 方法会阻塞调用方所在的线程，直到有结果可返回。此时，result 方法可以接收可选的 timeout 参数，如果在指定的时间内期物没有运行完毕，会抛出 TimeoutError 异常。读到 18.1.1 节你会发现，asyncio.Future.result 方法不支持设定超时时间，在那个库中获取期物的结果最好使用 yield from 结构。不过，对 concurrency.futures.Future 实例不能这么做。

这两个库中有几个函数会返回期物，其他函数则使用期物，以用户易于理解的方式实现自身。使用 17-3 中的 Executor.map 方法属于后者：返回值是一个迭代器，迭代器的 __next__ 方法调用各个期物的 result 方法，因此我们得到的是各个期物的结果，而非期物本身。

为了从实用的角度理解期物，我们可以使用 concurrent.futures.as_completed 函数重写示例 17-3。这个函数的参数是一个期物列表，返回值是一个迭代器，在期物运行结束后产出期物。

为了使用 futures.as_completed 函数，只需修改 download_many 函数，把较抽象的 executor.map 调用换成两个 for 循环：一个用于创建并排定期物，另一个用于获取期物的结果。同时，我们会添加几个 print 调用，显示运行结束前后的期物。修改后的 download_many 函数如示例 17-4，代码行数由 5 变成 17，不过现在我们能一窥神秘的期物了。其他函数不变，与示例 17-3 中的一样。

示例 17-4　flags_threadpool_ac.py：把 download_many 函数中的 executor.map 方法换成 executor.submit 方法和 futures.as_completed 函数

def download_many(cc_list): cc_list = cc_list[:5] ➊ with futures.ThreadPoolExecutor(max_workers=3) as executor: ➋ to_do = [] for cc in sorted(cc_list): ➌ future = executor.submit(download_one, cc) ➍ to_do.append(future) ➎ msg = 'Scheduled for {}: {}' print(msg.format(cc, future)) ➏ results = [] for future in futures.as_completed(to_do): ➐ res = future.result() ➑ msg = '{} result: {!r}' print(msg.format(future, res)) ➒ results.append(res) return len(results)

❶ 这次演示只使用人口最多的 5 个国家。

❷ 把 max_workers 硬编码为 3，以便在输出中观察待完成的期物。

❸ 按照字母表顺序迭代国家代码，明确表明输出的顺序与输入一致。

❹ executor.submit 方法排定可调用对象的执行时间，然后返回一个期物，表示这个待执行的操作。

❺ 存储各个期物，后面传给 as_completed 函数。

❻ 显示一个消息，包含国家代码和对应的期物。

❼ as_completed 函数在期物运行结束后产出期物。

❽ 获取该期物的结果。

❾ 显示期物及其结果。

注意，在这个示例中调用 future.result () 方法绝不会阻塞，因为 future 由 as_completed 函数产出。运行示例 17-4 得到的输出如示例 17-5 所示。

示例 17-5　flags_threadpool_ac.py 脚本的输出

$ python3 flags_threadpool_ac.py Scheduled for BR: <Future at 0x100791518 state=running> ➊ Scheduled for CN: <Future at 0x100791710 state=running> Scheduled for ID: <Future at 0x100791a90 state=running> Scheduled for IN: <Future at 0x101807080 state=pending> ➋ Scheduled for US: <Future at 0x101807128 state=pending> CN <Future at 0x100791710 state=finished returned str> result: 'CN' ➌ BR ID <Future at 0x100791518 state=finished returned str> result: 'BR' ➍ <Future at 0x100791a90 state=finished returned str> result: 'ID' IN <Future at 0x101807080 state=finished returned str> result: 'IN' US <Future at 0x101807128 state=finished returned str> result: 'US' 5 flags downloaded in 0.70s

❶ 排定的期物按字母表排序；期物的 repr () 方法会显示期物的状态：前三个期物的状态是 running，因为有三个工作的线程。

❷ 后两个期物的状态是 pending，等待有线程可用。

❸ 这一行里的第一个 CN 是运行在一个工作线程中的 download_one 函数输出的，随后的内容是 download_many 函数输出的。

❹ 这里有两个线程输出国家代码，然后主线程中的 download_many 函数输出第一个线程的结果。

多次运行 flags_threadpool_ac.py 脚本，看到的结果有所不同。如果把 max_workers 参数的值增大到 5，结果的顺序变化更多。把 max_workers 参数的值设为 1，代码依序运行，结果的顺序始终与调用 submit 方法的顺序一致。

我们分析了两个版本的使用 concurrent.futures 库实现的下载脚本：使用 ThreadPoolExecutor.map 方法的示例 17-3 和使用 futures.as_completed 函数的示例 17-4。如果你对 flags_asyncio.py 脚本的代码好奇，可以看一眼第 18 章中的示例 18-5。

严格来说，我们目前测试的并发脚本都不能并行下载。使用 concurrent.futures 库实现的那两个示例受 GIL（Global Interpreter Lock，全局解释器锁）的限制，而 flags_asyncio.py 脚本在单个线程中运行。

读到这里，你可能会对前面做的非正规基准测试有下述疑问。

既然 Python 线程受 GIL 的限制，任何时候都只允许运行一个线程，那么 flags_threadpool.py 脚本的下载速度怎么会比 flags.py 脚本快 5 倍？

flags_asyncio.py 脚本和 flags.py 脚本都在单个线程中运行，前者怎么会比后者快 5 倍？

第二个问题在 18.3 节解答。

GIL 几乎对 I/O 密集型处理无害，原因参见下一节。

17.2　阻塞型 I/O 和 GIL

CPython 解释器本身就不是线程安全的，因此有全局解释器锁（GIL），一次只允许使用一个线程执行 Python 字节码。因此，一个 Python 进程通常不能同时使用多个 CPU 核心。5

5 这是 CPython 解释器的局限，与 Python 语言本身无关。Jython 和 IronPython 没有这种限制。不过，目前最快的 Python 解释器 PyPy 也有 GIL。

编写 Python 代码时无法控制 GIL；不过，执行耗时的任务时，可以使用一个内置的函数或一个使用 C 语言编写的扩展释放 GIL。其实，有个使用 C 语言编写的 Python 库能管理 GIL，自行启动操作系统线程，利用全部可用的 CPU 核心。这样做会极大地增加库代码的复杂度，因此大多数库的作者都不这么做。

然而，标准库中所有执行阻塞型 I/O 操作的函数，在等待操作系统返回结果时都会释放 GIL。这意味着在 Python 语言这个层次上可以使用多线程，而 I/O 密集型 Python 程序能从中受益：一个 Python 线程等待网络响应时，阻塞型 I/O 函数会释放 GIL，再运行一个线程。

因此 David Beazley 才说：「Python 线程毫无作用。」6

6 出自「Generators: The Final Frontier」，第 106 张幻灯片。

Python 标准库中的所有阻塞型 I/O 函数都会释放 GIL，允许其他线程运行。time.sleep () 函数也会释放 GIL。因此，尽管有 GIL，Python 线程还是能在 I/O 密集型应用中发挥作用。

下面简单说明如何在 CPU 密集型作业中使用 concurrent.futures 模块轻松绕开 GIL。

17.3　使用 concurrent.futures 模块启动进程

concurrent.futures 模块的文档副标题是「Launching parallel tasks」（执行并行任务）。这个模块实现的是真正的并行计算，因为它使用 ProcessPoolExecutor 类把工作分配给多个 Python 进程处理。因此，如果需要做 CPU 密集型处理，使用这个模块能绕开 GIL，利用所有可用的 CPU 核心。

ProcessPoolExecutor 和 ThreadPoolExecutor 类都实现了通用的 Executor 接口，因此使用 concurrent.futures 模块能特别轻松地把基于线程的方案转成基于进程的方案。

下载国旗的示例或其他 I/O 密集型作业使用 ProcessPoolExecutor 类得不到任何好处。这一点易于验证，只需把示例 17-3 中下面这几行：

def download_many(cc_list): workers = min(MAX_WORKERS, len(cc_list)) with futures.ThreadPoolExecutor(workers) as executor:

改成：

def download_many(cc_list): with futures.ProcessPoolExecutor() as executor:

对简单的用途来说，这两个实现 Executor 接口的类唯一值得注意的区别是，ThreadPoolExecutor.__init__ 方法需要 max_workers 参数，指定线程池中线程的数量。在 ProcessPoolExecutor 类中，那个参数是可选的，而且大多数情况下不使用 —— 默认值是 os.cpu_count () 函数返回的 CPU 数量。这样处理说得通，因为对 CPU 密集型的处理来说，不可能要求使用超过 CPU 数量的职程。而对 I/O 密集型处理来说，可以在一个 ThreadPoolExecutor 实例中使用 10 个、100 个或 1000 个线程；最佳线程数取决于做的是什么事，以及可用内存有多少，因此要仔细测试才能找到最佳的线程数。

经过几次测试，我发现使用 ProcessPoolExecutor 实例下载 20 面国旗的时间增加到了 1.8 秒，而原来使用 ThreadPoolExecutor 的版本是 1.4 秒。主要原因可能是，我的电脑用的是四核 CPU，因此限制只能有 4 个并发下载，而使用线程池的版本有 20 个工作的线程。

ProcessPoolExecutor 的价值体现在 CPU 密集型作业上。我用两个 CPU 密集型脚本做了一些性能测试。

arcfour_futures.py

这个脚本（代码清单参见示例 A-7）纯粹使用 Python 实现 RC4 算法。我加密并解密了 12 个字节数组，大小从 149KB 到 384KB 不等。

sha_futures.py

这个脚本（代码清单参见示例 A-9）使用标准库中的 hashlib 模块（使用 OpenSSL 库实现）实现 SHA-256 算法。我计算了 12 个 1MB 字节数组的 SHA-256 散列值。

这两个脚本除了显示汇总结果之外，没有使用 I/O。构建和处理数据的过程都在内存中完成，因此 I/O 对执行时间没有影响。

我运行了 64 次 RC4 示例，48 次 SHA 示例，平均时间如表 17-1 所示。统计的时间中包含派生工作进程的时间。

表 17-1：在配有 Intel Core i7 2.7 GHz 四核 CPU 的设备中，使用 Python 3.4 运行 RC4 和 SHA 示例，分别使用 1~4 个职程得到的时间和提速倍数

职程数

运行 RC4 示例的时间

RC4 示例的提速倍数

运行 SHA 示例的时间

SHA 示例的提速倍数

1

11.48s

1.00×

22.66s

1.00×

2

8.65s

1.33×

14.90s

1.52×

3

6.04s

1.90×

11.91s

1.90×

4

5.58s

2.06×

10.89s

2.08×

可以看出，对加密算法来说，使用 ProcessPoolExecutor 类派生 4 个工作的进程后（如果有 4 个 CPU 核心的话），性能可以提高两倍。

对那个纯粹使用 Python 实现的 RC4 示例来说，如果使用 PyPy 和 4 个职程，与使用 CPython 和 4 个职程相比，速度能提高 3.8 倍。以表 17-1 中使用 CPython 和一个职程的运行时间为基准，速度提升了 7.8 倍。

如果使用 Python 处理 CPU 密集型工作，应该试试 PyPy。使用 PyPy 运行 arcfour_futures.py 脚本，速度快了 3.8~5.1 倍；具体的倍数由职程的数量决定。我测试时使用的是 PyPy 2.4.0，这一版与 Python 3.2.5 兼容，因此标准库中有 concurrent.futures 模块。

下面通过一个演示程序来研究线程池的行为。这个程序会创建一个包含 3 个职程的线程池，运行 5 个可调用的对象，输出带有时间戳的消息。

17.4　实验 Executor.map 方法

若想并发运行多个可调用的对象，最简单的方式是使用示例 17-3 中见过的 Executor.map 方法。示例 17-6 中的脚本演示了 Executor.map 方法的某些运作细节。这个脚本的输出在示例 17-7 中。

示例 17-6　demo_executor_map.py：简单演示 ThreadPoolExecutor 类的 map 方法

from time import sleep, strftime from concurrent import futures def display(*args): ➊ print(strftime('[%H:%M:%S]'), end=' ') print(*args) def loiter(n): ➋ msg = '{}loiter({}): doing nothing for {}s...' display(msg.format('\t'*n, n, n)) sleep(n) msg = '{}loiter({}): done.' display(msg.format('\t'*n, n)) return n * 10 ➌ def main(): display('Script starting.') executor = futures.ThreadPoolExecutor(max_workers=3) ➍ results = executor.map(loiter, range(5)) ➎ display('results:', results) ➏ display('Waiting for individual results:') for i, result in enumerate(results): ➐ display('result {}: {}'.format(i, result)) main()

❶ 这个函数的作用很简单，把传入的参数打印出来，并在前面加上 [HH:MM:SS] 格式的时间戳。

❷ loiter 函数什么也没做，只是在开始时显示一个消息，然后休眠 n 秒，最后在结束时再显示一个消息；消息使用制表符缩进，缩进的量由 n 的值确定。

❸ loiter 函数返回 n * 10，以便让我们了解收集结果的方式。

❹ 创建 ThreadPoolExecutor 实例，有 3 个线程。

❺ 把五个任务提交给 executor（因为只有 3 个线程，所以只有 3 个任务会立即开始：loiter (0)、loiter (1) 和 loiter (2)）；这是非阻塞调用。

❻ 立即显示调用 executor.map 方法的结果：一个生成器，如示例 17-7 中的输出所示。

❼ for 循环中的 enumerate 函数会隐式调用 next (results)，这个函数又会在（内部）表示第一个任务（loiter (0)）的 _f 期物上调用 _f.result () 方法。result 方法会阻塞，直到期物运行结束，因此这个循环每次迭代时都要等待下一个结果做好准备。

我建议你运行示例 17-6，看着结果逐渐显示出来。此外，还可以修改 ThreadPoolExecutor 构造方法的 max_workers 参数，以及 executor.map 方法中 range 函数的参数；或者自己挑选几个值，以列表的形式传给 map 方法，得到不同的延迟。

示例 17-7 是运行示例 17-6 得到的输出示例。

示例 17-7　示例 17-6 中 demo_executor_map.py 脚本的运行示例

$ python3 demo_executor_map.py [15:56:50] Script starting. ➊ [15:56:50] loiter(0): doing nothing for 0s... ➋ [15:56:50] loiter(0): done. [15:56:50] loiter(1): doing nothing for 1s... ➌ [15:56:50] loiter(2): doing nothing for 2s... [15:56:50] results: <generator object result_iterator at 0x106517168> ➍ [15:56:50] loiter(3): doing nothing for 3s... ➎ [15:56:50] Waiting for individual results: [15:56:50] result 0: 0 ➏ [15:56:51] loiter(1): done. ➐ [15:56:51] loiter(4): doing nothing for 4s... [15:56:51] result 1: 10 ➑ [15:56:52] loiter(2): done. ➒ [15:56:52] result 2: 20 [15:56:53] loiter(3): done. [15:56:53] result 3: 30 [15:56:55] loiter(4): done. ➓ [15:56:55] result 4: 40

❶ 这次运行从 15:56:50 开始。

❷ 第一个线程执行 loiter (0)，因此休眠 0 秒，甚至会在第二个线程开始之前就结束，不过具体情况因人而异。7

7 具体情况因人而异：对线程来说，你永远不知道某一时刻事件的具体排序；有可能在另一台设备中会看到 loiter (1) 在 loiter (0) 结束之前开始，这是因为 sleep 函数总会释放 GIL。因此，即使休眠 0 秒，Python 也可能会切换到另一个线程。

❸ loiter (1) 和 loiter (2) 立即开始（因为线程池中有三个职程，可以并发运行三个函数）。

❹ 这一行表明，executor.map 方法返回的结果（results）是生成器；不管有多少任务，也不管 max_workers 的值是多少，目前不会阻塞。

❺ loiter (0) 运行结束了，第一个职程可以启动第四个线程，运行 loiter (3)。

❻ 此时执行过程可能阻塞，具体情况取决于传给 loiter 函数的参数：results 生成器的 __next__ 方法必须等到第一个期物运行结束。此时不会阻塞，因为 loiter (0) 在循环开始前结束。注意，这一点之前的所有事件都在同一刻发生 ——15:56:50。

❼ 一秒钟后，即 15:56:51，loiter (1) 运行完毕。这个线程闲置，可以开始运行 loiter (4)。

❽ 显示 loiter (1) 的结果：10。现在，for 循环会阻塞，等待 loiter (2) 的结果。

❾ 同上：loiter (2) 运行结束，显示结果；loiter (3) 也一样。

❿ 2 秒钟后 loiter (4) 运行结束，因为 loiter (4) 在 15:56:51 时开始，休眠了 4 秒。

Executor.map 函数易于使用，不过有个特性可能有用，也可能没用，具体情况取决于需求：这个函数返回结果的顺序与调用开始的顺序一致。如果第一个调用生成结果用时 10 秒，而其他调用只用 1 秒，代码会阻塞 10 秒，获取 map 方法返回的生成器产出的第一个结果。在此之后，获取后续结果时不会阻塞，因为后续的调用已经结束。如果必须等到获取所有结果后再处理，这种行为没问题；不过，通常更可取的方式是，不管提交的顺序，只要有结果就获取。为此，要把 Executor.submit 方法和 futures.as_completed 函数结合起来使用，像示例 17-4 中那样。17.5.2 节会继续讨论这种方式。

executor.submit 和 futures.as_completed 这个组合比 executor.map 更灵活，因为 submit 方法能处理不同的可调用对象和参数，而 executor.map 只能处理参数不同的同一个可调用对象。此外，传给 futures.as_completed 函数的期物集合可以来自多个 Executor 实例，例如一些由 ThreadPoolExecutor 实例创建，另一些由 ProcessPoolExecutor 实例创建。

下一节根据新的需求继续实现下载国旗的示例，这一次不使用 executor.map 方法，而是迭代 futures.as_completed 函数返回的结果。

17.5　显示下载进度并处理错误

前面说过，17.1 节中的几个脚本没有处理错误，这样做是为了便于阅读和比较三种方案（依序、多线程和异步）的结构。

为了处理各种错误，我创建了 flags2 系列示例。

flags2_common.py

这个模块中包含所有 flags2 示例通用的函数和设置，例如 main 函数，负责解析命令行参数、计时和报告结果。这个脚本中的代码其实是提供支持的，与本章的话题没有直接关系，因此我把源码放在附录 A 里的示例 A-10 中。

flags2_sequential.py

能正确处理错误，以及显示进度条的 HTTP 依序下载客户端。flags2_threadpool.py 脚本会用到这个模块里的 download_one 函数。

flags2_threadpool.py

基于 futures.ThreadPoolExecutor 类实现的 HTTP 并发客户端，演示如何处理错误，以及集成进度条。

flags2_asyncio.py

与前一个脚本的作用相同，不过使用 asyncio 和 aiohttp 实现。这个脚本在第 18 章的 18.4 节中分析。

测试并发客户端时要小心

在公开的 HTTP 服务器上测试 HTTP 并发客户端时要小心，因为每秒可能会发起很多请求，这相当于是拒绝服务（DoS）攻击。我们不想攻击任何人，只是在学习如何开发高性能的客户端。访问公开的服务器时一定要管好自己的客户端。做高并发试验时，应该在本地架设 HTTP 服务器供测试。本书代码仓库中的 17-futures/countries/ 目录里有个 README.rst 文件，那里有架设说明。

flags2 系列示例最明显的特色是，有使用 TQDM 包实现的文本动画进度条。我在 YouTube 上发布了一个 108 秒的视频，展示了这个进度条，还对比了三个 flags 脚本的下载速度。在那个视频中，我先运行依序下载的脚本，不过 32 秒后中断了，因为那个脚本要用 5 分多钟访问 676 个 URL，下载 194 面国旗；然后，我分别运行多线程和 asyncio 版三次，每次都在 6 秒之内（即快了 60 多倍）完成任务。图 17-1 中有两个截图，分别是 flags2_threadpool.py 脚本运行中和运行结束后。

图 17-1：（左上）flags2_threadpool.py 运行中，显示着 tqdm 包生成的进度条；（右下）同一个终端窗口，脚本运行完毕后

TQDM 包特别易于使用，项目的 README.md 文件中有个 GIF 动画，演示了最简单的用法。安装 tqdm 包之后，8 在 Python 控制台中输入下述代码，会在注释那里看到进度条动画：

8 可以使用 pip install tqdm 命令安装 tqdm 包。—— 编者注

>>> import time >>> from tqdm import tqdm >>> for i in tqdm (range (1000)): ... time.sleep (.01) ... >>> # -> 进度条会出现在这里 <-

除了这个灵巧的效果之外，tqdm 函数的实现方式也很有趣：能处理任何可迭代的对象，生成一个迭代器；使用这个迭代器时，显示进度条和完成全部迭代预计的剩余时间。为了计算预计剩余时间，tqdm 函数要获取一个能使用 len 函数确定大小的可迭代对象，或者在第二个参数中指定预期的元素数量。借助在 flags2 系列示例中集成 TQDM，我们可以深入了解这几个脚本的运作方式，因为我们必须使用 futures.as_completed 函数和 asyncio.as_completed 函数，这样 tqdm 函数才能在每个期物运行结束后更新进度。

flags2 系列示例的另一个特色是，提供了命令行接口。三个脚本接受的选项相同，运行任意一个脚本时指定 -h 选项就能看到所有选项。示例 17-8 显示的是帮助文本。

示例 17-8　flags2 系列脚本的帮助界面

$ python3 flags2_threadpool.py -h usage: flags2_threadpool.py [-h] [-a] [-e] [-l N] [-m CONCURRENT] [-s LABEL] [-v] [CC [CC ...]] Download flags for country codes. Default: top 20 countries by population. positional arguments: CC country code or 1st letter (eg. B for BA...BZ) optional arguments: -h, --help show this help message and exit -a, --all get all available flags (AD to ZW) -e, --every get flags for every possible code (AA...ZZ) -l N, --limit N limit to N first codes -m CONCURRENT, --max_req CONCURRENT maximum concurrent requests (default=30) -s LABEL, --server LABEL Server to hit; one of DELAY, ERROR, LOCAL, REMOTE (default=LOCAL) -v, --verbose output detailed progress info

所有选项都是可选的。下面说明最重要的选项。

不能忽略的选项是 -s/--server：用于选择测试时使用的 HTTP 服务器和基 URL。这个选项的值可以设为下述 4 个字符串（不区分大小写），用于确定脚本从哪里下载国旗。

LOCAL

使用 http://localhost:8001/flags；这是默认值。你应该配置一个本地 HTTP 服务器，响应 8001 端口的请求。我测试时使用 Nginx。本章示例代码中的 README.rst 文件说明了如何安装和配置 Nginx。

REMOTE

使用 http://flupy.org/data/flags；这是我搭建的公开网站，托管在一个共享服务器中。请不要使用太多并发请求访问这个网站。flupy.org 域名由 Cloudflare CDN 的一个免费账户管理，因此第一次下载时会发现很慢，不过一旦 CDN 有了缓存，速度就会变快。9

9 测试这些脚本时，我向那个廉价的虚拟主机发起了一些并发请求，但是得到的响应是「HTTP 503 errors—Service Temporarily Unavailable」。后来我配置了 Cloudflare，现在没有这个错误了。

DELAY

使用 http://localhost:8002/flags；这是一个代理，会延迟 HTTP 响应，监听的端口是 8002。我在本地的 Nginx 服务器前加上了 Mozilla Vaurien，以此引入延迟。前面提到的那个 README.rst 文件中有运行 Vaurien 代理的说明。

ERROR

使用 http://localhost:8003/flags；这是一个代理，监听 8003 端口，引入了 HTTP 错误，并延迟响应。这个服务器使用的 Vaurien 配置与前面不同。

仅当在本地架设 HTTP 服务器，并且监听 8001 端口时，才能使用 LOCAL 选项。DELAY 和 ERROR 选项需要代理，分别监听 8002 和 8003 端口。在 GitHub 上本书的代码仓库中有个 17-futures/countries/README.rst 文件，说明了如何配置 Nginx 和 Mozilla Vaurien，以实现这些选项的要求。

默认情况下，各个 flags2 脚本会使用默认的并发连接数（各脚本有所不同）从 LOCAL 服务器中下载人口最多的 20 个国家的国旗。示例 17-9 是全部使用默认值运行 flags2_sequential.py 脚本得到的输出。

示例 17-9　全部使用默认值运行 flags2_sequential.py 脚本：LOCAL 服务器，人口最多的 20 国国旗，1 个并发连接

$ python3 flags2_sequential.py LOCAL site: http://localhost:8001/flags Searching for 20 flags: from BD to VN 1 concurrent connection will be used. -------------------- 20 flags downloaded. Elapsed time: 0.10s

我们可以使用多种不同的方式选择下载哪些国家的国旗。示例 17-10 展示如何下载国家代码以字母 A、B 或 C 开头的所有国旗。

示例 17-10　运行 flags2_threadpool.py 脚本，从 DELAY 服务器中下载国家代码以 A、B 或 C 开头的所有国旗

$ python3 flags2_threadpool.py -s DELAY a b c DELAY site: http://localhost:8002/flags Searching for 78 flags: from AA to CZ 30 concurrent connections will be used. -------------------- 43 flags downloaded. 35 not found. Elapsed time: 1.72s

不管使用什么方式选择国家代码，下载的国旗数量都可以使用 -l/--limit 选项限制。示例 17-11 演示如何发起 100 个请求，结合 -a 和 -l 选项下载 100 面国旗。

示例 17-11　运行 flags2_asyncio.py 脚本，使用 100 个并发请求（-m 100）从 ERROR 服务器中下载 100 面国旗（-al 100）

$ python3 flags2_asyncio.py -s ERROR -al 100 -m 100 ERROR site: http://localhost:8003/flags Searching for 100 flags: from AD to LK 100 concurrent connections will be used. -------------------- 73 flags downloaded. 27 errors. Elapsed time: 0.64s

以上是 flags2 系列示例的用户界面。下面分析实现方式。

17.5.1　flags2 系列示例处理错误的方式

这三个示例在负责下载一个文件的函数（download_one）中使用相同的策略处理 HTTP 404 错误（未找到）。其他异常则向上冒泡，交给 download_many 函数处理。

我们还是先分析依序下载的代码，因为这些代码更易于理解，而且使用线程池的脚本重用了这里的大部分代码。示例 17-12 列出的是 flags2_sequential.py 和 flags2_threadpool.py 脚本真正用于下载的函数。

示例 17-12　flags2_sequential.py：负责下载的基本函数；flags2_threadpool.py 脚本重用了这两个函数

def get_flag(base_url, cc): url = '{}/{cc}/{cc}.gif'.format(base_url, cc=cc.lower()) resp = requests.get(url) if resp.status_code != 200: ➊ resp.raise_for_status() return resp.content def download_one(cc, base_url, verbose=False): try: image = get_flag(base_url, cc) except requests.exceptions.HTTPError as exc: ➋ res = exc.response if res.status_code == 404: status = HTTPStatus.not_found ➌ msg = 'not found' else: ➍ raise else: save_flag(image, cc.lower() + '.gif') status = HTTPStatus.ok msg = 'OK' if verbose: ➎ print(cc, msg) return Result(status, cc) ➏

❶ get_flag 函数没有处理错误，当 HTTP 代码不是 200 时，使用 requests.Response.raise_for_status 方法抛出异常。10

10HTTP 代码 200 表示成功完成 HTTP 请求。—— 编者注

❷ download_one 函数捕获 requests.exceptions.HTTPError 异常，特别处理 HTTP 404 错误……

❸ …… 方法是，把局部变量 status 设为 HTTPStatus.not_found；HTTPStatus 是从 flags2_common 模块（见示例 A-10）中导入的 Enum 对象。

❹ 重新抛出其他 HTTPError 异常；这些异常会向上冒泡，传给调用方。

❺ 如果在命令行中设定了 -v/--verbose 选项，显示国家代码和状态消息；这就是详细模式中看到的进度信息。

➏ download_one 函数的返回值是一个 namedtuple——Result，其中有个 status 字段，其值是 HTTPStatus.not_found 或 HTTPStatus.ok。

示例 17-13 列出的是 download_many 函数的依序下载版。代码虽然简单，不过值得分析一下，以便后面与并发版对比。我们要关注的是报告进度、处理错误和统计下载数量的方式。

示例 17-13　flags2_sequential.py：实现依序下载的 download_many 函数

def download_many(cc_list, base_url, verbose, max_req): counter = collections.Counter() ➊ cc_iter = sorted(cc_list) ➋ if not verbose: cc_iter = tqdm.tqdm(cc_iter) ➌ for cc in cc_iter: ➍ try: res = download_one(cc, base_url, verbose) ➎ except requests.exceptions.HTTPError as exc: ➏ error_msg = 'HTTP error {res.status_code} - {res.reason}' error_msg = error_msg.format(res=exc.response) except requests.exceptions.ConnectionError as exc: ➐ error_msg = 'Connection error' else: ➑ error_msg = '' status = res.status if error_msg: status = HTTPStatus.error ➒ counter[status] += 1 ➓ if verbose and error_msg: ⓫ print('*** Error for {}: {}'.format(cc, error_msg)) return counter ⓬

❶ 这个 Counter 实例用于统计不同的下载状态：HTTPStatus.ok、HTTPStatus.not_found 或 HTTPStatus.error。

❷ 按字母顺序传入的国家代码列表，保存在 cc_iter 变量中。

❸ 如果不是详细模式，把 cc_iter 传给 tqdm 函数，返回一个迭代器，产出 cc_iter 中的元素，还会显示进度条动画。

❹ 这个 for 循环迭代 cc_iter……

❺ …… 不断调用 download_one 函数，执行下载。

❻ 处理 get_flag 函数抛出的与 HTTP 有关的且 download_one 函数没有处理的异常。

❼ 处理其他与网络有关的异常。其他异常会中止这个脚本，因为调用 download_many 函数的 flags2_common.main 函数中没有 try/except 块。

❽ 如果没有异常从 download_one 函数中逃出，从 download_one 函数返回的 namedtuple（HTTPStatus）中获取 status。

❾ 如果有错误，把局部变量 status 设为相应的状态。

❿ 以 HTTPStatus（一个 Enum）中的值为键，增加计数器。

⓫ 如果是详细模式，而且有错误，显示带有当前国家代码的错误消息。

⓬ 返回 counter，以便 main 函数能在最终的报告中显示数量。

下面分析重构后的线程池示例 ——flags2_threadpool.py。

17.5.2　使用 futures.as_completed 函数

为了集成 TQDM 进度条，并处理各次请求中的错误，flags2_threadpool.py 脚本用到我们见过的 futures.ThreadPoolExecutor 类和 futures.as_completed 函数。示例 17-14 是 flags2_threadpool.py 脚本的完整代码清单。这个脚本只实现了 download_many 函数，其他函数都重用 flags2_common 和 flags2_sequential 模块里的。

示例 17-14　flags2_threadpool.py：完整的代码清单

import collections from concurrent import futures import requests import tqdm ➊ from flags2_common import main, HTTPStatus ➋ from flags2_sequential import download_one ➌ DEFAULT_CONCUR_REQ = 30 ➍ MAX_CONCUR_REQ = 1000 ➎ def download_many(cc_list, base_url, verbose, concur_req): counter = collections.Counter() with futures.ThreadPoolExecutor(max_workers=concur_req) as executor: ➏ to_do_map = {} ➐ for cc in sorted(cc_list): ➑ future = executor.submit(download_one, cc, base_url, verbose) ➒ to_do_map[future] = cc ➓ done_iter = futures.as_completed(to_do_map) ⓫ if not verbose: done_iter = tqdm.tqdm(done_iter, total=len(cc_list)) ⓬ for future in done_iter: ⓭ try: res = future.result() ⓮ except requests.exceptions.HTTPError as exc: ⓯ error_msg = 'HTTP {res.status_code} - {res.reason}' error_msg = error_msg.format(res=exc.response) except requests.exceptions.ConnectionError as exc: error_msg = 'Connection error' else: error_msg = '' status = res.status if error_msg: status = HTTPStatus.error counter[status] += 1 if verbose and error_msg: cc = to_do_map[future] ⓰ print('*** Error for {}: {}'.format(cc, error_msg)) return counter if __name__ == '__main__': main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ)

❶ 导入显示进度条的库。

❷ 从 flags2_common 模块中导入一个函数和一个 Enum。

❸ 重用 flags2_sequential 模块（见示例 17-12）里的 download_one 函数。

❹ 如果没有在命令行中指定 -m/--max_req 选项，使用这个值作为并发请求数的最大值，也就是线程池的大小；真实的数量可能会比这少，例如下载的国旗数量较少。

❺ 不管要下载多少国旗，也不管 -m/--max_req 命令行选项的值是多少，MAX_CONCUR_REQ 会限制最大的并发请求数；这是一项安全预防措施。

❻ 把 max_workers 设为 concur_req，创建 ThreadPoolExecutor 实例；main 函数会把下面这三个值中最小的那个赋值给 concur_req：MAX_CONCUR_REQ、cc_list 的长度、-m/--max_req 命令行选项的值。这样能避免创建超过所需的线程。

❼ 这个字典把各个 Future 实例（表示一次下载）映射到相应的国家代码上，在处理错误时使用。

❽ 按字母顺序迭代国家代码列表。结果的顺序主要由 HTTP 响应的时间长短决定，不过，如果线程池的大小（由 concur_req 设定）比 len (cc_list) 小得多，可能会发现有按字母顺序批量下载的情况。

❾ 每次调用 executor.submit 方法排定一个可调用对象的执行时间，然后返回一个 Future 实例。第一个参数是可调用的对象，其余的参数是传给可调用对象的参数。

❿ 把返回的 future 和国家代码存储在字典中。

⓫ futures.as_completed 函数返回一个迭代器，在期物运行结束后产出期物。

⓬ 如果不是详细模式，把 as_completed 函数返回的结果传给 tqdm 函数，显示进度条；因为 done_iter 没有 len 函数，所以我们必须通过 total= 参数告诉 tqdm 函数预期的元素数量，这样 tqdm 才能预计剩余的工作量。

⓭ 迭代运行结束后的期物。

⓮ 在期物上调用 result 方法，要么返回可调用对象的返回值，要么抛出可调用的对象在执行过程中捕获的异常。这个方法可能会阻塞，等待确定结果；不过，在这个示例中不会阻塞，因为 as_completed 函数只返回已经运行结束的期物。

⓯ 处理可能出现的异常；这个函数余下的代码与依序下载版 download_many 函数一样（见示例 17-13），不过下一点除外。

⓰ 为了给错误消息提供上下文，以当前的 future 为键，从 to_do_map 中获取国家代码。在依序下载版中无须这么做，因为那一版迭代的是国家代码，所以知道当前国家的代码；而这里迭代的是期物。

示例 17-14 用到了一个对 futures.as_completed 函数特别有用的惯用法：构建一个字典，把各个期物映射到其他数据（期物运行结束后可能有用）上。这里，在 to_do_map 中，我们把各个期物映射到对应的国家代码上。这样，尽管期物生成的结果顺序已经乱了，依然便于使用结果做后续处理。

Python 线程特别适合 I/O 密集型应用，concurrent.futures 模块大大简化了某些使用场景下 Python 线程的用法。我们对 concurrent.futures 模块基本用法的介绍到此结束。下面讨论不适合使用 ThreadPoolExecutor 或 ProcessPoolExecutor 类时，有哪些替代方案。

17.5.3　线程和多进程的替代方案

Python 自 0.9.8 版（1993 年）就支持线程了，concurrent.futures 只不过是使用线程的最新方式。Python 3 废弃了原来的 thread 模块，换成了高级的 threading 模块。11 如果 futures.ThreadPoolExecutor 类对某个作业来说不够灵活，可能要使用 threading 模块中的组件（如 Thread、Lock、Semaphore 等）自行制定方案，比如说使用 queue 模块创建线程安全的队列，在线程之间传递数据。futures.ThreadPoolExecutor 类已经封装了这些组件。

11threading 模块自 Python 1.5.1（1998 年）就已存在，不过有些人仍然继续使用旧的 thread 模块。Python 3 把 thread 模块重命名为 _thread，以此强调这是低层实现，不应该在应用代码中使用。

对 CPU 密集型工作来说，要启动多个进程，规避 GIL。创建多个进程最简单的方式是，使用 futures.ProcessPoolExecutor 类。不过和前面一样，如果使用场景较复杂，需要更高级的工具。multiprocessing 模块的 API 与 threading 模块相仿，不过作业交给多个进程处理。对简单的程序来说，可以用 multiprocessing 模块代替 threading 模块，少量改动即可。不过，multiprocessing 模块还能解决协作进程遇到的最大挑战：在进程之间传递数据。

17.6　本章小结

本章开头对两个 HTTP 并发客户端和一个依序下载的客户端做了对比，结果是并发版比依序下载的脚本性能高很多。

分析过使用 concurrent.futures 实现的第一个示例后，我们深入探讨了期物对象，即 concurrent.futures.Future 或 asyncio.Future 类的实例，着重说明了二者的共同点（区别在第 18 章详述）。我们说明了如何使用 Executor.submit (...) 方法创建期物，以及如何使用 concurrent.futures.as_completed (...) 函数迭代运行结束的期物。

接下来，我们分析了为什么尽管有 GIL，Python 线程仍然适合 I/O 密集型应用：标准库中每个使用 C 语言编写的 I/O 函数都会释放 GIL，因此，当某个线程在等待 I/O 时，Python 调度程序会切换到另一个线程。然后，我们讨论了如何借助 concurrent.futures.ProcessPoolExecutor 类使用多进程，以此绕开 GIL，使用多个 CPU 核心运行加密算法，并通过四个职程实现一倍多的速度提升。

在随后的一节中，我们深入分析了 concurrent.futures.ThreadPoolExecutor 类的运作方式。为了说明问题，我特意举了一个示例，创建几个任务，但是休眠几秒钟，什么也不做，只是显示带有时间戳的状态。

接下来，本章回到下载国旗的示例，增加了进度条和错误处理代码，并且进一步探索了 future.as_completed 生成器函数。我们得知一个常见的做法：把期物存储在一个字典中，提交期物时把期物与相关的信息联系起来；这样，as_completed 迭代器产出期物后，就可以使用那些信息。

最后，本章简要说明了多线程和多进程并发的低层实现（但却更灵活）——threading 和 multiprocessing 模块。这两个模块代表在 Python 中使用线程和进程的传统方式。

17.7　延伸阅读

Brian Quinlan 是 concurrent.futures 包的贡献者，他在 PyCon Australia 2010 上所做的「The Future Is Soon!」演讲对这个包做了介绍。Quinlan 演讲时没用幻灯片，而是直接在 Python 控制台中输入代码，以此说明这个库的用途。作为引子，他在演讲中推荐了 XKCD 漫画家和程序员 Randall Munroe 制作的一个视频，Randall 在这个视频中对 Google Maps 发起了 DoS 攻击（非有意为之），绘制一个彩色地图，显示他驾车绕城的路线。这个库的正式介绍文件是「PEP 3148—futures—execute computations asynchronously」。在这个 PEP 中，Quinlan 写道，concurrent.futures 库「受 Java 的 java.util.concurrent 包影响很大」。

Jan Palach 写的 Parallel Programming with Python（Packt 出版社）一书介绍了几个并发编程的工具，包括 concurrent.futures、threading 和 multiprocessing 库。除了标准库之外，这本书还讨论了 Celery。这是一个任务队列，用于把工作分配给多个线程和进程，甚至是不同的设备。在 Django 社区中，为了减轻繁重任务的负担（例如，把生成 PDF 的工作交给其他进程，防止 HTTP 响应延迟生成），Celery 可能是使用最广泛的系统。

Beazley 与 Jones 的著作《Python Cookbook（第 3 版）中文版》有多个使用 concurrent.futures 的诀窍，首先是「11.12 理解事件驱动型 I/O」。「12.7 创建线程池」展示了一个简单的 TCP 回显服务器，「12.8 实现简单的并行编程」提供了一个特别实用的示例：借助 ProcessPoolExecutor 实例分析一整个目录中使用 gzip 压缩的 Apache 日志文件。这本书的第 12 章对线程做了更多介绍，特别值得一提的是「12.10 定义一个 Actor 任务」，这个诀窍演示了参与者模型：通过传递消息协调多个线程的可行方式。

Brett Slatkin 写的《Effective Python：编写高质量 Python 代码的 59 个有效方法》一书中有一章探讨了并发的多个话题，包括：协程；使用 concurrent.futures 库处理线程和进程；不使用 ThreadPoolExecutor 类，而使用锁和队列做线程编程。

Micha Gorelick 与 Ian Ozsvald 写的 High Performance Python（O'Reilly 出版社）和 Doug Hellmann 写的《Python 标准库》都涵盖了线程和进程。

若想了解不使用线程或回调的现代并发方式，推荐阅读 Paul Butcher 写的《七周七并发模型》。12 我喜欢这本书的副标题「When　Threads Unravel」（线程束手无策之时）。这本书的第 1 章简单介绍了线程和锁，后面的六章探讨了不同语言（不包括 Python、Ruby 和 JavaScript）为并发编程提供的现代化替代方案。

12 该书已由人民邮电出版社出版，书号：978-7-115-38606-9。—— 编者注

如果对 GIL 感兴趣，请先阅读 Python 文档中的「Python Library and Extension FAQ」（「Can't we get rid of the Global Interpreter Lock?」）。Guido van Rossum 写的「It isn't Easy to Remove the GIL」和 Jesse Noller（multiprocessing 包的贡献者）写的「Python Threads and the Global Interpreter Lock」也值得一读。此外，David Beazley 在「Understanding the Python GIL」中详细探讨了 GIL 的内部运作。13 在这次演讲的第 54 张幻灯片中，Beazley 得出了一些令人担忧的结果，例如，使用 Python 3.2 引入的新 GIL 算法做基准测试时，他发现处理时间增加了 20 倍。不过，Beazley 似乎使用一个空的 while True: pass 循环模拟 CPU 密集型工作，而现实中不会这样做。在 Beazley 提交的缺陷报告中，根据 Antoine Pitrou（实现新 GIL 算法的人）的评论，这个问题与工作负载没有太大关系。

13 感谢 Lucas Brunialti 把这个演讲的链接发给我。

GIL 是实际存在的问题，而且短时间内不可能消失，不过 Jesse Noller 和 Richard Oudkerk 开发了一个库，能让 CPU 密集型应用轻松地绕开这个问题 ——multiprocessing 包。这个包在多个进程中模拟 threading 模块的 API，而且支持基础设施的锁、队列、管道、共享内存，等等。这个包由「PEP 371—Addition of the multiprocessing package to the standard library」引入。这个包的官方文档是个 93KB 的 .rst 文件（大约 63 页），是 Python 标准库文档中最长的一章。多进程是 concurrent.futures.ProcessPoolExecutor 类的基础。

对于 CPU 密集型和数据密集型并行处理，现在有个新工具可用 —— 分布式计算引擎 Apache Spark。Spark 在大数据领域发展势头强劲，提供了友好的 Python API，支持把 Python 对象当作数据，如示例页面所示。

João S. O. Bueno 开发的 lelo 库和 Nat Pryce 开发的 python-parallelize 库简洁且十分易于使用，它们的作用是使用多个进程处理并行任务。lelo 包定义了一个 @parallel 装饰器，可以应用到任何函数上，把函数变成非阻塞：调用被装饰的函数时，函数在一个新进程中执行。Nat Pryce 开发的 python-parallelize 包提供了一个 parallelize 生成器，能把 for 循环分配给多个 CPU 执行。这两个包在内部都使用了 multiprocessing 模块。

杂谈

远离线程

并发是计算机科学中最难的概念之一（通常最好别去招惹它）。14

——David Beazley

Python 教练和科学狂人

上面引自 David Beazley 的话与本章开头引自 Michele Simionato 的话明显矛盾，但我都同意。在大学学过一门并发课程之后（那门课把「并发编程」与管理线程和锁划上等号），我得出一个结论，我不该自己管理线程和锁，而应该管理内存分配和释放。线程和锁最好由懂行的系统程序员管理，他们有这种爱好，也有时间去管理（但愿如此）。

因此我觉得 concurrent.futures 包很棒，它把线程、进程和队列视作服务的基础设施，不用自己动手直接处理。当然，这个包针对的是简单的作业，也就是所谓的「高度并行」问题。可是，正如本章开头 Simionato 所说的那样，编写应用（而非操作系统或数据库服务器）时，遇到的大部分并发问题都属于这一种。

对于并发程度不高的问题来说，线程和锁也不是解决之道。在操作系统层面，线程永远不会消失；不过，过去七年我觉得让人眼前一亮的编程语言（包括 Go、Elixir 和 Clojure）都对并发做了更好、更高层的抽象，正如《七周七并发模型》一书所述。Erlang（实现 Elixir 的语言）是典型示例，设计这门语言时彻底考虑到了并发。我对这门语言不感兴趣的原因很简单 —— 句法丑陋。我被 Python 的句法宠坏了。

José Valim 是著名的 Ruby on Rails 核心贡献者，他设计的 Elixir 提供了友好而现代的句法。与 Lisp 和 Clojure 一样，Elixir 也实现了句法宏。这是把双刃剑。使用句法宏能实现强大的 DSL，可是衍生语言多起来之后，代码基会出现兼容问题，社区会分裂。大量涌现的宏导致 Lisp 没落，因为各种 Lisp 实现都使用独特难懂的方言。标准化的 Common Lisp 则开始复苏。我希望 José Valim 能引领 Elixir 社区，不要重蹈覆辙。

与 Elixir 类似，Go 也是一门充满新意的现代语言。可是，与 Elixir 相比，某些方面有点保守。Go 不支持宏，句法比 Python 简单。Go 也不支持继承和运算符重载，而且提供的元编程支持没有 Python 多。这些限制被认为是 Go 语言的特点，因为行为和性能更可预料。这对高并发来说是好事，而 Go 的重要使命是取代 C++、Java 和 Python。

虽然 Elixir 和 Go 在高并发领域是直接的竞争者，但是设计原理的不同则吸引了不同的用户群。这两门语言都可能蓬勃发展。可是纵观编程语言的历史，保守的语言更能吸引程序员。我希望自己能精通 Go 和 Elixir。

关于 GIL

GIL 简化了 CPython 解释器和 C 语言扩展的实现。得益于 GIL，Python 有很多 C 语言扩展 —— 这绝对是如今 Python 如此受欢迎的主要原因之一。

多年以来，我一直觉得 GIL 导致 Python 线程几乎没有用武之地，只能开发一些玩具应用。直到发现标准库中每一个阻塞型 I/O 函数都会释放 GIL 之后，我才意识到 Python 线程特别适合在 I/O 密集型系统（鉴于我的工作经验，客户经常付费让我开发这种应用）中使用。

竞争对手对并发的支持

MRI（推荐使用的 Ruby 实现）也有 GIL，因此，Ruby 线程与 Python 线程受到同样的限制。相比之下，JavaScript 解释器则根本不支持用户层级的线程。在 JavaScript 中，只能通过回调式异步编程实现并发。我提到这些是因为，Ruby 和 JavaScript 是最能直接与 Python 竞争的通用动态编程语言。

在深谙并发的这一批新语言中，Go 和 Elixir 或许是最能蚕食 Python 的语言。不过，现在有 asyncio 了。既然这么多人相信纯粹使用回调的 Node.js 平台可以做并发编程，那么 asyncio 生态系统成熟后，Python 赢回这些人能有多难呢？不过，这是下一章「杂谈」的话题。

14 摘自 PyCon 2009 教程「A Curious Course on Coroutines and Concurrency」的第 9 张幻灯片。

第 18 章　使用 asyncio 包处理并发

并发是指一次处理多件事。

并行是指一次做多件事。

二者不同，但是有联系。

一个关于结构，一个关于执行。

并发用于制定方案，用来解决可能（但未必）并行的问题。1

——Rob Pike

Go 语言的创造者之一

1 摘自「Concurrency Is Not Parallelism (It's Better)」演讲的第 5 张幻灯片。

Imre Simon 教授 2 说过，科学界有两个重要过错：使用不同的词表示相同的事物，以及使用同一个词表示不同的事物。如果你研究过并发编程或并行编程，会发现「并发」和「并行」有不同的定义。我将采用上述引文中 Rob Pike 的非正式定义。

2Imre Simon（1943—2009）是巴西的计算机科学先驱，对自动机理论（Automata Theory）有杰出的贡献，开创了热带数学（Tropical Mathematics）这一领域。他还是自由软件和自由文化的拥护者。我有幸曾与他一起学习、工作和相处。

真正的并行需要多个核心。现代的笔记本电脑有４个 CPU 核心，但是通常不经意间就有超过 100 个进程同时运行。因此，实际上大多数过程都是并发处理的，而不是并行处理。计算机始终运行着 100 多个进程，确保每个进程都有机会取得进展，不过 CPU 本身同时做的事情不能超过四件。十年前使用的设备也能并发处理 100 个进程，不过都在同一个核心里。鉴于此，Rob Pike 才把那次演讲取名为「Concurrency Is Not Parallelism (It's Better)」[「并发不是并行（并发更好）」]。

本章介绍 asyncio 包，这个包使用事件循环驱动的协程实现并发。这是 Python 中最大也是最具雄心壮志的库之一。Guido van Rossum 在 Python 仓库之外开发 asyncio 包，把这个项目的代号命名为「Tulip」（郁金香）。因此，在网上搜索这方面的资料时，会经常看到这种花的名称。例如，这个项目的主要讨论组仍叫 python-tulip。

Python 3.4 把 Tulip 添加到标准库中时，把它重命名为 asyncio。这个包也兼容 Python 3.3，在 PyPI 中可以通过新的官方名称找到（https://pypi.python.org/pypi/asyncio）。asyncio 大量使用 yield from 表达式，因此与 Python 旧版不兼容。

Trollius 项目（也以花名命名，http://trollius.readthedocs.org/）移植了 asyncio，把 yield from 替换成 yield 和精巧的回调（From 和 Return），以便支持 Python 2.6 及以上版本。yield from ... 表达式变成了 yield From (...)；如果协程需要返回结果，那么要把 return result 替换成 raise Return (result)。Trollius 由 Victor Stinner 主导，他也是 asyncio 包的核心开发者。Victor 人很好，在本书付梓之前同意审核本章。

本章讨论以下话题：

对比一个简单的多线程程序和对应的 asyncio 版，说明多线程和异步任务之间的关系

asyncio.Future 类与 concurrent.futures.Future 类之间的区别

第 17 章中下载国旗那些示例的异步版

摒弃线程或进程，如何使用异步编程管理网络应用中的高并发

在异步编程中，与回调相比，协程显著提升性能的方式

如何把阻塞的操作交给线程池处理，从而避免阻塞事件循环

使用 asyncio 编写服务器，重新审视 Web 应用对高并发的处理方式

为什么 asyncio 已经准备好对 Python 生态系统产生重大影响

首先，本章通过简单的示例来对比 threading 模块和 asyncio 包。

18.1　线程与协程对比

有一次讨论线程和 GIL 时，Michele Simionato 发布了一个简单但有趣的示例：在长时间计算的过程中，使用 multiprocessing 包在控制台中显示一个由 ASCII 字符 "|/-\" 构成的动画旋转指针。

我改写了 Simionato 的示例，一个借由 threading 模块使用线程实现，一个借由 asyncio 包使用协程实现。我这么做是为了让你对比两种实现，理解如何不使用线程来实现并发行为。

示例 18-1 和示例 18-2 的输出是动态的，因此你一定要运行这两个脚本，看看结果如何。如果你在坐地铁（或者在某个没有 Wi-Fi 连接的地方），可以看图 18-1，想象单词「thinking」之前的 \ 线是旋转的。

图 18-1：spinner_thread.py 和 spinner_asyncio.py 两个脚本的输出类似：旋转指针对象的字符串表示形式和文本「Answer: 42」。在这个截图中，spinner_asyncio.py 脚本仍在运行中，旋转指针显示的是「\ thinking!」消息；脚本运行结束后，那一行会替换成「Answer: 42」

首先，分析 spinner_thread.py 脚本（见示例 18-1）。

示例 18-1　spinner_thread.py：通过线程以动画形式显示文本式旋转指针

import threading import itertools import time import sys class Signal: ➊ go = True def spin (msg, signal): ➋ write, flush = sys.stdout.write, sys.stdout.flush for char in itertools.cycle ('|/-\\'): ➌ status = char + ' ' + msg write (status) flush () write ('\x08' * len (status)) ➍ time.sleep (.1) if not signal.go: ➎ break write (' ' * len (status) + '\x08' * len (status)) ➏ def slow_function (): ➐ # 假装等待 I/O 一段时间 time.sleep (3) ➑ return 42 def supervisor (): ➒ signal = Signal () spinner = threading.Thread (target=spin, args=('thinking!', signal)) print ('spinner object:', spinner) ➓ spinner.start () ⓫ result = slow_function () ⓬ signal.go = False ⓭ spinner.join () ⓮ return result def main (): result = supervisor () ⓯ print ('Answer:', result) if __name__ == '__main__': main ()

❶ 这个类定义一个简单的可变对象；其中有个 go 属性，用于从外部控制线程。

❷ 这个函数会在单独的线程中运行。signal 参数是前面定义的 Signal 类的实例。

❸ 这其实是个无限循环，因为 itertools.cycle 函数会从指定的序列中反复不断地生成元素。

❹ 这是显示文本式动画的诀窍所在：使用退格符（\x08）把光标移回来。

❺ 如果 go 属性的值不是 True 了，那就退出循环。

❻ 使用空格清除状态消息，把光标移回开头。

❼ 假设这是耗时的计算。

❽ 调用 sleep 函数会阻塞主线程，不过一定要这么做，以便释放 GIL，创建从属线程。

❾ 这个函数设置从属线程，显示线程对象，运行耗时的计算，最后杀死线程。

❿ 显示从属线程对象。输出类似于 <Thread (Thread-1, initial)>。

⓫ 启动从属线程。

⓬ 运行 slow_function 函数，阻塞主线程。同时，从属线程以动画形式显示旋转指针。

⓭ 改变 signal 的状态；这会终止 spin 函数中的那个 for 循环。

⓮ 等待 spinner 线程结束。

⓯ 运行 supervisor 函数。

注意，Python 没有提供终止线程的 API，这是有意为之的。若想关闭线程，必须给线程发送消息。这里，我使用的是 signal.go 属性：在主线程中把它设为 False 后，spinner 线程最终会注意到，然后干净地退出。

下面来看如何使用 @asyncio.coroutine 装饰器替代线程，实现相同的行为。

第 16 章的小结说过，asyncio 包使用的「协程」是较严格的定义。适合 asyncio API 的协程在定义体中必须使用 yield from，而不能使用 yield。此外，适合 asyncio 的协程要由调用方驱动，并由调用方通过 yield from 调用；或者把协程传给 asyncio 包中的某个函数，例如 asyncio.async (...) 和本章要介绍的其他函数，从而驱动协程。最后，@asyncio.coroutine 装饰器应该应用在协程上，如下述示例所示。

我们来分析示例 18-2。

示例 18-2　spinner_asyncio.py：通过协程以动画形式显示文本式旋转指针

import asyncio import itertools import sys @asyncio.coroutine ➊ def spin (msg): ➋ write, flush = sys.stdout.write, sys.stdout.flush for char in itertools.cycle ('|/-\\'): status = char + ' ' + msg write (status) flush () write ('\x08' * len (status)) try: yield from asyncio.sleep (.1) ➌ except asyncio.CancelledError: ➍ break write (' ' * len (status) + '\x08' * len (status)) @asyncio.coroutine def slow_function (): ➎ # 假装等待 I/O 一段时间 yield from asyncio.sleep (3) ➏ return 42 @asyncio.coroutine def supervisor (): ➐ spinner = asyncio.async (spin ('thinking!')) ➑ print ('spinner object:', spinner) ➒ result = yield from slow_function () ➓ spinner.cancel () ⓫ return result def main (): loop = asyncio.get_event_loop () ⓬ result = loop.run_until_complete (supervisor ()) ⓭ loop.close () print ('Answer:', result) if __name__ == '__main__': main ()

❶ 打算交给 asyncio 处理的协程要使用 @asyncio.coroutine 装饰。这不是强制要求，但是强烈建议这么做。原因在本列表后面。

❷ 这里不需要示例 18-1 中 spin 函数中用来关闭线程的 signal 参数。

❸ 使用 yield from asyncio.sleep (.1) 代替 time.sleep (.1)，这样的休眠不会阻塞事件循环。

❹ 如果 spin 函数苏醒后抛出 asyncio.CancelledError 异常，其原因是发出了取消请求，因此退出循环。

❺ 现在，slow_function 函数是协程，在用休眠假装进行 I/O 操作时，使用 yield from 继续执行事件循环。

❻ yield from asyncio.sleep (3) 表达式把控制权交给主循环，在休眠结束后恢复这个协程。

❼ 现在，supervisor 函数也是协程，因此可以使用 yield from 驱动 slow_function 函数。

❽ asyncio.async (...) 函数排定 spin 协程的运行时间，使用一个 Task 对象包装 spin 协程，并立即返回。

❾ 显示 Task 对象。输出类似于 <Task pending coro=<spin () running at spinner_ asyncio.py:12>>。

❿ 驱动 slow_function () 函数。结束后，获取返回值。同时，事件循环继续运行，因为 slow_function 函数最后使用 yield from asyncio.sleep (3) 表达式把控制权交回给了主循环。

⓫ Task 对象可以取消；取消后会在协程当前暂停的 yield 处抛出 asyncio.CancelledError 异常。协程可以捕获这个异常，也可以延迟取消，甚至拒绝取消。

⓬ 获取事件循环的引用。

⓭ 驱动 supervisor 协程，让它运行完毕；这个协程的返回值是这次调用的返回值。

除非想阻塞主线程，从而冻结事件循环或整个应用，否则不要在 asyncio 协程中使用 time.sleep (...)。如果协程需要在一段时间内什么也不做，应该使用 yield from asyncio.sleep (DELAY)。

使用 @asyncio.coroutine 装饰器不是强制要求，但是强烈建议这么做，因为这样能在一众普通的函数中把协程凸显出来，也有助于调试：如果还没从中产出值，协程就被垃圾回收了（意味着有操作未完成，因此有可能是个缺陷），那就可以发出警告。这个装饰器不会预激协程。

注意，spinner_thread.py 和 spinner_asyncio.py 两个脚本的代码行数差不多。supervisor 函数是这两个示例的核心。下面详细对比二者。示例 18-3 只列出了线程版示例中的 supervisor 函数。

示例 18-3　spinner_thread.py：线程版 supervisor 函数

def supervisor(): signal = Signal() spinner = threading.Thread(target=spin, args=('thinking!', signal)) print('spinner object:', spinner) spinner.start() result = slow_function() signal.go = False spinner.join() return result

为了对比，示例 18-4 列出了 supervisor 协程。

示例 18-4　spinner_asyncio.py：异步版 supervisor 协程

@asyncio.coroutine def supervisor(): spinner = asyncio.async(spin('thinking!')) print('spinner object:', spinner) result = yield from slow_function() spinner.cancel() return result

这两种 supervisor 实现之间的主要区别概述如下。

asyncio.Task 对象差不多与 threading.Thread 对象等效。Victor Stinner（本章的特约技术审校）指出，「Task 对象像是实现协作式多任务的库（例如 gevent）中的绿色线程（green thread）」。

Task 对象用于驱动协程，Thread 对象用于调用可调用的对象。

Task 对象不由自己动手实例化，而是通过把协程传给 asyncio.async (...) 函数或 loop.create_task (...) 方法获取。

获取的 Task 对象已经排定了运行时间（例如，由 asyncio.async 函数排定）；Thread 实例则必须调用 start 方法，明确告知让它运行。

在线程版 supervisor 函数中，slow_function 函数是普通的函数，直接由线程调用。在异步版 supervisor 函数中，slow_function 函数是协程，由 yield from 驱动。

没有 API 能从外部终止线程，因为线程随时可能被中断，导致系统处于无效状态。如果想终止任务，可以使用 Task.cancel () 实例方法，在协程内部抛出 CancelledError 异常。协程可以在暂停的 yield 处捕获这个异常，处理终止请求。

supervisor 协程必须在 main 函数中由 loop.run_until_complete 方法执行。

上述比较应该能帮助你理解，与更熟悉的 threading 模型相比，asyncio 是如何编排并发作业的。

线程与协程之间的比较还有最后一点要说明：如果使用线程做过重要的编程，你就知道写出程序有多么困难，因为调度程序任何时候都能中断线程。必须记住保留锁，去保护程序中的重要部分，防止多步操作在执行的过程中中断，防止数据处于无效状态。

而协程默认会做好全方位保护，以防止中断。我们必须显式产出才能让程序的余下部分运行。对协程来说，无需保留锁，在多个线程之间同步操作，协程自身就会同步，因为在任意时刻只有一个协程运行。想交出控制权时，可以使用 yield 或 yield from 把控制权交还调度程序。这就是能够安全地取消协程的原因：按照定义，协程只能在暂停的 yield 处取消，因此可以处理 CancelledError 异常，执行清理操作。

下面说明 asyncio.Future 类与第 17 章所用的 concurrent.futures.Future 类之间的区别。

18.1.1　asyncio.Future：故意不阻塞

asyncio.Future 类与 concurrent.futures.Future 类的接口基本一致，不过实现方式不同，不可以互换。「PEP 3156—Asynchronous IO Support Rebooted: the‘asyncio’Module」对这个不幸状况是这样说的：

未来可能会统一 asyncio.Future 和 concurrent.futures.Future 类实现的期物（例如，为后者添加兼容 yield from 的 __iter__ 方法）。

如 17.1.3 节所述，期物只是调度执行某物的结果。在 asyncio 包中，BaseEventLoop.create_task (...) 方法接收一个协程，排定它的运行时间，然后返回一个 asyncio.Task 实例 —— 也是 asyncio.Future 类的实例，因为 Task 是 Future 的子类，用于包装协程。这与调用 Executor.submit (...) 方法创建 concurrent.futures.Future 实例是一个道理。

与 concurrent.futures.Future 类似，asyncio.Future 类也提供了 .done ()、.add_done_callback (...) 和 .result () 等方法。前两个方法的用法与 17.1.3 节所述的一样，不过 .result () 方法差别很大。

asyncio.Future 类的 .result () 方法没有参数，因此不能指定超时时间。此外，如果调用 .result () 方法时期物还没运行完毕，那么 .result () 方法不会阻塞去等待结果，而是抛出 asyncio.InvalidStateError 异常。

然而，获取 asyncio.Future 对象的结果通常使用 yield from，从中产出结果，如示例 18-8 所示。

使用 yield from 处理期物，等待期物运行完毕这一步无需我们关心，而且不会阻塞事件循环，因为在 asyncio 包中，yield from 的作用是把控制权还给事件循环。

注意，使用 yield from 处理期物与使用 add_done_callback 方法处理协程的作用一样：延迟的操作结束后，事件循环不会触发回调对象，而是设置期物的返回值；而 yield from 表达式则在暂停的协程中生成返回值，恢复执行协程。

总之，因为 asyncio.Future 类的目的是与 yield from 一起使用，所以通常不需要使用以下方法。

无需调用 my_future.add_done_callback (...)，因为可以直接把想在期物运行结束后执行的操作放在协程中 yield from my_future 表达式的后面。这是协程的一大优势：协程是可以暂停和恢复的函数。

无需调用 my_future.result ()，因为 yield from 从期物中产出的值就是结果（例如，result = yield from my_future）。

当然，有时也需要使用 .done ()、.add_done_callback (...) 和 .result () 方法。但是一般情况下，asyncio.Future 对象由 yield from 驱动，而不是靠调用这些方法驱动。

下面分析 yield from 和 asyncio 包的 API 如何拉近期物、任务和协程的关系。

18.1.2　从期物、任务和协程中产出

在 asyncio 包中，期物和协程关系紧密，因为可以使用 yield from 从 asyncio.Future 对象中产出结果。这意味着，如果 foo 是协程函数（调用后返回协程对象），抑或是返回 Future 或 Task 实例的普通函数，那么可以这样写：res = yield from foo ()。这是 asyncio 包的 API 中很多地方可以互换协程与期物的原因之一。

为了执行这些操作，必须排定协程的运行时间，然后使用 asyncio.Task 对象包装协程。对协程来说，获取 Task 对象有两种主要方式。

asyncio.async(coro_or_future, *, loop=None)

这个函数统一了协程和期物：第一个参数可以是二者中的任何一个。如果是 Future 或 Task 对象，那就原封不动地返回。如果是协程，那么 async 函数会调用 loop.create_task (...) 方法创建 Task 对象。loop= 关键字参数是可选的，用于传入事件循环；如果没有传入，那么 async 函数会通过调用 asyncio.get_event_loop () 函数获取循环对象。

BaseEventLoop.create_task(coro)

这个方法排定协程的执行时间，返回一个 asyncio.Task 对象。如果在自定义的 BaseEventLoop 子类上调用，返回的对象可能是外部库（如 Tornado）中与 Task 类兼容的某个类的实例。

BaseEventLoop.create_task (...) 方法只在 Python 3.4.2 及以上版本中可用。如果是 Python 3.3 或 Python 3.4 的旧版，要使用 asyncio.async (...) 函数，或者从 PyPI 中安装较新的 asyncio 版本。

asyncio 包中有多个函数会自动（内部使用的是 asyncio.async 函数）把参数指定的协程包装在 asyncio.Task 对象中，例如 BaseEventLoop.run_until_complete (...) 方法。

如果想在 Python 控制台或者小型测试脚本中试验期物和协程，可以使用下述代码片段：3

3 摘自 Petr Viktorin 于 2014 年 9 月 11 日在 Python-ideas 邮件列表中发布的消息。

>>> import asyncio >>> def run_sync(coro_or_future): ... loop = asyncio.get_event_loop() ... return loop.run_until_complete(coro_or_future) ... >>> a = run_sync(some_coroutine())

在 asyncio 包的文档中，「18.5.3. Tasks and coroutines」一节说明了协程、期物和任务之间的关系。其中有个注解说道：

这份文档把一些方法说成是协程，即使它们其实是返回 Future 对象的普通 Python 函数。这是故意的，为的是给以后修改这些函数的实现留下余地。

掌握这些基础知识后，接下来要分析异步下载国旗的 flags_asyncio.py 脚本。这个脚本的用法在示例 17-1（第 17 章）中与依序下载版和线程池版一同演示过。

18.2　使用 asyncio 和 aiohttp 包下载

从 Python 3.4 起，asyncio 包只直接支持 TCP 和 UDP。如果想使用 HTTP 或其他协议，那么要借助第三方包。当下，使用 asyncio 实现 HTTP 客户端和服务器时，使用的似乎都是 aiohttp 包。

示例 18-5 是下载国旗的 flags_asyncio.py 脚本的完整代码清单。运作方式简述如下。

(1) 首先，在 download_many 函数中获取一个事件循环，处理调用 download_one 函数生成的几个协程对象。

(2) asyncio 事件循环依次激活各个协程。

(3) 客户代码中的协程（如 get_flag）使用 yield from 把职责委托给库里的协程（如 aiohttp.request）时，控制权交还事件循环，执行之前排定的协程。

(4) 事件循环通过基于回调的低层 API，在阻塞的操作执行完毕后获得通知。

(5) 获得通知后，主循环把结果发给暂停的协程。

(6) 协程向前执行到下一个 yield from 表达式，例如 get_flag 函数中的 yield from resp.read ()。事件循环再次得到控制权，重复第 4~6 步，直到事件循环终止。

这与 16.9.2 节所见的示例类似。在那个示例中，主循环依次启动多个出租车进程；各个出租车进程产出结果后，主循环调度各个出租车的下一个事件（未来发生的事），然后激活队列中的下一个出租车进程。那个出租车仿真简单得多，主循环易于理解。不过，在 asyncio 中，基本的流程是一样的：在一个单线程程序中使用主循环依次激活队列里的协程。各个协程向前执行几步，然后把控制权让给主循环，主循环再激活队列里的下一个协程。

下面详细分析示例 18-5。

示例 18-5　flags_asyncio.py：使用 asyncio 和 aiohttp 包实现的异步下载脚本

import asyncio import aiohttp ➊ from flags import BASE_URL, save_flag, show, main ➋ @asyncio.coroutine ➌ def get_flag(cc): url = '{}/{cc}/{cc}.gif'.format(BASE_URL, cc=cc.lower()) resp = yield from aiohttp.request('GET', url) ➍ image = yield from resp.read() ➎ return image @asyncio.coroutine def download_one(cc): ➏ image = yield from get_flag(cc) ➐ show(cc) save_flag(image, cc.lower() + '.gif') return cc def download_many(cc_list): loop = asyncio.get_event_loop() ➑ to_do = [download_one(cc) for cc in sorted(cc_list)] ➒ wait_coro = asyncio.wait(to_do) ➓ res, _ = loop.run_until_complete(wait_coro) ⓫ loop.close() ⓬ return len(res) if __name__ == '__main__': main(download_many)

❶ 必须安装 aiohttp 包，它不在标准库中。4

4 可以使用 pip install aiohttp 命令安装 aiohttp 包。—— 编者注

❷ 重用 flags 模块（见示例 17-2）中的一些函数。

❸ 协程应该使用 @asyncio.coroutine 装饰。

❹ 阻塞的操作通过协程实现，客户代码通过 yield from 把职责委托给协程，以便异步运行协程。

❺ 读取响应内容是一项单独的异步操作。

❻ download_one 函数也必须是协程，因为用到了 yield from。

❼ 与依序下载版 download_one 函数唯一的区别是这一行中的 yield from；函数定义体中的其他代码与之前完全一样。

❽ 获取事件循环底层实现的引用。

❾ 调用 download_one 函数获取各个国旗，然后构建一个生成器对象列表。

❿ 虽然函数的名称是 wait，但它不是阻塞型函数。wait 是一个协程，等传给它的所有协程运行完毕后结束（这是 wait 函数的默认行为；参见这个示例后面的说明）。

⓫ 执行事件循环，直到 wait_coro 运行结束；事件循环运行的过程中，这个脚本会在这里阻塞。我们忽略 run_until_complete 方法返回的第二个元素。下文说明原因。

⓬关闭事件循环。

如果事件循环是上下文管理器就好了，这样我们就可以使用 with 块确保循环会被关闭。然而，实际情况是复杂的，客户代码绝不会直接创建事件循环，而是调用 asyncio.get_event_loop () 函数，获取事件循环的引用。而且有时我们的代码不「拥有」事件循环，因此关闭事件循环会出错。例如，使用 Quamash 这种包实现的外部 GUI 事件循环时，Qt 库负责在退出应用时关闭事件循环。

asyncio.wait (...) 协程的参数是一个由期物或协程构成的可迭代对象；wait 会分别把各个协程包装进一个 Task 对象。最终的结果是，wait 处理的所有对象都通过某种方式变成 Future 类的实例。wait 是协程函数，因此返回的是一个协程或生成器对象；wait_coro 变量中存储的正是这种对象。为了驱动协程，我们把协程传给 loop.run_until_complete (...) 方法。

loop.run_until_complete 方法的参数是一个期物或协程。如果是协程，run_until_complete 方法与 wait 函数一样，把协程包装进一个 Task 对象中。协程、期物和任务都能由 yield from 驱动，这正是 run_until_complete 方法对 wait 函数返回的 wait_coro 对象所做的事。wait_coro 运行结束后返回一个元组，第一个元素是一系列结束的期物，第二个元素是一系列未结束的期物。在示例 18-5 中，第二个元素始终为空，因此我们把它赋值给 _，将其忽略。但是 wait 函数有两个关键字参数，如果设定了可能会返回未结束的期物；这两个参数是 timeout 和 return_when。详情参见 asyncio.wait 函数的文档。

注意，在示例 18-5 中不能重用 flags.py 脚本（见示例 17-2）中的 get_flag 函数，因为那个函数用到了 requests 库，执行的是阻塞型 I/O 操作。为了使用 asyncio 包，我们必须把每个访问网络的函数改成异步版，使用 yield from 处理网络操作，这样才能把控制权交还给事件循环。在 get_flag 函数中使用 yield from，意味着它必须像协程那样驱动。

因此，不能重用 flags_threadpool.py 脚本（见示例 17-3）中的 download_one 函数。示例 18-5 中的代码使用 yield from 驱动 get_flag 函数，因此 download_one 函数本身也得是协程。每次请求时，download_many 函数会创建一个 download_one 协程对象；这些协程对象先使用 asyncio.wait 协程包装，然后由 loop.run_until_complete 方法驱动。

asyncio 包中有很多新概念要掌握，不过，如果你采用 Guido van Rossum 建议的一个技巧，就能轻松地理解示例 18-5 的总体逻辑：眯着眼，假装没有 yield from 关键字。这样做之后，你会发现示例 18-5 中的代码与纯粹依序下载的代码一样易于阅读。

比如说，以这个协程为例：

@asyncio.coroutine def get_flag(cc): url = '{}/{cc}/{cc}.gif'.format(BASE_URL, cc=cc.lower()) resp = yield from aiohttp.request('GET', url) image = yield from resp.read() return image

我们可以假设它与下述函数的作用相同，只不过协程版从不阻塞：

def get_flag(cc): url = '{}/{cc}/{cc}.gif'.format(BASE_URL, cc=cc.lower()) resp = aiohttp.request('GET', url) image = resp.read() return image

yield from foo 句法能防止阻塞，是因为当前协程（即包含 yield from 代码的委派生成器）暂停后，控制权回到事件循环手中，再去驱动其他协程；foo 期物或协程运行完毕后，把结果返回给暂停的协程，将其恢复。

在 16.7 节的末尾，我对 yield from 的用法做了两点陈述，摘要如下。

使用 yield from 链接的多个协程最终必须由不是协程的调用方驱动，调用方显式或隐式（例如，在 for 循环中）在最外层委派生成器上调用 next (...) 函数或 .send (...) 方法。

链条中最内层的子生成器必须是简单的生成器（只使用 yield）或可迭代的对象。

在 asyncio 包的 API 中使用 yield from 时，这两点都成立，不过要注意下述细节。

我们编写的协程链条始终通过把最外层委派生成器传给 asyncio 包 API 中的某个函数（如 loop.run_until_complete (...)）驱动。

也就是说，使用 asyncio 包时，我们编写的代码不通过调用 next (...) 函数或 .send (...) 方法驱动协程 —— 这一点由 asyncio 包实现的事件循环去做。

我们编写的协程链条最终通过 yield from 把职责委托给 asyncio 包中的某个协程函数或协程方法（例如示例 18-2 中的 yield from asyncio.sleep (...)），或者其他库中实现高层协议的协程（例如示例 18-5 中 get_flag 协程里的 resp = yield from aiohttp. request ('GET', url)）。

也就是说，最内层的子生成器是库中真正执行 I/O 操作的函数，而不是我们自己编写的函数。

概括起来就是：使用 asyncio 包时，我们编写的异步代码中包含由 asyncio 本身驱动的协程（即委派生成器），而生成器最终把职责委托给 asyncio 包或第三方库（如 aiohttp）中的协程。这种处理方式相当于架起了管道，让 asyncio 事件循环（通过我们编写的协程）驱动执行低层异步 I/O 操作的库函数。

现在可以回答第 17 章提出的那个问题了。

flags_asyncio.py 脚本和 flags.py 脚本都在单个线程中运行，前者怎么会比后者快 5 倍？

18.3　避免阻塞型调用

Ryan Dahl（Node.js 的发明者）在介绍他的项目背后的哲学时说：「我们处理 I/O 的方式彻底错了。」5 他把执行硬盘或网络 I/O 操作的函数定义为阻塞型函数，主张不能像对待非阻塞型函数那样对待阻塞型函数。为了说明原因，他展示了表 18-1 中的前两列。

5「Introduction to Node.js」视频 4:55 处。

表 18-1：使用现代的电脑从不同的存储介质中读取数据的延迟情况；第三栏按比例换算成具体的时间，便于人类理解

存储介质

CPU 周期

按比例换算成「人类时间」

L1 缓存

3

3 秒

L2 缓存

14

14 秒

RAM

250

250 秒

硬盘

41 000 000

1.3 年

网络

240 000 000

7.6 年

为了理解表 18-1，请记住一点：现代的 CPU 拥有 GHz 数量级的时钟频率，每秒钟能运行几十亿个周期。假设 CPU 每秒正好运行十亿个周期，那么 CPU 可以在一秒钟内读取 L1 缓存 333 333 333 次，读取网络 4 次（只有 4 次）。表 18-1 中的第三栏是拿第二栏中的各个值乘以固定的因子得到的。因此，在另一个世界中，如果读取 L1 缓存要用 3 秒，那么读取网络要用 7.6 年！

有两种方法能避免阻塞型调用中止整个应用程序的进程：

在单独的线程中运行各个阻塞型操作

把每个阻塞型操作转换成非阻塞的异步调用使用

多个线程是可以的，但是各个操作系统线程（Python 使用的是这种线程）消耗的内存达兆字节（具体的量取决于操作系统种类）。如果要处理几千个连接，而每个连接都使用一个线程的话，我们负担不起。

为了降低内存的消耗，通常使用回调来实现异步调用。这是一种低层概念，类似于所有并发机制中最古老、最原始的那种 —— 硬件中断。使用回调时，我们不等待响应，而是注册一个函数，在发生某件事时调用。这样，所有调用都是非阻塞的。因为回调简单，而且消耗低，所以 Ryan Dahl 拥护这种方式。

当然，只有异步应用程序底层的事件循环能依靠基础设置的中断、线程、轮询和后台进程等，确保多个并发请求能取得进展并最终完成，这样才能使用回调。6 事件循环获得响应后，会回过头来调用我们指定的回调。不过，如果做法正确，事件循环和应用代码共用的主线程绝不会阻塞。

6 其实，虽然 Node.js 不支持使用 JavaScript 编写的用户级线程，但是在背后却借助 libeio 库使用 C 语言实现了线程池，以此提供基于回调的文件 API—— 因为从 2014 年起，大多数操作系统都不提供稳定且便携的异步文件处理 API 了。

把生成器当作协程使用是异步编程的另一种方式。对事件循环来说，调用回调与在暂停的协程上调用 .send () 方法效果差不多。各个暂停的协程是要消耗内存，但是比线程消耗的内存数量级小。而且，协程能避免可怕的「回调地狱」；这一点会在 18.5 节讨论。

现在你应该能理解为什么 flags_asyncio.py 脚本的性能比 flags.py 脚本高 5 倍了：flags.py 脚本依序下载，而每次下载都要用几十亿个 CPU 周期等待结果。其实，CPU 同时做了很多事，只是没有运行你的程序。与此相比，在 flags_asyncio.py 脚本中，在 download_many 函数中调用 loop.run_until_complete 方法时，事件循环驱动各个 download_one 协程，运行到第一个 yield from 表达式处，那个表达式又驱动各个 get_flag 协程，运行到第一个 yield from 表达式处，调用 aiohttp.request (...) 函数。这些调用都不会阻塞，因此在零点几秒内所有请求全部开始。

asyncio 的基础设施获得第一个响应后，事件循环把响应发给等待结果的 get_flag 协程。得到响应后，get_flag 向前执行到下一个 yield from 表达式处，调用 resp.read () 方法，然后把控制权还给主循环。其他响应会陆续返回（因为请求几乎同时发出）。所有 get_ flag 协程都获得结果后，委派生成器 download_one 恢复，保存图像文件。

为了尽量提高性能，save_flag 函数应该执行异步操作，可是 asyncio 包目前没有提供异步文件系统 API（Node 有）。如果这是应用的瓶颈，可以使用 loop.run_in_executor 方法，在线程池中运行 save_flag 函数。示例 18-9 会说明做法。

因为异步操作是交叉执行的，所以并发下载多张图像所需的总时间比依序下载少得多。我使用 asyncio 包发起了 600 个 HTTP 请求，获得所有结果的时间比依序下载快 70 倍。

现在回到那个 HTTP 客户端示例，看看如何显示动态的进度条，并且恰当地处理错误。

18.4　改进 asyncio 下载脚本

17.5 节说过，flags2 系列示例的命令行接口相同。本节要分析这个系列中的 flags2_asyncio.py 脚本。例如，示例 18-6 展示如何使用 100 个并发请求（-m 100）从 ERROR 服务器中下载 100 面国旗（-al 100）。

示例 18-6　运行 flags2_asyncio.py 脚本

$ python3 flags2_asyncio.py -s ERROR -al 100 -m 100 ERROR site: http://localhost:8003/flags Searching for 100 flags: from AD to LK 100 concurrent connections will be used. -------------------- 73 flags downloaded. 27 errors. Elapsed time: 0.64s

测试并发客户端要谨慎

尽管线程版和 asyncio 版 HTTP 客户端的下载总时间相差无几，但是 asyncio 版发送请求的速度更快，因此很有可能对服务器发起 DoS 攻击。为了全速测试这些并发客户端，应该在本地搭建 HTTP 服务器，详情参见本书代码仓库中 17-futures/countries/ 目录里的 README.rst 文件。

下面分析 flags2_asyncio.py 脚本的实现方式。

18.4.1　使用 asyncio.as_completed 函数

在示例 18-5 中，我把一个协程列表传给 asyncio.wait 函数，经由 loop.run_until_complete 方法驱动，全部协程运行完毕后，这个函数会返回所有下载结果。可是，为了更新进度条，各个协程运行结束后就要立即获取结果。在线程池版示例中（见示例 17-14），为了集成进度条，我们使用的是 as_completed 生成器函数；幸好，asyncio 包提供了这个生成器函数的相应版本。

为了使用 asyncio 包实现 flags2 示例，我们要重写几个函数；重写后的函数可以供 concurrent.future 版重用。之所以要重写，是因为在使用 asyncio 包的程序中只有一个主线程，而在这个线程中不能有阻塞型调用，因为事件循环也在这个线程中运行。所以，我要重写 get_flag 函数，使用 yield from 访问网络。现在，由于 get_flag 是协程，download_one 函数必须使用 yield from 驱动它，因此 download_one 自己也要变成协程。之前，在示例 18-5 中，download_one 由 download_many 驱动：download_one 函数由 asyncio. wait 函数调用，然后传给 loop.run_until_complete 方法。现在，为了报告进度并处理错误，我们要更精确地控制，所以我把 download_many 函数中的大多数逻辑移到一个新的协程 downloader_coro 中，只在 download_many 函数中设置事件循环，以及调度 downloader_coro 协程。

示例 18-7 展示的是 flags2_asyncio.py 脚本的前半部分，定义 get_flag 和 download_one 协程。示例 18-8 列出余下的源码，定义 downloader_coro 协程和 download_many 函数。

示例 18-7　flags2_asyncio.py：脚本的前半部分；余下的代码在示例 18-8 中

import asyncio import collections import aiohttp from aiohttp import web import tqdm from flags2_common import main, HTTPStatus, Result, save_flag # 默认设为较小的值，防止远程网站出错 # 例如 503 - Service Temporarily Unavailable DEFAULT_CONCUR_REQ = 5 MAX_CONCUR_REQ = 1000 class FetchError (Exception): ➊ def __init__(self, country_code): self.country_code = country_code @asyncio.coroutine def get_flag (base_url, cc): ➋ url = '{}/{cc}/{cc}.gif'.format (base_url, cc=cc.lower ()) resp = yield from aiohttp.request ('GET', url) if resp.status == 200: image = yield from resp.read () return image elif resp.status == 404: raise web.HTTPNotFound () else: raise aiohttp.HttpProcessingError (code=resp.status, message=resp.reason, headers=resp.headers) @asyncio.coroutine def download_one (cc, base_url, semaphore, verbose): ➌ try: with (yield from semaphore): ➍ image = yield from get_flag (base_url, cc) ➎ except web.HTTPNotFound: ➏ status = HTTPStatus.not_found msg = 'not found' except Exception as exc: raise FetchError (cc) from exc ➐ else: save_flag (image, cc.lower () + '.gif') ➑ status = HTTPStatus.ok msg = 'OK' if verbose and msg: print (cc, msg) return Result (status, cc)

❶ 这个自定义的异常用于包装其他 HTTP 或网络异常，并获取 country_code，以便报告错误。

❷ get_flag 协程有三种返回结果：返回下载得到的图像；HTTP 响应码为 404 时，抛出 web.HTTPNotFound 异常；返回其他 HTTP 状态码时，抛出 aiohttp.HttpProcessingError 异常。

❸ semaphore 参数是 asyncio.Semaphore 类的实例。Semaphore 类是同步装置，用于限制并发请求数量。

❹ 在 yield from 表达式中把 semaphore 当成上下文管理器使用，防止阻塞整个系统：如果 semaphore 计数器的值是所允许的最大值，只有这个协程会阻塞。

❺ 退出这个 with 语句后，semaphore 计数器的值会递减，解除阻塞可能在等待同一个 semaphore 对象的其他协程实例。

❻ 如果没找到国旗，相应地设置 Result 的状态。

❼ 其他异常当作 FetchError 抛出，传入国家代码，并使用「PEP 3134 — Exception Chaining and Embedded Tracebacks」引入的 raise X from Y 句法链接原来的异常。

❽ 这个函数的作用是把国旗文件保存到硬盘中。

可以看出，与依序下载版相比，示例 18-7 中的 get_flag 和 download_one 函数改动幅度很大，因为现在这两个函数是协程，要使用 yield from 做异步调用。

对于我们分析的这种网络客户端代码来说，一定要使用某种限流机制，防止向服务器发起太多并发请求，因为如果服务器过载，那么系统的整体性能可能会下降。flags2_threadpool.py 脚本（见示例 17-14）限流的方法是，在 download_many 函数中实例化 ThreadPoolExecutor 类时把 max_workers 参数的值设为 concur_req，只在线程池中启动 concur_req 个线程。在 flags2_asyncio.py 脚本中我的做法是，在 downloader_coro 函数中创建一个 asyncio.Semaphore 实例（在后面的示例 18-8 中），然后把它传给示例 18-7 中 download_one 函数的 semaphore 参数。7

7 感谢 Guto Maia 指出本书的草稿没有说明 Semaphore 类。

Semaphore 对象维护着一个内部计数器，若在对象上调用 .acquire () 协程方法，计数器则递减；若在对象上调用 .release () 协程方法，计数器则递增。计数器的初始值在实例化 Semaphore 时设定，如 downloader_coro 函数中的这一行所示：

semaphore = asyncio.Semaphore(concur_req)

如果计数器大于零，那么调用 .acquire () 方法不会阻塞；可是，如果计数器为零，那么 .acquire () 方法会阻塞调用这个方法的协程，直到其他协程在同一个 Semaphore 对象上调用 .release () 方法，让计数器递增。在示例 18-7 中，我没有调用 .acquire () 或 .release () 方法，而是在 download_one 函数中的下述代码块中把 semaphore 当作上下文管理器使用：

with (yield from semaphore): image = yield from get_flag(base_url, cc)

这段代码保证，任何时候都不会有超过 concur_req 个 get_flag 协程启动。

现在来分析示例 18-8 中这个脚本余下的代码。注意，download_many 函数中以前的大多数功能现在都在 downloader_coro 协程中。我们必须这么做，因为必须使用 yield from 获取 asyncio.as_completed 函数产出的期物的结果，所以 as_completed 函数必须在协程中调用。可是，我不能直接把 download_many 函数改成协程，因为必须在脚本的最后一行把 download_many 函数传给 flags2_common 模块中定义的 main 函数，可 main 函数的参数不是协程，而是一个普通的函数。因此，我定义了 downloader_coro 协程，让它运行 as_completed 循环。现在，download_many 函数只用于设置事件循环，并把 downloader_coro 协程传给 loop.run_until_complete 方法，调度 downloader_coro。

示例 18-8　flags2_asyncio.py：接续示例 18-7

@asyncio.coroutine def downloader_coro(cc_list, base_url, verbose, concur_req): ➊ counter = collections.Counter() semaphore = asyncio.Semaphore(concur_req) ➋ to_do = [download_one(cc, base_url, semaphore, verbose) for cc in sorted(cc_list)] ➌ to_do_iter = asyncio.as_completed(to_do) ➍ if not verbose: to_do_iter = tqdm.tqdm(to_do_iter, total=len(cc_list)) ➎ for future in to_do_iter: ➏ try: res = yield from future ➐ except FetchError as exc: ➑ country_code = exc.country_code ➒ try: error_msg = exc.__cause__.args[0] ➓ except IndexError: error_msg = exc.__cause__.__class__.__name__ ⓫ if verbose and error_msg: msg = '*** Error for {}: {}' print(msg.format(country_code, error_msg)) status = HTTPStatus.error else: status = res.status counter[status] += 1 ⓬ return counter ⓭ def download_many(cc_list, base_url, verbose, concur_req): loop = asyncio.get_event_loop() coro = downloader_coro(cc_list, base_url, verbose, concur_req) counts = loop.run_until_complete(coro) ⓮ loop.close() ⓯ return counts if __name__ == '__main__': main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ)

❶ 这个协程的参数与 download_many 函数一样，但是不能直接调用，因为它是协程函数，而不是像 download_many 那样的普通函数。

❷ 创建一个 asyncio.Semaphore 实例，最多允许激活 concur_req 个使用这个计数器的协程。

❸ 多次调用 download_one 协程，创建一个协程对象列表。

❹ 获取一个迭代器，这个迭代器会在期物运行结束后返回期物。

❺ 把迭代器传给 tqdm 函数，显示进度。

❻ 迭代运行结束的期物；这个循环与示例 17-14 中 download_many 函数里的那个十分相似；不同的部分主要是异常处理，因为两个 HTTP 库（requests 和 aiohttp）之间有差异。

❼ 获取 asyncio.Future 对象的结果，最简单的方法是使用 yield from，而不是调用 future.result () 方法。

❽ download_one 函数抛出的各个异常都包装在 FetchError 对象里，并且链接原来的异常。

❾ 从 FetchError 异常中获取错误发生时的国家代码。

❿ 尝试从原来的异常（__cause__）中获取错误消息。

⓫ 如果在原来的异常中找不到错误消息，使用所链接异常的类名作为错误消息。

⓬ 记录结果。

⓭ 与其他脚本一样，返回计数器。

⓮ download_many 函数只是实例化 downloader_coro 协程，然后通过 run_until_complete 方法把它传给事件循环。

⓯ 所有工作做完后，关闭事件循环，返回 counts。

在示例 18-8 中不能像示例 17-14 那样把期物映射到国家代码上，因为 asyncio.as_completed 函数返回的期物与传给 as_completed 函数的期物可能不同。在 asyncio 包内部，我们提供的期物会被替换成生成相同结果的期物。8

8 关于这一点的详细讨论，可以阅读我在 python-tulip 讨论组中发起的话题，题为「Which other futures my come out of asyncio.as_completed?」。Guido 回复了，而且深入分析了 as_completed 函数的实现，还说明了 asyncio 包中期物与协程之间的紧密关系。

因为失败时不能以期物为键从字典中获取国家代码，所以我实现了自定义的 FetchError 异常（如示例 18-7 所示）。FetchError 包装网络异常，并关联相应的国家代码，因此在详细模式中报告错误时能显示国家代码。如果没有错误，那么国家代码是 for 循环顶部那个 yield from future 表达式的结果。

我们使用 asyncio 包实现的这个示例与前面的 flags2_threadpool.py 脚本具有相同的功能，这一话题到此结束。接下来，我们要改进 flags2_asyncio.py 脚本，进一步探索 asyncio 包。

在分析示例 18-7 的过程中，我发现 save_flag 函数会执行硬盘 I/O 操作，而这应该异步执行。下一节说明做法。

18.4.2　使用 Executor 对象，防止阻塞事件循环

Python 社区往往会忽略一个事实 —— 访问本地文件系统会阻塞，想当然地认为这种操作不会受网络访问的高延迟影响（这也极难预料）。与之相比，Node.js 程序员则始终谨记，所有文件系统函数都会阻塞，因为这些函数的签名中指明了要有回调。表 18-1 已经指出，硬盘 I/O 阻塞会浪费几百万个 CPU 周期，而这可能会对应用程序的性能产生重大影响。

在示例 18-7 中，阻塞型函数是 save_flag。在这个脚本的线程版中（见示例 17-14），save_flag 函数会阻塞运行 download_one 函数的线程，但是阻塞的只是众多工作线程中的一个。阻塞型 I/O 调用在背后会释放 GIL，因此另一个线程可以继续。但是在 flags2_asyncio.py 脚本中，save_flag 函数阻塞了客户代码与 asyncio 事件循环共用的唯一线程，因此保存文件时，整个应用程序都会冻结。这个问题的解决方法是，使用事件循环对象的 run_in_executor 方法。

asyncio 的事件循环在背后维护着一个 ThreadPoolExecutor 对象，我们可以调用 run_in_executor 方法，把可调用的对象发给它执行。若想在这个示例中使用这个功能，download_one 协程只有几行代码需要改动，如示例 18-9 所示。

示例 18-9　flags2_asyncio_executor.py：使用默认的 ThreadPoolExecutor 对象运行 save_flag 函数

@asyncio.coroutine def download_one(cc, base_url, semaphore, verbose): try: with (yield from semaphore): image = yield from get_flag(base_url, cc) except web.HTTPNotFound: status = HTTPStatus.not_found msg = 'not found' except Exception as exc: raise FetchError(cc) from exc else: loop = asyncio.get_event_loop() ➊ loop.run_in_executor(None, ➋ save_flag, image, cc.lower() + '.gif') ➌ status = HTTPStatus.ok msg = 'OK' if verbose and msg: print(cc, msg) return Result(status, cc)

❶ 获取事件循环对象的引用。

❷ run_in_executor 方法的第一个参数是 Executor 实例；如果设为 None，使用事件循环的默认 ThreadPoolExecutor 实例。

❸ 余下的参数是可调用的对象，以及可调用对象的位置参数。

我测试示例 18-9 时，没有发现改用 run_in_executor 方法保存图像文件后性能有明显变化，因为图像都不大（平均 13KB）。不过，如果编辑 flags2_common.py 脚本中的 save_flag 函数，把各个文件保存的字节数变成原来的 10 倍（只需把 fp.write (img) 改成 fp.write (img*10)），此时便会看到效果。下载的平均字节数变成 130KB 后，使用 run_in_executor 方法的好处就体现出来了。如果下载包含百万像素的图像，速度提升更明显。

如果需要协调异步请求，而不只是发起完全独立的请求，协程较之回调的好处会变得显而易见。下一节说明回调的问题，并给出解决方法。

18.5　从回调到期物和协程

使用协程做面向事件编程，需要下一番功夫才能掌握，因此最好知道，与经典的回调式编程相比，协程有哪些改进。这就是本节的话题。

只要对回调式面向事件编程有一定的经验，就知道「回调地狱」这个术语：如果一个操作需要依赖之前操作的结果，那就得嵌套回调。如果要连续做 3 次异步调用，那就需要嵌套 3 层回调。示例 18-10 是一个使用 JavaScript 编写的例子。

示例 18-10　JavaScript 中的回调地狱：嵌套匿名函数，也称为灾难金字塔

api_call1 (request1, function (response1) {// 第一步 var request2 = step1 (response1); api_call2 (request2, function (response2) {// 第二步 var request3 = step2 (response2); api_call3 (request3, function (response3) {// 第三步 step3 (response3); }); }); });

在示例 18-10 中，api_call1、api_call2 和 api_call3 是库函数，用于异步获取结果。例如，api_call1 从数据库中获取结果，api_call2 从 Web 服务器中获取结果。这 3 个函数都有回调。在 JavaScript 中，回调通常使用匿名函数实现（在下述 Python 示例中分别把这 3 个回调命名为 stage1、stage2 和 stage3）。这里的 step1、step2 和 step3 是应用程序中的常规函数，用于处理回调接收到的响应。

示例 18-11 展示 Python 中的回调地狱是什么样子。

示例 18-11　Python 中的回调地狱：链式回调

def stage1(response1): request2 = step1(response1) api_call2(request2, stage2) def stage2(response2): request3 = step2(response2) api_call3(request3, stage3) def stage3(response3): step3(response3) api_call1(request1, stage1)

虽然示例 18-11 中的代码与示例 18-10 的排布方式差异很大，但是作用却完全相同。前述 JavaScript 示例也能改写成这种排布方式（但是这段 Python 代码不能改写成 JavaScript 那种风格，因为 lambda 表达式句法上有限制）。

示例 18-10 和示例 18-11 组织代码的方式导致代码难以阅读，也更难编写：每个函数做一部分工作，设置下一个回调，然后返回，让事件循环继续运行。这样，所有本地的上下文都会丢失。执行下一个回调时（例如 stage2），就无法获取 request2 的值。如果需要那个值，那就必须依靠闭包，或者把它存储在外部数据结构中，以便在处理过程的不同阶段使用。

在这个问题上，协程能发挥很大的作用。在协程中，如果要连续执行 3 个异步操作，只需使用 yield3 次，让事件循环继续运行。准备好结果后，调用 .send () 方法，激活协程。对事件循环来说，这种做法与调用回调类似。但是对使用协程式异步 API 的用户来说，情况就大为不同了：3 次操作都在同一个函数定义体中，像是顺序代码，能在处理过程中使用局部变量保留整个任务的上下文。请看示例 18-12。

示例 18-12　使用协程和 yield from 结构做异步编程，无需使用回调

@asyncio.coroutine def three_stages (request1): response1 = yield from api_call1 (request1) # 第一步 request2 = step1 (response1) response2 = yield from api_call2 (request2) # 第二步 request3 = step2 (response2) response3 = yield from api_call3 (request3) # 第三步 step3 (response3) loop.create_task (three_stages (request1)) # 必须显式调度执行

与前面的 JavaScript 和 Python 示例相比，示例 18-12 容易理解多了：操作的 3 个步骤依次写在同一个函数中。这样，后续处理便于使用前一步的结果；而且提供了上下文，能通过异常来报告错误。

假设在示例 18-11 中处理 api_call2 (request2, stage2) 调用（stage1 函数最后一行）时抛出了 I/O 异常，这个异常无法在 stage1 函数中捕获，因为 api_call2 是异步调用，还未执行任何 I/O 操作就会立即返回。在基于回调的 API 中，这个问题的解决方法是为每个异步调用注册两个回调，一个用于处理操作成功时返回的结果，另一个用于处理错误。一旦涉及错误处理，回调地狱的危害程度就会迅速增大。

与此相比，在示例 18-12 中，那个三步操作的所有异步调用都在同一个函数中（three_stages），如果异步调用 api_call1、api_call2 和 api_call3 会抛出异常，那么可以把相应的 yield from 表达式放在 try/except 块中处理异常。

这么做比陷入回调地狱好多了，但是我不会把这种方式称为协程天堂，毕竟我们还要付出代价。我们不能使用常规的函数，必须使用协程，而且要习惯 yield from—— 这是第一个障碍。只要函数中有 yield from，函数就会变成协程，而协程不能直接调用，即不能像示例 18-11 中那样调用 api_call1 (request1, stage1) 来启动回调链。我们必须使用事件循环显式排定协程的执行时间，或者在其他排定了执行时间的协程中使用 yield from 表达式把它激活。如果示例 18-12 没有最后一行（loop.create_task (three_stages (request1))），那么什么也不会发生。

下面举个例子来实践这个理论。

每次下载发起多次请求

假设保存每面国旗时，我们不仅想在文件名中使用国家代码，还想加上国家名称。那么，下载每面国旗时要发起两个请求：一个请求用于获取国旗，另一个请求用于获取图像所在目录里的 metadata.json 文件（记录着国家名称）。

在同一个任务中发起多个请求，这对线程版脚本来说很容易：只需接连发起两次请求，阻塞线程两次，把国家代码和国家名称保存在局部变量中，在保存文件时使用。如果想在异步脚本中使用回调做到这一点，你会闻到回调地狱中飘来的硫磺味道：国家代码和名称要放在闭包中传来传去，或者保存在某个地方，在保存文件时使用，这么做是因为各个回调在不同的局部上下文中运行。协程和 yield from 结构能缓解这个问题。解决方法虽然没有使用多个线程那么简单，但是比链式或嵌套回调易于管理。

示例 18-13 是使用 asyncio 包下载国旗脚本的第 3 版，这一次国旗的文件名中有国家名称。flags2_asyncio.py 脚本（示例 18-7 和示例 18-8）中的 download_many 函数和 downloader_coro 协程没变，有变化的是下面的内容。

download_one

现在，这个协程使用 yield from 把职责委托给 get_flag 协程和新添的 get_country 协程。

get_flag

这个协程的大多数代码移到新添的 http_get 协程中了，以便也能在 get_country 协程中使用。

get_country

这个协程获取国家代码相应的 metadata.json 文件，从文件中读取国家名称。

http_get

从 Web 获取文件的通用代码。

示例 18-13　 flags3_asyncio.py：再定义几个协程，把职责委托出去，每次下载国旗时发起两次请求

@asyncio.coroutine def http_get(url): res = yield from aiohttp.request('GET', url) if res.status == 200: ctype = res.headers.get('Content-type', '').lower() if 'json' in ctype or url.endswith('json'): data = yield from res.json() ➊ else: data = yield from res.read() ➋ return data elif res.status == 404: raise web.HTTPNotFound() else: raise aiohttp.errors.HttpProcessingError( code=res.status, message=res.reason, headers=res.headers) @asyncio.coroutine def get_country(base_url, cc): url = '{}/{cc}/metadata.json'.format(base_url, cc=cc.lower()) metadata = yield from http_get(url) ➌ return metadata['country'] @asyncio.coroutine def get_flag(base_url, cc): url = '{}/{cc}/{cc}.gif'.format(base_url, cc=cc.lower()) return (yield from http_get(url)) ➍ @asyncio.coroutine def download_one(cc, base_url, semaphore, verbose): try: with (yield from semaphore): ➎ image = yield from get_flag(base_url, cc) with (yield from semaphore): country = yield from get_country(base_url, cc) except web.HTTPNotFound: status = HTTPStatus.not_found msg = 'not found' except Exception as exc: raise FetchError(cc) from exc else: country = country.replace(' ', '_') filename = '{}-{}.gif'.format(country, cc) loop = asyncio.get_event_loop() loop.run_in_executor(None, save_flag, image, filename) status = HTTPStatus.ok msg = 'OK' if verbose and msg: print(cc, msg) return Result(status, cc)

❶ 如果内容类型中包含 'json'，或者 url 以 .json 结尾，那么在响应上调用 .json () 方法，解析响应，返回一个 Python 数据结构 —— 在这里是一个字典。

❷ 否则，使用 .read () 方法读取原始字节。

❸ metadata 变量的值是一个由 JSON 内容构建的 Python 字典。

❹ 这里必须在外层加上括号，如果直接写 return yield from，Python 解析器会不明所以，报告句法错误。

❺ 我分别在 semaphore 控制的两个 with 块中调用 get_flag 和 get_country，因为我想尽量缩减下载时间。

在示例 18-13 中，yield from 句法出现了 9 次。现在，你应该已经熟知如何在协程中使用这个结构把职责委托给另一个协程，而不阻塞事件循环。

问题的关键是，知道何时该使用 yield from，何时不该使用。基本原则很简单，yield from 只能用于协程和 asyncio.Future 实例（包括 Task 实例）。可是有些 API 很棘手，肆意混淆协程和普通的函数，例如下一节实现某个服务器时使用的 StreamWriter 类。

示例 18-13 是本书最后一次讨论 flags2 系列示例。我建议你自己运行那些示例，有助于对 HTTP 并发客户端的运作方式建立直观认识。你可以使用 -a、-e 和 -l 这三个命令行选项控制下载的国旗数量，还可以使用 -m 选项设置并发下载数。此外，还可以分别使用 LOCAL、REMOTE、DELAY 和 ERROR 服务器测试，找出能最大限度地利用各个服务器的吞吐量的并发下载数。如果想去掉错误或延迟，可以修改 vaurien_error_delay.sh 脚本中的设置。

客户端脚本到此结束，接下来使用 asyncio 包编写服务器。

18.6　使用 asyncio 包编写服务器

演示 TCP 服务器时通常使用回显服务器。我们要构建更好玩一点的示例服务器，用于查找 Unicode 字符，分别使用简单的 TCP 协议和 HTTP 协议实现。这两个服务器的作用是，让客户端使用 4.8 节讨论过的 unicodedata 模块，通过规范名称查找 Unicode 字符。图 18-2 展示了在一个 Telnet 会话中访问 TCP 版字符查找服务器所做的两次查询，一次查询国际象棋棋子字符，一次查询名称中包含「sun」的字符。

图 18-2：在一个 Telnet 会话中访问 tcp_charfinder.py 服务器 —— 查询「chess black」和「sun」

接下来讨论实现方式。

18.6.1　使用 asyncio 包编写 TCP 服务器

下面几个示例的大多数逻辑在 charfinder.py 模块中，这个模块没有任何并发。你可以在命令行中使用 charfinder.py 脚本查找字符，不过这个脚本更为重要的作用是为使用 asyncio 包编写的服务器提供支持。charfinder.py 脚本的代码在本书的代码仓库中。

charfinder 模块读取 Python 内建的 Unicode 数据库，为每个字符名称中的每个单词建立索引，然后倒排索引，存进一个字典。例如，在倒排索引中，'SUN' 键对应的条目是一个集合（set），里面是名称中包含 'SUN' 这个词的 10 个 Unicode 字符。9 倒排索引保存在本地一个名为 charfinder_index.pickle 的文件中。如果查询多个单词，charfinder 会计算从索引中所得集合的交集。

9 在 Python 3.5 中，新增了 4 个名称中包含 'SUN' 的 Unicode 字符：U+1F323（WHITE SUN）、U+1F324（WHITE SUN WITH SMALL CLOUD）、U+1F325（WHITE SUN BEHIND CLOUD）和 U+1F326（WHITE SUN BEHIND CLOUD WITH RAIN）。—— 编者注

下面我们把注意力集中在响应图 18-2 中那两个查询的 tcp_charfinder.py 脚本上。我要对这个脚本中的代码做大量说明，因此把它分为两部分，分别在示例 18-14 和示例 18-15 中列出。

示例 18-14　tcp_charfinder.py：使用 asyncio.start_server 函数实现的简易 TCP 服务器；这个模块余下的代码在示例 18-15 中

import sys import asyncio from charfinder import UnicodeNameIndex ➊ CRLF = b'\r\n' PROMPT = b'?> ' index = UnicodeNameIndex () ➋ @asyncio.coroutine def handle_queries (reader, writer): ➌ while True: ➍ writer.write (PROMPT) # 不能使用 yield from！ ➎ yield from writer.drain () # 必须使用 yield from！ ➏ data = yield from reader.readline () ➐ try: query = data.decode ().strip () except UnicodeDecodeError: ➑ query = '\x00' client = writer.get_extra_info ('peername') ➒ print ('Received from {}: {!r}'.format (client, query)) ➓ if query: if ord (query [:1]) <32: ⓫ break lines = list (index.find_description_strs (query)) ⓬ if lines: writer.writelines (line.encode () + CRLF for line in lines) ⓭ writer.write (index.status (query, len (lines)).encode () + CRLF) ⓮ yield from writer.drain () ⓯ print ('Sent {} results'.format (len (lines))) ⓰ print ('Close the client socket') ⓱ writer.close () ⓲

❶ UnicodeNameIndex 类用于构建名称索引，提供查询方法。

❷ 实例化 UnicodeNameIndex 类时，它会使用 charfinder_index.pickle 文件（如果有的话），或者构建这个文件，因此第一次运行时可能要等几秒钟服务器才能启动。10

10Leonardo Rochael 指出，可以在示例 18-15 中的 main 函数里使用 loop.run_with_executor () 方法，在另一个线程中构建 Unicode 名称索引，这样索引构建好之后，服务器能立即开始接收请求。他说得对，不过这个应用的唯一用途是查询索引，因此那样做没有多大好处。不过，Leo 建议的做法是个不错的练习，有兴趣的话你可以去做。

❸ 这个协程要传给 asyncio.start_server 函数，接收的两个参数是 asyncio.StreamReader 对象和 asyncio.StreamWriter 对象。

❹ 这个循环处理会话，直到从客户端收到控制字符后退出。

❺ StreamWriter.write 方法不是协程，只是普通的函数；这一行代码发送？> 提示符。

❻ StreamWriter.drain 方法刷新 writer 缓冲；因为它是协程，所以必须使用 yield from 调用。

❼ StreamReader.readline 方法是协程，返回一个 bytes 对象。

❽ Telnet 客户端发送控制字符时，可能会抛出 UnicodeDecodeError 异常；遇到这种情况时，为了简单起见，假装发送的是空字符。

❾ 返回与套接字连接的远程地址。

❿ 在服务器的控制台中记录查询。

⓫ 如果收到控制字符或者空字符，退出循环。

⓬ 返回一个生成器，产出包含 Unicode 码位、真正的字符和字符名称的字符串（例如，U+0039\t9\tDIGIT NINE）；为了简单起见，我从中构建了一个列表。

⓭ 使用默认的 UTF-8 编码把 lines 转换成 bytes 对象，并在每一行末尾添加回车符和换行符；注意，参数是一个生成器表达式。

⓮ 输出状态，例如 627 matches for 'digit'。11

11 在 Python 3.5 中，是 755 matches for 'digit'。—— 编者注

⓯ 刷新输出缓冲。

⓰ 在服务器的控制台中记录响应。

⓱ 在服务器的控制台中记录会话结束。

⓲ 关闭 StreamWriter 流。

handle_queries 协程的名称是复数，因为它启动交互式会话后能处理各个客户端发来的多次请求。

注意，示例 18-14 中所有的 I/O 操作都使用 bytes 格式。因此，我们要解码从网络中收到的字符串，还要编码发出的字符串。Python 3 默认使用的编码是 UTF-8，这里就隐式使用了这个编码。

注意一点，有些 I/O 方法是协程，必须由 yield from 驱动，而另一些则是普通的函数。例如，StreamWriter.write 是普通的函数，我们假定它大多数时候都不会阻塞，因为它把数据写入缓冲；而刷新缓冲并真正执行 I/O 操作的 StreamWriter.drain 是协程，StreamReader.readline 也是协程。写作本书时，asyncio 包的 API 文档有重大的改进，明确标识出了哪些方法是协程。

示例 18-15 接续示例 18-14，列出这个模块的 main 函数。

示例 18-15　tcp_charfinder.py（接续示例 18-14）：main 函数创建并销毁事件循环和套接字服务器

def main (address='127.0.0.1', port=2323): ➊ port = int (port) loop = asyncio.get_event_loop () server_coro = asyncio.start_server (handle_queries, address, port, loop=loop) ➋ server = loop.run_until_complete (server_coro) ➌ host = server.sockets [0].getsockname () ➍ print ('Serving on {}. Hit CTRL-C to stop.'.format (host)) ➎ try: loop.run_forever () ➏ except KeyboardInterrupt: # 按 CTRL-C 键 pass print ('Server shutting down.') server.close () ➐ loop.run_until_complete (server.wait_closed ()) ➑ loop.close () ➒ if __name__ == '__main__': main (*sys.argv [1:]) ➓

❶ 调用 main 函数时可以不传入参数。

❷ asyncio.start_server 协程运行结束后，返回的协程对象返回一个 asyncio.Server 实例，即一个 TCP 套接字服务器。

❸ 驱动 server_coro 协程，启动服务器（server）。

❹ 获取这个服务器的第一个套接字的地址和端口，然后……

❺ …… 在服务器的控制台中显示出来。这是这个脚本在服务器的控制台中显示的第一个输出。

❻ 运行事件循环；main 函数在这里阻塞，直到在服务器的控制台中按 CTRL-C 键才会关闭。

❼ 关闭服务器。

❽ server.wait_closed () 方法返回一个期物；调用 loop.run_until_complete 方法，运行期物。

❾ 终止事件循环。

❿ 这是处理可选的命令行参数的简便方式：展开 sys.argv [1:]，传给 main 函数，未指定的参数使用相应的默认值。

注意，run_until_complete 方法的参数是一个协程（start_server 方法返回的结果）或一个 Future 对象（server.wait_closed 方法返回的结果）。如果传给 run_until_complete 方法的参数是协程，会把协程包装在 Task 对象中。

仔细查看 tcp_charfinder.py 脚本在服务器控制台中生成的输出（如示例 18-16），更易于理解脚本中控制权的流动。

示例 18-16　tcp_charfinder.py：这是图 18-2 所示会话在服务器端的输出

$ python3 tcp_charfinder.py Serving on ('127.0.0.1', 2323). Hit CTRL-C to stop. ➊ Received from ('127.0.0.1', 62910): 'chess black' ➋ Sent 6 results Received from ('127.0.0.1', 62910): 'sun' ➌ Sent 10 results Received from ('127.0.0.1', 62910): '\x00' ➍ Close the client socket ➎

❶ 这是 main 函数的输出。

❷ handle_queries 协程中那个 while 循环第一次迭代的输出。

❸ 那个 while 循环第二次迭代的输出。12

12 在 Python 3.5 中是 Sent 14 results。参见本小节开头的编者注。—— 编者注

❹ 用户按下 CTRL-C 键；服务器收到控制字符，关闭会话。

❺ 客户端套接字关闭了，但是服务器仍在运行，准备为其他客户端提供服务。

注意，main 函数几乎会立即显示 Serving on... 消息，然后在调用 loop.run_forever () 方法时阻塞。在那一点，控制权流动到事件循环中，而且一直待在那里，不过偶尔会回到 handle_queries 协程，这个协程需要等待网络发送或接收数据时，控制权又交还事件循环。在事件循环运行期间，只要有新客户端连接服务器就会启动一个 handle_queries 协程实例。因此，这个简单的服务器可以并发处理多个客户端。出现 KeyboardInterrupt 异常，或者操作系统把进程杀死，服务器会关闭。

tcp_charfinder.py 脚本利用 asyncio 包提供的高层流 API，有现成的服务器可用，所以我们只需实现一个处理程序（普通的回调或协程）。此外，asyncio 包受 Twisted 框架中抽象的传送和协议启发，还提供了低层传送和协议 API。详情请参见 asyncio 包的文档，里面有一个使用低层 API 实现的 TCP 回显服务器。

下一节实现 HTTP 版字符查找服务器。

18.6.2　使用 aiohttp 包编写 Web 服务器

asyncio 版国旗下载示例使用的 aiohttp 库也支持服务器端 HTTP，我就使用这个库实现了 http_charfinder.py 脚本。图 18-3 是这个简易服务器的 Web 界面，显示搜索「cat face」表情符号得到的结果。

图 18-3：浏览器窗口中显示在 http_charfinder.py 服务器中搜索「cat face」得到的结果

有些浏览器显示 Unicode 字符的效果比其他浏览器好。图 18-3 中的截图在 OS X 版 Firefox 浏览器中截取，我在 Safari 中也得到了相同的结果。但是，运行在同一台设备中的最新版 Chrome 和 Opera 却不能显示猫脸等表情符号。不过其他搜索结果（例如「chess」）正常，因此这可能是 OS X 版 Chrome 和 Opera 的字体问题。

我们先分析 http_charfinder.py 脚本中最重要的后半部分：启动和关闭事件循环与 HTTP 服务器。参见示例 18-17。

示例 18-17　http_charfinder.py：main 和 init 函数

@asyncio.coroutine def init (loop, address, port): ➊ app = web.Application (loop=loop) ➋ app.router.add_route ('GET', '/', home) ➌ handler = app.make_handler () ➍ server = yield from loop.create_server (handler, address, port) ➎ return server.sockets [0].getsockname () ➏ def main (address="127.0.0.1", port=8888): port = int (port) loop = asyncio.get_event_loop () host = loop.run_until_complete (init (loop, address, port)) ➐ print ('Serving on {}. Hit CTRL-C to stop.'.format (host)) try: loop.run_forever () ➑ except KeyboardInterrupt: # 按 CTRL-C 键 pass print ('Server shutting down.') loop.close () ➒ if __name__ == '__main__': main (*sys.argv [1:])

❶ init 协程产出一个服务器，交给事件循环驱动。

❷ aiohttp.web.Application 类表示 Web 应用……

❸ …… 通过路由把 URL 模式映射到处理函数上；这里，把 GET / 路由映射到 home 函数上（参见示例 18-18）。

❹ app.make_handler 方法返回一个 aiohttp.web.RequestHandler 实例，根据 app 对象设置的路由处理 HTTP 请求。

❺ create_server 方法创建服务器，以 handler 为协议处理程序，并把服务器绑定在指定的地址（address）和端口（port）上。

❻ 返回第一个服务器套接字的地址和端口。

❼ 运行 init 函数，启动服务器，获取服务器的地址和端口。

❽ 运行事件循环；控制权在事件循环手上时，main 函数会在这里阻塞。

❾ 关闭事件循环。

我们已经熟悉了 asyncio 包的 API，现在可以对比一下示例 18-17 与前面的 TCP 示例（见示例 18-15），看它们创建服务器的方式有何不同。

在前面的 TCP 示例中，服务器通过 main 函数中的下面两行代码创建并排定运行时间：

server_coro = asyncio.start_server(handle_queries, address, port, loop=loop) server = loop.run_until_complete(server_coro)

在这个 HTTP 示例中，init 函数通过下述方式创建服务器：

server = yield from loop.create_server(handler, address, port)

但是 init 是协程，驱动它运行的是 main 函数中的这一行：

host = loop.run_until_complete(init(loop, address, port))

asyncio.start_server 函数和 loop.create_server 方法都是协程，返回的结果都是 asyncio.Server 对象。为了启动服务器并返回服务器的引用，这两个协程都要由他人驱动，完成运行。在 TCP 示例中，做法是调用 loop.run_until_complete (server_coro)，其中 server_coro 是 asyncio.start_server 函数返回的结果。在 HTTP 示例中，create_server 方法在 init 协程中的一个 yield from 表达式里调用，而 init 协程则由 main 函数中的 loop.run_until_complete (init (...)) 调用驱动。

我提到这一点是为了强调之前讨论过的一个基本事实：只有驱动协程，协程才能做事，而驱动 asyncio.coroutine 装饰的协程有两种方法，要么使用 yield from，要么传给 asyncio 包中某个参数为协程或期物的函数，例如 run_until_complete。

示例 18-18 列出 home 函数。根据这个 HTTP 服务器的配置，home 函数用于处理 /（根）URL。

示例 18-18　http_charfinder.py：home 函数

def home(request): ➊ query = request.GET.get('query', '').strip() ➋ print('Query: {!r}'.format(query)) ➌ if query: ➍ descriptions = list(index.find_descriptions(query)) res = '\n'.join(ROW_TPL.format(**vars(descr)) for descr in descriptions) msg = index.status(query, len(descriptions)) else: descriptions = [] res = '' msg = 'Enter words describing characters.' html = template.format(query=query, result=res, ➎ message=msg) print('Sending {} results'.format(len(descriptions))) ➏ return web.Response(content_type=CONTENT_TYPE, text=html) ➐

❶ 一个路由处理函数，参数是一个 aiohttp.web.Request 实例。

❷ 获取查询字符串，去掉首尾的空白。

❸ 在服务器的控制台中记录查询。

❹ 如果有查询字符串，从索引（index）中找到结果，使用 HTML 表格中的行渲染结果，把结果赋值给 res 变量，再把状态消息赋值给 msg 变量。

❺ 渲染 HTML 页面。

❻ 在服务器的控制台中记录响应。

❼ 构建 Response 对象，将其返回。

注意，home 不是协程，既然定义体中没有 yield from 表达式，也没必要是协程。在 aiohttp 包的文档中，add_route 方法的条目下面说道，「如果处理程序是普通的函数，在内部会将其转换成协程」。

示例 18-18 中的 home 函数虽然简单，却有一个缺点。home 是普通的函数，而不是协程，这一事实预示着一个更大的问题：我们需要重新思考如何实现 Web 应用，以获得高并发。下面来分析这个问题。

18.6.3　更好地支持并发的智能客户端

示例 18-18 中的 home 函数很像是 Django 或 Flask 中的视图函数，实现方式完全没有考虑异步：获取请求，从数据库中读取数据，然后构建响应，渲染完整的 HTML 页面。在这个示例中，存储在内存中的 UnicodeNameIndex 对象是「数据库」。但是，对真正的数据库来说，应该异步访问，否则在等待数据库查询结果的过程中，事件循环会阻塞。例如，aiopg 包提供了一个异步 PostgreSQL 驱动，与 asyncio 包兼容；这个包支持使用 yield from 发送查询和获取结果，因此视图函数的表现与真正的协程一样。

除了防止阻塞调用之外，高并发的系统还必须把复杂的工作分成多步，以保持敏捷。http_charfinder.py 服务器表明了这一点：如果搜索「cjk」，得到的结果是 75 821 个中文、日文和韩文象形文字。13 此时，home 函数会返回一个 5.3MB 的 HTML 文档，显示一个有 75 821 行的表格。

13 这正是 CJK 表示的意思：不断增加的中文、日文和韩文字符。以后的 Python 版本支持的 CJK 象形文字数量可能会比 Python 3.4 多。

我在自己的设备中使用命令行 HTTP 客户端 curl 访问架设在本地的 http_charfinder.py 服务器，查询「cjk」，2 秒钟后获得响应。浏览器要布局包含这么大一个表格的页面，用的时间会更长。当然，大多数查询返回的响应要小得多：查询「braille」返回 256 行结果，页面大小为 19KB，在我的设备中用时 0.017 秒。可是，如果服务器要用 2 秒钟处理「cjk」查询，那么其他所有客户端都至少要等 2 秒 —— 这是不可接受的。

避免响应时间太长的方法是实现分页：首次至多返回（比如说）200 行，用户点击链接或滚动页面时再获取更多结果。如果查看本书代码仓库中的 charfinder.py 模块，你会发现 UnicodeNameIndex.find_descriptions 方法有两个可选的参数 ——start 和 stop，这是偏移值，用于支持分页。因此，我们可以返回前 200 个结果，当用户想查看更多结果时，再使用 AJAX 或 WebSockets 发送下一批结果。

实现分批发送结果所需的大多数代码都在浏览器这一端，因此 Google 和所有大型互联网公司都大量依赖客户端代码构建服务：智能的异步客户端能更好地使用服务器资源。

虽然智能的客户端甚至对老式 Django 应用也有帮助，但是要想真正为这种客户端服务，我们需要全方位支持异步编程的框架，从处理 HTTP 请求和响应到访问数据库，全都支持异步。如果想实现实时服务，例如游戏和以 WebSockets 支持的媒体流，那就尤其应该这么做。14

14 在「杂谈」中我会进一步说明这个趋势。

这里留一个练习给读者：改进 http_charfinder.py 脚本，添加下载进度条。此外还有一个附加题：实现 Twitter 那样的「无限滚动」。做完这个练习后，我们对如何使用 asyncio 包做异步编程的讨论就结束了。

18.7　本章小结

本章介绍了在 Python 中做并发编程的一种全新方式，这种方式使用 yield from、协程、期物和 asyncio 事件循环。首先，我们分析了两个简单的示例 —— 两个旋转指针脚本，仔细对比了使用 threading 模块和 asyncio 包处理并发的异同。

然后，本章讨论了 asyncio.Future 类的细节，重点讲述它对 yield from 的支持，以及与协程和 asyncio.Task 类的关系。接下来分析了 asyncio 版国旗下载脚本。

然后，本章分析了 Ryan Dahl 对 I/O 延迟所做的统计数据，还说明了阻塞调用的影响。尽管有些函数必然会阻塞，但是为了让程序持续运行，有两种解决方案可用：使用多个线程，或者异步调用 —— 后者以回调或协程的形式实现。

其实，异步库依赖于低层线程（直至内核级线程），但是这些库的用户无需创建线程，也无需知道用到了基础设施中的低层线程。在应用中，我们只需确保没有阻塞的代码，事件循环会在背后处理并发。异步系统能避免用户级线程的开销，这是它能比多线程系统管理更多并发连接的主要原因。

之后，我们又回到下载国旗的脚本，添加进度条并处理错误。这需要大幅度重构，特别是要把 asyncio.wait 函数换成 asyncio.as_completed 函数，因此不得不把 download_many 函数的大多数功能移到新添的 downloader_coro 协程中，这样我们才能使用 yield from 从 asyncio.as_completed 函数生成的多个期物中逐个获得结果。

然后，本章说明了如何使用 loop.run_in_executor 方法把阻塞的作业（例如保存文件）委托给线程池做。

接着，本章讨论了如何使用协程解决回调的主要问题：执行分成多步的异步任务时丢失上下文，以及缺少处理错误所需的上下文。

然后又举了一个例子，在下载国旗图像的同时获取国家名称，以此说明如何结合协程和 yield from 避免所谓的回调地狱。如果忽略 yield from 关键字，使用 yield from 结构实现异步调用的多步过程看起来类似于顺序执行的代码。

本章最后两个示例是使用 asyncio 包实现的 TCP 和 HTTP 服务器，用于按名称搜索 Unicode 字符。在分析 HTTP 服务器的最后，我们讨论了客户端 JavaScript 对服务器端提供高并发支持的重要性。使用 JavaScript，客户端可以按需发起小型请求，而不用下载较大的 HTML 页面。

18.8　延伸阅读

Python 核心开发者 Nick Coghlan 在 2013 年 1 月对「PEP 3156—Asynchronous IO Support Rebooted: the‘asyncio’Module」草案评论如下：

在这个 PEP 的开头部分应该言简意赅地说明等待异步期物返回结果的两个 API：

(1) f.add_done_callback(...)

(2) 协程中的 yield from f（期物运行结束后恢复协程，期物要么返回结果，要么抛出合适的异常）

此刻，这两个 API 深埋在众多的 API 中，而它们是理解核心事件循环层之上各种事物交互方式的关键。15

15 摘自 2013 年 1 月 20 日发布在 python-ideas 邮件列表中的一个消息，在这个消息中，Coghlan 对 PEP 3156 做出了上述评论。

PEP 3156 的作者 Guido van Rossum 没有采纳 Coghlan 的建议。实现 PEP 3156 的初期，asyncio 包的文档虽然十分详细，但对用户并不友好。asyncio 包的文档有 9 个 .rst 文件，128KB，将近 71 页。在标准库文档中，只有「Built-in Types」一章有这么长，而那一章内容众多，涵盖了数字类型、序列类型、生成器、映射、集合、bool、上下文管理器，等等。

asyncio 包的文档大部分是在讲概念和 API，其中夹杂着有用的示意图和示例，不过特别实用的一节是「18.5.11. Develop with asyncio」，16 其中说明了极为重要的使用模式。asyncio 包的文档需要用更多的内容来说明如何使用 asyncio。

16 目前是：18.5.9. Develop with asyncio。—— 编者注

asyncio 包很新，已出版的书中少有涉及。我发现只有 Jan Palach 写的 Parallel Programming with Python（Packt 出版社，2014 年）一书中有一章讲到了 asyncio，可惜那一章很短。

不过，有很多关于 asyncio 的精彩演讲。我觉得最棒的是 Brett Slatkin 在蒙特利尔 PyCon 2014 大会上发表的演讲，题为「Fan-In and Fan-Out: The Crucial Components of Concurrency」，副标题是「Why do we need Tulip? (a.k.a., PEP 3156—asyncio)」（视频）。在 30 分钟内，Slatkin 实现了一个简单的 Web 爬虫示例，强调了 asyncio 包的正确用法。身为观众的 Guido van Rossum 提到，为了引荐 asyncio 包，他也写了一个 Web 爬虫。Guido 写的代码不依赖 aiohttp 包，只用到了标准库。Slatkin 还写了一篇见解深刻的文章，题为「Python's asyncio Is for Composition, Not Raw Performance」。

Guido van Rossum 自己的几个演讲也是必看的，包括在 PyCon US 2013 上所做的主题演讲，以及在 LinkedIn 公司和 Twitter 大学所做的演讲。此外，还推荐 Saúl Ibarra Corretgé 的演讲 ——「A Deep Dive into PEP-3156 and the New asyncio Module」[（幻灯片，视频]。

在 PyCon US 2013 大会上，Dino Viehland 做了一场演讲，题为「Using futures for async GUI programming in Python 3.3」，说明如何把 asyncio 包集成到 Tkinter 事件循环中。Viehland 展示了在另一个事件循环之上实现 asyncio.AbstractEventLoop 接口的重要部分是多么容易。他的代码使用 Tulip 编写，这是 asyncio 包添加到标准库中之前的名称。我修改了他的代码，以便支持 Python 3.4 中的 asyncio 包。我重构后的新版在 GitHub 中。

Victor Stinner [asyncio 包的核心贡献者，asyncio 包的移植版 Trollius 的作者] 经常更新相关资源的链接列表 ——「The new Python asyncio module aka‘tulip’」。此外，收集 asyncio 资源的还有 Asyncio.org 网站 和 GitHub 中的 aio-libs 组织，在这两个网站中能找到 PostgreSQL、MySQL 和多种 NoSQL 数据库的异步驱动。我没有测试过这些驱动，不过写作本书时，这些项目好像十分活跃。

Web 服务将成为 asyncio 包的重要使用场景。你的代码有可能要依赖 Andrew Svetlov 领衔开发的 aiohttp 库。你可能还想架设环境，测试错误处理代码，在这方面，Alexis Métaireau 和 Tarek Ziadé 开发的 Vaurien（「混沌 TCP 代理」）极其有用。Vaurien 是为 Mozilla Services 项目开发的，用于在程序与后端服务器（例如，数据库和 Web 服务提供方）之间的 TCP 流量中引入延迟和随机错误。

杂谈

至尊循环

有很长一段时间，大多数 Python 高手开发网络应用时喜欢使用异步编程，但是总会遇到一个问题 —— 挑选的库之间不兼容。Ryan Dahl 提到，Twisted 是 Node.js 的灵感来源之一；而在 Python 中，Tornado 拥护使用协程做面向事件编程。

在 JavaScript 社区里还有争论，有些人推崇使用简单的回调，而有些人提倡使用与回调处于竞争地位的各种高层抽象方式。Node.js 早期版本的 API 使用的是 Promise 对象（类似于 Python 中的期物），但是后来 Ryan Dahl 决定统一只用回调。James Coglan 认为，Node.js 在这一点上错过了大好良机（https://blog.jcoglan.com/2013/03/30/callbacksare-imperative-promises-are-functional-nodes-biggest-missed-opportunity/）。

Python 社区的争论已经结束：asyncio 包添加到标准库中之后，协程和期物被确定为符合 Python 风格的异步代码编写方式。此外，asyncio 包为异步期物和事件循环定义了标准接口，为二者提供了实现参考。

正如「Python 之禅」所说：

肯定有一种 —— 通常也是唯一一种 —— 最佳的解决方案

不过这并不容易找到，因为你不是 Python 之父

或许变成荷兰人才能理解 yield from 吧。17 对我这个巴西人来说，一开始并不易于理解，不过一段时间之后我理解了。

更重要的是，设计 asyncio 包时考虑到了使用外部包替换自身的事件循环，因此才有 asyncio.get_event_loop 和 set_event_loop 函数 —— 二者是抽象的事件循环策略 API 的一部分。

Tornado 已经有实现 asyncio.AbstractEventLoop 接口的类 ——AsyncIOMainLoop（http://tornado.readthedocs.org/en/latest/asyncio.html），因此在同一个事件循环中可以使用这两个库运行异步代码。此外，Quamash 项目也很有趣，它把 asyncio 包集成到 Qt 事件循环中，以便使用 PyQt 或 PySide 开发 GUI 应用。我只是举两个例子，说明 asyncio 包能把面向事件的包集成在一起。

智能的 HTTP 客户端，例如单页 Web 应用（如 Gmail）或智能手机应用，需要快速、轻量级的响应和推送更新。鉴于这样的需求，服务器端最好使用异步框架，不要使用传统的 Web 框架（如 Django）。传统框架的目的是渲染完整的 HTML 网页，而且不支持异步访问数据库。

WebSockets 协议的作用是为始终连接的客户端（例如游戏和流式应用）提供实时更新，因此，高并发的异步服务器要不间断地与成百上千个客户端交互。asyncio 包的架构能很好地支持 WebSockets，而且至少有两个库已经在 asyncio 包的基础上实现了 WebSockets 协议：Autobahn|Python 和 WebSockets。

「实时 Web」的整体发展趋势迅猛，这是 Node.js 需求量不断攀升的主要因素，也是 Python 生态系统积极向 asyncio 靠拢的重要原因。不过，要做的事还有很多。为了便于入门，我们要在标准库中提供异步 HTTP 服务器和客户端 API，异步数据库 API 3.0，18 以及使用 asyncio 包构建的新数据库驱动。

与 Node.js 相比，含有 asyncio 包的 Python 3.4 最大的优势是 Python 本身：Python 语言设计良好，使用协程和 yield from 结构编写的异步代码比 JavaScript 采用的古老回调易于维护。而我们最大的劣势是库，Python 自带了很多库，但是那些库不支持异步编程。Node.js 库的生态系统丰富，完全建构在异步调用之上。但是，Python 和 Node. js 都有一个问题，而 Go 和 Erlang 从一开始就解决了这个问题：我们编写的代码无法轻松地利用所有可用的 CPU 核心。

Python 标准化了事件循环接口，还提供了一个异步库，这是一大进步，而且只有我们仁慈的独裁者能在众多深入人心且高质量的替代方案中选择这种方式。具体实现时，他咨询了多个重要的 Python 异步框架的作者，其中受 Glyph Lefkowitz（Twisted 的主要开发者）的影响最深。如果你想知道为什么 asyncio.Future 类与 Twisted 中的 Deferred 类不同，一定要阅读 Guido 在 Python-tulip 讨论组中发布的一篇文章，题为「Deconstructing Deferred」。Guido 对 Twisted 这个最古老也是最大的 Python 异步框架充满敬意，在 python-twisted 讨论组中讨论设计方案时，他甚至说，「What Would Twisted Do（WWTD）」。19

幸好有 Guido van Rossum 打头阵，让 Python 以更好的姿态应对当前的并发挑战。若想精通 asyncio 包，一定要下一番功夫。可是，如果你计划使用 Python 编写并发网络应用，那就去寻求至尊循环（the One Loop）：

至尊循环驭众生，至尊循环寻众生，

至尊循环引众生，普照众生欣欣荣。

17Python 之父 Guido van Rossum 是荷兰人。—— 译者注

18 应该是：PEP 249—Python Database API Specification v2.0。—— 编者注

19 出自 Guido 于 2015 年 1 月 29 日发布的消息，然后 Glyph 立即回复了这一消息。

第六部分　元编程

第 19 章　动态属性和特性

特性至关重要的地方在于，特性的存在使得开发者可以非常安全并且确定可行地将公共数据属性作为类的公共接口的一部分开放出来。1

——Alex Martelli

Python 贡献者和图书作者

1《Python 技术手册（第 2 版）》第 101 页。（该书中文版把「property」译为属性，这里改为「特性」，其他内容与原来的翻译相同。—— 译者注）

在 Python 中，数据的属性和处理数据的方法统称属性（attribute）。其实，方法只是可调用的属性。除了这二者之外，我们还可以创建特性（property），在不改变类接口的前提下，使用存取方法（即读值方法和设值方法）修改数据属性。这与统一访问原则相符：

不管服务是由存储还是计算实现的，一个模块提供的所有服务都应该通过统一的方式使用。2

2Bertrand Meyer, Object-Oriented Software Construction, 2E, p. 57.

除了特性，Python 还提供了丰富的 API，用于控制属性的访问权限，以及实现动态属性。使用点号访问属性时（如 obj.attr），Python 解释器会调用特殊的方法（如 __getattr__ 和 __setattr__）计算属性。用户自己定义的类可以通过 __getattr__ 方法实现「虚拟属性」，当访问不存在的属性时（如 obj.no_such_attribute），即时计算属性的值。

动态创建属性是一种元编程，框架的作者经常这么做。然而，在 Python 中，相关的基础技术十分简单，任何人都可以使用，甚至在日常的数据转换任务中也能用到。下面以这种任务开启本章的话题。

19.1　使用动态属性转换数据

在接下来的几个示例中，我们要使用动态属性处理 O'Reilly 为 OSCON 2014 大会提供的 JSON 格式数据源。示例 19-1 是那个数据源中的 4 个记录。3

3 关于这个数据源及其使用规则，请阅读「DIY: OSCON schedule」一文。那个 JSON 文件有 744KB，我写作本书时还在网上。本书代码仓库中的 oscon-schedule/data/ 目录里有个副本，文件名为 osconfeed.json。

示例 19-1　osconfeed.json 文件中的记录示例；节略了部分字段的内容

{ "Schedule": { "conferences": [{"serial": 115 }], "events": [ { "serial": 34505, "name": "Why Schools Don´t Use Open Source to Teach Programming", "event_type": "40-minute conference session", "time_start": "2014-07-23 11:30:00", "time_stop": "2014-07-23 12:10:00", "venue_serial": 1462, "description": "Aside from the fact that high school programming...", "website_url": "http://oscon.com/oscon2014/public/schedule/detail/34505", "speakers": [157509], "categories": ["Education"] } ], "speakers": [ { "serial": 157509, "name": "Robert Lefkowitz", "photo": null, "url": "http://sharewave.com/", "position": "CTO", "affiliation": "Sharewave", "twitter": "sharewaveteam", "bio": "Robert ´r0ml´ Lefkowitz is the CTO at Sharewave, a startup..." } ], "venues": [ { "serial": 1462, "name": "F151", "category": "Conference Venues" } ] } }

那个 JSON 源中有 895 条记录，示例 19-1 只列出了 4 条。可以看出，整个数据集是一个 JSON 对象，里面有一个键，名为 "Schedule"；这个键对应的值也是一个映像，有 4 个键： "conferences"、"events"、"speakers" 和 "venues"。这 4 个键对应的值都是一个记录列表。在示例 19-1 中，各个列表中只有一条记录。然而，在完整的数据集中，列表中有成百上千条记录。不过，"conferences" 键对应的列表中只有一条记录，如上述示例所示。这 4 个列表中的每个元素都有一个名为 "serial" 的字段，这是元素在各个列表中的唯一标识符。

我编写的第一个脚本只用于下载那个 OSCON 数据源。为了避免浪费流量，我会先检查本地有没有副本。这么做是合理的，因为 OSCON 2014 大会已经结束，数据源不会再更新。

示例 19-2 没用到元编程，几乎所有代码的作用可以用这一个表达式概括：json.load (fp)。不过，这样足以处理那个数据集了。osconfeed.load 函数会在后面几个示例中用到。

示例 19-2　osconfeed.py：下载 osconfeed.json（doctest 在示例 19-3 中）

from urllib.request import urlopen import warnings import os import json URL = 'http://www.oreilly.com/pub/sc/osconfeed' JSON = 'data/osconfeed.json' def load(): if not os.path.exists(JSON): msg = 'downloading {} to {}'.format(URL, JSON) warnings.warn(msg) ➊ with urlopen(URL) as remote, open(JSON, 'wb') as local: ➋ local.write(remote.read()) with open(JSON) as fp: return json.load(fp) ➌

❶ 如果需要下载，就发出提醒。

❷ 在 with 语句中使用两个上下文管理器（从 Python 2.7 和 Python 3.1 起允许这么做），分别用于读取和保存远程文件。

❸ json.load 函数解析 JSON 文件，返回 Python 原生对象。在这个数据源中有这几种数据类型：dict、list、str 和 int。

有了示例 19-2 中的代码，我们可以审查数据源中的任何字段，如示例 19-3 所示。

示例 19-3　osconfeed.py：示例 19-2 的 doctest

>>> feed = load() ➊ >>> sorted(feed['Schedule'].keys()) ➋ ['conferences', 'events', 'speakers', 'venues'] >>> for key, value in sorted(feed['Schedule'].items()): ... print('{:3} {}'.format(len(value), key)) ➌ ... 1 conferences 494 events 357 speakers 53 venues >>> feed['Schedule']['speakers'][-1]['name'] ➍ 'Carina C. Zona' >>> feed['Schedule']['speakers'][-1]['serial'] ➎ 141590 >>> feed['Schedule']['events'][40]['name'] 'There *Will* Be Bugs' >>> feed['Schedule']['events'][40]['speakers'] ➏ [3471, 5199]

❶ feed 的值是一个字典，里面嵌套着字典和列表，存储着字符串和整数。

❷ 列出 "Schedule" 键中的 4 个记录集合。

❸ 显示各个集合中的记录数量。

❹ 深入嵌套的字典和列表，获取最后一个演讲者的名字。

❺ 获取那位演讲者的编号。

❻ 每个事件都有一个'speakers' 字段，列出 0 个或多个演讲者的编号。

19.1.1　使用动态属性访问 JSON 类数据

示例 19-2 十分简单，不过，feed ['Schedule']['events'][40]['name'] 这种句法很冗长。在 JavaScript 中，可以使用 feed.Schedule.events [40].name 获取那个值。在 Python 中，可以实现一个近似字典的类（网上有大量实现）4，达到同样的效果。我自己实现了 FrozenJSON 类，比大多数实现都简单，因为只支持读取，即只能访问数据。不过，这个类能递归，自动处理嵌套的映射和列表。

4 最常提到的一个实现是 AttrDict，还有一个实现能快速创建嵌套的映射 ——addict。

示例 19-4 演示 FrozenJSON 类的用法，源代码在示例 19-5 中。

示例 19-4　 示例 19-5 定义的 FrozenJSON 类能读取属性，如 name，还能调用方法，如 .keys () 和 .items ()

>>> from osconfeed import load >>> raw_feed = load() >>> feed = FrozenJSON(raw_feed) ➊ >>> len(feed.Schedule.speakers) ➋ 357 >>> sorted(feed.Schedule.keys()) ➌ ['conferences', 'events', 'speakers', 'venues'] >>> for key, value in sorted(feed.Schedule.items()): ➍ ... print('{:3} {}'.format(len(value), key)) ... 1 conferences 494 events 357 speakers 53 venues >>> feed.Schedule.speakers[-1].name ➎ 'Carina C. Zona' >>> talk = feed.Schedule.events[40] >>> type(talk) ➏ <class 'explore0.FrozenJSON'> >>> talk.name 'There *Will* Be Bugs' >>> talk.speakers ➐ [3471, 5199] >>> talk.flavor ➑ Traceback (most recent call last): ... KeyError: 'flavor'

❶ 传入嵌套的字典和列表组成的 raw_feed，创建一个 FrozenJSON 实例。

❷ FrozenJSON 实例能使用属性表示法遍历嵌套的字典；这里，我们获取演讲者列表的元素数量。

❸ 也可以使用底层字典的方法，例如 .keys ()，获取记录集合的名称。

❹ 使用 items () 方法获取各个记录集合及其内容，然后显示各个记录集合中的元素数量。

❺ 列表，例如 feed.Schedule.speakers，仍是列表；但是，如果里面的元素是映射，会转换成 FrozenJSON 对象。

❻ events 列表中的 40 号元素是一个 JSON 对象，现在则变成一个 FrozenJSON 实例。

❼ 事件记录中有一个 speakers 列表，列出演讲者的编号。

❽ 读取不存在的属性会抛出 KeyError 异常，而不是通常抛出的 AttributeError 异常。

FrozenJSON 类的关键是 __getattr__ 方法。我们在 10.5 节的 Vector 示例中用过这个方法，那时用于通过字母获取 Vector 对象的分量（例如 v.x、v.y、v.z）。我们要记住重要的一点，仅当无法使用常规的方式获取属性（即在实例、类或超类中找不到指定的属性），解释器才会调用特殊的 __getattr__ 方法。

示例 19-4 的最后一行揭露了这个实现的一个小问题：理论上，尝试读取不存在的属性应该抛出 AttributeError 异常。其实，一开始我对这个异常做了处理，但是 __getattr__ 方法的代码量增加了一倍，而且偏离了我最想展示的重要逻辑，因此为了教学，后来我把那部分代码去掉了。

如示例 19-5 所示，FrozenJSON 类只有两个方法（__init__ 和 __getattr__）和一个实例属性 __data。因此，尝试获取其他属性会触发解释器调用 __getattr__ 方法。这个方法首先查看 self.__data 字典有没有指定名称的属性（不是键），这样 FrozenJSON 实例便可以处理字典的所有方法，例如把 items 方法委托给 self.__data.items () 方法。如果 self.__data 没有指定名称的属性，那么 __getattr__ 方法以那个名称为键，从 self.__data 中获取一个元素，传给 FrozenJSON.build 方法。这样就能深入 JSON 数据的嵌套结构，使用类方法 build 把每一层嵌套转换成一个 FrozenJSON 实例。

示例 19-5　 explore0.py：把一个 JSON 数据集转换成一个嵌套着 FrozenJSON 对象、列表和简单类型的 FrozenJSON 对象

from collections import abc class FrozenJSON: """一个只读接口，使用属性表示法访问 JSON 类对象""" def __init__(self, mapping): self.__data = dict (mapping) ➊ def __getattr__(self, name): ➋ if hasattr (self.__data, name): return getattr (self.__data, name) ➌ else: return FrozenJSON.build (self.__data [name]) ➍ @classmethod def build (cls, obj): ➎ if isinstance (obj, abc.Mapping): ➏ return cls (obj) elif isinstance (obj, abc.MutableSequence): ➐ return [cls.build (item) for item in obj] else: ➑ return obj

❶ 使用 mapping 参数构建一个字典。这么做有两个目的：(1) 确保传入的是字典（或者是能转换成字典的对象）；(2) 安全起见，创建一个副本。

❷ 仅当没有指定名称（name）的属性时才调用 __getattr__ 方法。

❸ 如果 name 是实例属性 __data 的属性，返回那个属性。调用 keys 等方法就是通过这种方式处理的。

❹ 否则，从 self.__data 中获取 name 键对应的元素，返回调用 FrozenJSON.build () 方法得到的结果。5

5 这一行中的 self.__data [name] 表达式可能抛出 KeyError 异常。我们应该处理这个异常，抛出 AttributeError 异常，因为这才是 __getattr__ 方法应该抛出的异常种类。建议勤奋的读者实现错误处理代码，当作一个练习。

❺ 这是一个备选构造方法，@classmethod 装饰器经常这么用。

❻ 如果 obj 是映射，那就构建一个 FrozenJSON 对象。

❼ 如果是 MutableSequence 对象，必然是列表，6 因此，我们把 obj 中的每个元素递归地传给 .build () 方法，构建一个列表。

6 数据源是 JSON 格式，而在 JSON 中，只有字典和列表是集合类型。

❽ 如果既不是字典也不是列表，那么原封不动地返回元素。

注意，我们没有缓存或转换原始数据源。在迭代数据源的过程中，嵌套的数据结构不断被转换成 FrozenJSON 对象。这么做没问题，因为数据集不大，而且这个脚本只用于访问或转换数据。

从随机源中生成或仿效动态属性名的脚本都必须处理一个问题：原始数据中的键可能不适合作为属性名。下一节处理这个问题。

19.1.2　处理无效属性名

FrozenJSON 类有个缺陷：没有对名称为 Python 关键字的属性做特殊处理。比如说像下面这样构建一个对象：

>>> grad = FrozenJSON({'name': 'Jim Bo', 'class': 1982})

此时无法读取 grad.class 的值，因为在 Python 中 class 是保留字：

>>> grad.class File "<stdin>", line 1 grad.class ^ SyntaxError: invalid syntax

当然，可以这么做：

>>> getattr(grad, 'class') 1982

但是，FrozenJSON 类的目的是为了便于访问数据，因此更好的方法是检查传给 FrozenJSON.__init__ 方法的映射中是否有键的名称为关键字，如果有，那么在键名后加上 _，然后通过下述方式读取：

>>> grad.class_ 1982

为此，我们可以把示例 19-5 中只有一行代码的 __init__ 方法改成示例 19-6 中的版本。

示例 19-6　explore1.py：在名称为 Python 关键字的属性后面加上 _

def __init__(self, mapping): self.__data = {} for key, value in mapping.items(): if keyword.iskeyword(key): ➊ key += '_' self.__data[key] = value

➊ keyword.iskeyword (...) 正是我们所需的函数；为了使用它，必须导入 keyword 模块；这个代码片段没有列出导入语句。

如果 JSON 对象中的键不是有效的 Python 标识符，也会遇到类似的问题：

>>> x = FrozenJSON({'2be':'or not'}) >>> x.2be File "<stdin>", line 1 x.2be ^ SyntaxError: invalid syntax

这种有问题的键在 Python 3 中易于检测，因为 str 类提供的 s.isidentifier () 方法能根据语言的语法判断 s 是否为有效的 Python 标识符。但是，把无效的标识符变成有效的属性名却不容易。对此，有两个简单的解决方法，一个是抛出异常，另一个是把无效的键换成通用名称，例如 attr_0、attr_1，等等。为了简单起见，我将忽略这个问题。

对动态属性的名称做了一些处理之后，我们要分析 FrozenJSON 类的另一个重要功能 —— 类方法 build 的逻辑。这个方法把嵌套结构转换成 FrozenJSON 实例或 FrozenJSON 实例列表，因此 __getattr__ 方法使用这个方法访问属性时，能为不同的值返回不同类型的对象。

除了在类方法中实现这样的逻辑之外，还可以在特殊的 __new__ 方法中实现，如下一节所述。

19.1.3　使用 __new__ 方法以灵活的方式创建对象

我们通常把 __init__ 称为构造方法，这是从其他语言借鉴过来的术语。其实，用于构建实例的是特殊方法 __new__：这是个类方法（使用特殊方式处理，因此不必使用 @classmethod 装饰器），必须返回一个实例。返回的实例会作为第一个参数（即 self）传给 __init__ 方法。因为调用 __init__ 方法时要传入实例，而且禁止返回任何值，所以 __init__ 方法其实是「初始化方法」。真正的构造方法是 __new__。我们几乎不需要自己编写 __new__ 方法，因为从 object 类继承的实现已经足够了。

刚才说明的过程，即从 __new__ 方法到 __init__ 方法，是最常见的，但不是唯一的。__new__ 方法也可以返回其他类的实例，此时，解释器不会调用 __init__ 方法。

也就是说，Python 构建对象的过程可以使用下述伪代码概括：

# 构建对象的伪代码 def object_maker (the_class, some_arg): new_object = the_class.__new__(some_arg) if isinstance (new_object, the_class): the_class.__init__(new_object, some_arg) return new_object # 下述两个语句的作用基本等效 x = Foo ('bar') x = object_maker (Foo, 'bar')

示例 19-7 是 FrozenJSON 类的另一个版本，把之前在类方法 build 中的逻辑移到了 __new__ 方法中。

示例 19-7　explore2.py：使用 __new__ 方法取代 build 方法，构建可能是也可能不是 FrozenJSON 实例的新对象

from collections import abc class FrozenJSON: """一个只读接口，使用属性表示法访问 JSON 类对象""" def __new__(cls, arg): ➊ if isinstance (arg, abc.Mapping): return super ().__new__(cls) ➋ elif isinstance (arg, abc.MutableSequence): ➌ return [cls (item) for item in arg] else: return arg def __init__(self, mapping): self.__data = {} for key, value in mapping.items (): if iskeyword (key): key += '_' self.__data [key] = value def __getattr__(self, name): if hasattr (self.__data, name): return getattr (self.__data, name) else: return FrozenJSON (self.__data [name]) ➍

❶ __new__ 是类方法，第一个参数是类本身，余下的参数与 __init__ 方法一样，只不过没有 self。

❷ 默认的行为是委托给超类的 __new__ 方法。这里调用的是 object 基类的 __new__ 方法，把唯一的参数设为 FrozenJSON。

❸ __new__ 方法中余下的代码与原先的 build 方法完全一样。

❹ 之前，这里调用的是 FrozenJSON.build 方法，现在只需调用 FrozenJSON 构造方法。

__new__ 方法的第一个参数是类，因为创建的对象通常是那个类的实例。所以，在 FrozenJSON.__new__ 方法中，super ().__new__(cls) 表达式会调用 object.__new__(FrozenJSON)，而 object 类构建的实例其实是 FrozenJSON 实例，即那个实例的 __class__ 属性存储的是 FrozenJSON 类的引用。不过，真正的构建操作由解释器调用 C 语言实现的 object.__new__ 方法执行。

OSCON 的 JSON 数据源有一个明显的缺点：索引为 40 的事件，即名为 'There *Will* Be Bugs' 的那个，有两位演讲者，3471 和 5199，但却不容易找到他们，因为提供的是编号，而 Schedule.speakers 列表没有使用编号建立索引。此外，每条事件记录中都有 venue_serial 字段，存储的值也是编号，但是如果想找到对应的记录，那就要线性搜索 Schedule.venues 列表。接下来的任务是，调整数据结构，以便自动获取所链接的记录。

19.1.4　使用 shelve 模块调整 OSCON 数据源的结构

标准库中有个 shelve（架子）模块，这名字听起来怪怪的，可是如果知道 pickle（泡菜）是 Python 对象序列化格式的名字，还是在那个格式与对象之间相互转换的某个模块的名字，就会觉得以 shelve 命名是合理的。泡菜坛子摆放在架子上，因此 shelve 模块提供了 pickle 存储方式。

shelve.open 高阶函数返回一个 shelve.Shelf 实例，这是简单的键值对象数据库，背后由 dbm 模块支持，具有下述特点。

shelve.Shelf 是 abc.MutableMapping 的子类，因此提供了处理映射类型的重要方法。

此外，shelve.Shelf 类还提供了几个管理 I/O 的方法，如 sync 和 close；它也是一个上下文管理器。

只要把新值赋予键，就会保存键和值。

键必须是字符串。

值必须是 pickle 模块能处理的对象。

shelve（https://docs.python.org/3/library/shelve.html）、dbm（https://docs.python.org/3/library/dbm.html）和 pickle 模块（https://docs.python.org/3/library/pickle.html）的详细用法和注意事项参见文档。现在值得关注的是，shelve 模块为识别 OSCON 的日程数据提供了一种简单有效的方式。我们将从 JSON 文件中读取所有记录，将其存在一个 shelve.Shelf 对象中，键由记录类型和编号组成（例如，'event.33950' 或'speaker.3471'），而值是我们即将定义的 Record 类的实例。

实例 19-8 是 schedule1.py 脚本的 doctest，使用 shelve 模块处理数据源。若想以交互式方式测试，要执行 python -i schedule1.py 命令运行脚本，启动加载了 schedule1 模块的控制台。主要工作由 load_db 函数完成：调用 osconfeed.load 方法（在示例 19-2 中定义）读取 JSON 数据，把通过 db 传入的 Shelf 对象中的各条记录存储为一个个 Record 实例。这样处理之后，获取演讲者的记录就容易了，例如 speaker = db ['speaker.3471']。

示例 19-8　测试 schedule1.py 脚本（见示例 19-9）提供的功能

>>> import shelve >>> db = shelve.open(DB_NAME) ➊ >>> if CONFERENCE not in db: ➋ ... load_db(db) ➌ ... >>> speaker = db['speaker.3471'] ➍ >>> type(speaker) ➎ <class 'schedule1.Record'> >>> speaker.name, speaker.twitter ➏ ('Anna Martelli Ravenscroft', 'annaraven') >>> db.close() ➐

❶ shelve.open 函数打开现有的数据库文件，或者新建一个。

❷ 判断数据库是否填充的简便方法是，检查某个已知的键是否存在；这里检查的键是 conference.115，即 conference 记录（只有一个）的键。7

7 也可以使用 len (db) 判断，不过，如果是大型 dbm 数据库，那就很耗费时间。

❸ 如果数据库是空的，那就调用 load_db (db)，加载数据。

❹ 获取一条 speaker 记录。

❺ 它是示例 19-9 中定义的 Record 类的实例。

❻ 各个 Record 实例都有一系列自定义的属性，对应于底层 JSON 记录里的字段。

❼ 一定要记得关闭 shelve.Shelf 对象。如果可以，使用 with 块确保 Shelf 对象会关闭。8

8doctest 有个突出的弱点：无法正确地设置资源并保证将其销毁。我使用 py.test 为 schedule1.py 脚本写了很多测试，在示例 A-12 中。

schedule1.py 脚本的代码在示例 19-9 中。

示例 19-9　schedule1.py：访问保存在 shelve.Shelf 对象里的 OSCON 日程数据

import warnings import osconfeed ➊ DB_NAME = 'data/schedule1_db' CONFERENCE = 'conference.115' class Record: def __init__(self, **kwargs): self.__dict__.update(kwargs) ➋ def load_db(db): raw_data = osconfeed.load() ➌ warnings.warn('loading ' + DB_NAME) for collection, rec_list in raw_data['Schedule'].items(): ➍ record_type = collection[:-1] ➎ for record in rec_list: key = '{}.{}'.format(record_type, record['serial']) ➏ record['serial'] = key ➐ db[key] = Record(**record) ➑

❶ 加载示例 19-2 中的 osconfeed.py 模块。

❷ 这是使用关键字参数传入的属性构建实例的常用简便方式（详情参见下文）。

❸ 如果本地没有副本，从网上下载 JSON 数据源。

❹ 迭代集合（例如 'conferences'、'events'，等等）。

❺ record_type 的值是去掉尾部's' 后的集合名（即把 'events' 变成 'event'）。

❻ 使用 record_type 和'serial' 字段构成 key。

❼ 把'serial' 字段的值设为完整的键。

❽ 构建 Record 实例，存储在数据库中的 key 键名下。

Record.__init__ 方法展示了一个流行的 Python 技巧。我们知道，对象的 __dict__ 属性中存储着对象的属性 —— 前提是类中没有声明 __slots__ 属性，如 9.8 节所述。因此，更新实例的 __dict__ 属性，把值设为一个映射，能快速地在那个实例中创建一堆属性。9

9 顺便说一下，2001 年 Alex Martelli 在「The simple but handy‘collector of a bunch of named stuff’class」诀窍中分享这个技巧时使用的类名是 Bunch。

我不会重述 19.1.2 节讨论的细节，不过要知道，在某些应用中，Record 类可能要处理不能作为属性名使用的键。

示例 19-9 中定义的 Record 类太简单了，因此你可能会问，为什么之前没用，而是使用更复杂的 FrozenJSON 类。原因有两个。第一，FrozenJSON 类要递归转换嵌套的映射和列表；而 Record 类不需要这么做，因为转换好的数据集中没有嵌套的映射和列表，记录中只有字符串、整数、字符串列表和整数列表。第二，FrozenJSON 类要访问内嵌的 __data 属性（值是字典，用于调用 keys 等方法），而现在我们也不需要这么做了。

Python 标准库中至少有两个与 Record 类似的类，其实例可以有任意个属性，由传给构造方法的关键字参数构建 ——multiprocessing.Namespace 类 [文档，源码] 和 argparse.Namespace 类 [文档，源码]。我之所以自己实现 Record，是为了说明一个重要的做法：在 __init__ 方法中更新实例的 __dict__ 属性。

像上面那样调整日程数据集之后，我们可以扩展 Record 类，让它提供一个有用的服务：自动获取 event 记录引用的 venue 和 speaker 记录。这与 Django ORM 访问 models.ForeignKey 字段时所做的事类似：得到的不是键，而是链接的模型对象。在下一个示例中，我们要使用特性来实现这个服务。

19.1.5　使用特性获取链接的记录

下一个版本的目标是，对于从 Shelf 对象中获取的 event 记录来说，读取它的 venue 或 speakers 属性时返回的不是编号，而是完整的记录对象。用法如示例 19-10 中的交互代码片段所示。

示例 19-10　摘自 schedule2.py 脚本的 doctest

>>> DbRecord.set_db(db) ➊ >>> event = DbRecord.fetch('event.33950') ➋ >>> event ➌ <Event 'There *Will* Be Bugs'> >>> event.venue ➍ <DbRecord serial='venue.1449'> >>> event.venue.name ➎ 'Portland 251' >>> for spkr in event.speakers: ➏ ... print('{0.serial}: {0.name}'.format(spkr)) ... speaker.3471: Anna Martelli Ravenscroft speaker.5199: Alex Martelli

❶ DbRecord 类扩展 Record 类，添加对数据库的支持：为了操作数据库，必须为 DbRecord 提供一个数据库的引用。

❷ DbRecord.fetch 类方法能获取任何类型的记录。

❸ 注意，event 是 Event 类的实例，而 Event 类扩展 DbRecord 类。

❹ event.venue 返回一个 DbRecord 实例。

❺ 现在，想找出 event.venue 的名称就容易了。这种自动取值是这个示例的目标。

❻ 还可以迭代 event.speakers 列表，获取表示各位演讲者的 DbRecord 对象。

图 19-1 绘出了本节要分析的几个类。

Record

__init__ 方法与 schedule1.py 脚本（见示例 19-9）中的一样；为了辅助测试，增加了 __eq__ 方法。

DbRecord

Record 类的子类，添加了 __db 类属性，用于设置和获取 __db 属性的 set_db 和 get_db 静态方法，用于从数据库中获取记录的 fetch 类方法，以及辅助调试和测试的 __repr__ 实例方法。

Event

DbRecord 类的子类，添加了用于获取所链接记录的 venue 和 speakers 属性，以及特殊的 __repr__ 方法。

图 19-1：改进的 Record 类和两个子类（DbRecord 和 Event）的 UML 类图

DbRecord.__db 类属性的作用是存储打开的 shelve.Shelf 数据库引用，以便在需要使用数据库的 DbRecord.fetch 方法及 Event.venue 和 Event.speakers 属性中使用。我把 __db 设为私有类属性，然后定义了普通的读值方法和设值方法，以防不小心覆盖 __db 属性的值。基于一个重要的原因，我没有使用特性去管理 __db 属性：特性是用于管理实例属性的类属性。10

10Stack Overflow 中有个题为「Class-level read only properties in Python」的问题，为类中的只读属性提供了解决方案，其中包括 Alex Martelli 提供的一个方案。这些方案要用到元类，因此学习那些方案之前可能要先读本书第 21 章。

本节的代码在本书仓库里的 schedule2.py 模块中。这个模块有 100 多行，因此我会分成几部分分析。

schedule2.py 脚本的前几个语句在示例 19-11 中。

示例 19-11　schedule2.py：导入模块，定义常量和增强的 Record 类

import warnings import inspect ➊ import osconfeed DB_NAME = 'data/schedule2_db' ➋ CONFERENCE = 'conference.115' class Record: def __init__(self, **kwargs): self.__dict__.update(kwargs) def __eq__(self, other): ➌ if isinstance(other, Record): return self.__dict__ == other.__dict__ else: return NotImplemented

➊ inspect 模块在 load_db 函数中使用（参见示例 19-14）。

➋ 因为要存储几个不同类的实例，所以我们要创建并使用不同的数据库文件；这里不用示例 19-9 中的'schedule1_db'，而是使用'schedule2_db'。

➌ __eq__ 方法对测试有重大帮助。

在 Python 2 中，只有「新式」类支持特性。在 Python 2 中定义新式类的方法是，直接或间接继承 object 类。示例 19-11 中的 Record 类是一个继承体系的基类，用到了特性；因此，在 Python 2 中声明 Record 类时，开头要这么写：11

class Record (object): # 余下的代码……

11 在 Python 3 中明确指明继承 object 类没有错，但是多余，因为现在所有类都是新式的。此例说明，与过去告别能让语言更简洁。如果要在 Python 2 和 Python 3 中运行同一段代码，应该显式继承 object 类。

接下来，schedule2.py 脚本定义了两个类 —— 一个自定义的异常类型和 DbRecord 类，参见示例 19-12。

示例 19-12　schedule2.py：MissingDatabaseError 类和 DbRecord 类

class MissingDatabaseError (RuntimeError): """需要数据库但没有指定数据库时抛出。""" ➊ class DbRecord (Record): ➋ __db = None ➌ @staticmethod ➍ def set_db (db): DbRecord.__db = db ➎ @staticmethod ➏ def get_db (): return DbRecord.__db @classmethod ➐ def fetch (cls, ident): db = cls.get_db () try: return db [ident] ➑ except TypeError: if db is None: ➒ msg = "database not set; call '{}.set_db (my_db)'" raise MissingDatabaseError (msg.format (cls.__name__)) else: ➓ raise def __repr__(self): if hasattr (self, 'serial'): ⓫ cls_name = self.__class__.__name__ return '<{} serial={!r}>'.format (cls_name, self.serial) else: return super ().__repr__() ⓬

❶ 自定义的异常通常是标志类，没有定义体。写一个文档字符串，说明异常的用途，比只写一个 pass 语句要好。

❷ DbRecord 类扩展 Record 类。

❸ __db 类属性存储一个打开的 shelve.Shelf 数据库引用。

❹ set_db 是静态方法，以此强调不管调用多少次，效果始终一样。

❺ 即使调用 Event.set_db (my_db)，__db 属性仍在 DbRecord 类中设置。

❻ get_db 也是静态方法，因为不管怎样调用，返回值始终是 DbRecord.__db 引用的对象。

❼ fetch 是类方法，因此在子类中易于定制它的行为。

❽ 从数据库中获取 ident 键对应的记录。

❾ 如果捕获到 TypeError 异常，而且 db 变量的值是 None，抛出自定义的异常，说明必须设置数据库。

❿ 否则，重新抛出 TypeError 异常，因为我们不知道怎么处理。

⓫ 如果记录有 serial 属性，在字符串表示形式中使用。

⓬ 否则，调用继承的 __repr__ 方法。

现在到这个示例的重要部分了 ——Event 类，如示例 19-13 所示。

示例 19-13　schedule2.py：Event 类

class Event(DbRecord): ➊ @property def venue(self): key = 'venue.{}'.format(self.venue_serial) return self.__class__.fetch(key) ➋ @property def speakers(self): if not hasattr(self, '_speaker_objs'): ➌ spkr_serials = self.__dict__['speakers'] ➍ fetch = self.__class__.fetch ➎ self._speaker_objs = [fetch('speaker.{}'.format(key)) for key in spkr_serials] ➏ return self._speaker_objs ➐ def __repr__(self): if hasattr(self, 'name'): ➑ cls_name = self.__class__.__name__ return '<{} {!r}>'.format(cls_name, self.name) else: return super().__repr__() ➒

❶ Event 类扩展 DbRecord 类。

❷ 在 venue 特性中使用 venue_serial 属性构建 key，然后传给继承自 DbRecord 类的 fetch 类方法（详情参见下文）。

❸ speakers 特性检查记录是否有 _speaker_objs 属性。

❹ 如果没有，直接从 __dict__ 实例属性中获取'speakers' 属性的值，防止无限递归，因为这个特性的公开名称也是 speakers。

❺ 获取 fetch 类方法的引用（稍后会说明这么做的原因）。

❻ 使用 fetch 获取 speaker 记录列表，然后赋值给 self._speaker_objs。

❼ 返回前面获取的列表。

❽ 如果记录有 name 属性，在字符串表示形式中使用。

❾ 否则，调用继承的 __repr__ 方法。

在示例 19-13 中的 venue 特性里，最后一行返回的是 self.__class__.fetch (key)，为什么不直接使用 self.fetch (key) 呢？对这个 OSCON 数据源来说，可以使用后者，因为事件记录都没有 'fetch' 键。哪怕只有一个事件记录有名为 'fetch' 的键，那么在那个 Event 实例中，self.fetch 获取的是 fetch 字段的值，而不是 Event 继承自 DbRecord 的 fetch 类方法。这个缺陷不明显，很容易被测试忽略；在生产环境中，如果会场或演讲者记录链接到那个事件记录，获取事件记录时才会暴露出来。

从数据中创建实例属性的名称时肯定有可能会引入缺陷，因为类属性（例如方法）可能被遮盖，或者由于意外覆盖现有的实例属性而丢失数据。这个问题可能是 Python 字典默认不能像 JavaScript 对象那样访问的主要原因。

如果 Record 类的行为更像映射，可以把动态的 __getattr__ 方法换成动态的 __getitem__ 方法，这样就不会出现由于覆盖或遮盖而引起的缺陷了。使用映射实现 Record 类或许更符合 Python 风格。可是，如果我采用那种方式，就发掘不了动态属性编程的技巧和陷阱了。

这个示例最后的代码是重写的 load_db 函数，如示例 19-14。

示例 19-14　schedule2.py：load_db 函数

def load_db(db): raw_data = osconfeed.load() warnings.warn('loading ' + DB_NAME) for collection, rec_list in raw_data['Schedule'].items(): record_type = collection[:-1] ➊ cls_name = record_type.capitalize() ➋ cls = globals().get(cls_name, DbRecord) ➌ if inspect.isclass(cls) and issubclass(cls, DbRecord): ➍ factory = cls ➎ else: factory = DbRecord ➏ for record in rec_list: ➐ key = '{}.{}'.format(record_type, record['serial']) record['serial'] = key db[key] = factory(**record) ➑

❶ 目前，与 schedule1.py 脚本（见示例 19-9）中的 load_db 函数一样。

❷ 把 record_type 变量的值首字母变成大写（例如，把 'event' 变成 'Event'），获取可能的类名。

❸ 从模块的全局作用域中获取那个名称对应的对象；如果找不到对象，使用 DbRecord。

❹ 如果获取的对象是类，而且是 DbRecord 的子类……

❺ …… 把对象赋值给 factory 变量。因此，factory 的值可能是 DbRecord 的任何一个子类，具体的类取决于 record_type 的值。

❻ 否则，把 DbRecord 赋值给 factory 变量。

❼ 这个 for 循环创建 key，然后保存记录，这与之前一样，不过……

❽ …… 存储在数据库中的对象由 factory 构建，factory 可能是 DbRecord 类，也可能是根据 record_type 的值确定的某个子类。

注意，只有事件类型的记录有自定义的类 ——Event。不过，如果定义了 Speaker 或 Venue 类，load_db 函数构建和保存记录时会自动使用这两个类，而不会使用默认的 DbRecord 类。

本章目前所举的示例是为了展示如何使用基本的工具，如 __getattr__ 方法、hasattr 函数、getattr 函数、@property 装饰器和 __dict__ 属性，来实现动态属性。

特性经常用于把公开的属性变成使用读值方法和设值方法管理的属性，且在不影响客户端代码的前提下实施业务规则，如下一节所述。

19.2　使用特性验证属性

目前，我们只介绍了如何使用 @property 装饰器实现只读特性。本节要创建一个可读写的特性。

19.2.1　LineItem 类第 1 版：表示订单中商品的类

假设有个销售散装有机食物的电商应用，客户可以按重量订购坚果、干果或杂粮。在这个系统中，每个订单中都有一系列商品，而每个商品都可以使用示例 19-15 中的类表示。

示例 19-15　bulkfood_v1：最简单的 LineItem 类

class LineItem: def __init__(self, description, weight, price): self.description = description self.weight = weight self.price = price def subtotal(self): return self.weight * self.price

这个类很精简，不过或许太简单了。示例 19-16 揭示了一个问题。

示例 19-16　重量为负值时，金额小计为负值

>>> raisins = LineItem ('Golden raisins', 10, 6.95) >>> raisins.subtotal () 69.5>>> raisins.weight = -20 # 无效输入…… >>> raisins.subtotal () # 无效输出…… -139.0

这个示例像玩具一样，但是没有想象中的那么好玩。下面是亚马逊早期的真实故事。

我们发现顾客买书时可以把数量设为负数！然后，我们把金额打到顾客的信用卡上，苦苦等待他们把书寄出（想得美）。12

——Jeff Bezos

亚马逊创始人和 CEO

12 摘自《华尔街日报》的文章，「Birth of a Salesman」（2011 年 10 月 15 日），这是 Jeff Bezos 的原话。

这个问题怎么解决呢？我们可以修改 LineItem 类的接口，使用读值方法和设值方法管理 weight 属性。这是 Java 采用的方式，这里也完全可行。

但是，如果能直接设定商品的 weight 属性，显得更自然。此外，系统可能在生产环境中，而其他部分已经直接访问 item.weight 了。此时，符合 Python 风格的做法是，把数据属性换成特性。

19.2.2　LineItem 类第 2 版：能验证值的特性

实现特性之后，我们可以使用读值方法和设值方法，但是 LineItem 类的接口保持不变（即，设置 LineItem 对象的 weight 属性依然写成 raisins.weight = 12）。

示例 19-17 列出可读写的 weight 特性的代码。

示例 19-17　bulkfood_v2.py：定义了 weight 特性的 LineItem 类

class LineItem: def __init__(self, description, weight, price): self.description = description self.weight = weight ➊ self.price = price def subtotal(self): return self.weight * self.price @property ➋ def weight(self): ➌ return self.__weight ➍ @weight.setter ➎ def weight(self, value): if value > 0: self.__weight = value ➏ else: raise ValueError('value must be > 0') ➐

❶ 这里已经使用特性的设值方法了，确保所创建实例的 weight 属性不能为负值。

❷ @property 装饰读值方法。

❸ 实现特性的方法，其名称都与公开属性的名称一样 ——weight。

❹ 真正的值存储在私有属性 __weight 中。

❺ 被装饰的读值方法有个 .setter 属性，这个属性也是装饰器；这个装饰器把读值方法和设值方法绑定在一起。

❻ 如果值大于零，设置私有属性 __weight。

❼ 否则，抛出 ValueError 异常。

注意，现在不能创建重量为无效值的 LineItem 对象：

>>> walnuts = LineItem('walnuts', 0, 10.00) Traceback (most recent call last): ... ValueError: value must be > 0

