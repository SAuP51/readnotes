第 4 章　文本和字节序列

人类使用文本，计算机使用字节序列。1

——Esther Nam 和 Travis Fischer

「Character Encoding and Unicode in Python」

1PyCon 2014，「Character Encoding and Unicode in Python」演讲的第 12 张幻灯片幻灯片，视频。

Python 3 明确区分了人类可读的文本字符串和原始的字节序列。隐式地把字节序列转换成 Unicode 文本已成过去。本章将要讨论 Unicode 字符串、二进制序列，以及在二者之间转换时使用的编码。

深入理解 Unicode 对你可能十分重要，也可能无关紧要，这取决于 Python 编程的场景。说到底，本章涵盖的问题对只处理 ASCII 文本的程序员没有影响。但是即便如此，也不能避而不谈字符串和字节序列的区别。此外，你会发现专门的二进制序列类型所提供的功能，有些是 Python 2 中「全功能」的 str 类型不具有的。

本章将讨论下述话题：

字符、码位和字节表述

bytes、bytearray 和 memoryview 等二进制序列的独特特性

全部 Unicode 和陈旧字符集的编解码器

避免和处理编码错误

处理文本文件的最佳实践

默认编码的陷阱和标准 I/O 的问题

规范化 Unicode 文本，进行安全的比较

规范化、大小写折叠和暴力移除音调符号的实用函数

使用 locale 模块和 PyUCA 库正确地排序 Unicode 文本

Unicode 数据库中的字符元数据

能处理字符串和字节序列的双模式 API

接下来先从字符、码位和字节序列开始。

4.1　字符问题

「字符串」是个相当简单的概念：一个字符串是一个字符序列。问题出在「字符」的定义上。

在 2015 年，「字符」的最佳定义是 Unicode 字符。因此，从 Python 3 的 str 对象中获取的元素是 Unicode 字符，这相当于从 Python 2 的 unicode 对象中获取的元素，而不是从 Python 2 的 str 对象中获取的原始字节序列。

Unicode 标准把字符的标识和具体的字节表述进行了如下的明确区分。

字符的标识，即码位，是 0~1 114 111 的数字（十进制），在 Unicode 标准中以 4~6 个十六进制数字表示，而且加前缀「U+」。例如，字母 A 的码位是 U+0041，欧元符号的码位是 U+20AC，高音谱号的码位是 U+1D11E。在 Unicode 6.3 中（这是 Python 3.4 使用的标准），约 10% 的有效码位有对应的字符。

字符的具体表述取决于所用的编码。编码是在码位和字节序列之间转换时使用的算法。在 UTF-8 编码中，A（U+0041）的码位编码成单个字节 \x41，而在 UTF-16LE 编码中编码成两个字节 \x41\x00。再举个例子，欧元符号（U+20AC）在 UTF-8 编码中是三个字节 ——\xe2\x82\xac，而在 UTF-16LE 中编码成两个字节：\xac\x20。

把码位转换成字节序列的过程是编码；把字节序列转换成码位的过程是解码。示例 4-1 阐释了这一区分。

示例 4-1　编码和解码

>>> s = 'café' >>> len(s) # ➊ 4 >>> b = s.encode('utf8') # ➋ >>> b b'caf\xc3\xa9' # ➌ >>> len(b) # ➍ 5 >>> b.decode('utf8') # ➎ 'café

❶ 'café' 字符串有 4 个 Unicode 字符。

❷ 使用 UTF-8 把 str 对象编码成 bytes 对象。

❸ bytes 字面量以 b 开头。

❹ 字节序列 b 有 5 个字节（在 UTF-8 中，「é」的码位编码成两个字节）。

❺ 使用 UTF-8 把 bytes 对象解码成 str 对象。

如果想帮助自己记住 .decode () 和 .encode () 的区别，可以把字节序列想成晦涩难懂的机器磁芯转储，把 Unicode 字符串想成「人类可读」的文本。那么，把字节序列变成人类可读的文本字符串就是解码，而把字符串变成用于存储或传输的字节序列就是编码。

虽然 Python 3 的 str 类型基本相当于 Python 2 的 unicode 类型，只不过是换了个新名称，但是 Python 3 的 bytes 类型却不是把 str 类型换个名称那么简单，而且还有关系紧密的 bytearray 类型。因此，在讨论编码和解码的问题之前，有必要先来介绍一下二进制序列类型。

4.2　字节概要

新的二进制序列类型在很多方面与 Python 2 的 str 类型不同。首先要知道，Python 内置了两种基本的二进制序列类型：Python 3 引入的不可变 bytes 类型和 Python 2.6 添加的可变 bytearray 类型。（Python 2.6 也引入了 bytes 类型，但那只不过是 str 类型的别名，与 Python 3 的 bytes 类型不同。）

bytes 或 bytearray 对象的各个元素是介于 0~255（含）之间的整数，而不像 Python 2 的 str 对象那样是单个的字符。然而，二进制序列的切片始终是同一类型的二进制序列，包括长度为 1 的切片，如示例 4-2 所示。

示例 4-2　包含 5 个字节的 bytes 和 bytearray 对象

>>> cafe = bytes('café', encoding='utf_8') ➊ >>> cafe b'caf\xc3\xa9' >>> cafe[0] ➋ 99 >>> cafe[:1] ➌ b'c' >>> cafe_arr = bytearray(cafe) >>> cafe_arr ➍ bytearray(b'caf\xc3\xa9') >>> cafe_arr[-1:] ➎ bytearray(b'\xa9')

❶ bytes 对象可以从 str 对象使用给定的编码构建。

❷ 各个元素是 range (256) 内的整数。

❸ bytes 对象的切片还是 bytes 对象，即使是只有一个字节的切片。

❹ bytearray 对象没有字面量句法，而是以 bytearray () 和字节序列字面量参数的形式显示。

❺ bytearray 对象的切片还是 bytearray 对象。

my_bytes [0] 获取的是一个整数，而 my_bytes [:1] 返回的是一个长度为 1 的 bytes 对象 —— 这一点应该不会让人意外。s [0] == s [:1] 只对 str 这个序列类型成立。不过，str 类型的这个行为十分罕见。对其他各个序列类型来说，s [i] 返回一个元素，而 s [i:i+1] 返回一个相同类型的序列，里面是 s [i] 元素。

虽然二进制序列其实是整数序列，但是它们的字面量表示法表明其中有 ASCII 文本。因此，各个字节的值可能会使用下列三种不同的方式显示。

可打印的 ASCII 范围内的字节（从空格到～），使用 ASCII 字符本身。

制表符、换行符、回车符和 \ 对应的字节，使用转义序列 \t、\n、\r 和 \\。

其他字节的值，使用十六进制转义序列（例如，\x00 是空字节）。

因此，在示例 4-2 中，我们看到的是 b'caf\xc3\xa9'：前 3 个字节 b'caf' 在可打印的 ASCII 范围内，后两个字节则不然。

除了格式化方法（format 和 format_map）和几个处理 Unicode 数据的方法（包括 casefold、isdecimal、isidentifier、isnumeric、isprintable 和 encode）之外，str 类型的其他方法都支持 bytes 和 bytearray 类型。这意味着，我们可以使用熟悉的字符串方法处理二进制序列，如 endswith、replace、strip、translate、upper 等，只有少数几个其他方法的参数是 bytes 对象，而不是 str 对象。此外，如果正则表达式编译自二进制序列而不是字符串，re 模块中的正则表达式函数也能处理二进制序列。Python 3.0~3.4 不能使用 % 运算符处理二进制序列，但是根据「PEP 461—Adding % formatting to bytes and bytearray」，Python 3.5 应该会支持。

二进制序列有个类方法是 str 没有的，名为 fromhex，它的作用是解析十六进制数字对（数字对之间的空格是可选的），构建二进制序列：

>>> bytes.fromhex('31 4B CE A9') b'1K\xce\xa9'

构建 bytes 或 bytearray 实例还可以调用各自的构造方法，传入下述参数。

一个 str 对象和一个 encoding 关键字参数。

一个可迭代对象，提供 0~255 之间的数值。

一个整数，使用空字节创建对应长度的二进制序列。[Python 3.5 会把这个构造方法标记为「过时的」，Python 3.6 会将其删除。参见「PEP 467—Minor API improvements for binary sequences」。]

一个实现了缓冲协议的对象（如 bytes、bytearray、memoryview、array.array）；此时，把源对象中的字节序列复制到新建的二进制序列中。

使用缓冲类对象构建二进制序列是一种低层操作，可能涉及类型转换。示例 4-3 做了演示。

示例 4-3　使用数组中的原始数据初始化 bytes 对象

>>> import array >>> numbers = array.array('h', [-2, -1, 0, 1, 2]) ➊ >>> octets = bytes(numbers) ➋ >>> octets b'\xfe\xff\xff\xff\x00\x00\x01\x00\x02\x00' ➌

➊ 指定类型代码 h，创建一个短整数（16 位）数组。

➋ octets 保存组成 numbers 的字节序列的副本。

➌ 这些是表示那 5 个短整数的 10 个字节。

使用缓冲类对象创建 bytes 或 bytearray 对象时，始终复制源对象中的字节序列。与之相反，memoryview 对象允许在二进制数据结构之间共享内存。如果想从二进制序列中提取结构化信息，struct 模块是重要的工具。下一节会使用这个模块处理 bytes 和 memoryview 对象。

结构体和内存视图

struct 模块提供了一些函数，把打包的字节序列转换成不同类型字段组成的元组，还有一些函数用于执行反向转换，把元组转换成打包的字节序列。struct 模块能处理 bytes、bytearray 和 memoryview 对象。

如 2.9.2 节所述，memoryview 类不是用于创建或存储字节序列的，而是共享内存，让你访问其他二进制序列、打包的数组和缓冲中的数据切片，而无需复制字节序列，例如 Python Imaging Library（PIL）2 就是这样处理图像的。

2Pillow 是 PIL 最活跃的派生库。

示例 4-4 展示了如何使用 memoryview 和 struct 提取一个 GIF 图像的宽度和高度。

示例 4-4　使用 memoryview 和 struct 查看一个 GIF 图像的首部

>>> import struct >>> fmt = '<3s3sHH' # ➊ >>> with open('filter.gif', 'rb') as fp: ... img = memoryview(fp.read()) # ➋ ... >>> header = img[:10] # ➌ >>> bytes(header) # ➍ b'GIF89a+\x02\xe6\x00' >>> struct.unpack(fmt, header) # ➎ (b'GIF', b'89a', 555, 230) >>> del header # ➏ >>> del img

❶ 结构体的格式：< 是小字节序，3s3s 是两个 3 字节序列，HH 是两个 16 位二进制整数。

❷ 使用内存中的文件内容创建一个 memoryview 对象……

❸ …… 然后使用它的切片再创建一个 memoryview 对象；这里不会复制字节序列。

❹ 转换成字节序列，这只是为了显示；这里复制了 10 字节。

❺ 拆包 memoryview 对象，得到一个元组，包含类型、版本、宽度和高度。

❻ 删除引用，释放 memoryview 实例所占的内存。

注意，memoryview 对象的切片是一个新 memoryview 对象，而且不会复制字节序列。[本书的技术审校之一 Leonardo Rochael 指出，如果使用 mmap 模块把图像打开为内存映射文件，那么会复制少量字节。本书不会讨论 mmap，如果你经常读取和修改二进制文件，可以阅读「mmap—Memory-mapped file support」来进一步学习。]

本书不会深入介绍 memoryview 和 struct 模块，如果要处理二进制数据，可以阅读它们的文档：「Built-in Types » Memory Views」和「struct—Interpret bytes as packed binary data」。

简要探讨 Python 的二进制序列类型之后，下面说明如何在它们和字符串之间转换。

4.3　基本的编解码器

Python 自带了超过 100 种编解码器（codec, encoder/decoder），用于在文本和字节之间相互转换。每个编解码器都有一个名称，如 'utf_8'，而且经常有几个别名，如 'utf8'、'utf-8' 和 'U8'。这些名称可以传给 open ()、str.encode ()、bytes.decode () 等函数的 encoding 参数。示例 4-5 使用 3 个编解码器把相同的文本编码成不同的字节序列。

示例 4-5　使用 3 个编解码器编码字符串「El Niño」，得到的字节序列差异很大

>>> for codec in ['latin_1', 'utf_8', 'utf_16']: ... print(codec, 'El Niño'.encode(codec), sep='\t') ... latin_1 b'El Ni\xf1o' utf_8 b'El Ni\xc3\xb1o' utf_16 b'\xff\xfeE\x00l\x00 \x00N\x00i\x00\xf1\x00o\x00'

图 4-1 展示了不同编解码器对「A」和高音谱号等字符编码后得到的字节序列。注意，后 3 种是可变长度的多字节编码。

图 4-1：12　个字符，它们的码位及不同编码的字节表述（十六进制，星号表明该编码不支持表示该字符）

图 4-1 中的星号表明，某些编码（如 ASCII 和多字节的 GB2312）不能表示所有 Unicode 字符。然而，UTF 编码的设计目的就是处理每一个 Unicode 码位。

图 4-1 中展示的是一些典型编码，介绍如下。

latin1（即 iso8859_1）

一种重要的编码，是其他编码的基础，例如 cp1252 和 Unicode（注意，latin1 与 cp1252 的字节值是一样的，甚至连码位也相同）。

cp1252

Microsoft 制定的 latin1 超集，添加了有用的符号，例如弯引号和€（欧元）；有些 Windows 应用把它称为「ANSI」，但它并不是 ANSI 标准。

cp437

IBM PC 最初的字符集，包含框图符号。与后来出现的 latin1 不兼容。

gb2312

用于编码简体中文的陈旧标准；这是亚洲语言中使用较广泛的多字节编码之一。

utf-8

目前 Web 中最常见的 8 位编码；3 与 ASCII 兼容（纯 ASCII 文本是有效的 UTF-8 文本）。

3W3Techs 发布的「Usage of character encodings for websites」报告指出，截至 2014 年 9 月，81.4% 的网站使用 UTF-8；而 Built With 发布的「Encoding Usage Statistics」估计的比例则是 79.4%。

utf-16le

UTF-16 的 16 位编码方案的一种形式；所有 UTF-16 支持通过转义序列（称为「代理对」，surrogate pair）表示超过 U+FFFF 的码位。

UTF-16 取代了 1996 年发布的 Unicode 1.0 编码（UCS-2）。这个编码在很多系统中仍在使用，但是支持的最大码位是 U+FFFF。从 Unicode 6.3 起，分配的码位中有超过 50% 在 U+10000 以上，包括逐渐流行的表情符号（emoji pictograph）。

概述常规的编码之后，下面要处理编码和解码过程中存在的问题。

4.4　了解编解码问题

虽然有个一般性的 UnicodeError 异常，但是报告错误时几乎都会指明具体的异常：UnicodeEncodeError（把字符串转换成二进制序列时）或 UnicodeDecodeError（把二进制序列转换成字符串时）。如果源码的编码与预期不符，加载 Python 模块时还可能抛出 SyntaxError。接下来的几节说明如何处理这些错误。

出现与 Unicode 有关的错误时，首先要明确异常的类型。导致编码问题的是 UnicodeEncodeError、UnicodeDecodeError，还是如 SyntaxError 的其他错误？解决问题之前必须清楚这一点。

4.4.1　处理 UnicodeEncodeError

多数非 UTF 编解码器只能处理 Unicode 字符的一小部分子集。把文本转换成字节序列时，如果目标编码中没有定义某个字符，那就会抛出 UnicodeEncodeError 异常，除非把 errors 参数传给编码方法或函数，对错误进行特殊处理。处理错误的方式如示例 4-6 所示。

示例 4-6　编码成字节序列：成功和错误处理

>>> city = 'São Paulo' >>> city.encode('utf_8') ➊ b'S\xc3\xa3o Paulo' >>> city.encode('utf_16') b'\xff\xfeS\x00\xe3\x00o\x00 \x00P\x00a\x00u\x00l\x00o\x00' >>> city.encode('iso8859_1') ➋ b'S\xe3o Paulo' >>> city.encode('cp437') ➌ Traceback (most recent call last): File "<stdin>", line 1, in <module> File "/.../lib/python3.4/encodings/cp437.py", line 12, in encode return codecs.charmap_encode(input,errors,encoding_map) UnicodeEncodeError: 'charmap' codec can't encode character '\xe3' in position 1: character maps to <undefined> >>> city.encode('cp437', errors='ignore') ➍ b'So Paulo' >>> city.encode('cp437', errors='replace') ➎ b'S?o Paulo' >>> city.encode('cp437', errors='xmlcharrefreplace') ➏ b'São Paulo'

❶ 'utf_?' 编码能处理任何字符串。

❷ 'iso8859_1' 编码也能处理字符串 'São Paulo'。

❸ 'cp437' 无法编码 'ã'（带波形符的「a」）。默认的错误处理方式'strict' 抛出 UnicodeEncodeError。

❹ error='ignore' 处理方式悄无声息地跳过无法编码的字符；这样做通常很是不妥。

❺ 编码时指定 error='replace'，把无法编码的字符替换成 '?'；数据损坏了，但是用户知道出了问题。

❻ 'xmlcharrefreplace' 把无法编码的字符替换成 XML 实体。

编解码器的错误处理方式是可扩展的。你可以为 errors 参数注册额外的字符串，方法是把一个名称和一个错误处理函数传给 codecs.register_error 函数。参见 codecs.register_error 函数的文档。

4.4.2　处理 UnicodeDecodeError

不是每一个字节都包含有效的 ASCII 字符，也不是每一个字符序列都是有效的 UTF-8 或 UTF-16。因此，把二进制序列转换成文本时，如果假设是这两个编码中的一个，遇到无法转换的字节序列时会抛出 UnicodeDecodeError。

另一方面，很多陈旧的 8 位编码 —— 如 'cp1252'、'iso8859_1' 和 'koi8_r'—— 能解码任何字节序列流而不抛出错误，例如随机噪声。因此，如果程序使用错误的 8 位编码，解码过程悄无声息，而得到的是无用输出。

乱码字符称为鬼符（gremlin）或 mojibake（文字化け，「变形文本」的日文）。

示例 4-7 演示了使用错误的编解码器可能出现鬼符或抛出 UnicodeDecodeError。

示例 4-7　把字节序列解码成字符串：成功和错误处理

>>> octets = b'Montr\xe9al' ➊ >>> octets.decode('cp1252') ➋ 'Montréal' >>> octets.decode('iso8859_7') ➌ 'Montrιal' >>> octets.decode('koi8_r') ➍ 'MontrИal' >>> octets.decode('utf_8') ➎ Traceback (most recent call last): File "<stdin>", line 1, in <module> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 5: invalid continuation byte >>> octets.decode('utf_8', errors='replace') ➏ 'Montr􀓠al'

❶ 这些字节序列是使用 latin1 编码的「Montréal」；'\xe9' 字节对应「é」。

❷ 可以使用 'cp1252'（Windows 1252）解码，因为它是 latin1 的有效超集。

❸ ISO-8859-7 用于编码希腊文，因此无法正确解释 '\xe9' 字节，而且没有抛出错误。

❹ KOI8-R 用于编码俄文；这里，'\xe9' 表示西里尔字母「И」。

❺ 'utf_8' 编解码器检测到 octets 不是有效的 UTF-8 字符串，抛出 UnicodeDecodeError。

❻ 使用 'replace' 错误处理方式，\xe9 替换成了「」（码位是 U+FFFD），这是官方指定的 REPLACEMENT CHARACTER（替换字符），表示未知字符。

4.4.3　使用预期之外的编码加载模块时抛出的 SyntaxError

Python 3 默认使用 UTF-8 编码源码，Python 2（从 2.5 开始）则默认使用 ASCII。如果加载的 .py 模块中包含 UTF-8 之外的数据，而且没有声明编码，会得到类似下面的消息：

SyntaxError: Non-UTF-8 code starting with '\xe1' in file ola.py on line 1, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details

GNU/Linux 和 OS X 系统大都使用 UTF-8，因此打开在 Windows 系统中使用 cp1252 编码的 .py 文件时可能发生这种情况。注意，这个错误在 Windows 版 Python 中也可能会发生，因为 Python 3 为所有平台设置的默认编码都是 UTF-8。

为了修正这个问题，可以在文件顶部添加一个神奇的 coding 注释，如示例 4-8 所示。

示例 4-8　ola.py：「你好，世界！」的葡萄牙语版

# coding: cp1252 print('Olá, Mundo!')

现在，Python 3 的源码不再限于使用 ASCII，而是默认使用优秀的 UTF-8 编码，因此要修正源码的陈旧编码（如 'cp1252'）问题，最好将其转换成 UTF-8，别去麻烦 coding 注释。如果你用的编辑器不支持 UTF-8，那么是时候换一个了。

源码中能不能使用非 ASCII 名称

Python 3 允许在源码中使用非 ASCII 标识符：

>>> ação = 'PBR' # ação = stock >>> ε = 10**-6 # ε = epsilon

有些人不喜欢这么做。支持始终使用 ASCII 标识符的人认为，这样便于所有人阅读和编辑代码。这些人没切中要害：源码应该便于目标群体阅读和编辑，而不是「所有人」。如果代码属于跨国公司，或者是开源的，想让来自世界各地的人作贡献，那么标识符应该使用英语，也就是说只能使用 ASCII 字符。

但是，如果你是巴西的一位老师，那么使用葡萄牙语正确拼写变量和函数名更便于学生阅读代码。而且，这些学生在本地化的键盘中不难打出变音符号和重音元音字母。

现在，Python 能解析 Unicode 名称，而且源码的默认编码是 UTF-8，我觉得没有任何理由使用不带重音符号的葡萄牙语编写标识符。在 Python 2 中确实不能这么做，除非你也想使用 Python 2 运行代码，否则不必如此。如果使用葡萄牙语命名标识符却不带重音符号的话，这样写出的代码对任何人来说都不易阅读。

这是我作为说葡萄牙语的巴西人的观点，不过我相信也适用于其他国家和文化：选择对团队而言易于阅读的人类语言，然后使用正确的字符拼写。

假如有个文本文件，里面保存的是源码或诗句，但是你不知道它的编码。如何查明真正的编码呢？下一节使用一个推荐的库回答这个问题。

4.4.4　如何找出字节序列的编码

如何找出字节序列的编码？简单来说，不能。必须有人告诉你。

有些通信协议和文件格式，如 HTTP 和 XML，包含明确指明内容编码的首部。可以肯定的是，某些字节流不是 ASCII，因为其中包含大于 127 的字节值，而且制定 UTF-8 和 UTF-16 的方式也限制了可用的字节序列。不过即便如此，我们也不能根据特定的位模式来 100% 确定二进制文件的编码是 ASCII 或 UTF-8。

然而，就像人类语言也有规则和限制一样，只要假定字节流是人类可读的纯文本，就可能通过试探和分析找出编码。例如，如果 b'\x00' 字节经常出现，那么可能是 16 位或 32 位编码，而不是 8 位编码方案，因为纯文本中不能包含空字符；如果字节序列 b'\x20\x00' 经常出现，那么可能是 UTF-16LE 编码中的空格字符（U+0020），而不是鲜为人知的 U+2000 EN QUAD 字符 —— 谁知道这是什么呢！

统一字符编码侦测包 Chardet 就是这样工作的，它能识别所支持的 30 种编码。Chardet 是一个 Python 库，可以在程序中使用，不过它也提供了命令行工具 chardetect。下面是它对本章书稿文件的检测报告：

$ chardetect 04-text-byte.asciidoc 04-text-byte.asciidoc: utf-8 with confidence 0.99

二进制序列编码文本通常不会明确指明自己的编码，但是 UTF 格式可以在文本内容的开头添加一个字节序标记。参见下一节。

4.4.5　BOM：有用的鬼符

在示例 4-5 中，你可能注意到了，UTF-16 编码的序列开头有几个额外的字节，如下所示：

>>> u16 = 'El Niño'.encode('utf_16') >>> u16 b'\xff\xfeE\x00l\x00 \x00N\x00i\x00\xf1\x00o\x00'

我指的是 b'\xff\xfe'。这是 BOM，即字节序标记（byte-order mark），指明编码时使用 Intel CPU 的小字节序。

在小字节序设备中，各个码位的最低有效字节在前面：字母 'E' 的码位是 U+0045（十进制数 69），在字节偏移的第 2 位和第 3 位编码为 69 和 0。

>>> list(u16) [255, 254, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]

在大字节序 CPU 中，编码顺序是相反的；'E' 编码为 0 和 69。

为了避免混淆，UTF-16 编码在要编码的文本前面加上特殊的不可见字符 ZERO WIDTH NO-BREAK SPACE（U+FEFF）。在小字节序系统中，这个字符编码为 b'\xff\xfe'（十进制数 255, 254）。因为按照设计，U+FFFE 字符不存在，在小字节序编码中，字节序列 b'\xff\xfe' 必定是 ZERO WIDTH NO-BREAK SPACE，所以编解码器知道该用哪个字节序。

UTF-16 有两个变种：UTF-16LE，显式指明使用小字节序；UTF-16BE，显式指明使用大字节序。如果使用这两个变种，不会生成 BOM：

>>> u16le = 'El Niño'.encode('utf_16le') >>> list(u16le) [69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0] >>> u16be = 'El Niño'.encode('utf_16be') >>> list(u16be) [0, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111]

如果有 BOM，UTF-16 编解码器会将其过滤掉，为你提供没有前导 ZERO WIDTH NO-BREAK SPACE 字符的真正文本。根据标准，如果文件使用 UTF-16 编码，而且没有 BOM，那么应该假定它使用的是 UTF-16BE（大字节序）编码。然而，Intel x86 架构用的是小字节序，因此有很多文件用的是不带 BOM 的小字节序 UTF-16 编码。

与字节序有关的问题只对一个字（word）占多个字节的编码（如 UTF-16 和 UTF-32）有影响。UTF-8 的一大优势是，不管设备使用哪种字节序，生成的字节序列始终一致，因此不需要 BOM。尽管如此，某些 Windows 应用（尤其是 Notepad）依然会在 UTF-8 编码的文件中添加 BOM；而且，Excel 会根据有没有 BOM 确定文件是不是 UTF-8 编码，否则，它假设内容使用 Windows 代码页（codepage）编码。UTF-8 编码的 U+FEFF 字符是一个三字节序列：b'\xef\xbb\xbf'。因此，如果文件以这三个字节开头，有可能是带有 BOM 的 UTF-8 文件。然而，Python 不会因为文件以 b'\xef\xbb\xbf' 开头就自动假定它是 UTF-8 编码的。

下面换个话题，讨论 Python 3 处理文本文件的方式。

4.5　处理文本文件

处理文本的最佳实践是「Unicode 三明治」（如图 4-2 所示）。4 意思是，要尽早把输入（例如读取文件时）的字节序列解码成字符串。这种三明治中的「肉片」是程序的业务逻辑，在这里只能处理字符串对象。在其他处理过程中，一定不能编码或解码。对输出来说，则要尽量晚地把字符串编码成字节序列。多数 Web 框架都是这样做的，使用框架时很少接触字节序列。例如，在 Django 中，视图应该输出 Unicode 字符串；Django 会负责把响应编码成字节序列，而且默认使用 UTF-8 编码。

4 我第一次见到「Unicode 三明治」这种说法是在 Ned Batchelder 在 US PyCon 2012 上所做的精彩演讲中：「Pragmatic Unicode」。

图 4-2：Unicode 三明治 —— 目前处理文本的最佳实践

在 Python 3 中能轻松地采纳 Unicode 三明治的建议，因为内置的 open 函数会在读取文件时做必要的解码，以文本模式写入文件时还会做必要的编码，所以调用 my_file.read () 方法得到的以及传给 my_file.write (text) 方法的都是字符串对象。5

5Python 2.6 或 Python 2.7 用户要使用 io.open () 函数才能得到读写文件时自动执行的解码和编码操作。

可以看出，处理文本文件很简单。但是，如果依赖默认编码，你会遇到麻烦。

看一下示例 4-9 中的控制台会话。你能发现问题吗？

示例 4-9　一个平台上的编码问题（如果在你的机器上运行，它可能会发生，也可能不会）

>>> open('cafe.txt', 'w', encoding='utf_8').write('café') 4 >>> open('cafe.txt').read() 'cafÃ©'

问题是：写入文件时指定了 UTF-8 编码，但是读取文件时没有这么做，因此 Python 假定要使用系统默认的编码（Windows 1252），于是文件的最后一个字节解码成了字符 'Ã©'，而不是 'é'。

我是在 Windows 7 中运行示例 4-9 的。在新版 GNU/Linux 或 Mac OS X 中运行同样的语句不会出问题，因为这几个操作系统的默认编码是 UTF-8，让人误以为一切正常。如果打开文件是为了写入，但是没有指定编码参数，会使用区域设置中的默认编码，而且使用那个编码也能正确读取文件。但是，如果脚本要生成文件，而字节的内容取决于平台或同一平台中的区域设置，那么就可能导致兼容问题。

需要在多台设备中或多种场合下运行的代码，一定不能依赖默认编码。打开文件时始终应该明确传入 encoding= 参数，因为不同的设备使用的默认编码可能不同，有时隔一天也会发生变化。

示例 4-9 中有个奇怪的细节：第一个语句中的 write 函数报告写入了 4 个字符，但是下一行读取时却得到了 5 个字符。示例 4-10 是对示例 4-9 的扩展，对这个问题以及其他细节做了说明。

示例 4-10　仔细分析在 Windows 中运行的示例 4-9，找出并修正问题

>>> fp = open('cafe.txt', 'w', encoding='utf_8') >>> fp ➊ <_io.TextIOWrapper name='cafe.txt' mode='w' encoding='utf_8'> >>> fp.write('café') 4 ➋ >>> fp.close() >>> import os >>> os.stat('cafe.txt').st_size 5 ➌ >>> fp2 = open('cafe.txt') >>> fp2 ➍ <_io.TextIOWrapper name='cafe.txt' mode='r' encoding='cp1252'> >>> fp2.encoding ➎ 'cp1252' >>> fp2.read() 'cafÃ©' ➏ >>> fp3 = open('cafe.txt', encoding='utf_8') ➐ >>> fp3 <_io.TextIOWrapper name='cafe.txt' mode='r' encoding='utf_8'> >>> fp3.read() 'café' ➑ >>> fp4 = open('cafe.txt', 'rb') ➒ >>> fp4 <_io.BufferedReader name='cafe.txt'> ➓ >>> fp4.read() ⓫ b'caf\xc3\xa9'

❶ 默认情况下，open 函数采用文本模式，返回一个 TextIOWrapper 对象。

❷ 在 TextIOWrapper 对象上调用 write 方法返回写入的 Unicode 字符数。

❸ os.stat 报告文件中有 5 个字节；UTF-8 编码的 'é' 占两个字节，0xc3 和 0xa9。

❹ 打开文本文件时没有显式指定编码，返回一个 TextIOWrapper 对象，编码是区域设置中的默认值。

❺ TextIOWrapper 对象有个 encoding 属性；查看它，发现这里的编码是 cp1252。

❻ 在 Windows cp1252 编码中，0xc3 字节是「Ã」（带波形符的 A），0xa9 字节是版权符号。

❼ 使用正确的编码打开那个文件。

❽ 结果符合预期：得到的是四个 Unicode 字符 'café'。

❾ 'rb' 标志指明在二进制模式中读取文件。

❿ 返回的是 BufferedReader 对象，而不是 TextIOWrapper 对象。

⓫读取返回的字节序列，结果与预期相符。

除非想判断编码，否则不要在二进制模式中打开文本文件；即便如此，也应该使用 Chardet，而不是重新发明轮子（参见 4.4.4 节）。常规代码只应该使用二进制模式打开二进制文件，如光栅图像。

示例 4-10 的问题是，打开文本文件时依赖默认设置。默认设置有许多来源，参见下一节。

编码默认值：一团糟

有几个设置对 Python I/O 的编码默认值有影响，如示例 4-11 中的 default_encodings.py 脚本所示。

示例 4-11　探索编码默认值

import sys, locale expressions = """ locale.getpreferredencoding() type(my_file) my_file.encoding sys.stdout.isatty() sys.stdout.encoding sys.stdin.isatty() sys.stdin.encoding sys.stderr.isatty() sys.stderr.encoding sys.getdefaultencoding() sys.getfilesystemencoding() """ my_file = open('dummy', 'w') for expression in expressions.split(): value = eval(expression) print(expression.rjust(30), '->', repr(value))

示例 4-11 在 GNU/Linux（Ubuntu 14.04）和 OS X（Mavericks 10.9）中的输出一样，表明这些系统中始终使用 UTF-8：

$ python3 default_encodings.py locale.getpreferredencoding() -> 'UTF-8' type(my_file) -> <class '_io.TextIOWrapper'> my_file.encoding -> 'UTF-8' sys.stdout.isatty() -> True sys.stdout.encoding -> 'UTF-8' sys.stdin.isatty() -> True sys.stdin.encoding -> 'UTF-8' sys.stderr.isatty() -> True sys.stderr.encoding -> 'UTF-8' sys.getdefaultencoding() -> 'utf-8' sys.getfilesystemencoding() -> 'utf-8'

然而，在 Windows 中的输出有所不同，如示例 4-12 所示。

示例 4-12　在 Windows 7（SP1）巴西版中的 cmd.exe 中输出的默认编码；PowerShell 输出的结果相同

Z:\>chcp ➊ Página de código ativa: 850 Z:\>python default_encodings.py ➋ locale.getpreferredencoding() -> 'cp1252' ➌ type(my_file) -> <class '_io.TextIOWrapper'> my_file.encoding -> 'cp1252' ➍ sys.stdout.isatty() -> True ➎ sys.stdout.encoding -> 'cp850' ➏ sys.stdin.isatty() -> True sys.stdin.encoding -> 'cp850' sys.stderr.isatty() -> True sys.stderr.encoding -> 'cp850' sys.getdefaultencoding() -> 'utf-8' sys.getfilesystemencoding() -> 'mbcs'

➊ chcp 输出当前控制台激活的代码页：850。

➋ 运行 default_encodings.py，把结果输出到控制台。

➌ locale.getpreferredencoding () 是最重要的设置。

➍ 文本文件默认使用 locale.getpreferredencoding ()。

➎ 输出到控制台中，因此 sys.stdout.isatty () 返回 True。

➏ 因此，sys.stdout.encoding 与控制台的编码相同。

如果把输出重定向到文件，如下所示：

Z:\>python default_encodings.py > encodings.log

sys.stdout.isatty () 的返回值会变成 False，sys.stdout.encoding 会设为 locale.getpreferredencoding ()，在那台设备中是 'cp1252'。

注意，示例 4-12 中有 4 种不同的编码。

如果打开文件时没有指定 encoding 参数，默认值由 locale.getpreferredencoding () 提供（在示例 4-12 中是 'cp1252'）。

如果设定了 PYTHONIOENCODING 环境变量，sys.stdout/stdin/stderr 的编码使用设定的值；否则，继承自所在的控制台；如果输入 / 输出重定向到文件，则由 locale.getpreferredencoding () 定义。

Python 在二进制数据和字符串之间转换时，内部使用 sys.getdefaultencoding () 获得的编码；Python 3 很少如此，但仍有发生。6 这个设置不能修改。7

sys.getfilesystemencoding () 用于编解码文件名（不是文件内容）。把字符串参数作为文件名传给 open () 函数时就会使用它；如果传入的文件名参数是字节序列，那就不经改动直接传给 OS API。「Unicode HOWTO」一文中说：「在 Windows 中，Python 使用 mbcs 这个名称引用当前配置的编码。」MBCS 是 Multi Byte Character Set（多字节字符集）的首字母缩写，在 Windows 中是陈旧的变长编码，如 gb2312 或 Shift_JIS，而不是 UTF-8。[关于这个话题，Stack Overflow 中有一个很好的回答，「Difference between MBCS and UTF-8 on Windows」。]

6 研究这个话题时，我在 Python 内部找不到把字节序列转换成字符串的情况。Python 核心开发者 Antoine Pitrou 在 comp.python.devel 邮件列表中说，CPython 的内部函数「在 py3k 中很少这么做」。

7Python 2 对 sys.setdefaultencoding 函数的使用方式不当，Python 3 的文档中已经没有这个函数。这个函数是供核心开发者使用的，用于在内部的默认编码未定时设置编码。在 comp.python.devel 邮件列表的那个话题中，Marc-André Lemburg 说，用户代码一定不能调用 sys.setdefaultencoding 函数，而且对 CPython 来说，它的值在 Python 2 中只能是 'ascii'，在 Python 3 中只能是 'utf-8'。

在 GNU/Linux 和 OS X 中，这些编码的默认值都是 UTF-8，而且多年来都是如此，因此 I/O 能处理所有 Unicode 字符。在 Windows 中，不仅同一个系统中使用不同的编码，还有只支持 ASCII 和 127 个额外的字符的代码页（如 'cp850' 或 'cp1252'），而且不同的代码页之间增加的字符也有所不同。因此，若不多加小心，Windows 用户更容易遇到编码问题。

综上，locale.getpreferredencoding () 返回的编码是最重要的：这是打开文件的默认编码，也是重定向到文件的 sys.stdout/stdin/stderr 的默认编码。然而，文档也说道（摘录部分）：

locale.getpreferredencoding(do_setlocale=True)

根据用户的偏好设置，返回文本数据的编码。用户的偏好设置在不同系统中的设定方式不同，而且在某些系统中可能无法通过编程方式设置，因此这个函数返回的只是猜测的编码……

因此，关于编码默认值的最佳建议是：别依赖默认值。

如果遵从 Unicode 三明治的建议，而且始终在程序中显式指定编码，那将避免很多问题。可惜，即使把字节序列正确地转换成字符串，Unicode 仍有不尽如人意的地方。接下来的两节讨论的话题对 ASCII 世界来说很简单，但是在 Unicode 领域就变得相当复杂：文本规范化（即为了比较而把文本转换成统一的表述）和排序。

4.6　为了正确比较而规范化 Unicode 字符串

因为 Unicode 有组合字符（变音符号和附加到前一个字符上的记号，打印时作为一个整体），所以字符串比较起来很复杂。

例如，「café」这个词可以使用两种方式构成，分别有 4 个和 5 个码位，但是结果完全一样：

>>> s1 = 'café' >>> s2 = 'cafe\u0301' >>> s1, s2 ('café', 'café') >>> len(s1), len(s2) (4, 5) >>> s1 == s2 False

U+0301 是 COMBINING ACUTE ACCENT，加在「e」后面得到「é」。在 Unicode 标准中，'é' 和 'e\u0301' 这样的序列叫「标准等价物」（canonical equivalent），应用程序应该把它们视作相同的字符。但是，Python 看到的是不同的码位序列，因此判定二者不相等。

这个问题的解决方案是使用 unicodedata.normalize 函数提供的 Unicode 规范化。这个函数的第一个参数是这 4 个字符串中的一个：'NFC'、'NFD'、'NFKC' 和 'NFKD'。下面先说明前两个。

NFC（Normalization Form C）使用最少的码位构成等价的字符串，而 NFD 把组合字符分解成基字符和单独的组合字符。这两种规范化方式都能让比较行为符合预期：

>>> from unicodedata import normalize >>> s1 = 'café' # 把 "e" 和重音符组合在一起 >>> s2 = 'cafe\u0301' # 分解成 "e" 和重音符 >>> len (s1), len (s2) (4, 5) >>> len (normalize ('NFC', s1)), len (normalize ('NFC', s2)) (4, 4) >>> len (normalize ('NFD', s1)), len (normalize ('NFD', s2)) (5, 5) >>> normalize ('NFC', s1) == normalize ('NFC', s2) True >>> normalize ('NFD', s1) == normalize ('NFD', s2) True

西方键盘通常能输出组合字符，因此用户输入的文本默认是 NFC 形式。不过，安全起见，保存文本之前，最好使用 normalize ('NFC', user_text) 清洗字符串。NFC 也是 W3C 的「Character Model for the World Wide Web: String Matching and Searching」规范推荐的规范化形式。

使用 NFC 时，有些单字符会被规范成另一个单字符。例如，电阻的单位欧姆（Ω）会被规范成希腊字母大写的欧米加。这两个字符在视觉上是一样的，但是比较时并不相等，因此要规范化，防止出现意外：

>>> from unicodedata import normalize, name >>> ohm = '\u2126' >>> name(ohm) 'OHM SIGN' >>> ohm_c = normalize('NFC', ohm) >>> name(ohm_c) 'GREEK CAPITAL LETTER OMEGA' >>> ohm == ohm_c False >>> normalize('NFC', ohm) == normalize('NFC', ohm_c) True

在另外两个规范化形式（NFKC 和 NFKD）的首字母缩略词中，字母 K 表示「compatibility」（兼容性）。这两种是较严格的规范化形式，对「兼容字符」有影响。虽然 Unicode 的目标是为各个字符提供「规范的」码位，但是为了兼容现有的标准，有些字符会出现多次。例如，虽然希腊字母表中有「μ」这个字母（码位是 U+03BC，GREEK SMALL LETTER MU），但是 Unicode 还是加入了微符号 'µ'（U+00B5），以便与 latin1 相互转换。因此，微符号是一个「兼容字符」。

在 NFKC 和 NFKD 形式中，各个兼容字符会被替换成一个或多个「兼容分解」字符，即便这样有些格式损失，但仍是「首选」表述 —— 理想情况下，格式化是外部标记的职责，不应该由 Unicode 处理。下面举个例子。二分之一 '½'（U+00BD）经过兼容分解后得到的是三个字符序列 '1/2'；微符号 'µ'（U+00B5）经过兼容分解后得到的是小写字母 'μ'（U+03BC）。8

8 微符号是「兼容字符」，而欧姆符号不是，这还真是奇怪。因此，NFC 不会改动微符号，但是会把欧姆符号改成大写的欧米加；而 NFKC 和 NFKD 会把欧姆和微符号都改成其他字符。

下面是 NFKC 的具体应用：

>>> from unicodedata import normalize, name >>> half = '½' >>> normalize('NFKC', half) '1⁄2' >>> four_squared = '4²' >>> normalize('NFKC', four_squared) '42' >>> micro = 'μ' >>> micro_kc = normalize('NFKC', micro) >>> micro, micro_kc ('μ', 'μ') >>> ord(micro), ord(micro_kc) (181, 956) >>> name(micro), name(micro_kc) ('MICRO SIGN', 'GREEK SMALL LETTER MU')

使用 '1/2' 替代 '½' 可以接受，微符号也确实是小写的希腊字母 'µ'，但是把 '4²' 转换成 '42' 就改变原意了。某些应用程序可以把 '4²' 保存为 '4<sup>2</sup>'，但是 normalize 函数对格式一无所知。因此，NFKC 或 NFKD 可能会损失或曲解信息，但是可以为搜索和索引提供便利的中间表述：用户搜索 '1 / 2 inch' 时，如果还能找到包含 '½ inch' 的文档，那么用户会感到满意。

使用 NFKC 和 NFKD 规范化形式时要小心，而且只能在特殊情况中使用，例如搜索和索引，而不能用于持久存储，因为这两种转换会导致数据损失。

为搜索或索引准备文本时，还有一个有用的操作，即下一节讨论的大小写折叠。

4.6.1　大小写折叠

大小写折叠其实就是把所有文本变成小写，再做些其他转换。这个功能由 str.casefold () 方法（Python 3.3 新增）支持。

对于只包含 latin1 字符的字符串 s，s.casefold () 得到的结果与 s.lower () 一样，唯有两个例外：微符号 'µ' 会变成小写的希腊字母「μ」（在多数字体中二者看起来一样）；德语 Eszett（「sharp s」，ß）会变成「ss」。

>>> micro = 'μ' >>> name(micro) 'MICRO SIGN' >>> micro_cf = micro.casefold() >>> name(micro_cf) 'GREEK SMALL LETTER MU' >>> micro, micro_cf ('μ', 'μ') >>> eszett = 'ß' >>> name(eszett) 'LATIN SMALL LETTER SHARP S' >>> eszett_cf = eszett.casefold() >>> eszett, eszett_cf ('ß', 'ss')

自 Python 3.4 起，str.casefold () 和 str.lower () 得到不同结果的有 116 个码位。Unicode 6.3 命名了 110 122 个字符，这只占 0.11%。

与 Unicode 相关的任何问题一样，大小写折叠是个复杂的问题，有很多语言上的特殊情况，但是 Python 核心团队尽力提供了一种方案，能满足大多数用户的需求。

接下来的几节将使用这些规范化知识来开发几个实用的函数。

4.6.2　规范化文本匹配实用函数

由前文可知，NFC 和 NFD 可以放心使用，而且能合理比较 Unicode 字符串。对大多数应用来说，NFC 是最好的规范化形式。不区分大小写的比较应该使用 str.casefold ()。

如果要处理多语言文本，工具箱中应该有示例 4-13 中的 nfc_equal 和 fold_equal 函数。

示例 4-13　normeq.py：比较规范化 Unicode 字符串

""" Utility functions for normalized Unicode string comparison. Using Normal Form C, case sensitive: >>> s1 = 'café' >>> s2 = 'cafe\u0301' >>> s1 == s2 False >>> nfc_equal(s1, s2) True >>> nfc_equal('A', 'a') False Using Normal Form C with case folding: >>> s3 = 'Straße' >>> s4 = 'strasse' >>> s3 == s4 False >>> nfc_equal(s3, s4) False >>> fold_equal(s3, s4) True >>> fold_equal(s1, s2) True >>> fold_equal('A', 'a') True """ from unicodedata import normalize def nfc_equal(str1, str2): return normalize('NFC', str1) == normalize('NFC', str2) def fold_equal(str1, str2): return (normalize('NFC', str1).casefold() == normalize('NFC', str2).casefold())

除了 Unicode 规范化和大小写折叠（二者都是 Unicode 标准的一部分）之外，有时需要进行更为深入的转换，例如把 'café' 变成 'cafe'。下一节说明何时以及如何进行这种转换。

4.6.3　极端「规范化」：去掉变音符号

Google 搜索涉及很多技术，其中一个显然是忽略变音符号（如重音符、下加符等），至少在某些情况下会这么做。去掉变音符号不是正确的规范化方式，因为这往往会改变词的意思，而且可能误判搜索结果。但是对现实生活却有所帮助：人们有时很懒，或者不知道怎么正确使用变音符号，而且拼写规则会随时间变化，因此实际语言中的重音经常变来变去。

除了搜索，去掉变音符号还能让 URL 更易于阅读，至少对拉丁语系语言是如此。下面是维基百科中介绍圣保罗市（São Paulo）的文章的 URL：

http://en.wikipedia.org/wiki/S%C3%A3o_Paulo

其中，「% C3% A3」是 UTF-8 编码「ã」字母（带有波形符的「a」）转义后得到的结果。下述形式更友好，尽管拼写是错误的：

http://en.wikipedia.org/wiki/Sao_Paulo

如果想把字符串中的所有变音符号都去掉，可以使用示例 4-14 中的函数。

示例 4-14　去掉全部组合记号的函数（在 sanitize.py 模块中）

import unicodedata import string def shave_marks (txt): """去掉全部变音符号""" norm_txt = unicodedata.normalize ('NFD', txt) ➊ shaved = ''.join (c for c in norm_txt if not unicodedata.combining (c)) ➋ return unicodedata.normalize ('NFC', shaved) ➌

➊ 把所有字符分解成基字符和组合记号。

➋ 过滤掉所有组合记号。

➌ 重组所有字符。

示例 4-15 是 shave_marks 函数的两个使用示例。

示例 4-15　示例 4-14 中 shave_marks 函数的两个使用示例

>>> order = '「Herr Voß: • ½ cup of OEtker™ caffè latte • bowl of açaí.」' >>> shave_marks(order) '「Herr Voß: • ½ cup of OEtker™ caffe latte • bowl of acai.」' ➊ >>> Greek = 'Zέφupoς, Zéfiro' >>> shave_marks(Greek) 'Ζεφupoς, Zefiro' ➋

➊ 只替换了「è」「ç」和「í」三个字符。

➋「έ」和「é」都被替换了。

示例 4-14 中定义的 shave_marks 函数使用起来没问题，但是也许做得太多了。通常，去掉变音符号是为了把拉丁文本变成纯粹的 ASCII，但是 shave_marks 函数还会修改非拉丁字符（如希腊字母），而只去掉重音符并不能把它们变成 ASCII 字符。因此，我们应该分析各个基字符，仅当字符在拉丁字母表中时才删除附加的记号，如示例 4-16 所示。

示例 4-16　删除拉丁字母中组合记号的函数（import 语句省略了，因为这是示例 4-14 中定义的 sanitize.py 模块的一部分）

def shave_marks_latin (txt): """把拉丁基字符中所有的变音符号删除""" norm_txt = unicodedata.normalize ('NFD', txt) ➊ latin_base = False keepers = [] for c in norm_txt: if unicodedata.combining (c) and latin_base: ➋ continue # 忽略拉丁基字符上的变音符号 keepers.append (c) ➌ # 如果不是组合字符，那就是新的基字符 if not unicodedata.combining (c): ➍ latin_base = c in string.ascii_letters shaved = ''.join (keepers) return unicodedata.normalize ('NFC', shaved) ➎

➊ 把所有字符分解成基字符和组合记号。

➋ 基字符为拉丁字母时，跳过组合记号。

➌ 否则，保存当前字符。

➍ 检测新的基字符，判断是不是拉丁字母。

➎ 重组所有字符。

更彻底的规范化步骤是把西文文本中的常见符号（如弯引号、长破折号、项目符号，等等）替换成 ASCII 中的对等字符。示例 4-17 中的 asciize 函数就是这么做的。

示例 4-17　把一些西文印刷字符转换成 ASCII 字符（这个代码片段也是示例 4-14 中 sanitize.py 模块的一部分）

single_map = str.maketrans (""",ƒ,,†ˆ‹‘’「」·––˜›""", ➊ """'f"*^<''""---~>""") multi_map = str.maketrans ({➋ '€': '<euro>', '…': '...', 'OE': 'OE', '™': '(TM)', 'oe': 'oe', '‰': '<per mille>', '‡': '**', }) multi_map.update (single_map) ➌ def dewinize (txt): """把 Win1252 符号替换成 ASCII 字符或序列""" return txt.translate (multi_map) ➍ def asciize (txt): no_marks = shave_marks_latin (dewinize (txt)) ➎ no_marks = no_marks.replace ('ß', 'ss') ➏ return unicodedata.normalize ('NFKC', no_marks) ➐

❶ 构建字符替换字符的映射表。

❷ 构建字符替换字符串的映射表。

❸ 合并两个映射表。

❹ dewinize 函数不影响 ASCII 或 latin1 文本，只替换 Microsoft 在 cp1252 中为 latin1 额外添加的字符。

❺ 调用 dewinize 函数，然后去掉变音符号。

❻ 把德语 Eszett 替换成「ss」（这里没有使用大小写折叠，因为我们想保留大小写）。

❼ 使用 NFKC 规范化形式把字符和与之兼容的码位组合起来。

示例 4-18 是 asciize 函数的使用示例。

示例 4-18　示例 4-17 中 asciize 函数的使用示例

>>> order = '「Herr Voß: • ½ cup of OEtker™ caffè latte • bowl of açaí.」' >>> dewinize(order) '"Herr Voß: - ½ cup of OEtker(TM) caffè latte - bowl of açaí."' ➊ >>> asciize(order) '"Herr Voss: - 1⁄2 cup of OEtker(TM) caffe latte - bowl of acai."' ➋

➊ dewinize 函数替换弯引号、项目符号和™（商标符号）。

➋ asciize 函数调用 dewinize 函数，去掉变音符号，还会替换 'ß'。

不同语言删除变音符号的规则也有所不同。例如，德语把 'ü' 变成 'ue'。我们定义的 asciize 函数没这么精确，因此可能适合你的语言，也可能不适合。不过，它对葡萄牙语的处理是可接受的。

综上，sanitize.py 中的函数做的事情超出了标准的规范化，而且会对文本做进一步处理，很有可能会改变原意。只有知道目标语言、目标用户群和转换后的用途，才能确定要不要做这么深入的规范化。

我们对 Unicode 文本规范化的讨论到此结束。

接下来要解决的 Unicode 问题是…… 排序。

4.7　Unicode 文本排序

Python 比较任何类型的序列时，会一一比较序列里的各个元素。对字符串来说，比较的是码位。可是在比较非 ASCII 字符时，得到的结果不尽如人意。

下面对一个生长在巴西的水果的列表进行排序：

>>> fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola'] >>> sorted(fruits) ['acerola', 'atemoia', 'açaí', 'caju', 'cajá']

不同的区域采用的排序规则有所不同，葡萄牙语等很多语言按照拉丁字母表排序，重音符号和下加符对排序几乎没什么影响。9 因此，排序时「cajá」视作「caja」，必定排在「caju」前面。

9 变音符号对排序有影响的情况很少发生，只有两个词之间唯有变音符号不同时才有影响。此时，带有变音符号的词排在常规词的后面。

排序后的 fruits 列表应该是：

['açaí', 'acerola', 'atemoia', 'cajá', 'caju']

在 Python 中，非 ASCII 文本的标准排序方式是使用 locale.strxfrm 函数，根据 locale 模块的文档，这 个函数会「把字符串转换成适合所在区域进行比较的形式」。

使用 locale.strxfrm 函数之前，必须先为应用设定合适的区域设置，还要祈祷操作系统支持这项设置。在区域设为 pt_BR 的 GNU/Linux（Ubuntu 14.04）中，可以使用示例 4-19 中的命令。

示例 4-19　使用 locale.strxfrm 函数做排序键

>>> import locale >>> locale.setlocale(locale.LC_COLLATE, 'pt_BR.UTF-8') 'pt_BR.UTF-8' >>> fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola'] >>> sorted_fruits = sorted(fruits, key=locale.strxfrm) >>> sorted_fruits ['açaí', 'acerola', 'atemoia', 'cajá', 'caju']

因此，使用 locale.strxfrm 函数做排序键之前，要调用 setlocale (LC_COLLATE, «your_locale»)。

不过，有几点要注意。

区域设置是全局的，因此不推荐在库中调用 setlocale 函数。应用或框架应该在进程启动时设定区域设置，而且此后不要再修改。

操作系统必须支持区域设置，否则 setlocale 函数会抛出 locale.Error: unsupported locale setting 异常。

必须知道如何拼写区域名称。它在 Unix 衍生系统中几乎已经形成标准，要通过 'language_code.encoding' 获取。10 但是在 Windows 中，句法复杂一些：Language Name-Language Variant_Region Name.codepage。注意，「Language Name」（语言名称）、「Language Variant」（语言变体）和「Region Name」（区域名）中可以包含空格；除了第一部分之外，其他部分的前面是不同的字符：一个连字符、一个下划线和一个点号。除了语言名称之外，其他部分好像都是可选的。例如，English_United States.850，它的语言名称是「English」，区域是「United States」，代码页是「850」。Windows 能理解的语言名称和区域名见于 MSDN 中的文章「Language Identifier Constants and Strings」，还有「Code Page Identifiers.aspx)」一文列出了最后一部分的代码页数字。11

操作系统的制作者必须正确实现了所设的区域。我在 Ubuntu 14.04 中成功了，但在 OS X（Mavericks 10.9）中却失败了。在两台 Mac 中，调用 setlocale (LC_COLLATE, 'pt_BR.UTF-8') 返回的都是字符串 'pt_BR.UTF-8'，没有任何问题。但是，sorted (fruits, key=locale.strxfrm) 得到的结果与 sorted (fruits) 一样，是错误的。我还在 OS X 中尝试了 fr_FR、es_ES 和 de_DE，但是 locale.strxfrm 并未起作用。12

10 在 Linux 操作系统中，中国大陆的读者可以使用 zh_CN.UTF-8，简体中文会按照汉语拼音顺序进行排序，它也能对葡萄牙语进行正确排序。—— 编者注

11 感谢 Leonardo Rochael，他所做的工作超出了身为技术审校的职责，虽然他是 GNU/Linux 用户，但却研究了这些 Windows 细节。

12 同样，我没找到解决方案，不过却发现其他人也报告了同样的问题。本书技术审校之一 Alex Martelli，在他装有 OS X 10.9 的 Mac 电脑中使用 setlocale 和 locale.strxfrm 时没有遇到问题。综上：结果因人而异。

因此，标准库提供的国际化排序方案可用，但是似乎只支持 GNU/Linux（可能也支持 Windows，但你得是专家）。即便如此，还要依赖区域设置，而这会为部署带来问题。

幸好，有个较为简单的方案：PyPI 中的 PyUCA 库。

使用 Unicode 排序算法排序

James Tauber，一位高产的 Django 贡献者，他一定是感受到了这一痛点，因此开发了 PyUCA 库，这是 Unicode 排序算法（Unicode Collation Algorithm，UCA）的纯 Python 实现。示例 4-20 展示了它的简单用法。

示例 4-20　使用 pyuca.Collator.sort_key 方法

>>> import pyuca >>> coll = pyuca.Collator() >>> fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola'] >>> sorted_fruits = sorted(fruits, key=coll.sort_key) >>> sorted_fruits ['açaí', 'acerola', 'atemoia', 'cajá', 'caju']

这样做更友好，而且恰好可用。我在 GNU/Linux、OS X 和 Windows 中做过测试。目前，PyUCA 只支持 Python 3.x。13

132015 年 5 月，PyUCA 重新支持 Python 2.x，参见：http://jktauber.com/2015/05/13/pyuca-supports-python-2-again。—— 编者注

PyUCA 没有考虑区域设置。如果想定制排序方式，可以把自定义的排序表路径传给 Collator () 构造方法。PyUCA 默认使用项目自带的 allkeys.txt，这就是 Unicode 6.3.0 的「Default Unicode Collation Element Table」的副本。

顺便说一下，那个表是 Unicode 数据库中众多表中的一个。下一节会讨论这个话题。

4.8　Unicode 数据库

Unicode 标准提供了一个完整的数据库（许多格式化的文本文件），不仅包括码位与字符名称之间的映射，还有各个字符的元数据，以及字符之间的关系。例如，Unicode 数据库记录了字符是否可以打印、是不是字母、是不是数字，或者是不是其他数值符号。字符串的 isidentifier、isprintable、isdecimal 和 isnumeric 等方法就是靠这些信息作判断的。str.casefold 方法也用到了 Unicode 表中的信息。

unicodedata 模块中有几个函数用于获取字符的元数据。例如，字符在标准中的官方名称是不是组合字符（如结合波形符构成的变音符号等），以及符号对应的人类可读数值（不是码位）。示例 4-21 展示了 unicodedata.name () 和 unicodedata.numeric () 函数，以及字符串的 .isdecimal () 和 .isnumeric () 方法的用法。

示例 4-21　Unicode 数据库中数值字符的元数据示例（各个标号说明输出中的各列）

import unicodedata import re re_digit = re.compile(r'\d') sample = '1\xbc\xb2\u0969\u136b\u216b\u2466\u2480\u3285' for char in sample: print('U+%04x' % ord(char), ➊ char.center(6), ➋ 're_dig' if re_digit.match(char) else '-', ➌ 'isdig' if char.isdigit() else '-', ➍ 'isnum' if char.isnumeric() else '-', ➎ format(unicodedata.numeric(char), '5.2f'), ➏ unicodedata.name(char), ➐ sep='\t')

➊ U+0000 格式的码位。

➋ 在长度为 6 的字符串中居中显示字符。

➌ 如果字符匹配正则表达式 r'\d'，显示 re_dig。

➍ 如果 char.isdigit () 返回 True，显示 isdig。

➎ 如果 char.isnumeric () 返回 True，显示 isnum。

➏ 使用长度为 5、小数点后保留 2 位的浮点数显示数值。

➐ Unicode 标准中字符的名称。

运行示例 4-21 得到的结果如图 4-3 所示。

图 4-3 中的第 6 列是在字符上调用 unicodedata.numeric (char) 函数得到的结果。这表明，Unicode 知道表示数字的符号的数值。因此，如果你想创建一个支持泰米尔数字和罗马数字的电子表格应用，那就尽管去做吧！

图 4-3：9 个数值字符及其元数据；re_dig 表示字符匹配正则表达式 r'\d'

图 4-3 表明，正则表达式 r'\d' 能匹配数字「1」和梵文数字 3，但是不能匹配 isdigit 方法判断为数字的其他字符。re 模块对 Unicode 的支持并不充分。PyPI 中有个新开发的 regex 模块，它的最终目的是取代 re 模块，以提供更好的 Unicode 支持。14 下一节会回过头来讨论 re 模块。

14 不过在这个示例中，它在识别数字方面的表现没有 re 模块好。

本章使用了 unicodedata 模块中的几个函数，但是还有很多没有用到。详情参阅标准库文档对 unicodedata 模块的说明。

在结束对字符串和字节序列的讨论之前，我们还要简要说明一个新的趋势 —— 双模式 API，即提供的函数能接受字符串或字节序列为参数，然后根据类型进行特殊处理。

4.9　支持字符串和字节序列的双模式 API

标准库中的一些函数能接受字符串或字节序列为参数，然后根据类型展现不同的行为。re 和 os 模块中就有这样的函数。

4.9.1　正则表达式中的字符串和字节序列

如果使用字节序列构建正则表达式，\d 和 \w 等模式只能匹配 ASCII 字符；相比之下，如果是字符串模式，就能匹配 ASCII 之外的 Unicode 数字或字母。示例 4-22 和图 4-4 展示了字符串模式和字节序列模式中字母、ASCII 数字、上标和泰米尔数字的匹配情况。

示例 4-22　ramanujan.py：比较简单的字符串正则表达式和字节序列正则表达式的行为

import re re_numbers_str = re.compile(r'\d+') ➊ re_words_str = re.compile(r'\w+') re_numbers_bytes = re.compile(rb'\d+') ➋ re_words_bytes = re.compile(rb'\w+') text_str = ("Ramanujan saw \u0be7\u0bed\u0be8\u0bef" ➌ " as 1729 = 1³ + 12³ = 9³ + 10³.") ➍ text_bytes = text_str.encode('utf_8') ➎ print('Text', repr(text_str), sep='\n ') print('Numbers') print(' str :', re_numbers_str.findall(text_str)) ➏ print(' bytes:', re_numbers_bytes.findall(text_bytes)) ➐ print('Words') print(' str :', re_words_str.findall(text_str)) ➑ print(' bytes:', re_words_bytes.findall(text_bytes)) ➒

❶ 前两个正则表达式是字符串类型。

❷ 后两个正则表达式是字节序列类型。

❸ 要搜索的 Unicode 文本，包括 1729 的泰米尔数字（逻辑行直到右括号才结束）。

❹ 这个字符串在编译时与前一个拼接起来（参见 Python 语言参考手册中的「2.4.2. String literal concatenation」）。

❺ 字节序列只能用字节序列正则表达式搜索。

❻ 字符串模式 r'\d+' 能匹配泰米尔数字和 ASCII 数字。

❼ 字节序列模式 rb'\d+' 只能匹配 ASCII 字节中的数字。

❽ 字符串模式 r'\w+' 能匹配字母、上标、泰米尔数字和 ASCII 数字。

❾ 字节序列模式 rb'\w+' 只能匹配 ASCII 字节中的字母和数字。

图 4-4：运行示例 4-22 中的 ramanujan.py 脚本时的截图

示例 4-22 是随便举的例子，为的是说明一个问题：可以使用正则表达式搜索字符串和字节序列，但是在后一种情况中，ASCII 范围外的字节不会当成数字和组成单词的字母。

字符串正则表达式有个 re.ASCII 标志，它让 \w、\W、\b、\B、\d、\D、\s 和 \S 只匹配 ASCII 字符。详情参阅 re 模块的文档。

另一个重要的双模式模块是 os。

4.9.2　os 函数中的字符串和字节序列

GNU/Linux 内核不理解 Unicode，因此你可能发现了，对任何合理的编码方案来说，在文件名中使用字节序列都是无效的，无法解码成字符串。在不同操作系统中使用各种客户端的文件服务器，在遇到这个问题时尤其容易出错。

为了规避这个问题，os 模块中的所有函数、文件名或路径名参数既能使用字符串，也能使用字节序列。如果这样的函数使用字符串参数调用，该参数会使用 sys.getfilesystemencoding () 得到的编解码器自动编码，然后操作系统会使用相同的编解码器解码。这几乎就是我们想要的行为，与 Unicode 三明治最佳实践一致。

但是，如果必须处理（也可能是修正）那些无法使用上述方式自动处理的文件名，可以把字节序列参数传给 os 模块中的函数，得到字节序列返回值。这一特性允许我们处理任何文件名或路径名，不管里面有多少鬼符，如示例 4-23 所示。

示例 4-23　把字符串和字节序列参数传给 listdir 函数得到的结果

>>> os.listdir('.') # ➊ ['abc.txt', 'digits-of-π.txt'] >>> os.listdir(b'.') # ➋ [b'abc.txt', b'digits-of-\xcf\x80.txt']

➊ 第二个文件名是「digits-of-π.txt」（有一个希腊字母 π）。

➋ 参数是字节序列，listdir 函数返回的文件名也是字节序列：b'\xcf\x80' 是希腊字母 π 的 UTF-8 编码。

为了便于手动处理字符串或字节序列形式的文件名或路径名，os 模块提供了特殊的编码和解码函数。

fsencode(filename)

如果 filename 是 str 类型（此外还可能是 bytes 类型），使用 sys.getfilesystemencoding () 返回的编解码器把 filename 编码成字节序列；否则，返回未经修改的 filename 字节序列。

fsdecode(filename)

如果 filename 是 bytes 类型（此外还可能是 str 类型），使用 sys.getfilesystemencoding () 返回的编解码器把 filename 解码成字符串；否则，返回未经修改的 filename 字符串。

在 Unix 衍生平台中，这些函数使用 surrogateescape 错误处理方式（参见下述附注栏）以避免遇到意外字节序列时卡住。Windows 使用的错误处理方式是 strict。

使用 surrogateescape 处理鬼符

Python 3.1 引入的 surrogateescape 编解码器错误处理方式是处理意外字节序列或未知编码的一种方式，它的说明参见「PEP 383 — Non-decodable Bytes in System Character Interfaces」。

这种错误处理方式会把每个无法解码的字节替换成 Unicode 中 U+DC00 到 U+DCFF 之间的码位（Unicode 标准把这些码位称为「Low Surrogate Area」），这些码位是保留的，没有分配字符，供应用程序内部使用。编码时，这些码位会转换成被替换的字节值，如示例 4-24 所示。

示例 4-24　使用 surrogateescape 错误处理方式

>>> os.listdir('.') ➊ ['abc.txt', 'digits-of-π.txt'] >>> os.listdir(b'.') ➋ [b'abc.txt', b'digits-of-\xcf\x80.txt'] >>> pi_name_bytes = os.listdir(b'.')[1] ➌ >>> pi_name_str = pi_name_bytes.decode('ascii', 'surrogateescape') ➍ >>> pi_name_str ➎ 'digits-of-\udccf\udc80.txt' >>> pi_name_str.encode('ascii', 'surrogateescape') ➏ b'digits-of-\xcf\x80.txt

➊ 列出目录里的文件，有个文件名中包含非 ASCII 字符。

➋ 假设我们不知道编码，获取文件名的字节序列形式。

➌ pi_names_bytes 是包含 π 的文件名。

➍ 使用 'ascii' 编解码器和'surrogateescape' 错误处理方式把它解码成字符串。

➎ 各个非 ASCII 字节替换成代替码位：'\xcf\x80' 变成了 '\udccf\udc80'。

➏ 编码成 ASCII 字节序列：各个代替码位还原成被替换的字节。

我们对字符串和字节序列的探讨到此结束。如果你坚持读到这里，恭喜你！

4.10　本章小结

本章首先澄清了人们对一个字符等于一个字节的误解。随着 Unicode 的广泛使用（80% 的网站已经使用 UTF-8），我们必须把文本字符串与它们在文件中的二进制序列表述区分开，而 Python 3 中这个区分是强制的。

对 bytes、bytearray 和 memoryview 等二进制序列数据类型做了简要概述之后，我们转到了编码和解码话题，通过示例展示了重要的编解码器；随后讨论了如何避免和处理臭名昭著的 UnicodeEncodeError 和 UnicodeDecodeError，以及由于 Python 源码文件编码错误导致的 SyntaxError。

讨论源码的编码问题时，我表明了自己对非 ASCII 标识符的观点：如果代码基的维护者想使用包含非 ASCII 字符的人类语言命名标识符，那就去做，除非还想在 Python 2 中运行代码。但是，如果项目想吸引世界各国的贡献者，那么标识符应该使用英语单词，此时 ASCII 就够用了。

然后，我们说明了在没有元数据的情况下检测编码的理论和实际情况：理论上，做不到这一点；但是实际上，Chardet 包能够正确处理一些流行的编码。随后介绍了字节序标记，这是 UTF-16 和 UTF-32 文件中常见的编码提示，某些 UTF-8 文件中也有。

随后的一节演示了如何打开文本文件，这是一项简单的任务，不过有个陷阱：打开文本文件时，encoding= 关键字参数不是必需的，但是应该指定。如果没有指定编码，那么程序会想方设法生成「纯文本」，如此一来，不一致的默认编码就会导致跨平台不兼容性。然后，我们说明了 Python 用作默认值的几个编码设置，以及如何检测它们：locale.getpreferredencoding ()、sys.getfilesystemencoding ()、sys.getdefaultencoding ()，以及标准 I/O 文件（如 sys.stdout.encoding）的编码。对 Windows 用户来说，现实不容乐观：这些设置在同一台设备中往往有不同的值，而且各个设置相互不兼容。而对 GNU/ Linux 和 OS X 用户来说，情况就好多了，几乎所有地方使用的默认值都是 UTF-8。

文本比较是个异常复杂的任务，因为 Unicode 为某些字符提供了不同的表示，所以匹配文本之前一定要先规范化。说明规范化和大小写折叠之后，我们提供了几个实用函数，你可以根据自己的需求改编。其中有个函数所做的是极端转换，比如去掉所有重音符号。随后，我们说明了如何使用标准库中的 locale 模块正确地排序 Unicode 文本（有一些注意事项）；此外，还可以使用外部的 PyUCA 包，从而无需依赖捉摸不定的区域配置。

最后简要介绍了 Unicode 数据库（包含每个字符的元数据），还简单讨论了双模式 API（例如 re 和 os 模块，这两个模块中的某些函数可以接受字符串或字节序列参数，返回不同但合适的结果）。

