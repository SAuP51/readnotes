## 附录1：一个经典的5层神经网络LeNet-5

图附1.1是由人工智能大神之一岩·拉孔（CNN的发明人）提出的典型的卷积网络LeNet-5。这个网络的左边是输入的待识别的图像，最右边有10个输出，因此可以识别10种不同的图像，例如手写体的0，1，2，…，9。

图附1.1岩·拉扎提出的卷积网络LeNet-5

图片来源：http://yann.lecun.com/exdb/lenet/。

输入图像的大小是32×32。网络的第一层是卷积层，所谓“卷积”就是拿着各种不同的小模板（图附1.1的模板大小是5×5）在一张大图上滑动找相似的图形（每个模板是一个特定的图形，例如三角、方块、直线、弧线等）。模板在大图中一个一个像素滑动，在每一个滑动位置，都把大图上的像素值和模板对应的像素值相乘再全部加起来，把这个加总之和作为一个新的输出像素。当模板滑动过所有像素时，这些新的像素就构成一张新的大图。在模板滑动过程中，每当碰上大图中有类似模板上的图形（即在大图上这个区域的图形和模板重合度高时），新的像素值就会很大。图附1.1的模板尺寸是5×5，因此在32×32的输入图像上横竖都只能滑动28次，所以卷积的结果是一张28×28的图片。在这个网络里，第一层使用了6个模板，所以第一层卷积后出来6张28×28的图片。

卷积的结果出来后，对每一个像素还要做一个“非线性”处理。一种“非线性”处理方法就是把所有小于零的像素值用零来代替，这样做的目的是让网络增加复杂性，可以识别更复杂的图形。

网络的第二层叫采样层（down-sampling）或汇集层（pooling），简单讲采样层就是把大图变小图。在这个网络里，采样的方法是用一个2×2的透明小窗在图片上滑动，每次挪动2个像素。每挪动一个位置，把小窗内的4个像素的最大值取出来作为下一层图片中的一个像素（除了取最大值，也可以有取平均值等其他方法）。经过这样的采样，就产生了下一层6张14×14的图片。

这个网络的第三层又是一个卷积层，这次有16个5×5大小的模板，在14×14的图片上产生16张10×10的图片（5×5的模板在14×14的图片的横竖方向上都只能滑动10次）。

第三层做完非线性处理后，第四层又是一个采样层，把16张10×10的图片变成16张5×5的图片。

第五层再次用120个5×5模板对16张5×5的图片做卷积。每个模板同时对所有16张图片一起卷积，每个模板产生一个像素，一共产生120个像素，这120个像素就成为下一层的输入。从这里开始，原始的一张32×32的输入图片变成了120个像素。每个像素都代表着某个特征。从这一层开始下面就不再做卷积和采样了，而是变为“全连接”的标准神经网络，即第五层的120个神经元和第六层的80个神经元中的每一个都连接。（不要问为什么第六层是80个而不是79个或81个，答案是没太多道理。）

第六层的80个神经元和第七层（最后一层，即输出层）的10个神经元的每一个都相连接。

以上卷积+非线性+采样可以看成卷积网络的一个单元，这个单元的作用就是“提取特征+压缩”，在一个卷积网络中可以不断地重复使用这个单元。今天大型的卷积网络可能有几百次这样的重复，在这个例子里，只重复了一次。

卷积网络为什么这样设计？为什么分为卷积和全连接两个不同的部分？简单说是省了计算。如果用全连接多层网络，那么输入层要有32×32=1 024个神经元，在LeNet中最后压缩到120个。压缩的原理就在于对识别一张图像来说，只有图像中的某些特征是相关的。例如识别人脸，主要是五官的特征有用，至于背景是蓝天白云还是绿树红花都无所谓。卷积的模板就是要提取出那些最相关的特征。注意，这里模板并不需要事先设计好，模板中的各像素的数值，就是神经网络各层的加权系数。根据训练数据的不同，通过前述的最陡下降法，模板各像素取值最终也不同（即训练数据都是人脸和训练数据都是动物最终演化出来的模板不同）。每一层使用多少个模板要在识别率和计算量之间找一个平衡，模板越多识别越准确，但计算量也越大。

## 附录2：循环神经网络RNN和长-短时记忆网络LSTM

循环神经网络RNN

CNN比较适合处理图像，因为每幅图像之间都没有太多相关性，所以可以一幅一幅地输入CNN里。但许多非图像信息例如语音、天气预报、股市等是一个连续的数值序列，不仅无法截成一段一段地输入，前后之间还有很强的相关性。例如我们在下面句子里猜词填空：“我是上海人，会讲————话。”在这里如果我们没有看到第一句“我是上海人”就很难填空。处理这一类问题最有效的神经元网络叫循环神经网络。图附2.1是一个最简单的RNN。

图附2.1最简单的RNN

这里Xt 是t时刻的输入向量，A是神经元网络，ht 是t时刻的输出。注意到方框A有一个自我反馈的箭头，网络A下一时刻的状态依赖于上一时刻的状态，这是RNN和CNN及全连接网络的最大的区别。为了更清楚地理解图附2.1，我们可以把每个离散时间网络的状态都画出来生成图附2.2。

图附2.2把RNN从时间上展开看

这个链状结构展现出RNN与列表、数据流等序列化数据的密切关系，而这类数据的处理也确实在使用RNN这样的神经网络。在过去几年中，将RNN应用于很多问题后，已经取得了难以想象的成功，例如语音识别、语言建模、翻译、图像抓取等，而它所涉及的领域还在不断地增加。

现在我们已经知道了RNN是什么，以及基本的工作原理。下面我们通过一个有趣的例子来加深对RNN的理解：训练一个基于字符的RNN语言模型。我们的做法是，给RNN“喂”大段的文字，要求它基于前一段字符建立下一个字母的概率分布，这样我们就可以通过前一个字母预测下一个字母，这样可以一个字母一个字母地生成新的文字。

举一个简单的例子，假设我们的字母库里只有4个字母可选：“h”“e”“l”“o”，我们想训练出一个能产生“hello”顺序的RNN，这个训练过程事实上是4个独立训练的组合。

（1）给出字母“h”后，后面大概率是字母“e”。

（2）给出字符“he”后，后面大概率是字母“l”。

（3）给出字符“hel”后，后面大概率还是字母“l”。

（4）给出字符“hell”后，后面大概率是字母“o”。

具体来说，我们可以将每个字母编码成一个向量，这个向量除了该字母顺序位是1，其余位置都是0（例如对于“h”而言，第一位是1，其余位置都是0，而“e”则第二位是1，其余位置是0）。之后我们把这些向量一次一个地“喂”给RNN，这样可以得到一个四维输出向量序列（每个维度对应一个字母），这个向量序列我们可以认为是RNN目前认为每个字母即将在下一个出现的概率。

图附2.3是一个输入和输出层为4维，隐含层为3个单元（神经元）的RNN。图中显示了当RNN被“喂”了字符“hell”作为输入后，前向传递是如何被激活的。输出层包括RNN为下一个可能出现的字母（字母表里只有h、e、l、o 4个字母）所分配的概率，我们希望输出层加粗的数越大越好，输出层其他的数越小越好。例如，我们可以看到在第一步时，RNN看到字母“h”，它认为下一个字母是“h”的概率是1.0，是“e”的概率是2.2，是“l”的概率是-3.0，是“o”的概率是4.1。由于在我们的训练数据（字符串“hello”）中，正确的下一个字母是“e”，所以我们希望提升“e”的概率，同时降低其他字母的概率。同样地，对于4步中的每一步，我们都希望网络能提升对某一个目标字母的概率。由于RNN完全由可分的操作组成，因此我们可以采用反向传播算法来计算出我们应该向哪个方向调整每一个权量，以便提升正确目标的概率（输出层里的加粗数字）。

图附2.3 RNN学习“hello”语句的训练过程

接下来我们可以进行参数调整，即将每一个权值向着刚才说的梯度方向微微调整一点，调整之后如果还将刚才那个输入字符“喂”给RNN，我们就会看到正确字母（例如第一步中的“e”）的分值提高了一点（例如从2.2提高到2.3），而其他字母的分值降低了一点。然后我们不断地重复这个步骤，直到整个网络的预测结果最终与训练数据一致，即每一步都能正确预测下一个字母。需要注意的是，第一次字母“l”作为输入时，目标输出是“l”，但第二次目标输出就变成了“o”，因此RNN不是仅仅根据输入判断，而是使用反复出现的关系来了解语言环境，以便完成任务。


LSTM：记忆增强版RNN


RNN可以根据前一个字母预测出下一个字母。但有时候可能需要更前面的信息，例如我们要填空“我在上海出生，一直待到高中毕业，所以我可以讲————话”。此时如果没有看到第一句“我在上海出生”就猜不到填空的内容。为了解决这个问题，人们对RNN进行了改造，使其可以有更长的记忆。这就是长-短时记忆网络（LSTM）。

所有的RNN都是链状的、模块不断重复的神经网络，标准的RNN中，重复的模块结构简单。如图附2.4所示，每个模块由单个的tanh层组成。

图附2.4在标准的RNN中，重复的模块是单层结构



LSTM也是链状的，和RNN相比，其重复模块中的神经网络并非单层，而是有4层，并且这4层以特殊的形式相互作用。（见图附2.5）

图附2.5在LSTM中，重复模块中含有相互作用的4层神经网络



在图附2.5中，σ（希腊字母sigma）是sigmoid函数，其定义如下：



这个sigmoid函数的图形如图附2.6所示。

图附2.6 sigmoid函数的图形



tanh函数是sigmoid函数的放大平移版，它们之间的关系如下：

tanh（x）=2σ（2·x）-1

图附2.7 sigmoid函数和tanh函数曲线



不要被上面这么多复杂的东西吓着，我们等一下会详述LSTM的结构。从现在开始，请试图熟悉和适应我们将使用的一些符号。

在图附2.8中左起，长方形表示已经学习的神经网络层；圆圈代表逐点运算，就像向量加法；单箭头代表一个完整的向量，从一个节点的输出指向另一个节点的输入；合并的箭头表示关联；分叉的箭头表示同样的内容被复制并发送到不同的地方。

图附2.8 LSTM中使用的符号





LSTM背后的核心


LSTM的关键是单元状态，即图附2.9中上方那条贯穿单元的线。单元状态有点像传送带，它在整个链条一直传送下去，只有少数节点对它有影响，信息可以很方便地传送过去，保持不变。

图附2.9 LSTM的单元状态



LSTM对单元状态有增加或移除信息的能力，但需要在一个叫作“门”的结构下按照一定的规范进行。“门”是有条件地让信息通过的路径，它由一个sigmoid神经网络层和一个逐点乘法运算构成。

图附2.10 LSTM的单元“门”结构



sigmoid层输出0 ~1的数值，表示每个要素可以通过的程度，0表示“什么都过不去”，1表示“全部通过”。一个LSTM有3个这样的“门”，用来保护和控制单元状态。





附录3：CPU、GPU和TPU


GPU是神经网络计算的引擎。图附3.1是一个典型的神经网络，用输出误差来调节各层的权重系数，输入阵列X通过参数矩阵运算进入下一层运算，每一层运算都是一次这样的矩阵运算。所谓矩阵运算，就是先把数字相乘再相加。

图附3.1典型的深度神经网络



为了帮助那些没有学过矩阵的读者加深理解，现在我们只看图附3.2中左边输入和第一层中一个神经元的关系：每一个输入数字和圆圈里的权重系数相乘，然后把所有的乘积加起来就得到一个值。例如，输入有三个单元，分别是（2，5，8），对应的权重系数分别是（2，-1，0.5），它们相乘后再相加是2×2+5×（-1）+8×0.5=4-5+4=3。这个值再经过一个“非线性”处理：这个值如果大于0就取原值，小于0就取值为0。注意这里所有的相乘运算都可以同时进行，这就是所谓的可“并行运算”。我们刚才描述的仅仅是一个单元的计算方法，其他单元的计算方法也都一样，也就是说不仅一个单元的计算是可以并行的，所有单元的计算都是可以同时进行的。

图附3.2神经网络的一个单元内的计算



而GPU与CPU相比的优点正是在这里。当年设计CPU时主要为了执行计算机程序，绝大部分计算机程序都是“串行”的，也就是后一个命令或计算要等前一个命令或计算的结果出来后才能执行。而GPU最初是为图形处理用的，图形处理的一个特点就是可以并行。例如，从一张图中把所有的黑点找出来，就可以把一张图分割成许多小图同时来找黑点。图附3.3是CPU和GPU的结构对比，图中左边的CPU通常由一个控制器（Control）来给少数几个功能强大的算术逻辑运算器（ALU）分配任务，而右边的GPU通常由许多简单的控制器（右图最左边一列方块）来控制更多的算术运算单元组成（那些小格子）。在图形和图像处理中大量的运算都是矩阵运算，所以GPU从第一天起就是为矩阵运算设计的，没想到几十年后的深度学习也主要是矩阵运算，这就是“天上的馅饼”。

图附3.3CPU结构和GPU结构对比

图片来源：https://www.quora.com/Does-CPU-vendors-feel-the-competition-from-GPUs-computational-power。



那么在神经网络深度学习计算中GPU比CPU能好多少呢？一个极端的例子是在深度学习使用GPU之前，谷歌使用16 000个CPU建造了一个超级深度学习网络，如图附3.4所示，成本为数百万美元。

几年后，斯坦福大学只用几个GPU就可以达到同样的性能，成本只有3万美元！即使考虑到芯片本身在几年内的发展，这个比较也是惊人的。当然这个比较仅仅是比较深度学习的矩阵运算，谷歌大脑还可以做很多其他的运算，例如强化学习等。总的来说，CPU适合串行运算，可以胜任从航天到手机的各种不同复杂运算和处理，而GPU主要用于简单的并行运算，并不会取代CPU。但是在图形处理和深度神经网络计算中，GPU可以比CPU快10倍甚至百倍。英伟达2017年推出的用于自动驾驶的芯片Xavier已经达到每秒20万亿次浮点计算。

图附3.4谷歌用16 000个CPU搭建的深度学习“谷歌大脑”

图片来源：https://amp.businessinsider.com/images/507ebdd2ecad045603000001- 480-360.jpg。



2006—2017年，单片CPU的处理能力提高了50倍。50倍的增长不是来自时钟速度的提高（即单次运算变快），而是来自在芯片中塞进更多的处理器。它的内核数量从4个变到28个，是原来的7倍。另外一个性能的提升来源于指令的宽度，2006年一条指令只能处理2个单精度的浮点运算，今天512位的指令集中，一条指令可以同时处理16个单精度的浮点运算，这就相当于8倍的性能提升。7×8=56，约50倍的运算速度提升就是靠更多的处理器得来的。

图附3.5 2006—2017年CPU运算速度的进展

图片来源：香港浸会大学褚晓文教授“深度学习框架大PK”中的五大深度学习框架三类神经网络全面测评。



再看GPU在近十年的发展，图附3.6是GPU和CPU的性能对比。2006年英伟达第一次发布通用计算的GPU8800GTX，当时它的性能已经达到0.5万亿次浮点运算（500 GFlops），接下来的十年，大家可以看到GPU相对CPU的计算能力一直维持在10~15倍的比例。GPU从过去的128个核心变成5 376个核心，这个增长速度与CPU相同，所以GPU运算速度与CPU运算速度的比值一直保持不变。

图附3.6 2006—2017年CPU和GPU计算能力对比

图片来源：香港浸会大学褚晓文教授“深度学习框架大PK”中的五大深度学习框架三类神经网络全面测评。



其他几家互联网巨头也不能眼睁睁地看着英伟达控制着深度学习的命脉。谷歌就撸起袖子做了一款自用的TPU，设计思路是这样的：既然GPU通过牺牲通用性换取了在图形处理方面比CPU快15倍的性能，为什么不能进一步专注于只把神经网络需要的矩阵运算做好，进一步提高速度呢？TPU设计的核心诀窍有以下四点。

第一，图形与图像处理需要很高的精度（通常用32比特浮点精度），而用于识别的神经网络的参数并不需要很高的精度。所以谷歌的第一款TPU就专门为识别设计，在运算上放弃32比特的浮点运算精度，全部采用8比特的整数精度。

第二，由于8比特的乘法器比32比特简单4×4=16倍，所以在同等芯片面积上可以多放许多运算单元。谷歌的第一款TPU就有65 000个乘加运算单元，相比最快的GPU只有5 300个单元，多了不止10倍。

第三，不论在多核的CPU还是GPU中，目前的运算速度瓶颈都是内存的读和写。因为要被运算的数据都存在中央存储器里，而这些数字在运算时要分配到几百上千个运算单元中，从存储器到运算单元可谓“千里迢迢”，往返很费时间。在TPU里为了解决这个问题，干脆把运算单元摆成矩阵一样，让被运算的数据（例如神经网络的输入数据）流淌过这些运算单元，从内存中提取一个数据就让它和所有的权重系数都做完乘法，而不是像传统CPU或GPU那样提取一个数据只做一次运算就放回到存储器，做下一次运算再千里迢迢从存储器去取。这样数据像波浪一样一波一波涌来，所以叫脉动式计算。

第四，一个专注于矩阵运算的芯片不用考虑图形处理时要考虑的许多其他事情，例如多线程、分叉预测、跳序执行等，这是由于脉动式计算省掉了许多暂存、缓存等。整个运算的指令只有矩阵运算和取非线性那么几个，例如读数据、读参数、相乘、累加、非线性、写数据等。整个芯片和软件都变得非常简单，这样可以做到每个时钟周期执行一次指令。

图附3.7向量（一维）计算、矩阵（二维）计算和张量（三维）计算

图片来源：https://mp.weixin.qq.com/s/e333KjLavEvvpNIL3u1Y4Q。



现在我们可以算一下TPU到底比GPU快多少了。谷歌第一代的TPU有256×256=65 536个8比特乘加器，时钟是700MHz，每秒能做的8比特乘加运算=65 536×700×106 =46×1012 次乘加运算，即92万亿次整数运算（92 TOPS，一次乘加运算是两次运算）。所以当谷歌宣称比GPU快时，是用整数运算次数OPS和浮点运算次数FLOPS比。而GPU是以浮点运算为测量单位的，前面说过最新的英伟达Xavier运算速度可达20 TFLOPS，这两个不可直接相比，但如果在神经网络用于识别时（而不是用于训练），浮点和整数运算造成的识别准确率差别不大，就可以说这款TPU比GPU快了92/20=4.6倍。对于谷歌这样需要大量矩阵运算的公司可以节省许多买GPU的钱，并且加快了识别速度（例如谷歌翻译、图片识别等的几亿用户非常在意处理速度），更重要的是把核心能力控制在了自己手里。谷歌在云服务方面没有亚马逊做得好，正在奋起直追，有了TPU则可以给用户提供更快、更便宜的深度学习云服务，所以谷歌的TPU目前只给自己用，不卖给别人。谷歌的第二代TPU已经能够进行32比特的单精度浮点运算，这样从训练到识别都不需要买别人的GPU了。用浮点运算做识别还有一个好处就是通过浮点运算训练出来的模型可以直接用于识别，而不是像第一代TPU那样先要把那些32比特的参数集都量化为8比特。但是通过刚才的讨论，我们知道TPU更快的一个重要原因是放弃浮点运算，当TPU也需要浮点运算时，相比GPU的性能提高就不会那么大了。谷歌的第二代（TPU2.0）可以达到每秒45万亿次单精度浮点运算，和英伟达Xavier芯片比只快了一倍（TPU2.0在Xavier之后出来，快一倍就不算什么）。在2018年谷歌开发者大会上，谷歌又宣布了第三代（TPU3.0），宣称比TPU2.0快8倍。由于功耗的提高，所以第三代芯片已经需要液体制冷。一个第三代的TPU集群（一个机柜）有64块板卡，每块板卡上有4个TPU，总运算速度可以达到每秒8×45×4×64=92 160万亿次浮点计算。





附录4：机器学习的主要编程框架




TensorFlow是由谷歌大脑团队开发的，主要用于机器学习和深度神经网络的研究。2016年5月，谷歌从Torch（一种编程框架）转移到TensorFlow，这对其他编程框架造成了打击，特别是torch和theano。许多人将TensorFlow描述成一个比theano更现代化的版本，吸取了这些年在新领域 / 技术的许多重要的经验教训。

TensorFlow以智能、灵活的方式而闻名，是一种高度可扩展的机器学习系统，使其更容易适应不同的新旧产品和研究，并且比较容易安装，还针对初学者提供了教程，涵盖神经网络的理论基础和实际应用。TensorFlow比theano和torch慢，但谷歌和开源社区正在解决这个问题。TensorBoard是TensorFlow的可视化模块，它提供了一个计算路径的直观视图。深度学习库Keras被移植到TensorFlow上运行，这意味着任何用Keras编写的模型现在都可以运行在TensorFlow上。最后，值得一提的是TensorFlow可以在各种硬件上运行。其特点如下。

（1）GPU加速：支持。

（2）语言 / 界面：Python、Numpy、C++。

（3）平台：跨平台。

（4）维护者：谷歌。



theano起源于2007年在蒙特利尔大学的知名MILA（学习算法研究所），是用Python编写的CPU/GPU符号表达式的深度学习编译器。theano功能强大，速度极快，并且灵活，但通常被认为是一个底层框架。因此，原生theano更像是一个研究平台和生态系统，而非深度学习库，它经常被用作高级程序库的底层平台，而这些高级库给用户提供简单的API。theano提供一些比较受欢迎的库包括Keras、Lasagne和Blocks。theano的缺点之一是仍然需要一个支持多GPU的方案。theano的特点如下。

（1）GPU加速：支持。

（2）语言 / 界面：Python，Numpy。

（3）平台：Linux、Mac OS X和Windows。

（4）维护者：蒙特利尔大学MILA实验室。



在所有常见的框架中，torch可能是最容易启动和运行的，特别是在使用Ubuntu（一种开源电脑操作系统）的情况下。它允许基于神经网络的算法在GPU硬件上运行，而不需要在硬件级别进行编码。torch在2002年由纽约大学开发，被Facebook和Twitter等大型科技公司广泛使用，并得到英伟达的支持。Torch是用一种叫作Lua的脚本语言编写的，这种语言很容易阅读，但并不像Python那样通用。有用的错误提示消息、大量的示例代码 / 教程以及Lua的简单性让torch很容易上手。其特点如下。

（1）GPU加速：支持。

（2）语言 / 界面：Lua。

（3）平台：Linux、Android、Mac OS X、iOS和Windows。

（4）维护者：Ronan、Clément、Koray和Soumith。

Caffe被开发用于利用卷积神经网络的图像分类 / 机器视觉，由1 000多名开发人员推动其发展。Caffe最出名的可能是ModelZoo模型，开发者无须编写任何代码就可以直接使用。

Caffe主要针对工业应用，而torch和theano是为研究量身打造的。Caffe不适用于非计算机视觉深度学习应用，如文本、声音或时间序列数据。Caffe可以在各种硬件上运行，并且CPU和GPU之间的切换可以通过设置单个标志来完成。Caffe的运行速度比theano和torch要慢。其特点如下。

（1）GPU加速：支持。

（2）语言 / 界面：C、C++、Python、MATLAB、CLI。

（3）平台：Ubuntu、Mac OS X、Windows实验版。

（4）维护者：伯克利视觉和学习中心（BVLC）。

CNTK是微软深度学习工具包，是微软的开源深度学习框架。CNTK在语音社区中比在一般深度学习社区中更为著名，可以用于图像和文本训练。CNTK支持多种算法，例如Feed Forward、CNN、RNN、LSTM和Sequence-to-Sequence。它可以运行在许多不同的硬件类型上，包括多个GPU。其特点如下。

（1）GPU加速：支持。

（3）语言 / 界面：Python、C++、C＃和CLI。

（4）平台：Windows、Linux。

（5）维护者：微软研究院。

H2O也称为H2O.ai，是世界上使用最广泛的开源深度学习平台之一。它被全球超过8万名数据科学家和研究人员以及超过9000家企业和组织所用，包括为全球最有影响力的一些公司开发关键任务数据产品。H2O提供基于Web的用户界面，同时可以访问机器学习软件库，并开启机器学习的过程。

维基百科中有一张表详细列出了各主要编程框架的参数和特点，链接如下：

https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software。




