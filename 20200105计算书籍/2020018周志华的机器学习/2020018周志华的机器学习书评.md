## 01. 这本书应该没你们说的那么好

用户 5616961672

2016-06-22 16:41:33

看到这本书豆瓣上评到 9.1，各种赞誉，很是惊奇，感叹国内作者能写出这样的书来真不容易。当然有好书我也想看，昨晚下单，今天到货，很不好意思，要泼点冷水，这本书没有你们说的你们好。

简单说：这本书作为自学教材，是绝对不合适的。这本书其实是本：<机器学习导论>。

以下是读这本书的感觉，都是个人观点。另外，周老师是机器学习的大牛，水平肯定不是盖的。以下评论对书不对人。

我学习人工神经网络有段时间了，即使这样第五章神经网络部分读起来都十分晦涩，很难想象对于想拿着本书入门机器学习的人来说，到底能有多少收获。至少神经网络部分的学习体验，这本书远不如《神经网络在应用科学和工程中的应用：从基本原理到复杂的模式识别》，以下简称《神经网络》。

简单说几个问题：

1、术语不规范。

BP 网络权值更新，按训练样本多少，可以逐次更新或者样本集训练完整体更新一次。这本书的叫法是「累积 BP 算法」和「标准 BP 算法」，这个叫法我觉得晦涩且不确切；《神经网络》的叫法是：批量学习（离线学习）与在线学习；还有权值和阈值的叫法，其实阈值这个叫法也不确切，当然更不是阀值，相对的，《神经网络》这本书在前半部分就着重提出，这是 b 值，其实是」偏置「，从出从感知器和线性网络开始，b 值就一直充当分类直线的偏置的作用。。。不一一描述。

2、实在是实在是，太简略。

正文 400 页，十几个算法，每页只印刷大半页。作者大概也没想正八景给你讲算法把。。。

正则化一部分，为了获得更好的 泛化能力，其实 方法有很多，提早停止、正则化、人为增加训练集大小等，当然本书只提到正则化，并且基本上只给了一个概念和一个公式。至于什么权重值的调整」常通过交叉验证法来估计。「这个交叉验证法，作者是有必要给出三五句描述的。。。

至于常用的径向基网络以及其他网络，基本只给了个概念。。。

3、配图及符号表示有些乱

是有些乱，不是特别乱。当然主要是我受《神经网络》那本书的影响，那本书的配图，及对各层输入输出权值偏置的表示非常清晰，阅读体验非常棒。（ps，那本书也有问题，这里并不是吹捧那本书。。。）这本书呢，各层输入输出权值偏置的表示的符号，不清晰，没规则，阅读体验差一些。想对着这本书的介绍自己写个程序实现神经网络的话，要先花点时间把错乱的符号理顺清楚才好。。。

4、免不了的学院派

既然这本书不适合做自学教材，适合做科普。。。按理说应该亲民一些，少些专业符号，公式可读性方面提高一些，至少能够让人清楚的理顺算法逻辑做到心中有数。但是这本书还是十分学院派，读起来很硬。国内作者写书的通病吧。对比的，还是《神经网络》那本书。。。

这本书印刷精良，字体大小合适，排版上乘，纸张优质，是本值得收藏的书。。。也确实是本很」流行「的书。正如作者一直在讲西瓜的例子，并且把西瓜作为了封面图片，似乎也预示了，这本书可能不是一本十分严肃的教材。。。如果作为培训，有老师在课上一边详细的给你讲算法，你做笔记，配合本书，或许能有收获。。。否则看这本书，只能是作为课外阅读。

怎么说呢，这可能是一本不错的读物。。。

我只能给 7-8 分。

      

## 02. 教材书籍

SCUT - 苏东

2016-02-06 08:03:23

周老师这本书用来当教材确实不错，不过自学的话跟李航老师的《统计学习方法》来比，确实不够详细，但周老师的书广度上要更加广泛。不过某些内容上，如线性判别函数，《模式分类》以及《现代模式识别》(第二版) 都用了一章去讲的，但周老师的书又略显简略。个人建议，周老师的书可以用来当做一本教材，自学的话，可以根据周老师的书，辅以《模式分类》(前面的三章可以通读)，《现代模式识别》(重公式推导)，《统计学习方法》来进行加强学习 2016.3.19 更新：认真看了前面 9 章，不得不说，周老师写书确实有一套，基本上对算法原理，优点缺点，适用条件讲得非常清楚，详略得当。当然，作为教材不可能面面俱到，上面提到过的书籍都可以用作参考。至于视频，理论性很强的台大林轩田的机器学习两门课可以配合着周老师的课本一起学习。至于英文参考，我觉得 cs229 的课件就非常适合理论入门，至于 esl，prml，mlapp 可以用来当做进阶参考 2016.7.24 更新再加一本实战书籍《Python machine learning》用 scikit-learn 等包进行一些操作至于 R 语言的，可以看看《R IN ACTION》第二版以及《applied predicted model》(都有中文版)

2017.10.10 更新

视频的话，台大李宏毅机器学习跟深度学习的两套视频都很深入浅出，值得一看

      

目前看到过的，算是最好的「中文」机器学习「教科书」了。

蜡笔小轩

2016-02-15 14:21:47

注意，这是一本「教科书」，所以其中有一些「不足」也就好理解了。

书本身没有很厚，看起来不至于压力很大。毕竟是「教科书」，每一章只有短短的几十页，结合课程用来教学是不错，但自学只看这本书估计有点不太够，可以结合 coursera 上 Andrew Ng 和林轩田的课来使用，要是这本书早点出就好了，相见恨晚。以前一直想找一本这样脉络比较好的机器学习教材，一直没找到好的。

除了前面提到的经典的公开课，顺带再介绍一些自学材料吧。EM 算法「西瓜书」里只有两页，我觉得还是 cs229 的 note 讲得比较清楚。神经网络可以参考 stanford 的 UFLDL 课程，做做里面的课程实验，加深理解。类别不均衡问题，最近刚跟大牛学了 EasyEnsemble 这个 trick，书里一句话带过，不注意可能就略过去了，这个 trick 还是挺简单好用的，值得了解一下。LDA 可以看看 "LDA 数学八卦"。

跟其他书比起来，可以说各有侧重，要结合自身情况阅读。经典的 PRML 打印了一本，因为现在已经上班了，远水解不了近渴，看了一些，然后暂时放边上吃灰了。我这种「实践派」，还不如看 sklearn 的文档来得实在，毕竟要搬砖赚钱吃饭。真羡慕那些已经「深入学习 PRML」的人呐！

把道理本身讲清楚是一方面，把道理讲得让别人容易懂是另一方面。所以就需要「西瓜书」绪论那样的引子和西瓜这种通俗易懂的例子。有人说李航的《统计学习方法》适合入门，我自己刚刚入门的时候倒觉得不好懂，不过后面开始摸着门道之后倒觉得不错，把原理和式子列一列，然后再举个例子，就差不多了。不需要太多「废话」。

反正这些书都是好书，但适合不同的段位和需求的人。有时间的话，多看看，从不同角度加深理解也是极好。

看完《机器学习》的总结与心得

kalashnikov

2016-07-31 23:33:19

这段时间利用下班晚上和周末在家的时间把《机器学习》看了一遍，总的来说，除了前两章是基础，其余章节都是介绍模型方法，应用场景、理论推导、优化过程、算法等，每章都可独立阅读而不受其他章节影响。

如果只是每一章都看完，顶多就是了解了一种方法，相互之间是割裂的，这样当碰到一个实际问题，仍然无法思路完整的求解，因而有必要在阅读时就要有意识甚至刻意的建立起知识架构。实际上，所谓的机器学习，是面对一个具体的问题，从给定的数据中产生模型的算法，也就是说脱离了实际问题谈机器学习算法是毫无意义的。

参考本书的章节内容，加上自己在读书、工作中的一些理解，简单总结一下基于机器学习的一般性问题解决方法。

前面提到，脱离实际问题谈机器学习是毫无意义的，那么在给定数据集（所谓大数据）和具体问题的前提下，一般解决问题的步骤可以概括如下：

1、数据抽象

将数据集和具体问题抽象成数学语言，以恰当的数学符号表示。这样做自然是为了方便表述和求解问题，而且也更加直观。

2、设定性能度量指标

机器学习是产生模型的算法，一般来说模型都有误差。如果模型学的太好，把训练样本自身的一些特点当成所有潜在样本具有的一般性质，这种情况称为过拟合，这样的模型在面对新样本时就会出现较大误差，专业表述就是导致模型的泛化性能下降。

与之相对的是欠拟合，模型对样本的一般性质都没学好，这种情况一般比较好解决，扩充数据集或者调整模型皆可。

而一般来说无论是机器学习还是现在很火的深度学习，面对的主要问题都是过拟合。那么为了保证模型的泛化能力足够强，必须要有衡量模型泛化能力的评价标准，也就是性能度量的设定。

很显然不同的性能度量会导致不同的评判结果，好的性能度量能够直观的显示模型的好坏，同时也能看到不同模型，或者模型的不同参数下对解决问题的程度好坏。

进一步，有的问题可以直接基于设定的性能度量直接做最优化，得出该问题的一般求解模型。

比如回归任务最常用的性能度量就是均方误差，目标就是让均方误差最小，这就直接转化成了一个最优化问题。

其他一些常用的有错误率与精度、查准查全率、ROC 与 AOC 等。

当然更为重要的是，仅仅设定好性能度量是不够的，不同模型或者不同参数下得到的性能度量结果一般是不同的，一般来说不能简单的比较结果，而应该基于统计假设检验来做效果判定。也就是说通过比较检验的方法，我们就可以判断，如果观察到 A 比 B 好，在统计意义上 A 的泛化性能是否优于 B，以及这个判断的把握有多大。

3、数据预处理

之所以要做数据预处理，是因为提供的数据集往往很少是可以直接拿来用的。

可能的情况有：

- 样本某些属性值缺失

- 有未标记样本

- 样本的属性太多

- 样本量不足

- 没有分出测试集和验证集

- 不同类的样本数相差比较大

- 不同类的样本数相差比较大

这就是所谓类别不平衡问题。举个例子，样本里有 998 个反例，2 个正例，如果一个模型对任何的新样本都预测为反例，那么它的精度为 99.8%，虽然很高，但没有任何价值。

这种情况使用的基本策略是再缩放，具体方法则是采样。通过不同的采样方法来使类别达到平衡。

没有分出测试集和验证集

再说第五种情况，为了方便训练和验证模型好坏，数据集一般会以 9：1 或者其他合适比例（比例选择主要基于实际问题）分为测试集和验证集。如果给定的数据集只是已经标记好的样本，那么划分时必须保证数据集和测试集的分布大致均匀，这就涉及到具体的划分算法了。

样本量不足

第四种情况一般图像问题比较常遇到，如果样本量不足，不够模型来学习，效果自然很差。常见的方法一般有两种：

基于原数据集做扩充。比如对于图片，可以做各种处理，比如旋转、crop、对比度、亮度等基于像素值的调整，使得一个样本能够生成多个样本，从而达到扩充数据集的目的。

通过人工标注生成样本。比如大名鼎鼎的 ImageNet 数据集就是通过全球众包完成的，当然这个不仅耗时长，人工成本也很高，需谨慎考虑。

样本的属性太多

对于第三种情况，如果样本属性太多，为了保证模型的泛化性能足够强，则理论上必须保证数据集包括有所有属性的所有值，而这随着属性数目以及属性值是呈指数上升，很有可能达到天文数字，不仅难以计算，在现实情况也不可能收集到这么多的样本。

从数学角度看，每个样本的属性可以看成向量，属性数目是向量的维数，解决第三种情况一般有两种方法：

- 降维

- 特征选择

特征选择比较好理解，就是选择有用相关的属性，或者用另外一种表达方式：选择样本中有用、跟问题相关的特征。事实上这也很正常，并不一定样本的所有属性对具体问题都是有用的，通过一定的方法选择合适的特征可以保证模型更优。常用的方法大致分三类：过滤式、包裹式和嵌入式。

所谓的降维，即是多属性意味着是高维空间，在很多时候可以等价的映射到低维而不丢失主要信息。从空间映射的角度看，我们可以通过主成分分析 PCA（线性映射）和核化主成分分析（非线性映射）来达到降维的目的。（补充：PCA 是无监督降维方法，线性判别分析 LDA 则是监督降维防范）

有未标记样本

现实情况下往往很多数据集都有大量的未标记样本，有标记的样本反而比较少。如果直接弃用，很大程度上会导致模型精度低。这种情况解决的思路往往是结合有标记的样本，通过估计的方法把未标记样本变为伪的有标记样本。基本的方法有主动学习和半监督学习两种方法。

样本某些属性值缺失

样本的属性值缺失是很常见的一种情况。比如做书籍、视频、音乐等的个性化推荐时，需要用户对不同种类的偏好或评价。而用户不一定听过所有种类的歌，也不一定做出了评价。这就需要通过他已产生的样本数据和与之相类似的用户的数据来恢复和补全。

从原理上讲，这和压缩感知根据部分信息恢复全部信息是有类似的。

常用的方法涉及到协同过滤、矩阵补全等技术和方法。

总的来说，数据预处理是一个非常重要的过程，实际上数据预处理往往会和模型选择结合在一起。

4、选定模型

在数据集完美的情况下，接下来就是根据具体问题选定恰当的模型了。

一种方式是根据有没有标记样本考虑。

如果是有标记样本，可以考虑有监督学习，反之则是无监督学习，兼而有之就看半监督学习是否派的上用场。

无监督学习方法主要提到的是聚类。随机选定几个样本，通过一定的算法不停迭代直至收敛或者达到停止条件，然后便将所有样本分成了几类。

对有监督学习而言，根据最终所需要的输出结果

如果涉及到分类，可以参考的模型有线性回归及其非线性扩展、决策树、神经网络、支持向量机 SVM、规则学习等

如果是回归问题，可以认为是分类的连续形式，方法便是以上模型的变种或扩展

如果涉及到概率，可以参考的有神经网络、贝叶斯、最大似然、EM、概率图、隐马尔科夫模型、强化学习等

5、训练及优化

选定了模型，如何训练和优化也是一个重要问题。

如果要评估训练集和验证集的划分效果，常用的有留出法、交叉验证法、自助法、模型调参等

如果模型计算时间太长，可以考虑剪枝

如果是过拟合，则可通过引入正则化项来抑制（补偿原理）

如果单个模型效果不佳，可以集成多个学习器通过一定策略结合，取长补短（集成学习）

6、应用

模型训练好之后，自然就是应用，针对未知样本做预测、推荐等。

最后再简单谈一下神经网络。本书中神经网络仅仅是作为一个章节叙述，包括经典的 BP 算法。

但技术书籍的推出总是落后于当前技术发展的。《机器学习》上市的时间，已经是深度学习大行其道了。

多层的神经网络伴随的往往是大量参数的计算和对大数据的渴求。随着互联网的发展，人们产生的数据呈爆炸式增长，这使得大数据集越来越多，同时以 NVIDIA 为代表推出的基于 GPU 的机器的计算能力大幅提高，这使得应用多层神经网络解决实际问题成为可能。尤其是在 ImageNet 竞赛中基于卷积神经网络的模型以大幅优势夺冠后，无论是学术界还是工业界，都投入大量人力财力研究和应用。以前认为人工智能不可能击败围棋的论点也在今年 AlphaGO 战胜李世石不攻自破，AlphaGO 赖以成功的基础就是几千万名家的对战棋谱（数据集）和多层神经网络（模型），当然还包括了强化学习和评估网络等辅助下棋策略。

一个非常有趣的事实是，神经网络在经历了几次寒冬后，科学家们为了保证科研成果能够更大概率的发表，启用了新名词深度学习。（以 neural network 为关键词的 paper 拒稿率很高）所以从本质上讲，深度学习可以简单的认为是很深层的神经网络。

机器学习理论大集合

偶得茶馆

2016-02-23 11:24:37

冲着周教授的学术名气买的这本书，拿到书以后，稍微感觉有点失望。

之前，曾经入手了 李航博士的《统计学习方法》，而且我是认认真真看了不止一遍的。

对比周教授的这本《机器学习》，在内容的广度上，《机器学习》介绍的内容范围相对广一些，但是就单个领域的论述和可接受方面，《机器学习》就不如《统计学习方法》更加让人容易接受了。比如，对比决策树一章的内容，周教授只是介绍了整个决策树的算法流程，举了一个例子把整个流程演示了一遍而已。反观李航博士的《统计学习方法》，首先介绍了决策树的模型理论，然后举了几个例子来说明各个环节的做法；后面再把我们平常用到的几个经典的决策树算法，比如 ID3 算法、C4.5 算法、CART 算法都进行了理论方面的梳理，这样的知识框架安排，就要合理得多了，给人感觉言简意赅，提纲挈领。

当然了，《机器学习》也有比较好的一面，比如对于各个算法的来龙去脉的历史介绍，后续阅读材料的介绍以及参考文献方面，都要比《统计学习方法》好些。

      

谁说这本书简单易懂的？你站出来，我保证不打死你。

一瓢之饮

2016-09-23 15:31:10

内容是挺严谨，因为显然，能用公式表达的地方它都用公式了……

带来的问题是，很多本来简单的道理也变得非常难懂。

幸好我是配合国外课程边听边看的，我在课程里听得挺明白的，但同一个章节，我再去这本书里看，却看得很累。

你可以想象书有多难懂了吧！

作为一本教科书，我觉得简单易懂应该是比内容严谨更加重要的，毕竟你又不是在出论文。

如果你拿这本书「自学入门」，那祝贺你选择了 hard 模式，你可能会被机器学习操得抬不起头来。

（我属于编程还可以，数学忘得差不多了，且没接触过太多机器学习的人。所以我的评价仅供和我类似的人参考。）

读这本西瓜书的正确方法！

HJ

2018-12-25 17:31:31

        

转自周志华老师微博

某推荐的读法：没基础的读者从头囫囵读，建议最好不超过两月，读不懂的跳过去。给读者搭建不偏学派的整体框架，建骨骼，是本书第一层用处。然后建议找别的材料读，长肉，这样的读物常见。再回来读本书，或许会发觉好多东西原来那么简单，这是提筋节，本书的第二层用处。再去别处学，全面积累，到一定时候回来或许能有点豁然贯通感，通经络，是第三层用处。写的时候就是冲着这去的，希望出一本有助不同层次读者的耐读之书。有读者体会了，作者就算没白下功夫。

纯自学第一遍读最好不超过两月，读不懂的跳过去，不求甚解。对一个发展迅速、知识尚未固化、外延极广的学科，先搭建大体框架要紧，开头不要试图弄懂所有细节。否则要么两年下来仍懵懵懂懂，要么对少量内容有了解、其他无知，失去进一步学习中触类旁通的机会。像画油画一样，一层层往上刷，后面再细琢

      

看了几章，还不错

eric

2016-02-14 17:31:35

大致翻了下，目前详细看了第 8 章集成学习，和前 5 章。感觉还不错，不由得拿李航的书来比较。（只谈谈我目前看到过的几点，仅供参考）

①内容方面，李的书要少于周的书，周写的内容更多一些。

②由于只看了第八章，两者关于 adaboost 的公式讲解都差不多，但是李航的书有例题，一步步的计算。周的书没有例子，但是有一些在相同数据集上不同参数的比较图。

③周的书有个好处就是，行文中穿插了各个算法的来龙去脉，有点历史的感觉。

④参考文献两本书都有，这点就不比较了。

⑤最后一点，周的书比李航的书要厚许多，也是彩印，内容当然也要涵盖很多，但是不能说周的书是包含了李航的书，我的建议是两本都可以看看，都是不错的读物。

可以当个随便翻翻的 Review，不懂评分为什么这么高

WenzheWang

2017-02-02 01:36:58

        

这本书的纸张选的很怪异，装在包里很难装，放书柜也不好放。从出版至今短短一两年的时间，这本破书就改了十版，可想而知这本书里到底是有多少错误，作者对待这本书是什么个态度。作者说要把这本书当成教科书，但真正学机器学习技术的人不会看这本书（至少我看完后悔了），只能勉勉强强算一个科普，因为即使你认认真真看完仍然什么都不会做。

我曾经很认真地看这本书，把它当作我的入门读物，一个一个公式的推导，结果发现这本书的公式定义尤其混乱，和前面的文字对不上，表意不明，有很多是作者自己的理解，还需要读者揣摩作者的意思。该给读者仔细指明的公式简而又简，那种无需在意过程只需看结果的公式长的不行。内容上也是前后文有重叠，最开始告诉读者十次十折交叉验证最常用，到后文就推荐别的交叉验证，也没有指明这两者孰优孰劣。作者只是把几个常用的机器学习技术罗列在那，没有进行系统的总结，也没有实际能应用的例子。

不过这本书也不是没有优点，这本书算是一个比较全的 Review，想大概了解一下机器学习的技术可以大概浏览一下本书的文字部分，不能细究。西瓜的例子还是很生动的，这本书本来想给一颗星，多那一颗是给西瓜的。

      

这不是一本用来自学的书

JudeLiu

2018-01-15 20:47:46

        

似乎中国的老师在写教材的时候是没打算让人自学的，经常写到懂得人不需要读，不懂的人读了也不明白的地步。这本书就对初学者很不友好，实在不是一本入门的自学读本，可能加上作者的授课倒是可以起到实际的效果。不过考虑到读者人数和作者能够教学的人数，恐怕大多数的读者都没有这个机会得到作者的讲解了。而且这本书的一个问题是内容过于的散了，没有对应的逻辑用来将各章节串联起来，也没有一个大纲给各个章节一个提纲挈领，让人总是在不同的章节中跳来跳去，这种感觉越到后面越严重。希望作者在改版的时候可以考虑一下这个情况，看看有没有办法讲一个完整系统的故事，可以让人将机器学习整个理顺，而不是仅仅写到机器学习导论为止。

      

周志华机器学习习题答案

Momo

2017-01-21 20:10:42

        

开个坑，慢慢填

第一章绪论：

http://wangzhinan.com/2017/01/08/zzh-machinelearning-exercise-1/

第二章模型评估与选择:

http://wangzhinan.com/2017/01/21/zzh-machinelearning-exercise-2/

      

什么鸡吧玩意

flanker

2016-03-23 18:27:14

一堆公式不给推导看个鸡吧啊好想好想好好许许多多家大家都觉得简简单单简简单单就得绝对绝对绝对绝对的经典打击打击打击打击打击

国内学者的写作通病

跑地比谁都快

2017-06-21 22:00:07

        

看网上评价很高，也是国内的第一本较为全面的介绍机器学习著作，作者的大名在学术界也是人尽皆知。于是立马买入此书。

大致翻了一下，明显有国内学者写书时的通病，上来就摆公式，把很多能用语言就简单描述的问题直接用公式符号表示。看似严谨，但其实对于初学者很不友好。做了一个测试。本人把一本国外的教材和这本书描述的同一个算法给公司里不懂机器学习的人看，国外的教材同事一下就看明白了，而这本教材同事看的云里雾里。

到底是大段大段地摆公式定理，显得严谨干练，把一个很简单的问题描述的很深奥，短平快的风格好？还是像国外很多学者一样，即使学术素养深厚，也会把一个很复杂的问题讲得通俗易懂好？

      

适合入门的一本书

阿杜

2016-11-27 22:54:06

历时一个月，利用每天早晨起床和晚上睡觉前的时间，前几天刚粗略过完一遍，趁着热乎劲写点评论。

周志华老师在这个领域确实很强，从书的结构组织和内容就能看出来，由浅入深，从经典算法到近期该领域的热门算法都讲到了，很有广度。但由于本书定位是教材，针对的是入门级别的人群，所以深度上有所欠缺。

整体上看本书前面 8 章写得比较详细，从推理各方面都比较细致，这些是机器学习比较基础的部分。后面的章节则感觉周老师想尽可能的多讲点内容，但又受限于书的篇幅（每章基本控制在 20 页左右），很多内容其实没讲清楚。可能也和自己的数学底子有关，后面章节的数学推导部分基本得跳着看，实在看不懂。不过好在每章后面都给出了参考文献，想继续深入了解的话可以自己找相关的文献继续学习。

总的来说，周老师这本书适合入门，想深入的话还要继续参考其他材料。之前看过李航老师写的《统计学习方法》，这边写的比较有深度，虽然介绍的内容没有周老师的这本广，但只要涉及到的算法，都由浅入深的讲，讲的比较细。另外台大林轩田教授讲授《机器学习基石》和《机器学习技法》的公开课也不错，比较偏理论，也是挺有深度。

看完之后我觉得还得补补数学的基础，感觉底子不扎实看这些内容有些确实比较费劲，打算看完再重新看下李航老师书和林轩田教授的视频。

中文领域最好、最完备、最适合入门的机器学习教材

风行水上

2019-01-11 12:02:23

        

从 18 年年底开始，做了一个决定：重新开始机器学习的学习之旅。逡巡了一下书架，决定选择本书作为主要教材，因为本书优点甚多：

一、结构合理，覆盖到了机器学习的几大板块：人工神经网络、SVM、集成学习、聚类分析等；

二、推导过程由浅入深，从简单原型发展到复杂的模型；

三、有适量的数学推导过程，且书后的附录有简明的数学补充知识，便于自学；

四、文笔很好，经常引用一些比喻，可读性佳，比李航老师的书更亲切一些。

花了两个礼拜，把前 11 章通读了一遍，收获蛮大的：

在头脑中基本形成了机器学习的架构，也提高了机器学习的一般思维

。我可以负责任地说，作者功力深厚，本书是中文领域数一数二的教材。

当然了，自学不能拘泥于一本书，要学好机器学习，必须参考很多资源，这里罗列下本人搜集的一些资源：

1）参考书：李航的《统计学习方法》、 M.Mitchell 的《机器学习》，吴喜之的《应用回归及分类》&《复杂数据统计方法》、杉山将的《图解机器学习》、陈希孺的《数理统计学教程》等等。

2）网络视频课程，台大林轩田老师的《机器学习基石》和《机器学习技巧》等。

3）其他网络资源，如 CSDN 上有海量大牛们的文章，有的解释很形象很多。每个人的学习习惯不同，我是以书本为主，网络资源作为参考的。

      

给高评分的，你们真的看完看懂了？

Basil

2018-04-09 11:43:35

        

我第一次看这本书的时候，给的四颗星。

大致浏览了一遍，觉得作者前面三章写得可以，感觉真的有四颗星。

第二遍要深读的时候，发现无法理解为什么这本书评价这么高。

原因就是我在短评里写的：简单的不用他写得如此通俗，也能易懂；不简单的他想通俗，可惜易懂的效果没起到，反而变成蜻蜓点水，让人更难懂了。

不得不说，这是通俗易懂风格的书的通病：除非你真的拿捏到位，否则很难体现出水平。

把复杂的事情用简单的方式解释清楚，这是大师。不是举几个西瓜例子就能做到的。

我想，这本书如此高分的原因，可能是周教授的名气。周教授是机器学习的专家，相信他本人实际上课和指导研究生时一定很有大家风范，不过写书，是一件更难的事情。

这本书如此收到好评的另一个重要原因，应该是这本书没有语言障碍，毕竟是中国大家写出来的书，不存在翻译问题。

我不知道给高分的人是不是真的看完了这本书并且看懂了。这本书后面的章节明显写得不太好，比如增强学习那张，非常牵强，基本上看了三遍还不能领会，公式推到也很晦涩，感觉是有意带过了很难描述的地方，然后在很容易懂的地方又花了很多的笔墨打磨。

诚然，机器学习本身这个领域就复杂，指望通过一本书就能领会是不太可能，不过这本书我觉得似乎也没有起到入门的作用。其实，这是目前你能看到的所有讲机器学习的书都有的问题。最多是做个提纲作用，然后大家有针对性地去深入吧。

      

感觉这本书不适合入门

scholarship

2016-11-27 09:08:20

之前已经有一定的数学知识储备，但第一次看这本书感觉没有什么头绪，基本上一章只能看懂前面的部分，后面较深入的就看不懂了，公式比较杂乱，上课用的模式分类感觉要比这本书好很多，讲的比较清楚，这本书应该比较适合作为一个字典或者有机器学习基础用来复习的，我旁边有很多人冲着周老师的名气买了这本书到现在这本书基本都躺在书架上吃灰，大家对这本书评价也不是很高，建议想入门机器学习的同学绕过此书。

求推荐配套相关的软件工具书籍或资源

redlion

2016-09-09 15:11:04

求推荐配套相关的工具书籍或资源。

这是一本理论书，各位有好的配套软件工具书籍或资源推荐的吗？感觉光看书看理论有时候容易头大，结合着软件工具练练手学习起来会比较好一些

一些地方能再详细讲解下就更好了

手机刚代码

2016-11-22 10:39:06

覆盖面没话说，也对一些知识有了初步的理解。

但说实话因为覆盖面比较广，一些推导就省略了不少，若能减少覆盖面，集中讲解几个算法，就更好了。

比如 CRF，HMM 只有两个子章节，导致看不太懂。

个人比较喜欢看完整的推导加十分详细的讲解。

总之就是一些地方能再详细讲解下就更好了。

本人正在阅读此书，加群 109180137 交流

spencer

2016-02-05 13:56:15

作为国内第一本大牛级别写的机器学习中文书，还是值得阅读研究一番的，欢迎加入此书读书 QQ 群：109180137。

这段时间正在阅读此书，大致翻阅了目录，范围比较广，书比较厚，等阅读完之后再深入评价。欢迎同仁加入一起研究和学习

太好懂了

Kijiang

2016-04-19 22:49:14

这本书写的太好了

好多之前一直看不懂的东西 看了其他也不理解的东西 看了这本书后特别容易理解！

这本书 1 月出版 印刷多次 居然已经冲上京东计算机排名第一

感受到了这个领域的热门以及众多的竞争对手…

能够解释为什么的机器学习教科书

a_31415926

2016-08-04 14:33:45

1. 这个书最好的地方在于他很多地方解释了某某模型或者参数为什么的问题：也就是某个东西的用途优点和动机 intuition。而一般的计算机类的机器学习书则侧重告诉你 "是什么" 的问题，而不知道所以然。这个优点是和一些机器学习书包括李航老师的，还有一些机器学习实战之类的书不太一样的地方。

2. 当然这是本教科书，提到的很多都是经典理论的东西，不会出现那么热门新潮的东西，深度学习还没有定型的时候，周老师也没有说那么多。

3. 另外教科书的推导也是教科书式的，图和例子不是很多，更多是公式，不太直观。

仍然是目前为止最好的机器学习中文书

钊钊

2017-08-05 12:23:19

前前后后花了一个学期时间才基本读完一遍，相对于刚拿到书时，读下来对这本书的喜爱不减。

首先，过于简略不能成为给这本书差评的理由。前言指出了这本书的定位是教科书，第十版印刷时增加了一些使用说明，作者表示作为一学期的教材，各章篇幅进行了仔细考量，避免每堂课涉及页码过多的情况。这其实也有一个好处，这本书拿来自学的时候，很快就能看完一章内容。虽然容易有意犹未尽的感觉，不过读完后也更有去查阅资料进一步深入的动力。

其次，这本书做到了文字语言和数学公式恰到好处的结合，可以看出来作者在努力讲清楚每个方法。总体上每个方法的介绍都依照「问题，解决办法 -- 总的抽象介绍 -- 形式说明 -- 算法 / 例子」的思路，期间穿插数学公式（和部分推导过程），用于对文字说明的辅助理解。事实上，我觉得对于对于数学出身的人来说，看到公式就像 CS 出身的人看到代码一样，觉得更容易懂吧：）另外，方法的优缺点介绍是一大亮点，可以重点 mark。

对于正文不到四百页的入门教科书，本书的内容涉及范围足够的广。由于先前看过台大林轩田的机器学习课程，所以前两部分（大概是 1~10 章）读起来比较顺畅，第三部分是一些相对进阶的知识，跳过了一两章，其他的通过其概述性的介绍作为入门感觉也不错。

最后，这本书需要多次重读，希望「于原不经意处或能有新得」。

不完美的好书

Vivian 陌水瑶

2017-10-21 00:34:56

        

上学期买的书，断续看了一点，这学期每周开了讨论会，于是开始精读。发现对初学者来说还是有很多难懂的地方没讲清楚，读的过程中需要查阅大量资料，公式推导省略了很多步骤，很多统计学、随机过程和矩阵论的知识也是一带而过，很多细节理解起来也比较艰难。个人感觉吴恩达的机器学习课程中文笔记更容易理解，不过较为浅显，李航的统计学习方法公式推导得很清晰明了，算法讲得相当细致而且还富有例子，对本书是一个很好的补充。我是先学了吴恩达的课（虽然已经忘得差不多了），然后两本书对照着看的，收获很多。

      

标准的教科书：理论知识，加比较新的研究总结

张晓帆字骥飞

2016-12-11 17:24:35

豆瓣评分 9 分的书，全是理论知识，应该说还不错。读完厚厚的一本书，能建立机器学习的框架，并有前瞻性的一些概念。掌握各种算法的数学基础，能解决什么业务问题，有什么注意事项，能更好的指导我们的工程实践。同时，补充书中没有的当前流行的计算环境和库，结合我们实际的各种业务问题，机器学习就能在我们的日常工作中开花结果了。

http://note.youdao.com/noteshare?id=17c39ed10781a12f636a232e9eee1385

      

学习记录

是得得啊

2018-05-23 17:36:52

        

习题答案参考：1、

https://blog.csdn.net/icefire_tyh/article/details/52064910

                          2、

https://blog.csdn.net/Snoopy_Yuan/article/details/62045353

2018/5/23  ——1、 绪论

主要了解版本空间的含义。

「析合范式」存在情况下，考虑到冗余存在的情况下，可能存在的假设

进入「栈」的概念：

https://baike.baidu.com/item/%E6%A0%88/12808149

栈作为一种

数据结构

，是一种只能在一端进行插入和删除操作的特殊

线性表

。它按照先进后出的原则存储数据，先进入的数据被压入栈底，最后的数据在栈顶，需要读数据的时候从栈顶开始弹出数据（最后一个数据被第一个读出来）。栈具有记忆作用，对栈的插入与删除操作中，不需要改变栈底

指针

。

栈是允许在同一端进行插入和删除操作的特殊

线性表

。允许进行插入和删除操作的一端称为栈顶 (top)，另一端为栈底 (bottom)；栈底固定，而栈顶浮动；栈中元素个数为零时称为空栈。插入一般称为

进栈

（PUSH），删除则称为退栈（POP）。栈也称为后进先出表。

在 git 上做备份就是简单的「栈」感念的运用。

2019/3/11

第二章 ，模型评估（可以结合 MOOC 看，视频讲解配图会对书有更好的理解，但是视频过于简单，配合书学习）

机器学习

（网易公开课 授课老师 蒋良孝、胡成玉）

公开课推荐基于 JAVA 的

WEKA 平台进行操作

由于没有很好的 JAVA 基础，改用 R 或者 python。

      

有点名不副实了

seal_hu

2018-11-19 14:35:13

        

之前有个同事被这本书劝退了放弃机器学习了，他应该还算挺聪明的那一类。

没搞懂这本书怎么定位的，如果说讲理论那就好好讲讲公式的推导，由来，可是这书里面像 em 这么重要的算法都没有多少篇幅。像 gibbs 采样，变分法这些挺难理解的东西也是随随便便就冒出来了。如果说讲思想这本书又很少讲算法背后的含义，不同算法之间的联系是什么。这本书更不是一本实战指导资料。

比吴恩达的公开课差远了，公开课很多公式都仔细的推导了确实能看懂，而且还有一些背景和思想的内容。这本书真有点坑，还不如只看目录，然后拿目录里的名词去网上找文章看可能效果会好很多。

      

即使是作为教材，也是严重不合格的

升仙

2019-12-07 11:59:08

        

教材，是一种依据课程标准编制的、系统反映学科内容的教学用书，是课程标准的具体化。但本书显然只做到了依据课程标准，却没有做到具体化。一本好的教材，难度要设置在普通学生初次学习所能达到的上限 —— 学生能在自己预习的时候，看个三五成明了，然后在书中标明问题，上课的时候由老师分析掉重难点，达到七八成水准，再根据参考文献，学生自己查阅相关内容补充知识，最后做习题升华检验，完成初次学习的历程。

然通读此书，全文各种语焉不详，推导跳步潇洒且采用极不聪明的方式，术语不规范，刻意回避问题，习题选取并不佳，检验能力和拓展性都不好，甚至还不能算是机器学习领域的谭浩强式教材。我实在想不出除了周本人以外，还有哪个能用此书达到应有的教学目的。

在机器学习日益发展的当今，初学者有更多的选择以学习机器学习理论，实在不必选取这样一本名不副实的教材，且不说你自己驾驭不了它，甚至选取这本书做讲义的老师们都会在教学过程中遇到困难，平添无数麻烦，也达不到预期效果。

指望国内出现优秀教材，看来在有生之年是不可能的

      

这本书和《统计学习方法》很配

潇潇哈哈

2016-03-31 17:14:25

《统计学习方法》和《机器学习》都读了一半这样。感觉《机器学习》比较全面一点，最近在研究概率图模型，神经网络和概率图模型这两章写的都不错，帮助我入门。然后两本书都讲到的章节，比如 LR，SVM，有的时候只看一本书看不懂，《机器学习》关于核函数的讲解比《统计》要浅一点。容易接受一点，所以看完《机器》再看《统计》。

总之，光靠一本书来学习算法是不可能的，搭配着看理解会更好一点。

作为一个自学机器学习的人来说，这是我读过最好的书 (好懂，知道了很多为什么)

javagood

2016-09-20 14:58:09

写的比较深入浅出，原来自学过机器学习课程。同时实践过一个简单的机器学习算法来预估广告点击率 (逻辑回归算法), 取得了一定效果.

但是很多基本原理也只是照搬硬套。看了这本书后有一种豁然开朗的感觉

例如为什么参数时，明明是两种情况，为什么要设置三个 (一个默认为空). 还有怎么校验一个学习算法的优劣，为什么这么校验。这本书告诉了你为什么，不错.

目前还只看了前两章，就感觉很爽.... 如果早几年有这种书就好了

机器学习最佳入门

Leuckart

2017-07-11 04:45:54

        

本书的前部分 (至第九章完) 是周老师在春季学期开设的机器学习课程的内容，后部分似乎是给研究生开设的高级机器学习课程的内容。校外人士似乎可以登陆课程网站下载课程习题，个人认为习题质量较高，基本是 4 道理论＋1 道编程 (如使用 python/matlab 实现神经网络 / 决策树 / 朴素贝叶斯 / 支持向量机等)。

至于这本书的内容，个人认为在国内当是机器学习领域最权威的教材之一，毕竟作者本身就是该领域翘楚。

但有个缺点是似乎经常用一大段话解释，却仍让人感到表意不清 (或许是我头脑不够好吧)，总之在这方面我觉得不如李航那本统计学习方法的言简意赅；证明过程阙失，这点和李书差不多；优点是常常有一语点醒梦中人的金句；内容多于李书。

周老师说过，这本书只是作为一本入门，我认为这确实是一本极为优秀的入门书籍，不论中外。

个人推荐参考书，prml，hastie 那本统计学习基础。

      

提纲掣领的好书

- 容澄 -

2017-09-26 18:58:40

        

刚出版的时候还在国外，第一时间拜托了朋友从国内带过来，对这本书充满了期待。初看之下挺失望的，知识点讲得看似面面俱到实则非常草率。但现在越发觉得，这本书能将各个知识点串在一起讲通，实在太难得了。算法细节完全可以从其他大部头里补充。学习算法难的地方就在于一课一课学习零散的知识，贯通只能靠自己。而西瓜书帮读者整理好了大部分脉络。我很难描述这种阅读时微妙的畅快感，只能说作为相关领域的学生到从业者，这本书给我的帮助是最大的，可供学习阅读，亦可作为参考书查阅。

      

《机器学习》周志华著，总结

Dr.Huang

2018-02-18 23:08:42

本书介绍了机器学习的基础知识、各种流派的学习算法，通读本书可对机器学习的原理、内容、功能、流派，应用有一个整体的认识。

人工只能经历了「推理期」、「知识期」到「学习期」，目前学习算法主要包括符号学派（代表算法：决策树、基于逻辑的学习）、连接学派（代表算法：人工神经网络）、统计学派（代表算法：支持向量机、核方法）等。各个学派的算法具有自己的优势，也有自己的短板。例如符号学派模拟人的概念推理过程，知识表达能力强，但难以处理大规模问题；连接主义学派对于现实问题的有效处理得到了认可，但是知识是个黑箱模型，无法显式地表达，且模型大量参数的设置缺乏有效理论指导；统计学派可有效解决一些大规模复杂问题，但是仍然存在核函数选取困难等问题。不同的时期，不同的学派占据主义地位，其顺序为：符号 -> 连接 -> 统计 -> 连续。目前连接学派里面的深度学习引起热潮，注意是得益于计算机硬件性能的提高和大数据时代，使得更多层的神经网络焕发青春。

对于工程技术人员而已，从应用角度上应该做到：

（1）清晰地认识自己要解决的问题，该问题很难手工或采用其它常规方法解决；

（2）没有数据就没有机器学习，因此首先要获取到足够的相关数据；

（3）了解各个机器学习算法的基本原理和功能，以方便选择合适的算法；

（4）计算结果要反馈到实际中查看效果，以做进一步的调整和优化。

好老师，好书，五星

Kevin

2018-03-02 17:20:43

写的太好了。看了前三章只给了三星，主要是感觉很多细节写的不清楚。看完全书来改为五星，发现本书本身就定义为科普 + 广度读物，适合作为每个小方向的引导读物，然后作者每个方向都写了类似综述的小结，提供了最靠谱的针对性文献，可以继续读文献深究。喜欢这种写书方式，不得不说，周老师确实是国内少数做学术和教学都很有一套的好老师。

书评

hello,world

2018-03-16 17:44:42

周老师写的这本西瓜书真的是可以作为一本机器学习的入门书，内容全面，对于机器学习整体概观的建立很有帮助。

1. 全书涵盖了机器学习的分类、回归、聚类、降维这些问题，后面部分还介绍了半监督学习和强化学习，内容是比较全面的。前面两章是对一些一些基本概念的介绍，包括归纳偏好（没有免费的午餐定理），模型的误差与评估等。第 3 章到第 11 章是本书的重点内容，每章都把算法讲述的很清楚，并且给出西瓜数据集的例子加以解释。能把每章的内容控制在 20 页，简洁却又不失全面。（大概这就是大佬的功力吧。。。）

2. 这本书是非常合适的入门书，我是真的很推荐。这本书与李航的统计学习方法相比，深度不是很够，对于公式原理的详细推导可能没有那么深。

总体来说，适合入门，内容全面，非常推荐。

对于初学者很不友好

my left foot

2018-06-09 15:02:53

        

我是看了吴恩达老师的视频然后去寻找对应的章节来看的，对于吴恩达老师很容易就搞明白，但是这本书基本上能用公式的就全用了公式，看起来非常吃力。而且对于数据基础不太好的同学，也是十分不友好的，周围的卖了这本书的同学都反映这本书，有些用语言描述十分简单的问题，用公式去表述的十分拗口。

刚开始学习机器学习，感觉还是吴恩达老师的视频更好一些。

      

修正里面的错误

特芬奇斯拉星人

2018-09-24 23:38:16

        

本书 230 有一处错，投影后样本的方差不是此公式，而是其代表的矩阵的迹。证明见图 2。

目前观感，此书推理跳跃了很多步骤，更倾向于给你一个思路，需要具备一定数学基础，然后根据其线索自己推理出来。

2,

175 页集成算法 boosting：计算 at 时，用的是 Dt，而不是最终目标函数的 D 分布，这里有一点错位，目前理解为计算过程中无法实施 ht 分类器在原始 D 分布下的最小指损函数，因此用过程中 Dt 替代，数学计算上的便易。

。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。字数限制。。。。。

      

对初学者不友好

小猪快跑

2018-11-07 17:10:20

        

花了两个月的时间断断续续读完了本书，如果只用一句话总结那大概是: "对初学者太不友好了！".

因为作者根本没想让你直接通过这本书就能入门机器学习，人家的定位是教材，是希望通过那些大学老师的在黑板上推算、在课堂上授课来使你明白机器学习里面深刻的内核。光凭对这么一本教材死磕，怎么能搞懂机器学习.

因此在阅读此书之前，我也先在网上找了 Python 机器学习实战的教程先过了一遍，然后也只是大概的浏览了一遍，没有仔细研读公式的证明过程，然后发现，工作当中能够直接用得上的算法，好像并不多 (甚至可能没有)... 可能也就只有 PCA 算法能在数据质量校验方面能提供些思想，以及深度学习在图像识别方面相对成熟的实践经验，至于其他算法想要直接用于解决实际问题，还真的是有点儿扯 (即使能用上，效果也不太好，数据的精度与准度的确差强人意).

因此建议后来的读者，前期也只要大概的关注些算法的应用流程、主体思想、能够解决的问题，其他更细节的内容还是等到真正的用到这些方法的时候再看仔细的研读吧.

      

非常好的书

唐僧洗头用飘柔

2018-11-28 11:15:05

        

这本书断断续续读了一年多，一直在反复读。这本书最大的好处在于满足不同层次读者的需求。对于新手别指望读一遍就学明白，那是不可能的。读这本书，不仅是读那些算法，而且还要读参考文献，余以为此书最大的亮点便在于习题和阅读材料那部分。然而多数读者在意的仅是所谓的那些算法，了解并实现就万事大吉，材料不看，习题不做。对此，说一句，简直可笑。将这本书读过 3 遍以上，中间在配以 PRML，书中不少点也将豁然开朗。此时，在看看一些所谓的前沿论文，便会发觉，真的没那么前沿，一切皆是有迹可循。

此书读到何种程度算是真正读通？读者可以先去看看周老师的 boosting 25 年演讲，如果你能做到向他那样对于问题的发展的脉络如数家珍一般，余以为那才是真正的将书读明白了。总之，不要太浮躁，一读二品三斟酌，四联系五归一。

      

非常好的书

唐僧洗头用飘柔

2018-11-28 11:24:52

        

这本书断断续续读了一年多，一直在反复读。这本书最大的好处在于满足不同层次读者的需求。对于新手别指望读一遍就学明白，那是不可能的。读这本书，不仅是读那些算法，而且还要读参考文献，余以为此书最大的亮点便在于习题和阅读材料那部分。然而多数读者在意的仅是所谓的那些算法，了解并实现就万事大吉，材料不看，习题不做。对此，说一句，简直可笑。将这本书读过 3 遍以上，中间在配以 PRML，书中不少点也将豁然开朗。此时，在看看一些所谓的前沿论文，便会发觉，真的没那么前沿，一切皆是有迹可循。

此书读到何种程度算是真正读通？读者可以先去看看周老师的 boosting 25 年演讲，如果你能做到向他那样对于问题的发展的脉络如数家珍一般，余以为那才是真正的将书读明白了。总之，不要太浮躁，一读二品三斟酌，四联系五归一。

最后说明一点，本人读书不喜欢那种事无巨细全都堆上去的那种。像西瓜书这类，像字典但又比字典略加详实的，同时又能给阅读资料，正式本人最喜欢的。通读之后，有一大概框架，之后便去度材料补充框架，最后再提炼。余以为这才是读书的一大乐趣。

      

不适合小萌新

息霜

2019-01-15 17:45:25

        

好书是好书，但是不适合初学者，行文有点枯燥，公式言简意赅很少有推导，推荐去看吴恩达的视频教程，网易公开课上有中文版，那才是对初学者友好的。

适合对机器学习已经很熟悉的人，然后对整个结构进行梳理，决定半年后机器学习有一定水平之后再重新捡起这本书

。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。

      

不适合自己

bd2qz

2019-05-13 11:52:26

        

最近在看机器学习和概率论相关的书籍，后来发现资料都是很好的，但现在可能不适合自己了，因为里面涉及到较多概念和公式，可能对于在校生或者有较多充裕时间，能不受干扰研究的人来说，可能很适合，但现在自己的状态，平时上班，兼顾盯盘，周末陪家人，已经很难能这么静下心来，或许之后有可能心情宁静，能超然看待得失，那时或许会做的更好，现在的话，只能先放弃，等待以后有机会在研究。

      

多少人是冲着机器学习的名字来的

lft.

2019-05-30 22:34:52

        

多少人是冲着机器学习的名头来的？？看过这本书，就产生了我高人一等的感觉？？如果不是周志华写的而是一个名气一般般的人写的会怎么样？抱着半天看不懂，还是要谷歌，忘了你们还是百度吧。硬要装这个逼到豆瓣来称赞一番，我不懂，别人厉害，你们不分青红皂白捧了别人，你们自己也厉害了吗？就跟现在的华为一样，说真的，希加廖夫主义好，这些书评看得我头疼。

      

算是读后感吧

marswu

2019-07-10 14:04:22

        

读完了，写点东西可以当作读后感吧，个人观点，大佬勿喷。

3.5 星给书，0.5 星给盛名。

1. 对于派系来说，这本书没有特别的倚重频率派与贝叶斯派哪一边，算是对大体算法的一个普遍介绍。

2. 对于公式，这本书公式是比较多，但还不至于到劝退的地步，最重要的是弄清你的 target 和变量的意义，公式便会简化许多。

3. 如何直接找到问题的本质，对于频率角度或统计机器学习来说，其最后的问题归纳于「优化问题」。就拿 SVM 来说，它将距离转化为一个 loss function，而这就是一个带约束的凸优化问题，拿拉格朗日求解就行。而对于贝叶斯角度来说，最终转化成一个积分问题，在概率图模型在可以广泛求证。

4. 对于数学，本人作为很久不碰数学以及刚入门 ML 的小白，感觉拿到以后多算两边就记住了啊，贝叶斯角度就是反复用贝叶斯定理。总结了一下几个用的比较多的数学知识，求偏导（包括对于矩阵和向量求导），奇异值分解，拉格朗日乘子法，梯度下降法，当然是对于高维的。

5. 说一些自己用的辅助资料，李航老师的统计学习方法，里面基本是 Frequentist 的东西，公式推导南瓜书 PumpkinBook，接下来就是我觉得最最最大的干货，b 站上的白板推导系列，大佬将每一个 detail 展现的淋漓尽致，不可所得的精品。接下来台大林轩田老师 foundation 里对于 12 章计算学习理论讲解颇为细致，从 cannot ML 到 why ML 让你完美理解 PAC。

6. 最后，总算把上学期学的东西串起来了，接下来 PRML 走起～

      

很一般的书

游荡天空

2019-09-28 20:12:47

        

「入门书」—— 因为里面的理论都太初等，不深入

「不适合新手」—— 因为里面的理论太简略，又太抽象，新手根本看不懂。

评论里面有两种不同的观点，其实说的都是这个书的缺点。。

但这个书也不是完全垃圾，因为机器学习本来就新，没什么权威教材可看。

可以看看周志华的序言，在国内没有机器学习课程的前提下编这一本书，挺难能可贵的。

但要好好学习，还是建议读李航的统计学习方法（第二版）（最新出的书），那里面先介绍了常用的统计决策方法，用统计决策理论把模型穿起来，每个模型的地位一清二楚。

同时也有不错的推导。

这个书，理论性太不明确，没有统一的线索，感觉是在重复学。

而且推导太简略，只是为了让你「相信」一下这里是有证明的，不是为了给你讲清楚。

      

好书，但不要盲目封神

努马

2020-01-29 20:58:53

        

周志华教授的这本《机器学习》，由于书中无处不在的西瓜元素，被广大网友亲切的称呼为「西瓜书」。在国内机器学习领域教材良莠不齐的当下，这本‘西瓜书’确实是一本不可多得的入门好书。正如周教授在书中所说，这本书的定位是「机器学习方向的入门级教科书」，因此，书中主要是分章阐述了机器学习领域的经典算法，数学推导涉及的并不是特别多。同时为了加深理解，每章大多有一个用本章算法挑选好西瓜的例子。加深了读者对算法的理解。在每章的最后，书中还附录了相关算法的经典文献，为读者下一步深入了解算法和科研入门提供了导向。不过，在书中的有些地方，由于数学推导可能过于简略，看起来显得有些跳跃。同时，作为一本老师上课使用的教材，这本书对于算法的讲解比较简略，我猜测可能是留待老师上课补充的，从这点上来看，对自学并不是特别友好。       除却这本书的内容，我还想多说一些，随着人工智能的大火，加之周志华教授作为国内人工智能领域的执牛耳者，使得不少人对这本书趋之若鹜。我记得在 18 年的时候有一则新闻，说记者在采访参加南大自主招生的学生里面，有一人表示在高考前就买了《机器学习》，说：「可能我现在还看不太懂，但假如能够进入南大学习人工智能，我想我一定会把这本书吃透。」其实周志华教授在西瓜书的序言里已经明确提到，这本书是‘’一本面向理工科高年级本科生和研究生的教科书「，虽然数学推导可能比较简单，但我认为在阅读之前，读者最好还是具有一定的微积分、线性代数和概率论的基础，才能更加深刻的理解算法内涵。否则买回来的话，更大概率是翻了两页之后就摆在书架上供着，进行自我满足和欺骗。

      

决策树

言兼

2020-02-13 17:43:28

        

决策树    ●划分选择（目标：纯度高）        ●信息熵        ●信息增益（ID3 算法）        ●增益率（C4. 5 算法）        ●基尼指数（CART 算法）    ●剪枝        ●如何判定泛化能力（验证集）        ●预剪枝、后剪枝    ●多变量决策树（OC1 算法）    ●增量学习（ID4）

      

