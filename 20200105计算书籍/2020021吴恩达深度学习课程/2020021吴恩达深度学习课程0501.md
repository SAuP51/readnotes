1.1 为什么选择序列模型？（Why Sequence Models?）

在本课程中你将学会序列模型，它是深度学习中最令人激动的内容之一。循环神经网络 （RNN）之类的模型在语音识别、自然语言处理和其他领域中引起变革。在本节课中，你将 学会如何自行创建这些模型。我们先看一些例子，这些例子都有效使用了序列模型。

在进行语音识别时，给定了一个输入音频片段 𝑥，并要求输出对应的文字记录 𝑦。这个 例子里输入和输出数据都是序列模型，因为 𝑥是一个按时播放的音频片段，输出 𝑦是一系列 单词。所以之后将要学到的一些序列模型，如循环神经网络等等在语音识别方面是非常有用 的。

音乐生成问题是使用序列数据的另一个例子，在这个例子中，只有输出数据 𝑦是序列，而输入数据可以是空集，也可以是个单一的整数，这个数可能指代你想要生成的音乐风格，也可能是你想要生成的那首曲子的头几个音符。输入的 𝑥可以是空的，或者就是个数字，然 后输出序列𝑦。

在处理情感分类时，输入数据𝑥是序列，你会得到类似这样的输入：「There is nothing to like in this movie.」，你认为这句评论对应几星？

525 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

系列模型在 DNA 序列分析中也十分有用，你的 DNA 可以用 A、C、G、T 四个字母来表 示。所以给定一段 DNA 序列，你能够标记出哪部分是匹配某种蛋白质的吗？

在机器翻译过程中，你会得到这样的输入句：「Voulez-vou chante avecmoi?」（法语：要 和我一起唱么？），然后要求你输出另一种语言的翻译结果。

在进行视频行为识别时，你可能会得到一系列视频帧，然后要求你识别其中的行为。

在进行命名实体识别时，可能会给定一个句子要你识别出句中的人名。

所以这些问题都可以被称作使用标签数据 (𝑥, 𝑦) 作为训练集的监督学习。但从这一系列 例子中你可以看出序列问题有很多不同类型。有些问题里，输入数据 𝑥和输出数据𝑦都是序 列，但就算在那种情况下，𝑥和𝑦有时也会不一样长。或者像上图编号 1 所示和上图编号 2 的 𝑥和𝑦有相同的数据长度。在另一些问题里，只有 𝑥或者只有𝑦是序列。

所以在本节我们学到适用于不同情况的序列模型，下节中我们会定义一些定义序列问题 要用到的符号。

526 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

1.2 数学符号（Notation）

本节先从定义符号开始一步步构建序列模型。比如说你想要建立一个序列模型，它的输入语句是这样的：「Harry Potter and Herminoe Granger invented a new spell.」，(这些人名都是出自于 J.K.Rowling 笔下的系列小说 Harry Potter)。假如你想要建立一个能够自动识别句中人名位置的序列模型，那么这就是一个命名 实体识别问题，这常用于搜索引擎，比如说索引过去 24 小时内所有新闻报道提及的人名，用这种方式就能够恰当地进行索引。命名实体识别系统可以用来查找不同类型的文本中的人 名、公司名、时间、地点、国家名和货币名等等。

现在给定这样的输入数据𝑥，假如你想要一个序列模型输出𝑦，使得输入的每个单词都对 应一个输出值，同时这个𝑦能够表明输入的单词是否是人名的一部分。技术上来说这也许不 是最好的输出形式，还有更加复杂的输出形式，它不仅能够表明输入词是否是人名的一部分，它还能够告诉你这个人名在这个句子里从哪里开始到哪里结束。比如 Harry Potter（上图编 号 1 所示）、Hermione Granger（上图标号 2 所示）。

更简单的那种输出形式:

这个输入数据是 9 个单词组成的序列，所以最终我们会有 9 个特征集和来表示这 9 个 单词，并按序列中的位置进行索引，𝑥<1> 、𝑥<2> 、𝑥<3> 等等一直到𝑥<9> 来索引不同的位置，我将用𝑥 <𝑡> 来索引这个序列的中间位置。𝑡意味着它们是时序序列，但不论是否是时序序列，我们都将用𝑡来索引序列中的位置。

输出数据也是一样，我们还是用𝑦<1> 、𝑦<2> 、𝑦<3> 等等一直到𝑦<9> 来表示输出数据。同时我们用𝑇 𝑥 来表示输入序列的长度，这个例子中输入是 9 个单词，所以𝑇 𝑥 = 9。我们用𝑇𝑦 来表示输出序列的长度。在这个例子里𝑇 𝑥 = 𝑇 𝑦 ，上个视频里你知道𝑇 𝑥 和𝑇 𝑦 可以有不同的值。

527 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

你应该记得我们之前用的符号，我们用𝑥 (𝑖) 来表示第𝑖个训练样本，所以为了指代第𝑡个 元素，或者说是训练样本𝑖的序列中第𝑡个元素用𝑥(𝑖)<𝑡> 这个符号来表示。如果𝑇 𝑥 是序列长度，那么你的训练集里不同的训练样本就会有不同的长度，所以𝑇 𝑥 (𝑖) 就代表第𝑖个训练样本的输入 序列长度。同样𝑦 (𝑖)<𝑡> 代表第𝑖个训练样本中第𝑡个元素，𝑇 𝑦 (𝑖) 就是第𝑖个训练样本的输出序列 的长度。

所以在这个例子中，𝑇 𝑥 (𝑖) = 9，但如果另一个样本是由 15 个单词组成的句子，那么对于 这个训练样本，𝑇 𝑥 (𝑖) = 15。

既然我们这个例子是 NLP，也就是自然语言处理，这是我们初次涉足自然语言处理，一 件我们需要事先决定的事是怎样表示一个序列里单独的单词，你会怎样表示像 Harry 这样的 单词，𝑥<1> 实际应该是什么？

接下来我们讨论一下怎样表示一个句子里单个的词。想要表示一个句子里的单词，第一 件事是做一张词表，有时也称为词典，意思是列一列你的表示方法中用到的单词。这个词表 （下图所示）中的第一个词是 a，也就是说词典中的第一个单词是 a，第二个单词是 Aaron，然后更下面一些是单词 and，再后面你会找到 Harry，然后找到 Potter，这样一直到最后，词典里最后一个单词可能是 Zulu。

因此 a 是第一个单词，Aaron 是第二个单词，在这个词典里，and 出现在 367 这个位置 上，Harry 是在 4075 这个位置，Potter 在 6830，词典里的最后一个单词 Zulu 可能是第 10,000 个单词。所以在这个例子中我用了 10,000 个单词大小的词典，这对现代自然语言处理应用 来说太小了。对于商业应用来说，或者对于一般规模的商业应用来说 30,000 到 50,000 词大 小的词典比较常见，但是 100,000 词的也不是没有，而且有些大型互联网公司会用百万词，甚至更大的词典。许多商业应用用的词典可能是 30,000 词，也可能是 50,000 词。不过我将 用 10,000 词大小的词典做说明，因为这是一个很好用的整数。

如果你选定了 10,000 词的词典，构建这个词典的一个方法是遍历你的训练集，并且找

528 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

到前 10,000 个常用词，你也可以去浏览一些网络词典，它能告诉你英语里最常用的 10,000 个单词，接下来你可以用 one-hot 表示法来表示词典里的每个单词。

举个例子，在这里𝑥<1> 表示 Harry 这个单词，它就是一个第 4075 行是 1，其余值都是 0 的向量（上图编号 1 所示），因为那是 Harry 在这个词典里的位置。

同样𝑥<2> 是个第 6830 行是 1，其余位置都是 0 的向量（上图编号 2 所示）。

and 在词典里排第 367，所以𝑥<3> 就是第 367 行是 1，其余值都是 0 的向量（上图编号 3 所示）。如果你的词典大小是 10,000 的话，那么这里的每个向量都是 10,000 维的。

因为 a 是字典第一个单词，𝑥<7> 对应 a，那么这个向量的第一个位置为 1，其余位置都 是 0 的向量（上图编号 4 所示）。

所以这种表示方法中，𝑥<𝑡> 指代句子里的任意词，它就是个 one-hot 向量，因为它只有 一个值是 1，其余值都是 0，所以你会有 9 个 one-hot 向量来表示这个句中的 9 个单词，目 的是用这样的表示方式表示𝑋，用序列模型在𝑋和目标输出𝑌之间学习建立一个映射。我会把 它当作监督学习的问题，我确信会给定带有 (𝑥, 𝑦) 标签的数据。

那么还剩下最后一件事，我们将在之后的视频讨论，如果你遇到了一个不在你词表中的 单词，答案就是创建一个新的标记，也就是一个叫做 Unknow Word 的伪单词，用 <UNK> 作 为标记，来表示不在词表中的单词，我们之后会讨论更多有关这个的内容。

总结一下本节课的内容，我们描述了一套符号用来表述你的训练集里的序列数据𝑥和𝑦，在下节课我们开始讲述循环神经网络中如何构建𝑋到𝑌的映射。

529 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

1.3 循环神经网络模型（Recurrent Neural Network Model）

上节视频中，你了解了我们用来定义序列学习问题的符号。现在我们讨论一下怎样才能 建立一个模型，建立一个神经网络来学习𝑋到𝑌的映射。

可以尝试的方法之一是使用标准神经网络，在我们之前的例子中，我们有 9 个输入单 词。想象一下，把这 9 个输入单词，可能是 9 个 one-hot 向量，然后将它们输入到一个标准 神经网络中，经过一些隐藏层，最终会输出 9 个值为 0 或 1 的项，它表明每个输入单词是否 是人名的一部分。

但结果表明这个方法并不好，主要有两个问题： 一、是输入和输出数据在不同例子中可以有不同的长度，不是所有的例子都有着同样输 入长度𝑇 𝑥 或是同样输出长度的𝑇 𝑦 。即使每个句子都有最大长度，也许你能够填充（pad）或 零填充（zero pad）使每个输入语句都达到最大长度，但仍然看起来不是一个好的表达方式。

二、一个像这样单纯的神经网络结构，它并不共享从文本的不同位置上学到的特征。具 体来说，如果神经网络已经学习到了在位置 1 出现的 Harry 可能是人名的一部分，那么如果 Harry 出现在其他位置，比如𝑥<𝑡> 时，它也能够自动识别其为人名的一部分的话，这就很棒 了。这可能类似于你在卷积神经网络中看到的，你希望将部分图片里学到的内容快速推广到 图片的其他部分，而我们希望对序列数据也有相似的效果。和你在卷积网络中学到的类似，用一个更好的表达方式也能够让你减少模型中参数的数量。

之前我们提到过这些（上图编号 1 所示的𝑥<1> ……𝑥 <𝑡> ……𝑥<𝑇 𝑥> ）都是 10,000 维的 onehot 向量，因此这会是十分庞大的输入层。如果总的输入大小是最大单词数乘以 10,000，那 么第一层的权重矩阵就会有着巨量的参数。但循环神经网络就没有上述的两个问题。

那么什么是循环神经网络呢？我们先建立一个（下图编号 1 所示）。如果你以从左到右

530 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

的顺序读这个句子，第一个单词就是，假如说是𝑥<1> ，我们要做的就是将第一个词输入一个 神经网络层，我打算这样画，第一个神经网络的隐藏层，我们可以让神经网络尝试预测输出，判断这是否是人名的一部分。循环神经网络做的是，当它读到句中的第二个单词时，假设是 𝑥<2> ，它不是仅用𝑥<2> 就预测出𝑦 ，他也会输入一些来自时间步 1 的信息。具体而言，

时间步 1 的激活值就会传递到时间步 2。然后，在下一个时间步，循环神经网络输入了单词 𝑥<3> ，然后它尝试预测输出了预测结果𝑦 ，等等，一直到最后一个时间步，输入了𝑥<𝑇 𝑥> ，

然后输出了𝑦 。至少在这个例子中𝑇 𝑥 = 𝑇 𝑦 ，同时如果𝑇 𝑥 和𝑇 𝑦 不相同，这个结构会需要作 出一些改变。所以在每一个时间步中，循环神经网络传递一个激活值到下一个时间步中用于 计算。

要开始整个流程，在零时刻需要构造一个激活值𝑎 <0> ，这通常是零向量。有些研究人员 会随机用其他方法初始化𝑎 <0> ，不过使用零向量作为零时刻的伪激活值是最常见的选择，因 此我们把它输入神经网络。

在一些研究论文中或是一些书中你会看到这类神经网络，用这样的图形来表示（上图编 号 2 所示），在每一个时间步中，你输入𝑥 <𝑡> 然后输出𝑦 <𝑡> 。然后为了表示循环连接有时人 们会像这样画个圈，表示输回网络层，有时他们会画一个黑色方块，来表示在这个黑色方块 处会延迟一个时间步。我个人认为这些循环图很难理解，所以在本次课程中，我画图更倾向 于使用左边这种分布画法（上图编号 1 所示）。不过如果你在教材中或是研究论文中看到了 右边这种图表的画法（上图编号 2 所示），它可以在心中将这图展开成左图那样。

循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的，所以下页幻灯 片中我们会详细讲述它的一套参数，我们用𝑊 来表示管理着从𝑥<1> 到隐藏层的连接的一系 ax

531 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

列参数，每个时间步使用的都是相同的参数𝑊 。而激活值也就是水平联系是由参数𝑊 决 ax 𝑎𝑎 定的，同时每一个时间步都使用相同的参数𝑊 ，同样的输出结果由𝑊 决定。下图详细讲述 𝑎𝑎 ya 这些参数是如何起作用。

在这个循环神经网络中，它的意思是在预测𝑦 时，不仅要使用𝑥 <3> 的信息，还要使用 来自𝑥<1> 和𝑥<2> 的信息，因为来自𝑥<1> 的信息可以通过这样的路径（上图编号 1 所示的路 径）来帮助预测𝑦 。这个循环神经网络的一个缺点就是它只使用了这个序列中之前的信息

来做出预测，尤其当预测𝑦 时，它没有用到𝑥<4> ，𝑥<5> ，𝑥<6> 等等的信息。所以这就有一 个问题，因为如果给定了这个句子，「Teddy Roosevelt was a great President.」，为了判断 Teddy 是否是人名的一部分，仅仅知道句中前两个词是完全不够的，还需要知道句中后部分的信息，这也是十分有用的，因为句子也可能是这样的，「Teddy bears are on sale!」。因此如果只给定 前三个单词，是不可能确切地知道 Teddy 是否是人名的一部分，第一个例子是人名，第二个 例子就不是，所以你不可能只看前三个单词就能分辨出其中的区别。所以这样特定的神经网络结构的一个限制是它在某一时刻的预测仅使用了从序列之前 的输入信息并没有使用序列中后部分的信息，我们会在之后的双向循环神经网络（BRNN） 的视频中处理这个问题。但对于现在，这个更简单的单向神经网络结构就够我们来解释关键 概念了，之后只要在此基础上作出修改就能同时使用序列中前面和后面的信息来预测𝑦 ，

不过我们会在之后的视频讲述这些内容，接下来我们具体地写出这个神经网络计算了些什 么。

532 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

这里是一张清理后的神经网络示意图，和我之前提及的一样，一般开始先输入𝑎 <0> ，它 是一个零向量。接着就是前向传播过程，先计算激活值𝑎 <1> ，然后再计算𝑦<1> 。

𝑎 <1> = 𝑔 1 (𝑊 𝑎 <0> + 𝑊 𝑥 <1> + 𝑏 𝑎 ) 𝑎𝑎 𝑎𝑥 

𝑦 <1> = 𝑔 2 (𝑊 𝑎 <1> + 𝑏 𝑦 ) 𝑦𝑎 

我将用这样的符号约定来表示这些矩阵下标，举个例子𝑊 ，第二个下标意味着𝑊 要乘 ax ax 以某个𝑥类型的量，然后第一个下标𝑎表示它是用来计算某个𝑎类型的变量。同样的，可以看 出这里的𝑊 乘上了某个𝑎类型的量，用来计算出某个𝑦类型的量。ya

循环神经网络用的激活函数经常是 tanh，不过有时候也会用 ReLU，但是 tanh 是更通常 的选择，我们有其他方法来避免梯度消失问题，我们将在之后进行讲述。选用哪个激活函数 是取决于你的输出𝑦，如果它是一个二分问题，那么我猜你会用 sigmoid 函数作为激活函数，如果是𝑘类别分类问题的话，那么可以选用 softmax 作为激活函数。不过这里激活函数的类 型取决于你有什么样类型的输出𝑦，对于命名实体识别来说𝑦只可能是 0 或者 1，那我猜这里 第二个激活函数𝑔可以是 sigmoid 激活函数。

更一般的情况下，在𝑡时刻，

𝑎 <𝑡> = 𝑔 1 (𝑊 𝑎 <𝑡−1> + 𝑊 𝑥 <𝑡> + 𝑏 𝑎 ) 𝑎𝑎 𝑎𝑥 

𝑦 <𝑡> = 𝑔 2 (𝑊 𝑎 <𝑡> + 𝑏 𝑦 ) 𝑦𝑎 

所以这些等式定义了神经网络的前向传播，你可以从零向量𝑎 <0> 开始，然后用𝑎 <0> 和 𝑥<1> 来计算出𝑎 <1> 和𝑦 ，然后用𝑥<2> 和𝑎 <1> 一起算出𝑎<2> 和𝑦 等等，像图中这样，从

左到右完成前向传播。

533 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

现在为了帮我们建立更复杂的神经网络，我实际要将这个符号简化一下，我在下一张幻 灯片里复制了这两个等式（上图编号 1 所示的两个等式）。

接下来为了简化这些符号，我要将这部分（𝑊 𝑎 <𝑡−1> + 𝑊 𝑥 <𝑡> ）（上图编号 1 所示） aa ax 以更简单的形式写出来，我把它写做𝑎 <𝑡> = 𝑔(𝑊 [𝑎 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑎 )（上图编号 2 所示），𝑎

那么左右两边划线部分应该是等价的。所以我们定义𝑊 的方式是将矩阵𝑊 和矩阵𝑊 水平 𝑎 𝑎𝑎 𝑎𝑥 并列放置，[𝑊 ⋮ 𝑊] = 𝑊 （上图编号 3 所示）。举个例子，如果𝑎是 100 维的，然后延续 𝑎𝑎 𝑎𝑥 𝑎

之前的例子，𝑥是 10,000 维的，那么𝑊 就是个 （100，100） 维的矩阵，𝑊 就是个 𝑎𝑎 𝑎𝑥 （100，10,000）维的矩阵，因此如果将这两个矩阵堆起来，𝑊 就会是个（100，10,100） 𝑎 维的矩阵。

用这个符号（[𝑎 <𝑡−1> , 𝑥 <𝑡> ]）的意思是将这两个向量堆在一起，我会用这个符号表示，即 [𝑎 <𝑡−1> 最终这就是个 10,100 维的向量。你可以自己检查一下，用 <𝑡> ]（上图编号 4 所示），

𝑥 ⋮ 𝑊 ] 乘以 [𝑎 <𝑡−1> 这个矩阵乘以这个向量，刚好能够得到原来的量，因为此时，矩阵 [𝑊 𝑎𝑎 𝑎𝑥 <𝑡> ]，𝑥 刚好等于𝑊 𝑎 <𝑡−1> + 𝑊 𝑥 <𝑡> ，刚好等于之前的这个结论（上图编号 5 所示）。这种记法 𝑎𝑎 𝑎𝑥 的好处是我们可以不使用两个参数矩阵𝑊 和𝑊 ，而是将其压缩成一个参数矩阵𝑊 ，所以 𝑎𝑎 𝑎𝑥 𝑎 当我们建立更复杂模型时这就能够简化我们要用到的符号。

同样对于这个例子（𝑦 = 𝑔(𝑊 𝑎 <𝑡> + 𝑏 𝑦 )），我会用更简单的方式重写，𝑦 <𝑡> = 𝑦𝑎

𝑔(𝑊 𝑎 <𝑡> + 𝑏 𝑦 )（上图编号 6 所示）。现在𝑊 和𝑏 𝑦 符号仅有一个下标，它表示在计算时会输 𝑦 𝑦

出什么类型的量，所以𝑊 就表明它是计算 y 类型的量的权重矩阵，而上面的𝑊 和𝑏 𝑎 则表示这 𝑦 𝑎 些参数是用来计算𝑎类型或者说是激活值的。

534 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

RNN 前向传播示意图：

你现在知道了基本的循环神经网络，下节课我们会一起来讨论反向传播，以及你如何能 够用 RNN 进行学习。

535 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

1.4 通过时间的反向传播（Backpropagation through time）

之前我们已经学过了循环神经网络的基础结构，在本节视频中我们将来了解反向传播是 怎样在循环神经网络中运行的。和之前一样，当你在编程框架中实现循环神经网络时，编程 框架通常会自动处理反向传播。但我认为，在循环神经网络中，对反向传播的运行有一个粗 略的认识还是非常有用的，让我们来一探究竟。

在之前你已经见过对于前向传播（上图蓝色箭头所指方向）怎样在神经网络中从左到右 地计算这些激活项，直到输出所有地预测结果。而对于反向传播，我想你已经猜到了，反向 传播地计算方向（上图红色箭头所指方向）与前向传播基本上是相反的。

我们来分析一下前向传播的计算，现在你有一个输入序列，𝑥 <1> ，𝑥 <2> ，𝑥 <3> 一直到 𝑥<𝑇 𝑥> ，然后用𝑥 <1> 还有𝑎 <0> 计算出时间步 1 的激活项，再用𝑥<2> 和𝑎 <1> 计算出𝑎 <2> ，然后 计算𝑎 <3> 等等，一直到𝑎<𝑇 𝑥> 。

536 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

为了真正计算出𝑎 <1> ，你还需要一些参数，𝑊 和𝑏 𝑎 ，用它们来计算出𝑎 <1> 。这些参数 𝑎 在之后的每一个时间步都会被用到，于是继续用这些参数计算𝑎 <2> ，𝑎 <3> 等等，所有的这些 激活项都要取决于参数𝑊 和𝑏 𝑎 。有了𝑎 <1> ，神经网络就可以计算第一个预测值𝑦 ，接着 𝑎

到下一个时间步，继续计算出𝑦 ，𝑦 和𝑏 𝑦 ，它们将被用于所有这些节点。

，等等，一直到𝑦

。为了计算出𝑦，需要参数𝑊 𝑦

然后为了计算反向传播，你还需要一个损失函数。我们先定义一个元素损失函数（上图 编号 1 所示）

𝐿 <𝑡> (𝑦 , 𝑦 <𝑡> ) = −𝑦 <𝑡> log 𝑦 <𝑡> − (1 − 𝑦 <𝑡> )𝑙𝑜𝑔(1 − 𝑦 <𝑡> ) 

它对应的是序列中一个具体的词，如果它是某个人的名字，那么𝑦 <𝑡> 的值就是 1，然后 神经网络将输出这个词是名字的概率值，比如 0.1。我将它定义为标准逻辑回归损失函数，也叫交叉熵损失函数（Cross Entropy Loss），它和之前我们在二分类问题中看到的公式很像。所以这是关于单个位置上或者说某个时间步𝑡上某个单词的预测值的损失函数。

现在我们来定义整个序列的损失函数，将𝐿定义为（上图编号 2 所示）

𝑇 𝑥 𝐿(𝑦 , 𝑦) = ∑ 𝐿 <𝑡> (𝑦 , 𝑦 <𝑡> ) 𝑡=1 

在这个计算图中，通过𝑦 可以计算对应的损失函数，于是计算出第一个时间步的损失 函数（上图编号 3 所示），然后计算出第二个时间步的损失函数，然后是第三个时间步，一 直到最后一个时间步，最后为了计算出总体损失函数，我们要把它们都加起来，通过下面的 等式（上图编号 2 所示的等式）计算出最后的𝐿（上图编号 4 所示），也就是把每个单独时

537 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

间步的损失函数都加起来。这就是完整的计算图，在之前的例子中，你已经见过反向传播，所以你应该能够想得到 反向传播算法需要在相反的方向上进行计算和传递信息，最终你做的就是把前向传播的箭头 都反过来，在这之后你就可以计算出所有合适的量，然后你就可以通过导数相关的参数，用 梯度下降法来更新参数。

在这个反向传播的过程中，最重要的信息传递或者说最重要的递归运算就是这个从右到 左的运算，这也就是为什么这个算法有一个很别致的名字，叫做「通过（穿越）时间反向传 播（backpropagation through time）」。取这个名字的原因是对于前向传播，你需要从左到右 进行计算，在这个过程中，时刻𝑡不断增加。而对于反向传播，你需要从右到左进行计算，就像时间倒流。「通过时间反向传播」，就像穿越时光，这种说法听起来就像是你需要一台时 光机来实现这个算法一样。

RNN 反向传播示意图：

希望你大致了解了前向和反向传播是如何在 RNN 中工作的，到目前为止，你只见到了 RNN 中一个主要的例子，其中输入序列的长度和输出序列的长度是一样的。在下节课将展示 更多的 RNN 架构，这将让你能够处理一些更广泛的应用。

538 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

1.5 不同类型的循环神经网络（Different types of RNNs）

现在你已经了解了一种 RNN 结构，它的输入量𝑇 𝑥 等于输出数量𝑇 𝑦 。事实上，对于其他 一些应用，𝑇 𝑥 和𝑇 𝑦 并不一定相等。在这个视频里，你会看到更多的 RNN 的结构。

你应该还记得这周第一个视频中的那个幻灯片，那里有很多例子输入𝑥和输出𝑦，有各种 类型，并不是所有的情况都满足𝑇 𝑥 = 𝑇 𝑦 。

比如音乐生成这个例子，𝑇 𝑥 可以是长度为 1 甚至为空集。再比如电影情感分类，输出𝑦 可以是 1 到 5 的整数，而输入是一个序列。在命名实体识别中，这个例子中输入长度和输出 长度是一样的。

还有一些情况，输入长度和输出长度不同，他们都是序列但长度不同，比如机器翻译，一个法语句子和一个英语句子不同数量的单词却能表达同一个意思。

所以我们应该修改基本的 RNN 结构来处理这些问题，这个视频的内容参考了 Andrej Karpathy 的博客，一篇叫做《循环神经网络的非理性效果》（「The Unreasonable Effectiveness of Recurrent Neural Networks」）的文章，我们看一些例子。

你已经见过𝑇 𝑥 = 𝑇 𝑦 的例子了（下图编号 1 所示），也就是我们输入序列𝑥<1> ，𝑥<2> ，一直到𝑥<𝑇 𝑥> ，我们的循环神经网络这样工作，输入𝑥<1> 来计算𝑦 ，𝑦 等等一直到𝑦 。

在原先的图里，我会画一串圆圈表示神经元，大部分时候为了让符号更加简单，此处就以简 单的小圈表示。这个就叫做「多对多」（many-to-many）的结构，因为输入序列有很多的输入，而输出序列也有很多输出。

539 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

现在我们看另外一个例子，假如说，你想处理情感分类问题（下图编号 2 所示），这里 𝑥可能是一段文本，比如一个电影的评论，「These is nothing to like in this movie.」（「这部电影

没什么还看的。」），所以𝑥就是一个序列，而𝑦可能是从 1 到 5 的一个数字，或者是 0 或 1，这代表正面评价和负面评价，而数字 1 到 5 代表电影是 1 星，2 星，3 星，4 星还是 5 星。所以在这个例子中，我们可以简化神经网络的结构，输入𝑥<1> ，𝑥<2> ，一次输入一个单词，如果输入文本是「These is nothing to like in this movie」，那么单词的对应如下图编号 2 所示。我们不再在每个时间上都有输出了，而是让这个 RNN 网络读入整个句子，然后在最后一个 时间上得到输出，这样输入的就是整个句子，所以这个神经网络叫做「多对一」（many-to-one） 结构，因为它有很多输入，很多的单词，然后输出一个数字。

为了完整性，还要补充一个「一对一」（one-to-one）的结构（上图编号 3 所示），这个 可能没有那么重要，这就是一个小型的标准的神经网络，输入𝑥然后得到输出𝑦，我们这个系 列课程的前两个课程已经讨论过这种类型的神经网络了。

540 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

除了「多对一」的结构，也可以有「一对多」（one-to-many）的结构。对于一个「一对多」神 经网络结构的例子就是音乐生成（上图编号 1 所示），事实上，你会在这个课后编程练习中 去实现这样的模型，你的目标是使用一个神经网络输出一些音符。对应于一段音乐，输入𝑥 可以是一个整数，表示你想要的音乐类型或者是你想要的音乐的第一个音符，并且如果你什 么都不想输入，𝑥可以是空的输入，可设为 0 向量。

这样这个神经网络的结构，首先是你的输入𝑥，然后得到 RNN 的输出，第一个值，然后 就没有输入了，再得到第二个输出，接着输出第三个值等等，一直到合成这个音乐作品的最 后一个音符，这里也可以写上输入𝑎<0> （上图编号 3 所示）。有一个后面才会讲到的技术细 节，当你生成序列时通常会把第一个合成的输出也喂给下一层（上图编号 4 所示），所以实 际的网络结构最终就像这个样子。

我们已经讨论了「多对多」、「多对一」、「一对一」和「一对多」的结构，对于「多对多」的结构 还有一个有趣的例子值得详细说一下，就是输入和输出长度不同的情况。你刚才看过的多对 多的例子，它的输入长度和输出长度是完全一样的。而对于像机器翻译这样的应用，输入句 子的单词的数量，比如说一个法语的句子，和输出句子的单词数量，比如翻译成英语，这两 个句子的长度可能不同，所以还需要一个新的网络结构，一个不同的神经网络（上图编号 2 所示）。首先读入这个句子，读入这个输入，比如你要将法语翻译成英语，读完之后，这个 网络就会输出翻译结果。有了这种结构𝑇 𝑥 和𝑇 𝑦 就可以是不同的长度了。同样，你也可以画上 这个𝑎 <0> 。这个网络的结构有两个不同的部分，这（上图编号 5 所示）是一个编码器，获取 输入，比如法语句子，这（上图编号 6 所示）是解码器，它会读取整个句子，然后输出翻译 成其他语言的结果。

541 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

这就是一个「多对多」结构的例子，到这周结束的时候，你就能对这些各种各样结构的基 本构件有一个很好的理解。严格来说，还有一种结构，我们会在第四周涉及到，就是「注意 力」（attention based）结构，但是根据我们现在画的这些图不好理解这个模型。

总结一下这些各种各样的 RNN 结构，这（上图编号 1 所示）是「一对一」的结构，当去 掉𝑎 <0> 时它就是一种标准类型的神经网络。还有一种「一对多」的结构（上图编号 2 所示），比如音乐生成或者序列生成。还有「多对一」，这（上图编号 3 所示）是情感分类的例子，首 先读取输入，一个电影评论的文本，然后判断他们是否喜欢电影还是不喜欢。还有「多对多」的结构（上图编号 4 所示），命名实体识别就是「多对多」的例子，其中𝑇 𝑥 = 𝑇 𝑦 。最后还有一 种「多对多」结构的其他版本（上图编号 5 所示），对于像机器翻译这样的应用，𝑇 𝑥 和𝑇 𝑦 就可 以不同了。

现在，你已经了解了大部分基本的模块，这些就是差不多所有的神经网络了，除了序列 生成，有些细节的问题我们会在下节课讲解。

我希望你从本视频中了解到用这些 RNN 的基本模块，把它们组合在一起就可以构建各 种各样的模型。但是正如我前面提到的，序列生成还有一些不一样的地方，在这周的练习里，你也会实现它，你需要构建一个语言模型，结果好的话会得到一些有趣的序列或者有意思的 文本。下节课深入探讨序列生成。

542 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

1.6 语言模型和序列生成（Language model and sequence generation）

在自然语言处理中，构建语言模型是最基础的也是最重要的工作之一，并且能用 RNN 很好地实现。在本视频中，你将学习用 RNN 构建一个语言模型，在本周结束的时候，还会 有一个很有趣的编程练习，你能在练习中构建一个语言模型，并用它来生成莎士比亚文风的 文本或其他类型文本。

所以什么是语言模型呢？比如你在做一个语音识别系统，你听到一个句子，「the apple and pear（pair） salad was delicious.」，所以我究竟说了什么？我说的是「the apple and pair salad」，还是「the apple and pear salad」？（pear 和 pair 是近音词）。你可能觉得我说的应该 更像第二种，事实上，这就是一个好的语音识别系统要帮助输出的东西，即使这两句话听起 来是如此相似。而让语音识别系统去选择第二个句子的方法就是使用一个语言模型，他能计 算出这两句话各自的可能性。

举个例子，一个语音识别模型可能算出第一句话的概率是:

𝑃(The apple and pair salad) = 3.2 × 10 −13 ，

而第二句话的概率是𝑃(The apple and pear salad) = 5.7 × 10 −10 ，

比较这两个概率值，显然我说的话更像是第二种，因为第二句话的概率比第一句高出 1000 倍以上，这就是为什么语音识别系统能够在这两句话中作出选择。

所以语言模型所做的就是，它会告诉你某个特定的句子它出现的概率是多少，根据我所 说的这个概率，假设你随机拿起一张报纸，打开任意邮件，或者任意网页或者听某人说下一 句话，并且这个人是你的朋友，这个你即将从世界上的某个地方得到的句子会是某个特定句 子的概率是多少，例如「the apple and pear salad」。它是两种系统的基本组成部分，一个刚才

543 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

所说的语音识别系统，还有机器翻译系统，它要能正确输出最接近的句子。而语言模型做的 最基本工作就是输入一个句子，准确地说是一个文本序列，𝑦<1> ，𝑦<2> 一直到𝑦<𝑇 𝑦> 。对于 语言模型来说，用𝑦来表示这些序列比用𝑥来表示要更好，然后语言模型会估计某个句子序列 中各个单词出现的可能性。那么如何建立一个语言模型呢？为了使用 RNN 建立出这样的模型，你首先需要一个训 练集，包含一个很大的英文文本语料库（corpus）或者其它的语言，你想用于构建模型的语 言的语料库。语料库是自然语言处理的一个专有名词，意思就是很长的或者说数量众多的英 文句子组成的文本。

假如说，你在训练集中得到这么一句话，「Cats average 15 hours of sleep a day.」(猫一天 睡 15 小时)，你要做的第一件事就是将这个句子标记化，意思就是像之前视频中一样，建立

一个字典，然后将每个单词都转换成对应的 one-hot 向量，也就是字典中的索引。可能还有 一件事就是你要定义句子的结尾，一般的做法就是增加一个额外的标记，叫做 EOS（上图编 号 1 所示），它表示句子的结尾，这样能够帮助你搞清楚一个句子什么时候结束，我们之后 会详细讨论这个。EOS 标记可以被附加到训练集中每一个句子的结尾，如果你想要你的模型 能够准确识别句子结尾的话。在本周的练习中我们不需要使用这个 EOS 标记，不过在某些 应用中你可能会用到它，不过稍后就能见到它的用处。于是在本例中我们，如果你加了 EOS 标记，这句话就会有 9 个输入，有𝑦<1> ，𝑦<2> 一直到𝑦<9> 。在标记化的过程中，你可以自 行决定要不要把标点符号看成标记，在本例中，我们忽略了标点符号，所以我们只把 day 看 成标志，不包括后面的句号，如果你想把句号或者其他符号也当作标志，那么你可以将句号 也加入你的字典中。现在还有一个问题如果你的训练集中有一些词并不在你的字典里，比如说你的字典有 10,000 个词，10,000 个最常用的英语单词。现在这个句，「The Egyptian Mau is a bread of cat.」

544 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

其中有一个词 Mau，它可能并不是预先的那 10,000 个最常用的单词，在这种情况下，你可 以把 Mau 替换成一个叫做 UNK 的代表未知词的标志，我们只针对 UNK 建立概率模型，而 不是针对这个具体的词 Mau。完成标识化的过程后，这意味着输入的句子都映射到了各个标志上，或者说字典中的各 个词上。下一步我们要构建一个 RNN 来构建这些序列的概率模型。在下一张幻灯片中会看 到的一件事就是最后你会将𝑥<𝑡> 设为𝑦<𝑡−1> 。

现在我们来建立 RNN 模型，我们继续使用「Cats average 15 hours of sleep a day.」这个句 子来作为我们的运行样例，我将会画出一个 RNN 结构。在第 0 个时间步，你要计算激活项

𝑎 <1> ，它是以𝑥<1> 作为输入的函数，而𝑥<1> 会被设为全为 0 的集合，也就是 0 向量。在之 前的𝑎 <0> 按照惯例也设为 0 向量，于是𝑎 <1> 要做的就是它会通过 softmax 进行一些预测来 计算出第一个词可能会是什么，其结果就是𝑦 （上图编号 1 所示），这一步其实就是通过

一个 softmax 层来预测字典中的任意单词会是第一个词的概率，比如说第一个词是𝑎的概率 有多少，第一个词是 Aaron 的概率有多少，第一个词是 cats 的概率又有多少，就这样一直 到 Zulu 是第一个词的概率是多少，还有第一个词是 UNK（未知词）的概率有多少，还有第 一个词是句子结尾标志的概率有多少，表示不必阅读。所以𝑦 的输出是 softmax 的计算结

果，它只是预测第一个词的概率，而不去管结果是什么。在我们的例子中，最终会得到单词 Cats。所以 softmax 层输出 10,000 种结果，因为你的字典中有 10,000 个词，或者会有 10,002 个结果，因为你可能加上了未知词，还有句子结尾这两个额外的标志。

545 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

然后 RNN 进入下个时间步，在下一时间步中，仍然使用激活项𝑎 <1> ，在这步要做的是 计算出第二个词会是什么。现在我们依然传给它正确的第一个词，我们会告诉它第一个词就 是 Cats，也就是𝑦 ，告诉它第一个词就是 Cats，这就是为什么𝑦 <1> = 𝑥<2> （上图编号 2

所示）。然后在第二个时间步中，输出结果同样经过 softmax 层进行预测，RNN 的职责就是 预测这些词的概率（上图编号 3 所示），而不会去管结果是什么，可能是 b 或者 arron，可 能是 Cats 或者 Zulu 或者 UNK（未知词）或者 EOS 或者其他词，它只会考虑之前得到的词。所以在这种情况下，我猜正确答案会是 average，因为句子确实就是 Cats average 开头的。

然后再进行 RNN 的下个时间步，现在要计算𝑎 <3> 。为了预测第三个词，也就是 15，我 们现在给它之前两个词，告诉它 Cats average 是句子的前两个词，所以这是下一个输入，𝑥 <3> = 𝑦<2> ，输入 average 以后，现在要计算出序列中下一个词是什么，或者说计算出字 典中每一个词的概率（上图编号 4 所示），通过之前得到的 Cats 和 average，在这种情况下，正确结果会是 15，以此类推。

一直到最后，没猜错的话，你会停在第 9 个时间步，然后把𝑥 <9> 也就是𝑦<8> 传给它（上 图编号 5 所示），也就是单词 day，这里是𝑎<9> ，它会输出𝑦<9> ，最后的得到结果会是 EOS 标志，在这一步中，通过前面这些得到的单词，不管它们是什么，我们希望能预测出 EOS 句 子结尾标志的概率会很高（上图编号 6 所示）。

所以 RNN 中的每一步都会考虑前面得到的单词，比如给它前 3 个单词（上图编号 7 所 示），让它给出下个词的分布，这就是 RNN 如何学习从左往右地每次预测一个词。

接下来为了训练这个网络，我们要定义代价函数。于是，在某个时间步𝑡，如果真正的 词是𝑦 <𝑡> ，而神经网络的 softmax 层预测结果值是𝑦 <𝑡> ，那么这（上图编号 8 所示）就是 softmax 损失函数，𝐿 (𝑦 , 𝑦 <𝑡> >) = − ∑ 𝑖 𝑦 𝑖 <𝑡> log 𝑦𝑖 <𝑡> 。而总体损失函数（上图编号 9 所

示）𝐿 = ∑ 𝑡 𝐿 <𝑡> (𝑦

, 𝑦 <𝑡> ) ，也就是把所有单个预测的损失函数都相加起来。

如果你用很大的训练集来训练这个 RNN，你就可以通过开头一系列单词像是 Cars average 15 或者 Cars average 15 hours of 来预测之后单词的概率。现在有一个新句子，它是 𝑦<1> ，𝑦<2> ，𝑦 <3> ，为了简单起见，它只包含 3 个词（如上图所示），现在要计算出整个

546 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

句子中各个单词的概率，方法就是第一个 softmax 层会告诉你𝑦<1> 的概率（上图编号 1 所 示），这也是第一个输出，然后第二个 softmax 层会告诉你在考虑𝑦<1> 的情况下𝑦<2> 的概率 （上图编号 2 所示），然后第三个 softmax 层告诉你在考虑𝑦<1> 和𝑦 <2> 的情况下𝑦<3> 的概 率（上图编号 3 所示），把这三个概率相乘，最后得到这个含 3 个词的整个句子的概率。这就是用 RNN 训练一个语言模型的基础结构，可能我说的这些东西听起来有些抽象，不过别担心，你可以在编程练习中亲自实现这些东西。下一节课用语言模型做的一件最有趣 的事就是从模型中进行采样。

547 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

1.7 对新序列采样（Sampling novel sequences）

在你训练一个序列模型之后，要想了解到这个模型学到了什么，一种非正式的方法就是 进行一次新序列采样，来看看到底应该怎么做。

记住一个序列模型模拟了任意特定单词序列的概率，我们要做的就是对这些概率分布进 行采样来生成一个新的单词序列。下图编号 1 所示的网络已经被上方所展示的结构训练训 练过了，而为了进行采样（下图编号 2 所示的网络），你要做一些截然不同的事情。

第一步要做的就是对你想要模型生成的第一个词进行采样，于是你输入𝑥 <1> = 0，𝑎 <0> = 0，现在你的第一个时间步得到的是所有可能的输出是经过 softmax 层后得到的概率，然后根据这个 softmax 的分布进行随机采样。Softmax 分布给你的信息就是第一个词 a 的概 率是多少，第一个词是 aaron 的概率是多少，第一个词是 zulu 的概率是多少，还有第一个词 是 UNK（未知标识）的概率是多少，这个标识可能代表句子的结尾，然后对这个向量使用例 如 numpy 命令，np.random.choice（上图编号 3 所示），来根据向量中这些概率的分布 进行采样，这样就能对第一个词进行采样了。

然后继续下一个时间步，记住第二个时间步需要𝑦 作为输入，而现在要做的是把刚刚

采样得到的𝑦 放到𝑎<2> （上图编号 4 所示），作为下一个时间步的输入，所以不管你在第 一个时间步得到的是什么词，都要把它传递到下一个位置作为输入，然后 softmax 层就会预 测𝑦 是什么。举个例子，假如说对第一个词进行抽样后，得到的是 The，The 作为第一个

548 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

词的情况很常见，然后把 The 当成𝑥<2> ，现在𝑥<2> 就是𝑦

，现在你要计算出在第一词是

The 的情况下，第二个词应该是什么（上图编号 5 所示），然后得到的结果就是𝑦

，然后

再次用这个采样函数来对𝑦 进行采样。然后再到下一个时间步，无论你得到什么样的用 one-hot 码表示的选择结果，都把它传 递到下一个时间步，然后对第三个词进行采样。不管得到什么都把它传递下去，一直这样直 到最后一个时间步。

那么你要怎样知道一个句子结束了呢？方法之一就是，如果代表句子结尾的标识在你的 字典中，你可以一直进行采样直到得到 EOS 标识（上图编号 6 所示），这代表着已经抵达结 尾，可以停止采样了。另一种情况是，如果你的字典中没有这个词，你可以决定从 20 个或 100 个或其他个单词进行采样，然后一直将采样进行下去直到达到所设定的时间步。不过这 种过程有时候会产生一些未知标识（上图编号 7 所示），如果你要确保你的算法不会输出这 种标识，你能做的一件事就是拒绝采样过程中产生任何未知的标识，一旦出现就继续在剩下 的词中进行重采样，直到得到一个不是未知标识的词。如果你不介意有未知标识产生的话，你也可以完全不管它们。

这就是你如何从你的 RNN 语言模型中生成一个随机选择的句子。直到现在我们所建立 的是基于词汇的 RNN 模型，意思就是字典中的词都是英语单词（下图编号 1 所示）。

根据你实际的应用，你还可以构建一个基于字符的 RNN 结构，在这种情况下，你的字 典仅包含从 a 到 z 的字母，可能还会有空格符，如果你需要的话，还可以有数字 0 到 9，如

549 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

果你想区分字母大小写，你可以再加上大写的字母，你还可以实际地看一看训练集中可能会 出现的字符，然后用这些字符组成你的字典（上图编号 2 所示）。如果你建立一个基于字符的语言模型，比起基于词汇的语言模型，你的序列𝑦 ，𝑦<2> ，

𝑦<3> 在你的训练数据中将会是单独的字符，而不是单独的词汇。所以对于前面的例子来说，

那个句子（上图编号 3 所示），「Cats average 15 hours of sleep a day.」，在该例中 C 就是𝑦

，

a 就是𝑦

，t 就是𝑦 ，空格符就是𝑦 等等。使用基于字符的语言模型有有点也有缺点，优点就是你不必担心会出现未知的标识，例 如基于字符的语言模型会将 Mau 这样的序列也视为可能性非零的序列。而对于基于词汇的 语言模型，如果 Mau 不在字典中，你只能把它当作未知标识 UNK。不过基于字符的语言模 型一个主要缺点就是你最后会得到太多太长的序列，大多数英语句子只有 10 到 20 个的单 词，但却可能包含很多很多字符。所以基于字符的语言模型在捕捉句子中的依赖关系也就是 句子较前部分如何影响较后部分不如基于词汇的语言模型那样可以捕捉长范围的关系，并且 基于字符的语言模型训练起来计算成本比较高昂。所以我见到的自然语言处理的趋势就是，绝大多数都是使用基于词汇的语言模型，但随着计算机性能越来越高，会有更多的应用。在 一些特殊情况下，会开始使用基于字符的模型。但是这确实需要更昂贵的计算力来训练，所 以现在并没有得到广泛地使用，除了一些比较专门需要处理大量未知的文本或者未知词汇的 应用，还有一些要面对很多专有词汇的应用。

在现有的方法下，现在你可以构建一个 RNN 结构，看一看英文文本的语料库，然后建 立一个基于词汇的或者基于字符的语言模型，然后从训练的语言模型中进行采样。

550 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

这里有一些样本，它们是从一个语言模型中采样得到的，准确来说是基于字符的语言模 型，你可以在编程练习中自己实现这样的模型。如果模型是用新闻文章训练的，它就会生成 左边这样的文本，这有点像一篇不太合乎语法的新闻文本，不过听起来，这句「Concussion epidemic」，to be examined，确实有点像新闻报道。用莎士比亚的文章训练后生成了右边这 篇东西，听起来很像是莎士比亚写的东西：

「The mortal moon hath her eclipse in love. 

And subject of this thou art another this fold. 

When besser be my love to me see sabl's. 

For whose are ruse of mine eyes heaves.」

这些就是基础的 RNN 结构和如何去建立一个语言模型并使用它，对于训练出的语言模 型进行采样。在之后的视频中，我想探讨在训练 RNN 时一些更加深入的挑战以及如何适应 这些挑战，特别是梯度消失问题来建立更加强大的 RNN 模型。下节课，我们将谈到梯度消 失并且会开始谈到 GRU，也就是门控循环单元和 LSTM 长期记忆网络模型。

551 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

1.8 循环神经网络的梯度消失（Vanishing gradients with RNNs）

你已经了解了 RNN 时如何工作的了，并且知道如何应用到具体问题上，比如命名实体 识别，比如语言模型，你也看到了怎么把反向传播用于 RNN。其实，基本的 RNN 算法还有 一个很大的问题，就是梯度消失的问题。这节课我们会讨论，在下几节课我们会讨论一些方 法用来解决这个问题。

你已经知道了 RNN 的样子，现在我们举个语言模型的例子，假如看到这个句子（上图 编号 1 所示），「The cat, which already ate ……, was full.」，前后应该保持一致，因为 cat 是单 数，所以应该用 was。「The cats, which ate ……, were full.」（上图编号 2 所示），cats 是复数，

所以用 were。这个例子中的句子有长期的依赖，最前面的单词对句子后面的单词有影响。但是我们目前见到的基本的 RNN 模型（上图编号 3 所示的网络模型），不擅长捕获这种长 期依赖效应，解释一下为什么。

你应该还记得之前讨论的训练很深的网络，我们讨论了梯度消失的问题。比如说一个很 深很深的网络（上图编号 4 所示），100 层，甚至更深，对这个网络从左到右做前向传播然 后再反向传播。我们知道如果这是个很深的神经网络，从输出𝑦得到的梯度很难传播回去，很难影响靠前层的权重，很难影响前面层（编号 5 所示的层）的计算。

对于有同样问题的 RNN，首先从左到右前向传播，然后反向传播。但是反向传播会很困 难，因为同样的梯度消失的问题，后面层的输出误差（上图编号 6 所示）很难影响前面层

552 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

（上图编号 7 所示的层）的计算。这就意味着，实际上很难让一个神经网络能够意识到它要 记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式的 was 或者 were。而且在英语里面，这中间的内容（上图编号 8 所示）可以任意长，对吧？所以你需要 长时间记住单词是单数还是复数，这样后面的句子才能用到这些信息。也正是这个原因，所 以基本的 RNN 模型会有很多局部影响，意味着这个输出𝑦 （上图编号 9 所示）主要受𝑦

附近的值（上图编号 10 所示）的影响，上图编号 11 所示的一个数值主要与附近的输入（上 图编号 12 所示）有关，上图编号 6 所示的输出，基本上很难受到序列靠前的输入（上图编 号 10 所示）的影响，这是因为不管输出是什么，不管是对的，还是错的，这个区域都很难 反向传播到序列的前面部分，也因此网络很难调整序列前面的计算。这是基本的 RNN 算法 的一个缺点，我们会在下几节视频里处理这个问题。如果不管的话，RNN 会不擅长处理长期 依赖的问题。

尽管我们一直在讨论梯度消失问题，但是，你应该记得我们在讲很深的神经网络时，我 们也提到了梯度爆炸，我们在反向传播的时候，随着层数的增多，梯度不仅可能指数型的下 降，也可能指数型的上升。事实上梯度消失在训练 RNN 时是首要的问题，尽管梯度爆炸也 是会出现，但是梯度爆炸很明显，因为指数级大的梯度会让你的参数变得极其大，以至于你 的网络参数崩溃。所以梯度爆炸很容易发现，因为参数会大到崩溃，你会看到很多 NaN，或 者不是数字的情况，这意味着你的网络计算出现了数值溢出。如果你发现了梯度爆炸的问题，一个解决方法就是用梯度修剪。梯度修剪的意思就是观察你的梯度向量，如果它大于某个阈 值，缩放梯度向量，保证它不会太大，这就是通过一些最大值来修剪的方法。所以如果你遇 到了梯度爆炸，如果导数值很大，或者出现了 NaN，就用梯度修剪，这是相对比较鲁棒的，这是梯度爆炸的解决方法。然而梯度消失更难解决，这也是我们下几节视频的主题。

总结一下，在前面的课程，我们了解了训练很深的神经网络时，随着层数的增加，导数 有可能指数型的下降或者指数型的增加，我们可能会遇到梯度消失或者梯度爆炸的问题。加 入一个 RNN 处理 1,000 个时间序列的数据集或者 10,000 个时间序列的数据集，这就是一个 1,000 层或者 10,000 层的神经网络，这样的网络就会遇到上述类型的问题。梯度爆炸基本上 用梯度修剪就可以应对，但梯度消失比较棘手。我们下节会介绍 GRU，门控循环单元网络，这个网络可以有效地解决梯度消失的问题，并且能够使你的神经网络捕获更长的长期依赖。

553 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

1.9 GRU 单元（Gated Recurrent Unit（GRU））

你已经了解了基础的 RNN 模型的运行机制，在本节视频中你将会学习门控循环单元，它改变了 RNN 的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题，让我们 看一看。

你已经见过了这个公式，𝑎 <𝑡> = 𝑔(𝑊 [𝑎 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑎 )，在 RNN 的时间𝑡处，计算激 𝑎 活值。我把这个画个图，把 RNN 的单元画个图，画一个方框，输入𝑎<𝑡−1> （上图编号 1 所 示），即上一个时间步的激活值，再输入𝑥<𝑡> （上图编号 2 所示），再把这两个并起来，然 后乘上权重项，在这个线性计算之后（上图编号 3 所示），如果𝑔是一个 tanh 激活函数，再 经过 tanh 计算之后，它会计算出激活值𝑎 <𝑡> 。然后激活值𝑎 <𝑡> 将会传 softmax 单元（上图 编号 4 所示），或者其他用于产生输出𝑦<𝑡> 的东西。就这张图而言，这就是 RNN 隐藏层的 单元的可视化呈现。我向展示这张图，因为我们将使用相似的图来讲解门控循环单元。

554 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

许多 GRU 的想法都来分别自于 Yu Young Chang, Kagawa，Gaza Hera, Chang Hung Chu 和 Jose Banjo 的两篇论文。我再引用上个视频中你已经见过的这个句子，「The cat, which already ate……, was full.」，你需要记得猫是单数的，为了确保你已经理解了为什么这里是 was 而不 是 were，「The cat was full.」或者是「The cats were full」。当我们从左到右读这个句子，GRU 单 元将会有个新的变量称为𝑐，代表细胞（cell），即记忆细胞（下图编号 1 所示）。记忆细胞 的作用是提供了记忆的能力，比如说一只猫是单数还是复数，所以当它看到之后的句子的时 候，它仍能够判断句子的主语是单数还是复数。于是在时间𝑡处，有记忆细胞𝑐<𝑡> ，然后我 们看的是，GRU 实际上输出了激活值𝑎 <𝑡> ，𝑐 <𝑡> = 𝑎<𝑡> （下图编号 2 所示）。于是我们想 要使用不同的符号𝑐和𝑎来表示记忆细胞的值和输出的激活值，即使它们是一样的。我现在使 用这个标记是因为当我们等会说到 LSTMs 的时候，这两个会是不同的值，但是现在对于 GRU，𝑐<𝑡> 的值等于𝑎<𝑡> 的激活值。

所以这些等式表示了 GRU 单元的计算，在每个时间步，我们将用一个候选值重写记忆 细胞，即𝑐̃ <𝑡> 的值，所以它就是个候选值，替代了𝑐<𝑡> 的值。然后我们用 tanh 激活函数来 计算，𝑐̃ = 𝑡𝑎𝑛ℎ(𝑊 [𝑐 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑐 )，所以𝑐̃ <𝑡> 的值就是个替代值，代替表示𝑐<𝑡> 的值 𝑐 （下图编号 3 所示）。

重点来了，在 GRU 中真正重要的思想是我们有一个门，我先把这个门叫做𝛤 （上图编 𝑢 号 4 所示），这是个下标为𝑢的大写希腊字母𝛤，𝑢代表更新门，这是一个 0 到 1 之间的值。为了让你直观思考 GRU 的工作机制，先思考𝛤 ，这个一直在 0 到 1 之间的门值，实际上这 𝑢 个值是把这个式子带入 sigmoid 函数得到的，𝛤 = 𝜎(𝑊 [𝑐 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑢 )。我们还记得 𝑢 𝑢 sigmoid 函数是上图编号 5 所示这样的，它的输出值总是在 0 到 1 之间，对于大多数可能的 输入，sigmoid 函数的输出总是非常接近 0 或者非常接近 1。在这样的直觉下，可以想到𝛤 𝑢

555 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

在大多数的情况下非常接近 0 或 1。然后这个字母 u 表示「update」，我选了字母𝛤是因为它 看起来像门。还有希腊字母 G，G 是门的首字母，所以 G 表示门。然后 GRU 的关键部分就是上图编号 3 所示的等式，我们刚才写出来的用𝑐̃更新𝑐的等式。然后门决定是否要真的更新它。于是我们这么看待它，记忆细胞𝑐<𝑡> 将被设定为 0 或者 1，这取决于你考虑的单词在句子中是单数还是复数，因为这里是单数情况，所以我们先假定它 被设为了 1，或者如果是复数的情况我们就把它设为 0。然后 GRU 单元将会一直记住𝑐<𝑡> 的 值，直到上图编号 7 所示的位置，𝑐<𝑡> 的值还是 1，这就告诉它，噢，这是单数，所以我们 用 was。于是门𝛤 的作用就是决定什么时候你会更新这个值，特别是当你看到词组 the cat，𝑢 即句子的主语猫，这就是一个好时机去更新这个值。然后当你使用完它的时候，「The cat, which already ate……, was full.」，然后你就知道，我不需要记住它了，我可以忘记它了。

所以我们接下来要给 GRU 用的式子就是𝑐 <𝑡> = 𝛤 ∗ 𝑐̃ + (1 − 𝛤) ∗ 𝑐<𝑡−1> （上图编号 𝑢 𝑢 1 所示）。你应该注意到了，如果这个更新值𝛤 = 1，也就是说把这个新值，即𝑐<𝑡> 设为候 𝑢

选值（𝛤 = 1 时简化上式，𝑐 <𝑡> = 𝑐̃ ）。将门值设为 1（上图编号 2 所示），然后往前再 𝑢 更新这个值。对于所有在这中间的值，你应该把门的值设为 0，即𝛤 = 0，意思就是说不更 𝑢 新它，就用旧的值。因为如果𝛤 = 0，则𝑐 <𝑡> = 𝑐 <𝑡−1> ，𝑐<𝑡> 等于旧的值。甚至你从左到右 𝑢 扫描这个句子，当门值为 0 的时候（上图编号 3 所示，中间𝛤 = 0 一直为 0，表示一直不更 𝑢 新），就是说不更新它的时候，不要更新它，就用旧的值，也不要忘记这个值是什么，这样 即使你一直处理句子到上图编号 4 所示，𝑐 <𝑡> 应该会一直等𝑐<𝑡−1> ，于是它仍然记得猫是单 数的。让我再画个图来（下图所示）解释一下 GRU 单元，顺便说一下，当你在看网络上的博

556 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

客或者教科书或者教程之类的，这些图对于解释 GRU 和我们稍后会讲的 LSTM 是相当流行 的，我个人感觉式子在图片中比较容易理解，那么即使看不懂图片也没关系，我就画画，万 一能帮得上忙就最好了。GRU 单元输入𝑐<𝑡−1> （下图编号 1 所示），对于上一个时间步，先假设它正好等于𝑎<𝑡−1> ，所以把这个作为输入。然后𝑥<𝑡> 也作为输入（下图编号 2 所示），然后把这两个用合适权重 结合在一起，再用𝑡𝑎𝑛ℎ计算，算出𝑐̃ <𝑡> ，𝑐̃ = 𝑡𝑎𝑛ℎ(𝑊 [𝑐 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑐 )，即𝑐<𝑡> 的替代 𝑐 值。

再用一个不同的参数集，通过 sigmoid 激活函数算出𝛤 𝑢 ，𝛤 = 𝜎(𝑊 [𝑐 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑢 )，𝑢 𝑢

即更新门。最后所有的值通过另一个运算符结合，我并不会写出公式，但是我用紫色阴影标 注的这个方框（下图编号 5 所示，其所代表的运算过程即下图编号 13 所示的等式），代表 了这个式子。所以这就是紫色运算符所表示的是，它输入一个门值（下图编号 6 所示），新 的候选值（下图编号 7 所示），这再有一个门值（下图编号 8 所示）和𝑐 <𝑡> 的旧值（下图编 号 9 所示），所以它把这个（下图编号 1 所示）、这个（下图编号 3 所示）和这个（下图编 号 4 所示）作为输入一起产生记忆细胞的新值𝑐 <𝑡> ，所以𝑐 <𝑡> 等于𝑎 <𝑡> 。如果你想，你也可 以也把这个带入 softmax 或者其他预测𝑦<𝑡> 的东西。

这就是 GRU 单元或者说是一个简化过的 GRU 单元，它的优点就是通过门决定，当你从 左（上图编号 10 所示）到右扫描一个句子的时候，这个时机是要更新某个记忆细胞，还是 不更新，不更新（上图编号 11 所示，中间𝛤 = 0 一直为 0，表示一直不更新）直到你到你真 𝑢 的需要使用记忆细胞的时候（上图编号 12 所示），这可能在句子之前就决定了。因为 sigmoid 的值，现在因为门很容易取到 0 值，只要这个值是一个很大的负数，再由于数值上的四舍五

557 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

入，上面这些门大体上就是 0，或者说非常非常非常接近 0。所以在这样的情况下，这个更 新式子（上图编号 13 所示的等式）就会变成𝑐 <𝑡> = 𝑐<𝑡−1> ，这非常有利于维持细胞的值。因为𝛤 很接近 0，可能是 0.000001 或者更小，这就不会有梯度消失的问题了。因为𝛤 很接近 𝑢 𝑢 0，这就是说𝑐<𝑡> 几乎就等于𝑐<𝑡−1> ，而且𝑐 <𝑡> 的值也很好地被维持了，即使经过很多很多的 时间步（上图编号 14 所示）。这就是缓解梯度消失问题的关键，因此允许神经网络运行在 非常庞大的依赖词上，比如说 cat 和 was 单词即使被中间的很多单词分割开。

现在我想说下一些实现的细节，在这个我写下的式子中𝑐<𝑡> 可以是一个向量（上图编号 1 所示），如果你有 100 维的隐藏的激活值，那么𝑐 <𝑡> 也是 100 维的，𝑐̃ 也是相同的维度 （𝑐̃ <𝑡> = 𝑡𝑎𝑛ℎ(𝑊 [𝑐 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑐 )），𝛤 也是相同的维度（𝛤 = 𝜎(𝑊 [𝑐 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑢 )），𝑐 𝑢 𝑢 𝑢

还有画在框中的其他值。这样的话「*」实际上就是元素对应的乘积（𝑐 <𝑡> = 𝛤 ∗ 𝑐̃ + (1 𝑢 𝛤) ∗ 𝑐<𝑡−1> ），所以这里的𝛤 ：（𝛤 = 𝜎(𝑊 [𝑐 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑢 )），即如果门是一个 100 维 𝑢 𝑢 𝑢 𝑢

的向量，𝛤 也就 100 维的向量，里面的值几乎都是 0 或者 1，就是说这 100 维的记忆细胞 𝑢 𝑐<𝑡> （𝑐 <𝑡> = 𝑎<𝑡> 上图编号 1 所示）就是你要更新的比特。

当然在实际应用中𝛤 不会真的等于 0 或者 1，有时候它是 0 到 1 的一个中间值（上图编 𝑢 号 5 所示），但是这对于直观思考是很方便的，就把它当成确切的 0，完全确切的 0 或者就 是确切的 1。元素对应的乘积做的就是告诉 GRU 单元哪个记忆细胞的向量维度在每个时间 步要做更新，所以你可以选择保存一些比特不变，而去更新其他的比特。比如说你可能需要 一个比特来记忆猫是单数还是复数，其他比特来理解你正在谈论食物，因为你在谈论吃饭或 者食物，然后你稍后可能就会谈论「The cat was full.」，你可以每个时间点只改变一些比特。

你现在已经理解 GRU 最重要的思想了，幻灯片中展示的实际上只是简化过的 GRU 单元，

558 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

现在来描述一下完整的 GRU 单元。对于完整的 GRU 单元我要做的一个改变就是在我们计算的第一个式子中给记忆细胞的 新候选值加上一个新的项，我要添加一个门𝛤 （下图编号 1 所示），你可以认为𝑟代表相关 𝑟 性（relevance）。这个𝛤 门告诉你计算出的下一个𝑐<𝑡> 的候选值𝑐̃<𝑡> 跟𝑐<𝑡−1> 有多大的相关 𝑟 性 。计 算 这 个 门 𝛤 需 要 参 数 ，正 如 你 看 到 的 这 个 ，一 个 新 的 参 数 矩 阵 𝑊 ，𝛤 = 𝑟 𝑟 𝑟

𝜎(𝑊 [𝑐 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑟 )。𝑟 

正如你所见，有很多方法可以来设计这些类型的神经网络，然后我们为什么有𝛤 ？为什 𝑟 么不用上一张幻灯片里的简单的版本？这是因为多年来研究者们试验过很多很多不同可能 的方法来设计这些单元，去尝试让神经网络有更深层的连接，去尝试产生更大范围的影响，还有解决梯度消失的问题，GRU 就是其中一个研究者们最常使用的版本，也被发现在很多不 同的问题上也是非常健壮和实用的。你可以尝试发明新版本的单元，只要你愿意。但是 GRU 是一个标准版本，也就是最常使用的。你可以想象到研究者们也尝试了很多其他版本，类似 这样的但不完全是，比如我这里写的这个。然后另一个常用的版本被称为 LSTM，表示长短 时记忆网络，这个我们会在下节视频中讲到，但是 GRU 和 LSTM 是在神经网络结构中最常 用的两个具体实例。

还有在符号上的一点，我尝试去定义固定的符号让这些概念容易理解，如果你看学术文 章的话，你有的时候会看到有些人使用另一种符号𝑥̃，𝑢，𝑟和ℎ表示这些量。但我试着在 GRU 和 LSTM 之间用一种更固定的符号，比如使用更固定的符号𝛤来表示门，所以希望这能让这 些概念更好理解。

所以这就是 GRU，即门控循环单元，这是 RNN 的其中之一。这个结构可以更好捕捉非 常长范围的依赖，让 RNN 更加有效。然后我简单提一下其他常用的神经网络，比较经典的 是这个叫做 LSTM，即长短时记忆网络，我们在下节视频中讲解。

559 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

（Chung J, Gulcehre C, Cho K H, et al. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling[J]. Eprint Arxiv, 2014. 

Cho K, Merrienboer B V, Bahdanau D, et al. On the Properties of Neural Machine Translation: Encoder-Decoder Approaches[J]. Computer Science, 2014.） 

560 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

1.10 长短期记忆（LSTM（long short term memory）unit）

在上一个视频中你已经学了 GRU（门控循环单元）。它能够让你可以在序列中学习非常 深的连接。其他类型的单元也可以让你做到这个，比如 LSTM 即长短时记忆网络，甚至比 GRU 更加有效，让我们看看。

这里是上个视频中的式子，对于 GRU 我们有𝑎 <𝑡> = 𝑐<𝑡> 。还有两个门:

更新门𝛤 （the update gate） 𝑢 相关门𝛤 （the relevance gate） 𝑟 𝑐̃ ，这是代替记忆细胞的候选值，然后我们使用更新门𝛤 来决定是否要用𝑐̃ <𝑡> 更新 𝑢 𝑐<𝑡> 。LSTM 是一个比 GRU 更加强大和通用的版本，这多亏了 Sepp Hochreiter 和 Jurgen Schmidhuber，感谢那篇开创性的论文，它在序列模型上有着巨大影响。我感觉这篇论文是 挺难读懂的，虽然我认为这篇论文在深度学习社群有着重大的影响，它深入讨论了梯度消失 的理论，我感觉大部分的人学到 LSTM 的细节是在其他的地方，而不是这篇论文。

561 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

这就是 LSTM 主要的式子（上图编号 2 所示），我们继续回到记忆细胞 c 上面来，使用 𝑐̃ = 𝑡𝑎𝑛ℎ(𝑊 [𝑎 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑐 来更新它的候选值𝑐̃<𝑡> （上图编号 3 所示）。注意了，在 𝑐 LSTM 中我们不再有𝑎 <𝑡> = 𝑐<𝑡> 的情况，这是现在我们用的是类似于左边这个式子（上图编 号 4 所示），但是有一些改变，现在我们专门使用𝑎 <𝑡> 或者𝑎<𝑡−1> ，而不是用𝑐<𝑡−1> ，我们 也不用𝛤 ，即相关门。虽然你可以使用 LSTM 的变体，然后把这些东西（左边所示的 GRU 公 𝑟 式）都放回来，但是在更加典型的 LSTM 里面，我们先不那样做。

我们像以前那样有一个更新门𝛤 和表示更新的参数𝑊 ，𝛤 = 𝜎(𝑊 [𝑎 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑢 ) 𝑢 𝑢 𝑢 𝑢

（上图编号 5 所示）。一个 LSTM 的新特性是不只有一个更新门控制，这里的这两项（上图 编号 6，7 所示），我们将用不同的项来代替它们，要用别的项来取代𝛤 和 1 − 𝛤 ，这里（上 𝑢 𝑢 图编号 6 所示）我们用𝛤 。𝑢

然后这里（上图编号 7 所示）用遗忘门（the forget gate），我们叫它𝛤 ，所以这个𝛤 = 𝑓 𝑓 𝜎(𝑊 𝑓 [𝑎 <𝑡−1> ,𝑥 <𝑡> ] + 𝑏 𝑓 )（上图编号 8 所示）；

然后我们有一个新的输出门，𝛤 𝑜 = 𝜎(𝑊 [𝑎 <𝑡−1> , 𝑥 <𝑡> ]+> 𝑏 𝑜 )（上图编号 9 所示）； 𝑜

于是记忆细胞的更新值𝑐 <𝑡> = 𝛤 ∗ 𝑐̃ + 𝛤 ∗ 𝑐<𝑡−1> （上图编号 10 所示）； 𝑢 𝑓

所以这给了记忆细胞选择权去维持旧的值𝑐<𝑡−1> 或者就加上新的值𝑐̃ <𝑡> ，所以这里用了 单独的更新门𝛤 和遗忘门𝛤 ，𝑢 𝑓

然后这个表示更新门（𝛤 = 𝜎(𝑊 [𝑎 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑢 ) 上图编号 5 所示）； 𝑢 𝑢

遗忘门（𝛤 = 𝜎(𝑊 𝑓 [𝑎 <𝑡−1> , 𝑥 <𝑡> ] + 𝑏 𝑓 ) 上图编号 8 所示）和输出门（上图编号 9 所示）。𝑓 最后𝑎 <𝑡> = 𝑐<𝑡> 的式子会变成𝑎 <𝑡> = 𝛤 ∗ 𝑐 <𝑡> 。这就是 LSTM 主要的式子了，然后这里 𝑜 （上图编号 11 所示）有三个门而不是两个，这有点复杂，它把门放到了和之前有点不同的

562 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

地方。

再提一下，这些式子就是控制 LSTM 行为的主要的式子了（上图编号 1 所示）。像之前 一样用图片稍微解释一下，先让我把图画在这里（上图编号 2 所示）。如果图片过于复杂，别担心，我个人感觉式子比图片好理解，但是我画图只是因为它比较直观。这个右上角的图 的灵感来自于 Chris Ola 的一篇博客，标题是《理解 LSTM 网络》（Understanding LSTM Network），这里的这张图跟他博客上的图是很相似的，但关键的不同可能是这里的这张图用了𝑎<𝑡−1> 和 𝑥<𝑡> 来计算所有门值（上图编号 3，4 所示），在这张图里是用𝑎<𝑡−1> ，𝑥<𝑡> 一起来计算遗 忘门𝛤 的值，还有更新门𝛤 以及输出门𝛤 （上图编号 4 所示）。然后它们也经过 tanh 函数来 𝑓 𝑢 𝑜 计算𝑐̃<𝑡> （上图编号 5 所示），这些值被用复杂的方式组合在一起，比如说元素对应的乘积 或者其他的方式来从之前的𝑐<𝑡−1> （上图编号 6 所示）中获得𝑐 <𝑡> （上图编号 7 所示）。

这里其中一个元素很有意思，如你在这一堆图（上图编号 8 所示的一系列图片）中看到 的，这是其中一个，再把他们连起来，就是把它们按时间次序连起来，这里（上图编号 9 所 示）输入𝑥<1> ，然后𝑥<2> ，𝑥<3> ，然后你可以把这些单元依次连起来，这里输出了上一个时 间的𝑎，𝑎会作为下一个时间步的输入，𝑐同理。在下面这一块，我把图简化了一下（相对上 图编号 2 所示的图有所简化）。然后这有个有意思的事情，你会注意到上面这里有条线（上 图编号 10 所示的线），这条线显示了只要你正确地设置了遗忘门和更新门，LSTM 是相当容 易把𝑐<0> 的值（上图编号 11 所示）一直往下传递到右边，比如𝑐 <3> = 𝑐<0> （上图编号 12 所 示）。这就是为什么 LSTM 和 GRU 非常擅长于长时间记忆某个值，对于存在记忆细胞中的某 个值，即使经过很长很长的时间步。

这就是 LSTM，你可能会想到这里和一般使用的版本会有些不同，最常用的版本可能是

563 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

门值不仅取决于𝑎<𝑡−1> 和𝑥<𝑡> ，有时候也可以偷窥一下𝑐<𝑡−1> 的值（上图编号 13 所示），这叫做「窥视孔连接」（peephole connection）。虽然不是个好听的名字，但是你想，「偷窥孔 连接」其实意思就是门值不仅取决于𝑎<𝑡−1> 和𝑥 <𝑡> ，也取决于上一个记忆细胞的值（𝑐 ），然后「偷窥孔连接」就可以结合这三个门（𝛤 、𝛤 、𝛤 ）来计算了。𝑢 𝑓 𝑜

如你所见 LSTM 主要的区别在于一个技术上的细节，比如这（上图编号 13 所示）有一 个 100 维的向量，你有一个 100 维的隐藏的记忆细胞单元，然后比如第 50 个𝑐<𝑡−1> 的元素 只会影响第 50 个元素对应的那个门，所以关系是一对一的，于是并不是任意这 100 维的 𝑐<𝑡−1> 可以影响所有的门元素。相反的，第一个𝑐<𝑡−1> 的元素只能影响门的第一个元素，第 二个元素影响对应的第二个元素，如此类推。但如果你读过论文，见人讨论「偷窥孔连接」，那就是在说𝑐 <𝑡−1> 也能影响门值。

LSTM 前向传播图：

LSTM 反向传播计算：

门求偏导：

d  o  t  = da next *tanh(c next )*  o  t  *(1 −  o  t  ) 

dc  t  = dc next *  i  t  +  o  t  (1 − tanh(c next ) 2 )* i t * da next * c  t  *(1 − tanh(c ) 2 ) 

d  u  t  = dc next * c  t  +  o  t  (1 − tanh(c next ) 2 )* c  t  * da next *  u  t  *(1 −  u  t  ) 

564 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

d  f  t  = dc next * c prev + o  t  (1 − tanh(c next ) 2 )* c prev * da next *  f  t  *(1 − f  t  ) 

参数求偏导 ：

T  a prev  dW f = d  f  t  *    x t  

T  a prev  dW u = d  u  t  *    x t  

T  a prev  dW c = dc  t  *    x t  

T  a prev  dW o = d  o  t  *    x t  

为了计算𝑑𝑏 𝑓 , 𝑑𝑏 𝑢 , 𝑑𝑏 𝑐 , 𝑑𝑏 𝑜 需要各自对𝑑𝛤 ⟨𝑡⟩ , 𝑑𝛤 ⟨𝑡⟩ , 𝑑 𝑐̃ , 𝑑𝛤 ⟨𝑡⟩ 求和。𝑓 𝑢 𝑜

最后，计算隐藏状态、记忆状态和输入的偏导数：

da prev = W f T * d  f  t  + W u T * d  u  t  + W c T * dc  t  + W o T * d o  t  

dc prev = dc next  f  t  + o  t  *(1 − tanh(c next ) 2 )*  f  t  * danext 

dx  t  = W f T * d  f  t  + W u T * d  u  t  + W c T * dc t + W o T * d o  t  

这就是 LSTM，我们什么时候应该用 GRU？什么时候用 LSTM？这里没有统一的准则。而且即使我先讲解了 GRU，在深度学习的历史上，LSTM 也是更早出现的，而 GRU 是最近才 发明出来的，它可能源于 Pavia 在更加复杂的 LSTM 模型中做出的简化。研究者们在很多不 同问题上尝试了这两种模型，看看在不同的问题不同的算法中哪个模型更好，所以这不是个 学术和高深的算法，我才想要把这两个模型展示给你。

GRU 的优点是这是个更加简单的模型，所以更容易创建一个更大的网络，而且它只有两 个门，在计算性上也运行得更快，然后它可以扩大模型的规模。

但是 LSTM 更加强大和灵活，因为它有三个门而不是两个。如果你想选一个使用，我认 为 LSTM 在历史进程上是个更优先的选择，所以如果你必须选一个，我感觉今天大部分的人 还是会把 LSTM 作为默认的选择来尝试。虽然我认为最近几年 GRU 获得了很多支持，而且 我感觉越来越多的团队也正在使用 GRU，因为它更加简单，而且还效果还不错，它更容易适 应规模更加大的问题。

所以这就是 LSTM，无论是 GRU 还是 LSTM，你都可以用它们来构建捕获更加深层连接

565 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

的神经网络。

（Hochreiter S, Schmidhuber J. Long Short-Term Memory[J]. Neural Computation, 1997, 9(8):1735-1780.） 

566 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

1.11 双向循环神经网络（Bidirectional RNN）

现在，你已经了解了大部分 RNN 模型的关键的构件，还有两个方法可以让你构建更好 的模型，其中之一就是双向 RNN 模型，这个模型可以让你在序列的某点处不仅可以获取之 前的信息，还可以获取未来的信息，我们会在这个视频里讲解。第二个就是深层的 RNN，我 们会在下个视频里见到，现在先从双向 RNN 开始吧。

为了了解双向 RNN 的动机，我们先看一下之前在命名实体识别中已经见过多次的神经 网络。这个网络有一个问题，在判断第三个词 Teddy（上图编号 1 所示）是不是人名的一部 分时，光看句子前面部分是不够的，为了判断𝑦 （上图编号 2 所示）是 0 还是 1，除了前

3 个单词，你还需要更多的信息，因为根据前 3 个单词无法判断他们说的是 Teddy 熊，还是 前美国总统 Teddy Roosevelt，所以这是一个非双向的或者说只有前向的 RNN。我刚才所说 的总是成立的，不管这些单元（上图编号 3 所示）是标准的 RNN 块，还是 GRU 单元或者是 LSTM 单元，只要这些构件都是只有前向的。那么一个双向的 RNN 是如何解决这个问题的？下面解释双向 RNN 的工作原理。为了简 单，我们用四个输入或者说一个只有 4 个单词的句子，这样输入只有 4 个，𝑥<1> 到𝑥<4> 。从这里开始的这个网络会有一个前向的循环单元叫做𝑎⃗ ，⃗𝑎 ，⃗𝑎 还有𝑎⃗ ，我在这 上面加个向右的箭头来表示前向的循环单元，并且他们这样连接（下图编号 1 所示）。这四 个循环单元都有一个当前输入𝑥输入进去，得到预测的𝑦 ，𝑦 ，𝑦<3> 和𝑦 。

到目前为止，我还没做什么，仅仅是把前面幻灯片里的 RNN 画在了这里，只是在这些 地方画上了箭头。我之所以在这些地方画上了箭头是因为我们想要增加一个反向循环层，这

567 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

里有个𝑎⃖⃗⃗ ，左箭头代表反向连接，⃖⃗𝑎⃗ 反向连接，⃖⃗𝑎⃗ 反向连接，⃖⃗𝑎⃗ 反向连接，所以这 里的左箭头代表反向连接。

同样，我们把网络这样向上连接，这个𝑎反向连接就依次反向向前连接（上图编号 2 所 示）。这样，这个网络就构成了一个无环图。给定一个输入序列𝑥<1> 到𝑥<4> ，这个序列首先 计算前向的𝑎⃗ ，然后计算前向的𝑎⃗ ，接着𝑎⃗ ，⃗𝑎 。而反向序列从计算𝑎⃖⃗⃗ 开始，反向进行，计算反向的𝑎⃖⃗⃗ 。你计算的是网络激活值，这不是反向而是前向的传播，而图中 这个前向传播一部分计算是从左到右，一部分计算是从右到左。计算完了反向的𝑎⃖⃗⃗ ，可以 用这些激活值计算反向的𝑎⃖⃗⃗ ，然后是反向的𝑎⃖⃗⃗ ，把所有这些激活值都计算完了就可以计 算预测结果了。

举个例子，为了预测结果，你的网络会有如𝑦 ，𝑦 <𝑡> = 𝑔(𝑊 [𝑎⃗ , ⃖⃗𝑎⃗] + 𝑏 𝑦 )（上 𝑔 图编号 1 所示）。比如你要观察时间 3 这里的预测结果，信息从𝑥<1> 过来，流经这里，前向

的𝑎⃗ 到前向的𝑎⃗ ，这些函数里都有表达，到前向的𝑎⃗ 再到𝑦

（上图编号 2 所示的

568 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

路径），所以从𝑥 <1> ，𝑥<2> ，𝑥<3> 来的信息都会考虑在内，而从𝑥<4> 来的信息会流过反向的 ⃖⃗𝑎⃗ ，到反向的𝑎⃖⃗⃗ 再到𝑦 （上图编号 3 所示的路径）。这样使得时间 3 的预测结果不

仅输入了过去的信息，还有现在的信息，这一步涉及了前向和反向的传播信息以及未来的信 息。给定一个句子 "He said Teddy Roosevelt..." 来预测 Teddy 是不是人名的一部分，你需要同 时考虑过去和未来的信息。

这就是双向循环神经网络，并且这些基本单元不仅仅是标准 RNN 单元，也可以是 GRU 单元或者 LSTM 单元。事实上，很多的 NLP 问题，对于大量有自然语言处理问题的文本，有 LSTM 单元的双向 RNN 模型是用的最多的。所以如果有 NLP 问题，并且文本句子都是完整 的，首先需要标定这些句子，一个有 LSTM 单元的双向 RNN 模型，有前向和反向过程是一 个不错的首选。

以上就是双向 RNN 的内容，这个改进的方法不仅能用于基本的 RNN 结构，也能用于 GRU 和 LSTM。通过这些改变，你就可以用一个用 RNN 或 GRU 或 LSTM 构建的模型，并且 能够预测任意位置，即使在句子的中间，因为模型能够考虑整个句子的信息。这个双向 RNN 网络模型的缺点就是你需要完整的数据的序列，你才能预测任意位置。比如说你要构建一个 语音识别系统，那么双向 RNN 模型需要你考虑整个语音表达，但是如果直接用这个去实现 的话，你需要等待这个人说完，然后获取整个语音表达才能处理这段语音，并进一步做语音 识别。对于实际的语音识别的应用通常会有更加复杂的模块，而不是仅仅用我们见过的标准 的双向 RNN 模型。但是对于很多自然语言处理的应用，如果你总是可以获取整个句子，这 个标准的双向 RNN 算法实际上很高效。

这就是双向 RNN，下一个视频，也是这周的最后一个，我们会讨论如何用这些概念，标 准的 RNN，LSTM 单元，GRU 单元，还有双向的版本，构建更深的网络。

569 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

1.12 深层循环神经网络（Deep RNNs）

目前你学到的不同 RNN 的版本，每一个都可以独当一面。但是要学习非常复杂的函数，通常我们会把 RNN 的多个层堆叠在一起构建更深的模型。这节视频里我们会学到如何构建 这些更深的 RNN。

一个标准的神经网络，首先是输入𝑥，然后堆叠上隐含层，所以这里应该有激活值，比 如说第一层是𝑎 [1] ，接着堆叠上下一层，激活值𝑎 [2] ，可以再加一层𝑎 [3] ，然后得到预测值𝑦。深层的 RNN 网络跟这个有点像，用手画的这个网络（下图编号 1 所示），然后把它按时间 展开就是了，我们看看。

这是我们一直见到的标准的 RNN（上图编号 3 所示方框内的 RNN），只是我把这里的 符号稍微改了一下，不再用原来的𝑎 <0> 表示 0 时刻的激活值了，而是用𝑎 [1]<0> 来表示第一层 （上图编号 4 所示），所以我们现在用𝑎 [𝑙]<𝑡> 来表示第 l 层的激活值，这个 <t> 表示第𝑡个时 间点，这样就可以表示。第一层第一个时间点的激活值𝑎 [1]<1> ，这（𝑎 [1]<2> ）就是第一层第 二个时间点的激活值，𝑎 [1]<3> 和𝑎 [1]<4> 。然后我们把这些（上图编号 4 方框内所示的部分） 堆叠在上面，这就是一个有三个隐层的新的网络。

我们看个具体的例子，看看这个值（𝑎 [2]<3> ，上图编号 5 所示）是怎么算的。激活值 𝑎 [2]<3> 有两个输入，一个是从下面过来的输入（上图编号 6 所示），还有一个是从左边过来

的输入（上图编号 7 所示），𝑎 [2]<3> = 𝑔(𝑊 [2] [𝑎 [2]<2> , 𝑎 [1]<3> ] + 𝑏 𝑎 [2] )，这就是这个激活值 𝑎 的计算方法。参数𝑊 [2] 和𝑏 𝑎 [2] 在这一层的计算里都一样，相对应地第一层也有自己的参数𝑊[1] 𝑎 𝑎

和𝑏 𝑎 [1] 。

570 

第五门课 序列模型 (Sequence Models)- 第一周 循环序列模型（Recurrent Neural Networks）

对于像左边这样标准的神经网络，你可能见过很深的网络，甚至于 100 层深，而对于 RNN 来说，有三层就已经不少了。由于时间的维度，RNN 网络会变得相当大，即使只有很 少的几层，很少会看到这种网络堆叠到 100 层。但有一种会容易见到，就是在每一个上面堆 叠循环层，把这里的输出去掉（上图编号 1 所示），然后换成一些深的层，这些层并不水平 连接，只是一个深层的网络，然后用来预测𝑦<1> 。同样这里（上图编号 2 所示）也加上一个 深层网络，然后预测𝑦<2> 。这种类型的网络结构用的会稍微多一点，这种结构有三个循环单 元，在时间上连接，接着一个网络在后面接一个网络，当然𝑦<3> 和𝑦<4> 也一样，这是一个深 层网络，但没有水平方向上的连接，所以这种类型的结构我们会见得多一点。通常这些单元 （上图编号 3 所示）没必要非是标准的 RNN，最简单的 RNN 模型，也可以是 GRU 单元或者 LSTM 单元，并且，你也可以构建深层的双向 RNN 网络。由于深层的 RNN 训练需要很多计 算资源，需要很长的时间，尽管看起来没有多少循环层，这个也就是在时间上连接了三个深 层的循环层，你看不到多少深层的循环层，不像卷积神经网络一样有大量的隐含层。

这就是深层 RNN 的内容，从基本的 RNN 网络，基本的循环单元到 GRU，LSTM，再到双 向 RNN，还有深层版的模型。这节课后，你已经可以构建很不错的学习序列的模型了。

571 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

2.1 词汇表征（Word Representation）

上周我们学习了 RNN、GRU 单元和 LSTM 单元。本周你会看到我们如何把这些知识用到 NLP 上，用于自然语言处理，深度学习已经给这一领域带来了革命性的变革。其中一个很关 键的概念就是词嵌入（word embeddings），这是语言表示的一种方式，可以让算法自动的 理解一些类似的词，比如男人对女人，比如国王对王后，还有其他很多的例子。通过词嵌入 的概念你就可以构建 NLP 应用了，即使你的模型标记的训练集相对较小。这周的最后我们 会消除词嵌入的偏差，就是去除不想要的特性，或者学习算法有时会学到的其他类型的偏差。

现在我们先开始讨论词汇表示，目前为止我们一直都是用词汇表来表示词，上周提到的 词汇表，可能是 10000 个单词，我们一直用 one-hot 向量来表示词。比如如果 man（上图编 号 1 所示）在词典里是第 5391 个，那么就可以表示成一个向量，只在第 5391 处为 1（上图 编号 2 所示），我们用𝑂5391 代表这个量，这里的𝑂代表 one-hot。接下来，如果 woman 是编 号 9853（上图编号 3 所示），那么就可以用𝑂 9853 来表示，这个向量只在 9853 处为 1（上图 编号 4 所示），其他为 0，其他的词 king、queen、apple、orange 都可以这样表示出来这种 表示方法的一大缺点就是它把每个词孤立起来，这样使得算法对相关词的泛化能力不强。

举个例子，假如你已经学习到了一个语言模型，当你看到「I want a glass of orange ___」，

572 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

那么下一个词会是什么？很可能是 juice。即使你的学习算法已经学到了「I want a glass of orange juice」这样一个很可能的句子，但如果看到「I want a glass of apple ___」，因为算法不 知道 apple 和 orange 的关系很接近，就像 man 和 woman，king 和 queen 一样。所以算法很 难从已经知道的 orange juice 是一个常见的东西，而明白 apple juice 也是很常见的东西或者 说常见的句子。这是因为任何两个 one-hot 向量的内积都是 0，如果你取两个向量，比如 king 和 queen，然后计算它们的内积，结果就是 0。如果用 apple 和 orange 来计算它们的内积，结果也是 0。很难区分它们之间的差别，因为这些向量内积都是一样的，所以无法知道 apple 和 orange 要比 king 和 orange，或者 queen 和 orange 相似地多。换一种表示方式会更好，如果我们不用 one-hot 表示，而是用特征化的表示来表示每个 词，man，woman，king，queen，apple，orange 或者词典里的任何一个单词，我们学习这 些词的特征或者数值。

举个例子，对于这些词，比如我们想知道这些词与 Gender（性别）的关系。假定男性 的性别为 - 1，女性的性别为 + 1，那么 man 的性别值可能就是 - 1，而 woman 就是 - 1。最终根 据经验 king 就是 - 0.95，queen 是 + 0.97，apple 和 orange 没有性别可言。

另一个特征可以是这些词有多 Royal（高贵），所以这些词，man，woman 和高贵没太 关系，所以它们的特征值接近 0。而 king 和 queen 很高贵，apple 和 orange 跟高贵也没太大 关系。

那么 Age（年龄）呢？man 和 woman 一般没有年龄的意思，也许 man 和 woman 隐含 着成年人的意思，但也可能是介于 young 和 old 之间，所以它们（man 和 woman）的值也 接近 0。而通常 king 和 queen 都是成年人，apple 和 orange 跟年龄更没什么关系了。

573 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

还有一个特征，这个词是否是 Food（食物），man 不是食物，woman 不是食物，king 和 queen 也不是，但 apple 和 orange 是食物。

当然还可以有很多的其他特征，从 Size（尺寸大小），Cost（花费多少），这个东西是 不是 alive（活的），是不是一个 Action（动作），或者是不是 Noun（名词）或者是不是 Verb （动词），还是其他的等等。

所以你可以想很多的特征，为了说明，我们假设有 300 个不同的特征，这样的话你就有 了这一列数字（上图编号 1 所示），这里我只写了 4 个，实际上是 300 个数字，这样就组成 了一个 300 维的向量来表示 man 这个词。接下来，我想用𝑒5391 这个符号来表示，就像这样 （上图编号 2 所示）。同样这个 300 维的向量，我用𝑒9853 代表这个 300 维的向量用来表示 woman 这个词（上图编号 3 所示），这些其他的例子也一样。现在，如果用这种表示方法 来表示 apple 和 orange 这些词，那么 apple 和 orange 的这种表示肯定会非常相似，可能有 些特征不太一样，因为 orange 的颜色口味，apple 的颜色口味，或者其他的一些特征会不太 一样，但总的来说 apple 和 orange 的大部分特征实际上都一样，或者说都有相似的值。这 样对于已经知道 orange juice 的算法很大几率上也会明白 apple juice 这个东西，这样对于不 同的单词算法会泛化的更好。

后面的几个视频，我们会找到一个学习词嵌入的方式，这里只是希望你能理解这种高维 特征的表示能够比 one-hot 更好的表示不同的单词。而我们最终学习的特征不会像这里一样 这么好理解，没有像第一个特征是性别，第二个特征是高贵，第三个特征是年龄等等这些，新的特征表示的东西肯定会更难搞清楚。尽管如此，接下来要学的特征表示方法却能使算法 高效地发现 apple 和 orange 会比 king 和 orange，queen 和 orange 更加相似。

574 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

如果我们能够学习到一个 300 维的特征向量，或者说 300 维的词嵌入，通常我们可以做 一件事，把这 300 维的数据嵌入到一个二维空间里，这样就可以可视化了。常用的可视化算 法是 t-SNE 算法，来自于 Laurens van der Maaten 和 Geoff Hinton 的论文。如果观察这种词 嵌入的表示方法，你会发现 man 和 woman 这些词聚集在一块（上图编号 1 所示），king 和 queen 聚集在一块（上图编号 2 所示），这些都是人，也都聚集在一起（上图编号 3 所示）。动物都聚集在一起（上图编号 4 所示），水果也都聚集在一起（上图编号 5 所示），像 1、 2、3、4 这些数字也聚集在一起（上图编号 6 所示）。如果把这些生物看成一个整体，他们 也聚集在一起（上图编号 7 所示）。

在网上你可能会看到像这样的图用来可视化，300 维或者更高维度的嵌入。希望你能有 个整体的概念，这种词嵌入算法对于相近的概念，学到的特征也比较类似，在对这些概念可 视化的时候，这些概念就比较相似，最终把它们映射为相似的特征向量。这种表示方式用的 是在 300 维空间里的特征表示，这叫做嵌入（embeddings）。之所以叫嵌入的原因是，你可 以想象一个 300 维的空间，我画不出来 300 维的空间，这里用个 3 维的代替（上图编号 8 所 示）。现在取每一个单词比如 orange，它对应一个 3 维的特征向量，所以这个词就被嵌在这 个 300 维空间里的一个点上了（上图编号 9 所示），apple 这个词就被嵌在这个 300 维空间 的另一个点上了（上图编号 10 所示）。为了可视化，t-SNE 算法把这个空间映射到低维空 间，你可以画出一个 2 维图像然后观察，这就是这个术语嵌入的来源。

词嵌入已经是 NLP 领域最重要的概念之一了，在自然语言处理领域。本节视频中你已 经知道为什么要学习或者使用词嵌入了，下节视频我们会深入讲解如何用这些算法构建 NLP 算法。

575 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

2.2 使用词嵌入（Using Word Embeddings）

上一个视频中，你已经了解不同单词的特征化表示了。这节你会看到我们如何把这种表 示方法应用到 NLP 应用中。

我们从一个例子开始，我们继续用命名实体识别的例子，如果你要找出人名，假如有一 个句子：「Sally Johnson is an orange farmer.」（Sally Johnson 是一个种橙子的农民），你会发 现 Sally Johnson 就是一个人名，所以这里的输出为 1。之所以能确定 Sally Johnson 是一个人 名而不是一个公司名，是因为你知道种橙子的农民一定是一个人，前面我们已经讨论过用 one-hot 来表示这些单词，𝑥 <1> ，𝑥<2> 等等。

但是如果你用特征化表示方法，嵌入的向量，也就是我们在上个视频中讨论的。那么用 词嵌入作为输入训练好的模型，如果你看到一个新的输入：「Robert Lin is an apple farmer.」（Robert Lin 是一个种苹果的农民），因为知道 orange 和 apple 很相近，那么你的算法很容 易就知道 Robert Lin 也是一个人，也是一个人的名字。一个有意思的情况是，要是测试集里 这句话不是「Robert Lin is an apple farmer.」，而是不太常见的词怎么办？要是你看到：「Robert Lin is a durian cultivator.」（Robert Lin 是一个榴莲培育家）怎么办？榴莲（durian）是一种比 较稀罕的水果，这种水果在新加坡和其他一些国家流行。如果对于一个命名实体识别任务，你只有一个很小的标记的训练集，你的训练集里甚至可能没有 durian（榴莲）或者 cultivator （培育家）这两个词。但是如果你有一个已经学好的词嵌入，它会告诉你 durian（榴莲）是 水果，就像 orange（橙子）一样，并且 cultivator（培育家），做培育工作的人其实跟 farmer （农民）差不多，那么你就有可能从你的训练集里的「an orange farmer」（种橙子的农民）归

576 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

纳出「a durian cultivator」（榴莲培育家）也是一个人。

词嵌入能够达到这种效果，其中一个原因就是学习词嵌入的算法会考察非常大的文本集，也许是从网上找到的，这样你可以考察很大的数据集可以是 1 亿个单词，甚至达到 100 亿也 都是合理的，大量的无标签的文本的训练集。通过考察大量的无标签文本，很多都是可以免 费下载的，你可以发现 orange（橙子）和 durian（榴莲）相近，farmer（农民）和 cultivator （培育家）相近。因此学习这种嵌入表达，把它们都聚集在一块，通过读取大量的互联网文 本发现了 orange（橙子）和 durian（榴莲）都是水果。接下来你可以把这个词嵌入应用到你 的命名实体识别任务当中，尽管你只有一个很小的训练集，也许训练集里有 100,000 个单词，甚至更小，这就使得你可以使用迁移学习，把你从互联网上免费获得的大量的无标签文本中 学习到的知识，能够分辨 orange（橙子）、apple（苹果）和 durian（榴莲）都是水果的知 识，然后把这些知识迁移到一个任务中，比如你只有少量标记的训练数据集的命名实体识别 任务中。当然了，这里为了简化我只画了单向的 RNN，事实上如果你想用在命名实体识别任 务上，你应该用一个双向的 RNN，而不是这样一个简单的。

总结一下，这是如何用词嵌入做迁移学习的步骤。第一步，先从大量的文本集中学习词嵌入。一个非常大的文本集，或者可以下载网上预 训练好的词嵌入模型，网上你可以找到不少，词嵌入模型并且都有许可。

第二步，你可以用这些词嵌入模型把它迁移到你的新的只有少量标注训练集的任务中，比如说用这个 300 维的词嵌入来表示你的单词。这样做的一个好处就是你可以用更低维度 的特征向量代替原来的 10000 维的 one-hot 向量，现在你可以用一个 300 维更加紧凑的向 量。尽管 one-hot 向量很快计算，而学到的用于词嵌入的 300 维的向量会更加紧凑。

第三步，当你在你新的任务上训练模型时，在你的命名实体识别任务上，只有少量的标 记数据集上，你可以自己选择要不要继续微调，用新的数据调整词嵌入。实际中，只有这个

577 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

第二步中有很大的数据集你才会这样做，如果你标记的数据集不是很大，通常我不会在微调 词嵌入上费力气。

当你的任务的训练集相对较小时，词嵌入的作用最明显，所以它广泛用于 NLP 领域。我 只提到一些，不要太担心这些术语（下问列举的一些 NLP 任务），它已经用在命名实体识 别，用在文本摘要，用在文本解析、指代消解，这些都是非常标准的 NLP 任务。

词嵌入在语言模型、机器翻译领域用的少一些，尤其是你做语言模型或者机器翻译任务 时，这些任务你有大量的数据。在其他的迁移学习情形中也一样，如果你从某一任务 A 迁移 到某个任务 B，只有 A 中有大量数据，而 B 中数据少时，迁移的过程才有用。所以对于很多 NLP 任务这些都是对的，而对于一些语言模型和机器翻译则不然。

最后，词嵌入和人脸编码之间有奇妙的关系，你已经在前面的课程学到了关于人脸编码 的知识了，如果你上了卷积神经网络的课程的话。你应该还记得对于人脸识别，我们训练了 一个 Siamese 网络结构，这个网络会学习不同人脸的一个 128 维表示，然后通过比较编码结 果来判断两个图片是否是同一个人脸，这个词嵌入的意思和这个差不多。在人脸识别领域大 家喜欢用编码这个词来指代这些向量𝑓(𝑥 (𝑖) )，𝑓(𝑥 (𝑗) )（上图编号 1 所示），人脸识别领域和 这里的词嵌入有一个不同就是，在人脸识别中我们训练一个网络，任给一个人脸照片，甚至 是没有见过的照片，神经网络都会计算出相应的一个编码结果。上完后面几节课，你会更明 白，我们学习词嵌入则是有一个固定的词汇表，比如 10000 个单词，我们学习向量𝑒 1 到𝑒 10000 ，学习一个固定的编码，每一个词汇表的单词的固定嵌入，这就是人脸识别与我们接下来几节 视频要讨论的算法之间的一个不同之处。这里的术语编码（encoding）和嵌入（embedding） 可以互换，所以刚才讲的差别不是因为术语不一样，这个差别就是，人脸识别中的算法未来

578 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

可能涉及到海量的人脸照片，而自然语言处理有一个固定的词汇表，而像一些没有出现过的 单词我们就记为未知单词。这节视频里，你看到如何用词嵌入来实现这种类型的迁移学习，并且通过替换原来的 one-hot 表示，而是用之前的嵌入的向量，你的算法会泛化的更好，你也可以从较少的标记 数据中进行学习。接下来我会给你展示一些词嵌入的特性，这之后再讨论学习这些词嵌入的 算法。下个视频我们会看到词嵌入在做类比推理中发挥的作用。

579 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

2.3 词嵌入的特性（Properties of Word Embeddings）

到现在，你应该明白了词嵌入是如何帮助你构建自然语言处理应用的。词嵌入还有一个 迷人的特性就是它还能帮助实现类比推理，尽管类比推理可能不是自然语言处理应用中最重 要的，不过它能帮助人们理解词嵌入做了什么，以及词嵌入能够做什么，让我们来一探究竟。

这是一系列你希望词嵌入可以捕捉的单词的特征表示，假如我提出一个问题，man 如果 对应 woman，那么 king 应该对应什么？你们应该都能猜到 king 应该对应 queen。能否有一 种算法来自动推导出这种关系，下面就是实现的方法。

我们用一个四维向量来表示 man，我们用𝑒5391 来表示，不过在这节视频中我们先把它 （上图编号 1 所示）称为𝑒man ，而旁边这个（上图编号 2 所示）表示 woman 的嵌入向量，称它为𝑒woman ，对 king 和 queen 也是用一样的表示方法。在该例中，假设你用的是四维的 嵌入向量，而不是比较典型的 50 到 1000 维的向量。这些向量有一个有趣的特性，就是假如 你有向量𝑒man 和𝑒 woman ，将它们进行减法运算，即 −1 1 −2 −2

0.01 0.02 −0.01 0 𝑒 man − 𝑒 woman = [ 0.03 0.02 0.01 0 

] − [] = [] ≈ [ ]   0.09 0.01 0.08 0 类似的，假如你用𝑒 king 减去𝑒queen ，最后也会得到一样的结果，即 −0.95 0.97 −1.92 −2

0.93 0.95 −0.02 0 𝑒 king − 𝑒 queen = [ 0.70 0.69 0.01 0 

] − [] = [] ≈ [ ]   0.02 0.01 0.01 0 这个结果表示，man 和 woman 主要的差异是 gender（性别）上的差异，而 king 和 queen

之间的主要差异，根据向量的表示，也是 gender（性别）上的差异，这就是为什么𝑒 man −

580 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

𝑒woman 和𝑒 king − 𝑒queen 结果是相同的。所以得出这种类比推理的结论的方法就是，当算法被 问及 man 对 woman 相当于 king 对什么时，算法所做的就是计算𝑒 man − 𝑒woman ，然后找出 一个向量也就是找出一个词，使得𝑒 man − 𝑒 woman ≈ 𝑒 king − 𝑒 ? ，也就是说，当这个新词是 queen 时，式子的左边会近似地等于右边。这种思想首先是被 Tomas Mikolov 和 Wen-tau Yih 还有 Geoffrey Zweig 提出的，这是词嵌入领域影响力最为惊人和显著的成果之一，这种思想帮助 了研究者们对词嵌入领域建立了更深刻的理解。（ Mikolov T, Yih W T, Zweig G. Linguistic regularities in continuous space word representations [J]. In HLT-NAACL, 2013.）

让我们来正式地探讨一下应该如何把这种思想写成算法。在图中，词嵌入向量在一个可 能有 300 维的空间里，于是单词 man 代表的就是空间中的一个点，另一个单词 woman 代表 空间另一个点，单词 king 也代表一个点，还有单词 queen 也在另一点上（上图编号 1 方框 内所示的点）。事实上，我们在上个幻灯片所展示的就是向量 man 和 woman 的差值非常接 近于向量 king 和 queen 之间的差值，我所画的这个箭头（上图编号 2 所示）代表的就是向 量在 gender（性别）这一维的差，不过不要忘了这些点是在 300 维的空间里。为了得出这样 的类比推理，计算当 man 对于 woman，那么 king 对于什么，你能做的就是找到单词 w 来 使得，𝑒 man − 𝑒 woman ≈ 𝑒 king − 𝑒 𝑤 这个等式成立，你需要的就是找到单词 w 来最大化𝑒 𝑤 与 𝑒 king − 𝑒 man + 𝑒 woman 的相似度，即

𝐹𝑖𝑛𝑑 𝑤𝑜𝑟𝑑 𝑤: 𝑎𝑟𝑔𝑚𝑎𝑥 𝑆𝑖𝑚(𝑒 𝑤 ,𝑒 king − 𝑒 man + 𝑒 woman ) 

所以我做的就是我把这个𝑒 𝑤 全部放到等式的一边，于是等式的另一边就会是𝑒 king −

581 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

𝑒 man + 𝑒woman 。我们有一些用于测算𝑒 𝑤 和𝑒 king − 𝑒 man + 𝑒woman 之间的相似度的函数，然后

通过方程找到一个使得相似度最大的单词，如果结果理想的话会得到单词 queen。值得注意 的是这种方法真的有效，如果你学习一些词嵌入，通过算法来找到使得相似度最大化的单词 w，你确实可以得到完全正确的答案。不过这取决于过程中的细节，如果你查看一些研究论 文就不难发现，通过这种方法来做类比推理准确率大概只有 30%~75%，只要算法猜中了单 词，就把该次计算视为正确，从而计算出准确率，在该例子中，算法选出了单词 queen。

在继续下一步之前，我想再说明一下左边的这幅图（上图编号 1 所示），在之前我们谈 到过用 t-SNE 算法来将单词可视化。t-SNE 算法所做的就是把这些 300 维的数据用一种非线 性的方式映射到 2 维平面上，可以得知 t-SNE 中这种映射很复杂而且很非线性。在进行 t-SNE 映射之后，你不能总是期望使等式成立的关系，会像左边那样成一个平行四边形，尽管在这 个例子最初的 300 维的空间内你可以依赖这种平行四边形的关系来找到使等式成立的一对 类比，通过 t-SNE 算法映射出的图像可能是正确的。但在大多数情况下，由于 t-SNE 的非线 性映射，你就没法再指望这种平行四边形了，很多这种平行四边形的类比关系在 t-SNE 映射 中都会失去原貌。

现在，再继续之前，我想再快速地列举一个最常用的相似度函数，这个最常用的相似度 函数叫做余弦相似度。这是我们上个幻灯片所得到的等式（下图编号 1 所示），在余弦相似 𝑢 𝑇 𝑣 度中，假如在向量𝑢和𝑣之间定义相似度:sim (𝑢, 𝑣) = ||𝑢|| ||𝑣||

2 

2 

现在我们先不看分母，分子其实就是𝑢和𝑣的内积。如果 u 和 v 非常相似，那么它们的 内积将会很大，把整个式子叫做余弦相似度，其实就是因为该式是𝑢和𝑣的夹角的余弦值，所 以这个角（下图编号 2 所示）就是 Φ 角，这个公式实际就是计算两向量夹角 Φ 角的余弦。你应该还记得在微积分中，Φ 角的余弦图像是这样的（下图编号 3 所示），所以夹角为 0 度 时，余弦相似度就是 1，当夹角是 90 度角时余弦相似度就是 0，当它们是 180 度时，图像完

582 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

全跑到了相反的方向，这时相似度等于 - 1，这就是为什么余弦相似度对于这种类比工作能起

到非常好的效果。距离用平方距离或者欧氏距离来表示:||𝑢 − 𝑣||2

参考资料：余弦相似度 为了测量两个词的相似程度，我们需要一种方法来测量两个词的两 个 嵌 入 向 量 之 间 的 相 似 程 度 。给 定 两 个 向 量 𝑢 和 𝑣 ，余 弦 相 似 度 定 义 如 下 ：

CosineSimilarity (u , v ) = 

u.v || || 2 || v || 

= cos ( ) 

u 

2 

其中 𝑢. 𝑣 是两个向量的点积（或内积），||𝑢|| 2 是向量𝑢的范数（或长度），并且 𝜃 是 向量𝑢和𝑣之间的角度。这种相似性取决于角度在向量𝑢和𝑣之间。如果向量𝑢和𝑣非常相似，它 们 的 余 弦 相 似 性 将 接 近 1; 如 果 它 们 不 相 似 ，则 余 弦 相 似 性 将 取 较 小 的 值 。

图 1：两个向量之间角度的余弦是衡量它们有多相似的指标，角度越小，两个向量越相似。从学术上来说，比起测量相似度，这个函数更容易测量的是相异度，所以我们需要对其 取负，这个函数才能正常工作，不过我还是觉得余弦相似度用得更多一点，这两者的主要区 别是它们对𝑢和𝑣之间的距离标准化的方式不同。

583 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

词嵌入的一个显著成果就是，可学习的类比关系的一般性。举个例子，它能学会 man 对 于 woman 相当于 boy 对于 girl，因为 man 和 woman 之间和 king 和 queen 之间，还有 boy 和 girl 之间的向量差在 gender（性别）这一维都是一样的。它还能学习 Canada（加拿大） 的首都是 Ottawa（渥太华），而渥太华对于加拿大相当于 Nairobi（内罗毕）对于 Kenya（肯 尼亚），这些都是国家中首都城市名字。它还能学习 big 对于 bigger 相当于 tall 对于 taller，还能学习 Yen（円）对于 Janpan（日本），円是日本的货币单位，相当于 Ruble（卢比）对 于 Russia（俄罗斯）。这些东西都能够学习，只要你在大型的文本语料库上实现一个词嵌入 学习算法，只要从足够大的语料库中进行学习，它就能自主地发现这些模式。

在本节视频中，你见到了词嵌入是如何被用于类比推理的，可能你不会自己动手构建一 个类比推理系统作为一项应用，不过希望在这些可学习的类特征的表示方式能够给你一些直 观的感受。你还看知道了余弦相似度可以作为一种衡量两个词嵌入向量间相似度的办法，我 们谈了许多有关这些嵌入的特性，以及如何使用它们。下节视频中，我们来讨论如何真正的 学习这些词嵌入。

584 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

2.4 嵌入矩阵（Embedding Matrix）

接下来我们要将学习词嵌入这一问题具体化，当你应用算法来学习词嵌入时，实际上是 学习一个嵌入矩阵，我们来看一下这是什么意思。

和之前一样，假设我们的词汇表含有 10,000 个单词，词汇表里有 a，aaron，orange，zulu，可能还有一个未知词标记 <UNK>。我们要做的就是学习一个嵌入矩阵𝐸，它将是一个 300×10,000 的矩阵，如果你的词汇表里有 10,000 个，或者加上未知词就是 10,001 维。这个 矩阵的各列代表的是词汇表中 10,000 个不同的单词所代表的不同向量。假设 orange 的单词 编号是 6257（下图编号 1 所示），代表词汇表中第 6257 个单词，我们用符号𝑂 6527 来表示 这个 one-hot 向量，这个向量除了第 6527 个位置上是 1（下图编号 2 所示），其余各处都为 0，显然它是一个 10,000 维的列向量，它只在一个位置上有 1，它不像图上画的那么短，它 的高度应该和左边的嵌入矩阵的宽度相等。

假设这个嵌入矩阵叫做矩阵𝐸，注意如果用𝐸去乘以右边的 one-hot 向量（上图编号 3 所 示），也就是𝑂 6527 ，那么就会得到一个 300 维的向量，𝐸是 300×10,000 的，𝑂 6527 是 10,000×1 的，所以它们的积是 300×1 的，即 300 维的向量。要计算这个向量的第一个元素，你需要做 的是把𝐸的第一行（上图编号 4 所示）和𝑂 6527 的整列相乘，不过𝑂 6527 的所有元素都是 0，只 有 6257 位置上是 1，最后你得到的这个向量的第一个元素（上图编号 5 所示）就是 orange 这一列下的数字（上图编号 6 所示）。然后我们要计算这个向量的第二个元素，就是把𝐸的 第二行（上图编号 7 所示）和这个𝑂 6527 相乘，和之前一样，然后得到第二个元素（上图编 号 8 所示），以此类推，直到你得到这个向量剩下的所有元素（上图编号 9 所示）。

这就是为什么把矩阵𝐸和这个 one-hot 向量相乘，最后得到的其实就是这个 300 维的列，

585 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

就是单词 orange 下的这一列，它等于𝑒 6257 ，这个符号是我们用来表示这个 300×1 的嵌入向 量的符号，它表示的单词是 orange。

更广泛来说，假如说有某个单词 w，那么𝑒 𝑤 就代表单词 w 的嵌入向量。同样，𝐸𝑂 𝑗 ，𝑂𝑗 就是只有第𝑗个位置是 1 的 one-hot 向量，得到的结果就是𝑒 𝑗 ，它表示的是字典中单词 j 的嵌 入向量。

在这一小节中，要记住的一件事就是我们的目标是学习一个嵌入矩阵𝐸。在下节视频中 你将会随机地初始化矩阵𝐸，然后使用梯度下降法来学习这个 300×10,000 的矩阵中的各个参 数，𝐸乘以这个 one-hot 向量（上图编号 1 所示）会得到嵌入向量。再多说一点，当我们写 这个等式（上图编号 2 所示）的时候，写出这些符号是很方便的，代表用矩阵𝐸乘以 one-hot 向量𝑂 𝑗 。但当你动手实现时，用大量的矩阵和向量相乘来计算它，效率是很低下的，因为 onehot 向量是一个维度非常高的向量，并且几乎所有元素都是 0，所以矩阵向量相乘效率太低，因为我们要乘以一大堆的 0。所以在实践中你会使用一个专门的函数来单独查找矩阵𝐸的某 列，而不是用通常的矩阵乘法来做，但是在画示意图时（上图所示，即矩阵𝐸乘以 one-hot 向 量示意图），这样写比较方便。但是例如在 Keras 中就有一个嵌入层，然后我们用这个嵌入 层更有效地从嵌入矩阵中提取出你需要的列，而不是对矩阵进行很慢很复杂的乘法运算。

在本视频中你见到了在学习嵌入向量的过程中用来描述这些算法的符号以及关键术语，矩阵𝐸它包含了词汇表中所有单词的嵌入向量。在下节视频中，我们将讨论学习矩阵𝐸的具 体算法。

586 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

2.5 学习词嵌入（Learning Word Embeddings）

在本节视频中，你将要学习一些具体的算法来学习词嵌入。在深度学习应用于学习词嵌 入的历史上，人们一开始使用的算法比较复杂，但随着时间推移，研究者们不断发现他们能 用更加简单的算法来达到一样好的效果，特别是在数据集很大的情况下。但有一件事情就是，现在很多最流行的算法都十分简单，如果我一开始就介绍这些简单的算法，你可能会觉得这 有点神奇，这么简单的算法究竟是怎么起作用的？稍微复杂一些的算法开始，因为我觉得这 样更容易对算法的运作方式有一个更直观的了解，之后我们会对这些算法进行简化，使你能 够明白即使一些简单的算法也能得到非常好的结果，我们开始吧。

假如你在构建一个语言模型，并且用神经网络来实现这个模型。于是在训练过程中，你 可能想要你的神经网络能够做到比如输入：「I want a glass of orange ___.」，然后预测这句话 的下一个词。在每个单词下面，我都写上了这些单词对应词汇表中的索引。实践证明，建立 一个语言模型是学习词嵌入的好方法，我提出的这些想法是源于 Yoshua Bengio，Rejean Ducharme，Pascal Vincent，Rejean Ducharme，Pascal Vincent 还有 Christian Jauvin。

下面我将介绍如何建立神经网络来预测序列中的下一个单词，让我为这些词列一个表格，「I want a glass of orange」，我们从第一个词 I 开始，建立一个 one-hot 向量表示这个单词 I。这是一个 one-hot 向量（上图编号 1 所示），在第 4343 个位置是 1，它是一个 10,000 维的 向量。然后要做的就是生成一个参数矩阵𝐸，然后用𝐸乘以𝑂 4343 ，得到嵌入向量𝑒 4343 ，这一 步意味着𝑒 4343 是由矩阵𝐸乘以 one-hot 向量得到的（上图编号 2 所示）。然后我们对其他的 词也做相同的操作，单词 want 在第 9665 个，我们将𝐸与这个 one-hot 向量（𝑂9665 ）相乘得

587 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

到嵌入向量𝑒9665 。对其他单词也是一样，a 是字典中的第一个词，因为 a 是第一个字母，由 𝑂1 得到𝑒 1 。同样地，其他单词也这样操作。于是现在你有许多 300 维的嵌入向量。我们能做的就是把它们全部放进神经网络中（上 图编号 3 所示），经过神经网络以后再通过 softmax 层（上图编号 4 所示），这个 softmax 也有自己的参数，然后这个 softmax 分类器会在 10,000 个可能的输出中预测结尾这个单词。假如说在训练集中有 juice 这个词，训练过程中 softmax 的目标就是预测出单词 juice，就是 结尾的这个单词。这个隐藏层（上图编号 3 所示）有自己的参数，我这里用𝑊 [1] 和𝑏 [1] 来表 示，这个 softmax 层（上图编号 4 所示）也有自己的参数𝑊 [2] 和𝑏 [2] 。如果它们用的是 300 维大小的嵌入向量，而这里有 6 个词，所以用 6×300，所以这个输入会是一个 1800 维的向 量，这是通过将这 6 个嵌入向量堆在一起得到的。

实际上更常见的是有一个固定的历史窗口，举个例子，你总是想预测给定四个单词（上 图编号 1 所示）后的下一个单词，注意这里的 4 是算法的超参数。这就是如何适应很长或者 很短的句子，方法就是总是只看前 4 个单词，所以说我只用这 4 个单词（上图编号 2 所示） 而不去看这几个词（上图编号 3 所示）。如果你一直使用一个 4 个词的历史窗口，这就意味 着你的神经网络会输入一个 1200 维的特征变量到这个层中（上图编号 4 所示），然后再通 过 softmax 来预测输出，选择有很多种，用一个固定的历史窗口就意味着你可以处理任意长 度的句子，因为输入的维度总是固定的。所以这个模型的参数就是矩阵𝐸，对所有的单词用 的都是同一个矩阵𝐸，而不是对应不同的位置上的不同单词用不同的矩阵。然后这些权重（上 图编号 5 所示）也都是算法的参数，你可以用反向传播来进行梯度下降来最大化训练集似 然，通过序列中给定的 4 个单词去重复地预测出语料库中下一个单词什么。

588 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

事实上通过这个算法能很好地学习词嵌入，原因是，如果你还记得我们的 orange jucie，apple juice 的例子，在这个算法的激励下，apple 和 orange 会学到很相似的嵌入，这样做能 够让算法更好地拟合训练集，因为它有时看到的是 orange juice，有时看到的是 apple juice。如果你只用一个 300 维的特征向量来表示所有这些词，算法会发现要想最好地拟合训练集，就要使 apple（苹果）、orange（橘子）、grape（葡萄）和 pear（梨）等等，还有像 durian （榴莲）这种很稀有的水果都拥有相似的特征向量。

这就是早期最成功的学习词嵌入，学习这个矩阵𝐸的算法之一。现在我们先概括一下这 个算法，看看我们该怎样来推导出更加简单的算法。现在我想用一个更复杂的句子作为例子 来解释这些算法，假设在你的训练集中有这样一个更长的句子：「I want a glass of orange juice to go along with my cereal.」。我们在上个幻灯片看到的是算法预测出了某个单词 juice，我们 把它叫做目标词（下图编号 1 所示），它是通过一些上下文，在本例中也就是这前 4 个词 （下图编号 2 所示）推导出来的。如果你的目标是学习一个嵌入向量，研究人员已经尝试过 很多不同类型的上下文。如果你要建立一个语言模型，那么一般选取目标词之前的几个词作 为上下文。但如果你的目标不是学习语言模型本身的话，那么你可以选择其他的上下文。

比如说，你可以提出这样一个学习问题，它的上下文是左边和右边的四个词，你可以把 目标词左右各 4 个词作为上下文（上图编号 3 所示）。这就意味着我们提出了一个这样的问 题，算法获得左边 4 个词，也就是 a glass of orange，还有右边四个词 to go along with，然后 要求预测出中间这个词（上图编号 4 所示）。提出这样一个问题，这个问题需要将左边的还 有右边这 4 个词的嵌入向量提供给神经网络，就像我们之前做的那样来预测中间的单词是 什么，来预测中间的目标词，这也可以用来学习词嵌入。

或者你想用一个更简单的上下文，也许只提供目标词的前一个词，比如只给出 orange

589 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

这个词来预测 orange 后面是什么（上图编号 5 所示），这将会是不同的学习问题。只给出 一个词 orange 来预测下一个词是什么（上图编号 6 所示），你可以构建一个神经网络，只 把目标词的前一个词或者说前一个词的嵌入向量输入神经网络来预测该词的下一个词。

还有一个效果非常好的做法就是上下文是附近一个单词，它可能会告诉你单词 glass（上 图编号 7 所示）是一个邻近的单词。或者说我看见了单词 glass，然后附近有一个词和 glass 位置相近，那么这个词会是什么（上图编号 8 所示）？这就是用附近的一个单词作为上下 文。我们将在下节视频中把它公式化，这用的是一种 Skip-Gram 模型的思想。这是一个简单 算法的例子，因为上下文相当的简单，比起之前 4 个词，现在只有 1 个，但是这种算法依然 能工作得很好。

研究者发现，如果你真想建立一个语言模型，用目标词的前几个单词作为上下文是常见 做法（上图编号 9 所示）。但如果你的目标是学习词嵌入，那么你就可以用这些其他类型的 上下文（上图编号 10 所示），它们也能得到很好的词嵌入。我会在下节视频详细介绍这些，我们会谈到 Word2Vec 模型。

总结一下，在本节视频中你学习了语言模型问题，模型提出了一个机器学习问题，即输 入一些上下文，例如目标词的前 4 个词然后预测出目标词，学习了提出这些问题是怎样帮助 学习词嵌入的。在下节视频，你将看到如何用更简单的上下文和更简单的算法来建立从上下 文到目标词的映射，这将让你能够更好地学习词嵌入，一起进入下节视频学习 Word2Vec 模 型。

590 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

2.6 Word2Vec 

在上个视频中你已经见到了如何学习一个神经语言模型来得到更好的词嵌入，在本视频 中你会见到 Word2Vec 算法，这是一种简单而且计算时更加高效的方式来学习这种类型的 嵌入，让我们来看看。

本视频中的大多数的想法来源于 Tomas Mikolov，Kai Chen，Greg Corrado 和 Jeff Dean。（Mikolov T, Chen K, Corrado G, et al. Efficient Estimation of Word Representations in Vector Space [J]. Computer Science, 2013.）

假设在训练集中给定了一个这样的句子：「I want a glass of orange juice to go along with my cereal.」，在 Skip-Gram 模型中，我们要做的是抽取上下文和目标词配对，来构造一个监 督学习问题。上下文不一定总是目标单词之前离得最近的四个单词，或最近的𝑛个单词。我 们要的做的是随机选一个词作为上下文词，比如选 orange 这个词，然后我们要做的是随机 在一定词距内选另一个词，比如在上下文词前后 5 个词内或者前后 10 个词内，我们就在这 个范围内选择目标词。可能你正好选到了 juice 作为目标词，正好是下一个词（表示 orange 的下一个词），也有可能你选到了前面第二个词，所以另一种配对目标词可以是 glass，还可 能正好选到了单词 my 作为目标词。

于是我们将构造一个监督学习问题，它给定上下文词，要求你预测在这个词正负 10 个 词距或者正负 5 个词距内随机选择的某个目标词。显然，这不是个非常简单的学习问题，因 为在单词 orange 的正负 10 个词距之间，可能会有很多不同的单词。但是构造这个监督学习 问题的目标并不是想要解决这个监督学习问题本身，而是想要使用这个学习问题来学到一个 好的词嵌入模型。

接下来说说模型的细节，我们继续假设使用一个 10,000 词的词汇表，有时训练使用的

591 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

词汇表会超过一百万词。但我们要解决的基本的监督学习问题是学习一种映射关系，从上下 文 c，比如单词 orange，到某个目标词，记为 t，可能是单词 juice 或者单词 glass 或者单词 my。延续上一张幻灯片的例子，在我们的词汇表中，orange 是第 6257 个单词，juice 是 10,000

个单词中的第 4834 个，这就是你想要的映射到输出𝑦的输入𝑥。

为了表示输入，比如单词 orange，你可以先从 one-hot 向量开始，我们将其写作𝑂 𝑐 ，这 就是上下文词的 one-hot 向量（上图编号 1 所示）。然后和你在上节视频中看到的类似，你 可以拿嵌入矩阵𝐸乘以向量𝑂 𝑐 ，然后得到了输入的上下文词的嵌入向量，于是这里𝑒 𝑐 = 𝐸𝑂 𝑐 。在这个神经网络中（上图编号 2 所示），我们将把向量𝑒 𝑐 喂入一个 softmax 单元。我通常把 softmax 单元画成神经网络中的一个节点（上图编号 3 所示），这不是字母 O，而是 softmax 单元，softmax 单元要做的就是输出𝑦。然后我们再写出模型的细节，这是 softmax 模型（上 图编号 4 所示），预测不同目标词的概率：

𝑒𝜃 𝑡 𝑇 𝑒 𝑐 𝑆𝑜𝑓𝑡𝑚𝑎𝑥: 𝑝(𝑡|𝑐) = ∑ 𝑗=1 10,000 𝑒𝜃 𝑗 𝑇 𝑒 𝑐 

这里𝜃 𝑡 是一个与输出𝑡有关的参数，即某个词𝑡和标签相符的概率是多少。我省略了 softmax 中的偏差项，想要加上的话也可以加上。

最终 softmax 的损失函数就会像之前一样，我们用𝑦表示目标词，我们这里用的𝑦和𝑦都 是用 one-hot 表示的，于是损失函数就会是：

10,000 𝐿(𝑦 , 𝑦) = − ∑ 𝑦 𝑖 log 𝑦𝑖 𝑖=1 

这是常用的 softmax 损失函数，𝑦 就是只有一个 1 其他都是 0 的 one-hot 向量，如果目

592 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

标词是 juice，那么第 4834 个元素就是 1，其余是 0（上图编号 5 所示）。类似的𝑦是一个从 softmax 单元输出的 10,000 维的向量，这个向量是所有可能目标词的概率。

总结一下，这大体上就是一个可以找到词嵌入的简化模型和神经网络（上图编号 2 所 示），其实就是个 softmax 单元。矩阵𝐸将会有很多参数，所以矩阵𝐸有对应所有嵌入向量𝑒𝑐 的参数（上图编号 6 所示），softmax 单元也有𝜃 𝑡 的参数（上图编号 3 所示）。如果优化这 个关于所有这些参数的损失函数，你就会得到一个较好的嵌入向量集，这个就叫做 Skip-Gram 模型。它把一个像 orange 这样的词作为输入，并预测这个输入词，从左数或从右数的某个 词，预测上下文词的前面一些或者后面一些是什么词。

实际上使用这个算法会遇到一些问题，首要的问题就是计算速度。尤其是在 softmax 模 型中，每次你想要计算这个概率，你需要对你词汇表中的所有 10,000 个词做求和计算，可 能 10,000 个词的情况还不算太差。如果你用了一个大小为 100,000 或 1,000,000 的词汇表，那么这个分母的求和操作是相当慢的，实际上 10,000 已经是相当慢的了，所以扩大词汇表 就更加困难了。

这里有一些解决方案，如分级（hierarchical）的 softmax 分类器和负采样（Negative Sampling）。

在文献中你会看到的方法是使用一个分级（hierarchical）的 softmax 分类器，意思就是 说不是一下子就确定到底是属于 10,000 类中的哪一类。想象如果你有一个分类器（上图编 号 1 所示），它告诉你目标词是在词汇表的前 5000 个中还是在词汇表的后 5000 个词中，假 如这个二分类器告诉你这个词在前 5000 个词中（上图编号 2 所示），然后第二个分类器会 告诉你这个词在词汇表的前 2500 个词中，或者在词汇表的第二组 2500 个词中，诸如此类，

593 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

直到最终你找到一个词准确所在的分类器（上图编号 3 所示），那么就是这棵树的一个叶子 节点。像这样有一个树形的分类器，意味着树上内部的每一个节点都可以是一个二分类器，比如逻辑回归分类器，所以你不需要再为单次分类，对词汇表中所有的 10,000 个词求和了。实际上用这样的分类树，计算成本与词汇表大小的对数成正比（上图编号 4 所示），而不是 词汇表大小的线性函数，这个就叫做分级 softmax 分类器。

我要提一下，在实践中分级 softmax 分类器不会使用一棵完美平衡的分类树或者说一棵 左边和右边分支的词数相同的对称树（上图编号 1 所示的分类树）。实际上，分级的 softmax 分类器会被构造成常用词在顶部，然而不常用的词像 durian 会在树的更深处（上图编号 2 所 示的分类树），因为你想更常见的词会更频繁，所以你可能只需要少量检索就可以获得常用 单词像 the 和 of。然而你更少见到的词比如 durian 就更合适在树的较深处，因为你一般不 需要到那样的深处，所以有不同的经验法则可以帮助构造分类树形成分级 softmax 分类器。所以这是你能在文献中见到的一个加速 softmax 分类的方法，但是我不会再花太多时间在这 上面了，你可以从我在第一张幻灯片中提到的 Tomas Mikolov 等人的论文中参阅更多的细节，所以我不会再花更多时间讲这个了。因为在下个视频中，我们会讲到另一个方法叫做负采样，我感觉这个会更简单一点，对于加速 softmax 和解决需要在分母中对整个词汇表求和的问题 也很有作用，下个视频中你会看到更多的细节。

但是在进入下个视频前，我想要你理解一个东西，那就是怎么对上下文 c 进行采样，一 旦你对上下文 c 进行采样，那么目标词 t 就会在上下文 c 的正负 10 个词距内进行采样。但 是你要如何选择上下文 c？一种选择是你可以就对语料库均匀且随机地采样，如果你那么做，

594 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

你会发现有一些词，像 the、of、a、and、to 诸如此类是出现得相当频繁的，于是你那么做 的话，你会发现你的上下文到目标词的映射会相当频繁地得到这些种类的词，但是其他词，像 orange、apple 或 durian 就不会那么频繁地出现了。你可能不会想要你的训练集都是这些 出现得很频繁的词，因为这会导致你花大部分的力气来更新这些频繁出现的单词的𝑒𝑐 （上图 编号 1 所示），但你想要的是花时间来更新像 durian 这些更少出现的词的嵌入，即𝑒durian 。实际上词𝑝(𝑐) 的分布并不是单纯的在训练集语料库上均匀且随机的采样得到的，而是采用了 不同的分级来平衡更常见的词和不那么常见的词。这就是 Word2Vec 的 Skip-Gram 模型，如果你读过我之前提到的论文原文，你会发现那 篇论文实际上有两个不同版本的 Word2Vec 模型，Skip-Gram 只是其中的一个，另一个叫做 CBOW，即连续词袋模型（Continuous Bag-Of-Words Model），它获得中间词两边的的上下 文，然后用周围的词去预测中间的词，这个模型也很有效，也有一些优点和缺点。

（下图左边为 CBOW，右边为 Skip-Gram）

CBOW 对小型数据库比较合适，而 Skip-Gram 在大型语料中表现更好。

总结下：CBOW 是从原始语句推测目标字词；而 Skip-Gram 正好相反，是从目标字词推 测出原始语句。而刚才讲的 Skip-Gram 模型，关键问题在于 softmax 这个步骤的计算成本非 常昂贵，因为它需要在分母里对词汇表中所有词求和。通常情况下，Skip-Gram 模型用到更 多点。在下个视频中，我会展示给你一个算法，它修改了训练目标使其可以运行得更有效，因此它可以让你应用在一个更大的训练集上面，也可以学到更好的词嵌入。

595 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

2.7 负采样（Negative Sampling）

在上个视频中，你见到了 Skip-Gram 模型如何帮助你构造一个监督学习任务，把上下文 映射到了目标词上，它如何让你学到一个实用的词嵌入。但是它的缺点就在于 softmax 计算 起来很慢。在本视频中，你会看到一个改善过的学习问题叫做负采样，它能做到与你刚才看 到的 Skip-Gram 模型相似的事情，但是用了一个更加有效的学习算法，让我们来看看这是怎 么做到的。

在本视频中大多数的想法源于 Tomas Mikolov，Ilya Sutskever，Kai Chen，Greg Corrado 和 Jeff Dean。

（Mikolov T, Sutskever I, Chen K, et al. Distributed Representations of Words and Phrases and their Compositionality[J]. 2013, 26:3111-3119.） 

我们在这个算法中要做的是构造一个新的监督学习问题，那么问题就是给定一对单词，比如 orange 和 juice，我们要去预测这是否是一对上下文词 - 目标词（context-target）。

在这个例子中 orange 和 juice 就是个正样本，那么 orange 和 king 就是个负样本，我们 把它标为 0。我们要做的就是采样得到一个上下文词和一个目标词，在这个例子中就是 orange 和 juice，我们用 1 作为标记，我把中间这列（下图编号 1 所示）叫做词（word）。这样生成一个正样本，正样本跟上个视频中生成的方式一模一样，先抽取一个上下文词，在 一定词距内比如说正负 10 个词距内选一个目标词，这就是生成这个表的第一行，即 orange–juice -1 的过程。然后为了生成一个负样本，你将用相同的上下文词，再在字典中随机选一 个词，在这里我随机选了单词 king，标记为 0。然后我们再拿 orange，再随机从词汇表中选 一个词，因为我们设想，如果随机选一个词，它很可能跟 orange 没关联，于是 orange–book0。我们再选点别的，orange 可能正好选到 the，然后是 0。还是 orange，再可能正好选到 of 这个词，再把这个标记为 0，注意 of 被标记为 0，即使 of 的确出现在 orange 词的前面。

596 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

总结一下，生成这些数据的方式是我们选择一个上下文词（上图编号 2 所示），再选一 个目标词（上图编号 3 所示），这（上图编号 4 所示）就是表的第一行，它给了一个正样本，上下文，目标词，并给定标签为 1。然后我们要做的是给定几次，比如𝐾次（上图编号 5 所 示），我们将用相同的上下文词，再从字典中选取随机的词，king、book、the、of 等，从词 典中任意选取的词，并标记 0，这些就会成为负样本（上图编号 6 所示）。出现以下情况也 没关系，就是如果我们从字典中随机选到的词，正好出现在了词距内，比如说在上下文词 orange 正负 10 个词之内。

接下来我们将构造一个监督学习问题，其中学习算法输入𝑥，输入这对词（上图编号 7 所示），要去预测目标的标签（上图编号 8 所示），即预测输出𝑦。因此问题就是给定一对 词，像 orange 和 juice，你觉得它们会一起出现么？你觉得这两个词是通过对靠近的两个词 采样获得的吗？或者你觉得我是分别在文本和字典中随机选取得到的？这个算法就是要分 辨这两种不同的采样方式，这就是如何生成训练集的方法。

那么如何选取𝐾？Mikolov 等人推荐小数据集的话，𝐾从 5 到 20 比较好。如果你的数据 集很大，𝐾就选的小一点。对于更大的数据集𝐾就等于 2 到 5，数据集越小𝐾就越大。那么在

这个例子中，我们就用𝐾 = 4。

597 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

下面我们讲讲学习从𝑥映射到𝑦的监督学习模型，这（上图编号 1 所示:𝑆𝑜𝑓𝑡𝑚𝑎𝑥: 𝑝(𝑡|𝑐) = 𝑒 𝜃 𝑇 𝑡 𝑒 𝑐 𝑇 𝑒 ）的 softmax 模型。这是我们从上张幻灯片中得到的训练集，这个（上图编号 2 所

∑𝑗=1 10,000 

𝜃 𝑒 

𝑗 

𝑐 

示）将是新的输入𝑥，这个（上图编号 3 所示）将是你要预测的值𝑦。为了定义模型，我们将 使用记号𝑐表示上下文词，记号𝑡表示可能的目标词，我再用𝑦表示 0 和 1，表示是否是一对 上下文 - 目标词。我们要做的就是定义一个逻辑回归模型，给定输入的𝑐，𝑡对的条件下，𝑦 = 1 的概率，即：

𝑃(𝑦 = 1|𝑐, 𝑡) = 𝜎(𝜃 𝑡 𝑇 𝑒 𝑐 ) 

这个模型基于逻辑回归模型，但不同的是我们将一个 sigmoid 函数作用于𝜃 𝑡 𝑇 𝑒 𝑐 ，参数和 之前一样，你对每一个可能的目标词有一个参数向量𝜃 𝑡 和另一个参数向量𝑒 𝑐 ，即每一个可能 上下文词的的嵌入向量，我们将用这个公式估计𝑦 = 1 的概率。如果你有𝐾个样本，你可以把 这个看作 1 𝐾 的正负样本比例，即每一个正样本你都有𝐾个对应的负样本来训练一个类似逻辑 回归的模型。

我们把这个画成一个神经网络，如果输入词是 orange，即词 6257，你要做的就是输入 one-hot 向量，再传递给𝐸，通过两者相乘获得嵌入向量𝑒 6257 ，你就得到了 10,000 个可能的 逻辑回归分类问题，其中一个（上图编号 4 所示）将会是用来判断目标词是否是 juice 的分 类器，还有其他的词，比如说可能下面的某个分类器（上图编号 5 所示）是用来预测 king 是 否是目标词，诸如此类，预测词汇表中这些可能的单词。把这些看作 10,000 个二分类逻辑 回归分类器，但并不是每次迭代都训练全部 10,000 个，我们只训练其中的 5 个，我们要训

598 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

练对应真正目标词那一个分类器，再训练 4 个随机选取的负样本，这就是𝐾 = 4 的情况。所 以不使用一个巨大的 10,000 维度的 softmax，因为计算成本很高，而是把它转变为 10,000 个 二分类问题，每个都很容易计算，每次迭代我们要做的只是训练它们其中的 5 个，一般而言 就是𝐾 + 1 个，其中𝐾个负样本和 1 个正样本。这也是为什么这个算法计算成本更低，因为 只需更新𝐾 + 1 个逻辑单元，𝐾 + 1 个二分类问题，相对而言每次迭代的成本比更新 10,000 维 的 softmax 分类器成本低。你也会在本周的编程练习中用到这个算法，这个技巧就叫负采样。因为你做的是，你有 一个正样本词 orange 和 juice，然后你会特意生成一系列负样本，这些（上图编号 6 所示） 是负样本，所以叫负采样，即用这 4 个负样本训练，4 个额外的二分类器，在每次迭代中你 选择 4 个不同的随机的负样本词去训练你的算法。

这个算法有一个重要的细节就是如何选取负样本，即在选取了上下文词 orange 之后，你如何对这些词进行采样生成负样本？一个办法是对中间的这些词进行采样，即候选的目标 词，你可以根据其在语料中的经验频率进行采样，就是通过词出现的频率对其进行采样。但 问题是这会导致你在 like、the、of、and 诸如此类的词上有很高的频率。另一个极端就是用 1 除以词汇表总词数，即 |𝑣| 1 ，均匀且随机地抽取负样本，这对于英文单词的分布是非常没有

代表性的。所以论文的作者 Mikolov 等人根据经验，他们发现这个经验值的效果最好，它位 于这两个极端的采样方法之间，既不用经验频率，也就是实际观察到的英文文本的分布，也 不用均匀分布，他们采用以下方式：

3 𝑓(𝑤 𝑖 ) 4 𝑃(𝑤 𝑖 ) = 3 ∑ 𝑗=1 10,000 𝑓(𝑤 𝑗 )4 

599 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

进行采样，所以如果𝑓(𝑤 𝑖) 是观测到的在语料库中的某个英文词的词频，通过 4 3 次方的计 算，使其处于完全独立的分布和训练集的观测分布两个极端之间。我并不确定这是否有理论 证明，但是很多研究者现在使用这个方法，似乎也效果不错。

总结一下，你已经知道了在 softmax 分类器中如何学到词向量，但是计算成本很高。在 这个视频中，你见到了如何通过将其转化为一系列二分类问题使你可以非常有效的学习词向 量。如果你使用这个算法，你将可以学到相当好的词向量。当然和深度学习的其他领域一样，有很多开源的实现，当然也有预训练过的词向量，就是其他人训练过的然后授权许可发布在 网上的，所以如果你想要在 NLP 问题上取得进展，去下载其他人的词向量是很好的方法，在 此基础上改进。

Skip-Gram 模型就介绍到这里，在下个视频中，我会跟你分享另一个版本的词嵌入学习 算法 GloVe，而且这可能比你之前看到的都要简单。

600 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

2.8 GloVe 词向量（GloVe Word Vectors）

你已经了解了几个计算词嵌入的算法，另一个在 NLP 社区有着一定势头的算法是 GloVe 算法，这个算法并不如 Word2Vec 或是 Skip-Gram 模型用的多，但是也有人热衷于它，我认 为可能是因为它简便吧，我们来看看这个算法。

Glove 算法是由 Jeffrey Pennington，Richard Socher 和 Chris Manning 发明的。(Pennington J, Socher R, Manning C. Glove: Global Vectors for Word Representation [C]// Conference on Empirical Methods in Natural Language Processing. 2014:1532-1543.)

GloVe 代表用词表示的全局变量（global vectors for word representation）。在此之前，我们曾通过挑选语料库中位置相近的两个词，列举出词对，即上下文和目标词，GloVe 算法 做的就是使其关系开始明确化。假定𝑋 𝑖𝑗 是单词𝑖在单词𝑗上下文中出现的次数，那么这里𝑖和𝑗 就和𝑡和𝑐的功能一样，所以你可以认为𝑋 𝑖𝑗 等同于𝑋 𝑡𝑐 。你也可以遍历你的训练集，然后数出 单词𝑖在不同单词𝑗上下文中出现的个数，单词𝑡在不同单词𝑐的上下文中共出现多少次。根据 上下文和目标词的定义，你大概会得出𝑋 𝑖𝑗 等于𝑋 𝑗𝑖 这个结论。事实上，如果你将上下文和目 标词的范围定义为出现于左右各 10 词以内的话，那么就会有一种对称关系。如果你对上下 文的选择是，上下文总是目标词前一个单词的话，那么𝑋 𝑖𝑗 和𝑋 𝑗𝑖 就不会像这样对称了。不过 对于 GloVe 算法，我们可以定义上下文和目标词为任意两个位置相近的单词，假设是左右各 10 词的距离，那么𝑋 𝑖𝑗 就是一个能够获取单词𝑖和单词𝑗出现位置相近时或是彼此接近的频率 的计数器。

GloVe 模型做的就是进行优化，我们将他们之间的差距进行最小化处理：

601 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

10,000 10,000 

2 minimize ∑ ∑ 𝑓(𝑋 𝑖𝑗 )(𝜃 𝑖 𝑇 𝑒 𝑗 + 𝑏 𝑖 + 𝑏 𝑗 ′ − 𝑙𝑜𝑔𝑋 𝑖𝑗 ) 

𝑖=1 

𝑗=1 

其中𝜃 𝑖 𝑇 𝑒 𝑗 ，想一下𝑖和𝑗与𝑡和𝑐的功能一样，因此这就和你之前看的有些类似了，即𝜃 𝑡 𝑇 𝑒 𝑐 。同时对于这个（𝜃 𝑡 𝑇 𝑒 𝑐 ，下图编号 1 所示）来说，你想要知道的是告诉你这两个单词之间有多 少联系，𝑡和𝑐之间有多紧密，𝑖和𝑗之间联系程度如何，换句话说就是他们同时出现的频率是 多少，这是由这个𝑋 𝑖𝑗 影响的。然后，我们要做的是解决参数𝜃和𝑒的问题，然后准备用梯度 下降来最小化上面的公式，你只想要学习一些向量，这样他们的输出能够对这两个单词同时 出现的频率进行良好的预测。

现在一些附加的细节是如果𝑋 𝑖𝑗 是等于 0 的话，那么𝑙𝑜𝑔0 就是未定义的，是负无穷大的，所以我们想要对𝑋 𝑖𝑗 为 0 时进行求和，因此要做的就是添加一个额外的加权项𝑓(𝑋 𝑖𝑗)（上图编 号 2 所示）。如果𝑋 𝑖𝑗 等于 0 的话，同时我们会用一个约定，即 0𝑙𝑜𝑔0 = 0，这个的意思是如 果𝑋 𝑖𝑗 = 0，先不要进行求和，所以这个𝑙𝑜𝑔0 项就是不相关项。上面的求和公式表明，这个和 仅是一个上下文和目标词关系里连续出现至少一次的词对的和。𝑓(𝑋 𝑖𝑗) 的另一个作用是，有 些词在英语里出现十分频繁，比如说 this，is，of，a 等等，有些情况，这叫做停止词，但是 在频繁词和不常用词之间也会有一个连续统（continuum）。不过也有一些不常用的词，比 如 durion，你还是想将其考虑在内，但又不像那些常用词这样频繁。因此，这个加权因子 𝑓(𝑋 𝑖𝑗) 就可以是一个函数，即使是像 durion 这样不常用的词，它也能给予大量有意义的运 算，同时也能够给像 this，is，of，a 这样在英语里出现更频繁的词更大但不至于过分的权重。因此有一些对加权函数𝑓的选择有着启发性的原则，就是既不给这些词（this，is，of，a）过 分的权重，也不给这些不常用词（durion）太小的权值。如果你想要知道 f 是怎么能够启发

602 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

性地完成这个功能的话，你可以看一下我之前的幻灯片里引用的 GloVe 算法论文。最后，一件有关这个算法有趣的事是𝜃和𝑒现在是完全对称的，所以那里的𝜃 𝑖 和𝑒 𝑗 就是对 称的。如果你只看数学式的话，他们（𝜃 𝑖 和𝑒 𝑗 ）的功能其实很相近，你可以将它们颠倒或者 将它们进行排序，实际上他们都输出了最佳结果。因此一种训练算法的方法是一致地初始化 𝜃和𝑒，然后使用梯度下降来最小化输出，当每个词都处理完之后取平均值，所以，给定一个 𝑒 𝑤 +𝜃 词𝑤，你就会有𝑒 𝑤 (𝑓𝑖𝑛𝑎𝑙) = 2 𝑤 。因为𝜃和𝑒在这个特定的公式里是对称的，而不像之前视频 里我们了解的模型，𝜃和𝑒功能不一样，因此也不能像那样取平均。

这就是 GloVe 算法的内容，我认为这个算法的一个疑惑之处是如果你看着这个等式，它 实在是太简单了，对吧？仅仅是最小化，像这样的一个二次代价函数（上图编号 3 所示）是 怎么能够让你学习有意义的词嵌入的呢？但是结果证明它确实有效，发明者们发明这个算法 的过程是他们以历史上更为复杂的算法，像是 newer language 模型，以及之后的 Word2Vec、 Skip-Gram 模型等等为基础，同时希望能够简化所有之前的算法才发明的。

在我们总结词嵌入学习算法之前，有一件更优先的事，我们会简单讨论一下。就是说，我们以这个特制的表格作为例子来开始学习词向量，我们说，第一行的嵌入向量是来表示 Gender 的，第二行是来表示 Royal 的，然后是是 Age，在之后是 Food 等等。但是当你在使 用我们了解过的算法的一种来学习一个词嵌入时，例如我们之前的幻灯片里提到的 GloVe 算 法，会发生一件事就是你不能保证嵌入向量的独立组成部分是能够理解的，为什么呢？

假设说有个空间，里面的第一个轴（上图编号 1 所示）是 Gender，第二个轴（上图编 号 2 所示）是 Royal，你能够保证的是第一个嵌入向量对应的轴（上图编号 3 所示）是和这 个轴（上面提到的第一和第二基轴，编号 1，2 所示）有联系的，它的意思可能是 Gender、

603 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

Royal、Age 和 Food。具体而言，这个学习算法会选择这个（上图编号 3 所示）作为第一维 的轴，所以给定一些上下文词，第一维可能是这个轴（上图编号 3 所示），第二维也许是这 个（上图编号 4 所示），或者它可能不是正交的，它也可能是第二个非正交轴（上图编号 5 所示），它可以是你学习到的词嵌入中的第二部分。当我们看到这个（上图编号 6 所示）的 时候，如果有某个可逆矩阵𝐴，那么这项（上图编号 6 所示）就可以简单地替换成 (𝐴𝜃) 𝑇 (𝐴 −𝑇 𝑒 𝑗)，因为我们将其展开：

(𝐴𝜃 𝑖 ) 𝑇 (𝐴 −𝑇 𝑒 𝑗 ) = 𝜃 𝑖 𝑇 𝐴 𝑇 𝐴 −𝑇 𝑒 𝑗 = 𝜃 𝑖 𝑇 𝑒𝑗 

不必担心，如果你没有学过线性代数的话会，和这个算法一样有一个简单证明过程。你 不能保证这些用来表示特征的轴能够等同于人类可能简单理解的轴，具体而言，第一个特征 可能是个 Gender、Roya、Age、Food Cost 和 Size 的组合，它也许是名词或是一个行为动词 和其他所有特征的组合，所以很难看出独立组成部分，即这个嵌入矩阵的单行部分，然后解 释出它的意思。尽管有这种类型的线性变换，这个平行四边形映射也说明了我们解决了这个 问题，当你在类比其他问题时，这个方法也是行得通的。因此尽管存在特征量潜在的任意线 性变换，你最终还是能学习出解决类似问题的平行四边形映射。

这就是词嵌入学习的内容，你现在已经了解了一些学习词嵌入的算法了，你可以在本周 的编程练习里更多地运用它们。下节课讲解怎样使用这些算法来解决情感分类问题。

604 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

2.9 情感分类（Sentiment Classification）

情感分类任务就是看一段文本，然后分辨这个人是否喜欢他们在讨论的这个东西，这是 NLP 中最重要的模块之一，经常用在许多应用中。情感分类一个最大的挑战就是可能标记的 训练集没有那么多，但是有了词嵌入，即使只有中等大小的标记的训练集，你也能构建一个 不错的情感分类器，让我们看看是怎么做到的。

这是一个情感分类问题的一个例子（上图所示），输入𝑥是一段文本，而输出𝑦是你要预 测的相应情感。比如说是一个餐馆评价的星级，

比如有人说，"The dessert is excellent."（甜点很棒），并给出了四星的评价；

"Service was quite slow"（服务太慢），两星评价；

"Good for a quick meal but nothing special"（适合吃快餐但没什么亮点），三星评价；

还有比较刁钻的评论，"Completely lacking in good taste, good service and good ambiance." （完全没有好的味道，好的服务，好的氛围），给出一星评价。

如果你能训练一个从𝑥到𝑦的映射，基于这样的标记的数据集，那么你就可以用来搜集大 家对你运营的餐馆的评价。一些人可能会把你的餐馆信息放到一些社交媒体上，Twitter、 Facebook、Instagram 或者其他的社交媒体，如果你有一个情感分类器，那么它就可以看一 段文本然后分析出这个人对你的餐馆的评论的情感是正面的还是负面的，这样你就可以一直 记录是否存在一些什么问题，或者你的餐馆是在蒸蒸日上还是每况愈下。

情感分类一个最大的挑战就是可能标记的训练集没有那么多。对于情感分类任务来说，训练集大小从 10,000 到 100,000 个单词都很常见，甚至有时会小于 10,000 个单词，采用了

605 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

词嵌入能够带来更好的效果，尤其是只有很小的训练集时。

接下来你可以这样做，这节我们会讲几个不同的算法。这是一个简单的情感分类的模型，假设有一个句子 "dessert is excellent"，然后在词典里找这些词，我们通常用 10,000 个词的词 汇表。我们要构建一个分类器能够把它映射成输出四个星，给定这四个词（"dessert is excellent"），我们取这些词，找到相应的 one-hot 向量，所以这里（上图编号 1 所示）就是 𝑜8928 ，乘以嵌入矩阵𝐸，𝐸可以从一个很大的文本集里学习到，比如它可以从一亿个词或者 一百亿个词里学习嵌入，然后用来提取单词 the 的嵌入向量𝑒8928 ，对 dessert、is、excellent 做同样的步骤。

如果在很大的训练集上训练𝐸，比如一百亿的单词，这样你就会获得很多知识，甚至从 有些不常用的词中获取，然后应用到你的问题上，即使你的标记数据集里没有这些词。我们 可以这样构建一个分类器，取这些向量（上图编号 2 所示），比如是 300 维度的向量。然后 把它们求和或者求平均，这里我画一个大点的平均值计算单元（上图编号 3 所示），你也可 以用求和或者平均。这个单元（上图编号 3 所示）会得到一个 300 维的特征向量，把这个特 征向量送进 softmax 分类器，然后输出𝑦。这个 softmax 能够输出 5 个可能结果的概率值，从一星到五星，这个就是 5 个可能输出的 softmax 结果用来预测𝑦的值。

这里用的平均值运算单元，这个算法适用于任何长短的评论，因为即使你的评论是 100 个词长，你也可以对这一百个词的特征向量求和或者平均它们，然后得到一个表示一个 300 维的特征向量表示，然后把它送进你的 softmax 分类器，所以这个平均值运算效果不错。它 实际上会把所有单词的意思给平均起来，或者把你的例子中所有单词的意思加起来就可以用 了。

这个算法有一个问题就是没考虑词序，尤其是这样一个负面的评价，"Completely lacking

606 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

in good taste, good service, and good ambiance."，但是 good 这个词出现了很多次，有 3 个 good，如果你用的算法跟这个一样，忽略词序，仅仅把所有单词的词嵌入加起来或者平均下 来，你最后的特征向量会有很多 good 的表示，你的分类器很可能认为这是一个好的评论，尽管事实上这是一个差评，只有一星的评价。

我们有一个更加复杂的模型，不用简单的把所有的词嵌入都加起来，我们用一个 RNN 来做情感分类。我们这样做，首先取这条评论，"Completely lacking in good taste, good service, and good ambiance."，找出每一个 one-hot 向量，这里我跳过去每一个 one-hot 向量的表示。用每一个 one-hot 向量乘以词嵌入矩阵𝐸，得到词嵌入表达𝑒，然后把它们送进 RNN 里。RNN 的工作就是在最后一步（上图编号 1 所示）计算一个特征表示，用来预测𝑦，这是一个多对 一的网络结构的例子，我们之前已经见过了。有了这样的算法，考虑词的顺序效果就更好了，它就能意识到 "things are lacking in good taste"，这是个负面的评价，「not good」也是一个负 面的评价。而不像原来的算法一样，只是把所有的加在一起得到一个大的向量，根本意识不 到「not good」和「good」不是一个意思，"lacking in good taste" 也是如此，等等。

如果你训练一个这样的算法，最后会得到一个很合适的情感分类的算法。由于你的词嵌 入是在一个更大的数据集里训练的，这样效果会更好，更好的泛化一些没有见过的新的单词。比如其他人可能会说，"Completely absent of good taste, good service, and good ambiance."，即使 absent 这个词不在标记的训练集里，如果是在一亿或者一百亿单词集里训练词嵌入，它仍然可以正确判断，并且泛化的很好，甚至这些词是在训练集中用于训练词嵌入的，但是 可以不在专门用来做情感分类问题的标记的训练集中。

以上就是情感分类的问题，我希望你能大体了解。一旦你学习到或者从网上下载词嵌入，你就可以很快构建一个很有效的 NLP 系统。

607 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

2.10 词嵌入除偏（Debiasing Word Embeddings）

现在机器学习和人工智能算法正渐渐地被信任用以辅助或是制定极其重要的决策，因此 我们想尽可能地确保它们不受非预期形式偏见影响，比如说性别歧视、种族歧视等等。本节 视频中我会向你展示词嵌入中一些有关减少或是消除这些形式的偏见的办法。

本节视频中当我使用术语 bias 时，我不是指 bias 本身这个词，或是偏见这种感觉，而 是指性别、种族、性取向方面的偏见，那是不同的偏见，同时这也通常用于机器学习的学术 讨论中。不过我们讨论的大部分内容是词嵌入是怎样学习类比像 Man：Woman，就像 King： Queen，不过如果你这样问，如果 Man 对应 Computer Programmer，那么 Woman 会对应什 么呢？所以这篇论文（上图编号 1 所示：Bolukbasi T, Chang K W, Zou J, et al. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings [J]. 2016.）的作者 Tolga Bolukbasi、Kai-Wei Chang、James Zou、Venkatesh Saligrama 和 Adam Kalai 发现了一个十分 可怕的结果，就是说一个已经完成学习的词嵌入可能会输出 Man：Computer Programmer，同时输出 Woman：Homemaker，那个结果看起来是错的，并且它执行了一个十分不良的性 别歧视。如果算法输出的是 Man：Computer Programmer，同时 Woman：Computer Programmer 这样子会更合理。同时他们也发现如果 Father：Doctor，那么 Mother 应该对应什么呢？一 个十分不幸的结果是，有些完成学习的词嵌入会输出 Mother：Nurse。

因此根据训练模型所使用的文本，词嵌入能够反映出性别、种族、年龄、性取向等其他 方面的偏见，一件我尤其热衷的事是，这些偏见都和社会经济状态相关，我认为每个人不论 你出身富裕还是贫穷，亦或是二者之间，我认为每个人都应当拥有好的机会，同时因为机器 学习算法正用来制定十分重要的决策，它也影响着世间万物，从大学录取到人们找工作的途 径，到贷款申请，不论你的的贷款申请是否会被批准，再到刑事司法系统，甚至是判决标准，

608 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

学习算法都在作出非常重要的决策，所以我认为我们尽量修改学习算法来尽可能减少或是理 想化消除这些非预期类型的偏见是十分重要的。至于词嵌入，它们能够轻易学会用来训练模型的文本中的偏见内容，所以算法获取到的 偏见内容就可以反映出人们写作中的偏见。在漫长的世纪里，我认为人类已经在减少这些类 型的偏见上取得了进展，幸运的是对于人工智能来说，实际上我认为有更好的办法来实现更 快地减少 AI 领域中相比与人类社会中的偏见。虽然我认为我们仍未实现人工智能，仍然有 许多研究许多难题需要完成来减少学习算法中这些类型的偏见。

本节视频里我想要做的是与你们分享一个例子，它是一篇论文的一套办法，就是下面引 用的这篇由 Bolukbasi 和其他人共同撰写的论文，它是研究减少词嵌入中偏见问题的。就是 这些，假设说我们已经完成一个词嵌入的学习，那么 babysitter 就是在这里，doctor 在这里，grandmother 在这里，grandfather 在这里，也许 girl 嵌入在这里，boy 嵌入在这里，也许 she 嵌在这里，he 在这里（上图编号 1 所示的区域内），所以首先我们要做的事就是辨别出我 们想要减少或想要消除的特定偏见的趋势。

为了便于说明，我会集中讨论性别歧视，不过这些想法对于所有我在上个幻灯片里提及 的其他类型的偏见都是通用的。这个例子中，你会怎样辨别出与这个偏见相似的趋势呢？主 要有以下三个步骤：

一、对于性别歧视这种情况来说，我们能做的是𝑒 he − 𝑒she ，因为它们的性别不同，然后 将𝑒 male − 𝑒female ，然后将这些值取平均（上图编号 2 所示），将这些差简单地求平均。这个 趋势（上图编号 3 所示）看起来就是性别趋势或说是偏见趋势，然后这个趋势（上图编号 4 所示）与我们想要尝试处理的特定偏见并不相关，因此这就是个无偏见趋势。在这种情况下，

609 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

偏见趋势可以将它看做 1D 子空间，所以这个无偏见趋势就会是 299D 的子空间。我已经略 微简化了，原文章中的描述这个偏见趋势可以比 1 维更高，同时相比于取平均值，如同我在 这里描述的这样，实际上它会用一个更加复杂的算法叫做 SVU，也就是奇异值分解，如果你 对主成分分析（Principle Component Analysis）很熟悉的话，奇异值分解这个算法的一些方 法和主成分分析 (PCA) 其实很类似。

二、中和步骤，所以对于那些定义不确切的词可以将其处理一下，避免偏见。有些词本 质上就和性别有关，像 grandmother、grandfather、girl、boy、she、he，他们的定义中本就 含有性别的内容，不过也有一些词像 doctor 和 babysitter 我们想使之在性别方面是中立的。同时在更通常的情况下，你可能会希望像 doctor 或 babysitter 这些词成为种族中立的，或是 性取向中立的等等，不过这里我们仍然只用性别来举例说明。对于那些定义不明确的词，它 的基本意思是不像 grandmother 和 grandfather 这种定义里有着十分合理的性别含义的，因 为从定义上来说 grandmothers 是女性，grandfather 是男性。所以对于像 doctor 和 babysitter 这种单词我们就可以将它们在这个轴（上图编号 1 所示）上进行处理，来减少或是消除他们 的性别歧视趋势的成分，也就是说减少他们在这个水平方向上的距离（上图编号 2 方框内所 示的投影），所以这就是第二个中和步。

610 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

三、均衡步，意思是说你可能会有这样的词对，grandmother 和 grandfather，或者是 girl 和 boy，对于这些词嵌入，你只希望性别是其区别。那为什么要那样呢？在这个例子中，babysitter 和 grandmother 之间的距离或者说是相似度实际上是小于 babysitter 和 grandfather 之间的（上图编号 1 所示），因此这可能会加重不良状态，或者可能是非预期的偏见，也就 是说 grandmothers 相比于 grandfathers 最终更有可能输出 babysitting。所以在最后的均衡 步中，我们想要确保的是像 grandmother 和 grandfather 这样的词都能够有一致的相似度，或者说是相等的距离，和 babysitter 或是 doctor 这样性别中立的词一样。这其中会有一些线 性代数的步骤，但它主要做的就是将 grandmother 和 grandfather 移至与中间轴线等距的一 对点上（上图编号 2 所示），现在性别歧视的影响也就是这两个词与 babysitter 的距离就完 全相同了（上图编号 3 所示）。所以总体来说，会有许多对像 grandmother-grandfather，boygirl，sorority-fraternity，girlhood-boyhood，sister-brother，niece-nephew，daughter-son 这 样的词对，你可能想要通过均衡步来解决他们。

最后一个细节是你怎样才能够决定哪个词是中立的呢？对于这个例子来说 doctor 看起 来像是一个应该对其中立的单词来使之性别不确定或是种族不确定。相反地，grandmother 和 grandfather 就不应是性别不确定的词。也会有一些像是 beard 词，一个统计学上的事实 是男性相比于比女性更有可能拥有胡子，因此也许 beard 应该比 female 更靠近 male 一些。

因此论文作者做的就是训练一个分类器来尝试解决哪些词是有明确定义的，哪些词是性 别确定的，哪些词不是。结果表明英语里大部分词在性别方面上是没有明确定义的，意思就 是说性别并是其定义的一部分，只有一小部分词像是 grandmother-grandfather，girl-boy，sorority-fraternity 等等，不是性别中立的。因此一个线性分类器能够告诉你哪些词能够通过

611 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

中和步来预测这个偏见趋势，或将其与这个本质是 299D 的子空间进行处理。最后，你需要平衡的词对的数实际上是很小的，至少对于性别歧视这个例子来说，用手 都能够数出来你需要平衡的大部分词对。完整的算法会比我在这里展示的更复杂一些，你可 以去看一下这篇论文了解详细内容，你也可以通过编程作业来练习一下这些想法。

参考资料：针对性别特定词汇的均衡算法

如何对两个单词除偏，比如："actress「（「女演员」）和「actor」（「演员」）。均衡算法适 用于您可能希望仅通过性别属性不同的单词对。举一个具体的例子，假设"actress「（「女演 员」）比「actor」（「演员」）更接近「保姆」。通过将中和应用于 "babysit"（「保姆」），我们可以 减少与保姆相关的性别刻板印象。但是这仍然不能保证 "actress「（「女演员」）和「actor」（「演 员」）与"babysit"（「保姆」）等距。均衡算法可以解决这个问题。

均衡背后的关键思想是确保一对特定的单词与 49 维𝑔⊥ 距离相等 。均衡步骤还可以确 保两个均衡步骤现在与𝑒 𝑟𝑒𝑐𝑒𝑝𝑡𝑖𝑜𝑛𝑖𝑠𝑡 𝑑𝑒𝑏𝑖𝑎𝑠𝑒𝑑 距离相同，或者用其他方法进行均衡。下图演示了均衡 算法的工作原理：

公式的推导有点复杂 (参考论文：Bolukbasi T, Chang K W, Zou J, et al. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings [J]. 2016.)

主要步骤如下:

e w 1 +ew 2 = 2 

 *bias a xis  B = + || bias a xis|| 2 *bias a xis || bias a xis||2 

 ⊥ = −B 

612 

第五门课 序列模型 (Sequence Models)- 第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）

(e w 1 −  ⊥ ) − B e w 1 B = |1− ||  ⊥ ||2 2 |* | (e w 1 −  ⊥ ) −  B ) | 

(e w2 −  ⊥ ) − B e w 2 B = |1− ||  ⊥ ||2 2 |* | (e w 2 −  ⊥ ) −  B ) | 

e 1 =e w 1 B + ⊥ e 2 =e w 2 B +⊥ 

总结一下，减少或者是消除学习算法中的偏见问题是个十分重要的问题，因为这些算法 会用来辅助制定越来越多的社会中的重要决策，在本节视频中分享了一套如何尝试处理偏见 问题的办法，不过这仍是一个许多学者正在进行主要研究的领域。

参考文献：

• The debiasing algorithm is from Bolukbasi et al., 2016, Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings 

• The GloVe word embeddings were due to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/) 

613 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

3.1 序列结构的各种序列（Various sequence to sequence architectures）

在这一周，你将会学习 seq2seq（sequence to sequence）模型，从机器翻译到语音识别，它们都能起到很大的作用，从最基本的模型开始。之后你还会学习集束搜索（Beam search） 和注意力模型（Attention Model），一直到最后的音频模型，比如语音。

现在就开始吧，比如你想通过输入一个法语句子，比如这句「Jane visite I'Afrique en septembre.」，将它翻译成一个英语句子，「Jane is visiting Africa in September.」。和之前一样，我们用𝑥 <1> 一直到𝑥<5> 来表示输入的句子的单词，然后我们用𝑦<1> 到𝑦<6> 来表示输出的句 子的单词，那么，如何训练出一个新的网络来输入序列𝑥和输出序列𝑦呢？

这里有一些方法，这些方法主要都来自于两篇论文，作者是 Sutskever，Oriol Vinyals 和 Quoc Le，另一篇的作者是 Kyunghyun Cho，Bart van Merrienboer，Caglar Gulcehre，Dzmitry Bahdanau，Fethi Bougares，Holger Schwen 和 Yoshua Bengio。

614 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

首先，我们先建立一个网络，这个网络叫做编码网络（encoder network）（上图编号 1 所示），它是一个 RNN 的结构，RNN 的单元可以是 GRU 也可以是 LSTM。每次只向该网络 中输入一个法语单词，将输入序列接收完毕后，这个 RNN 网络会输出一个向量来代表这个 输入序列。之后你可以建立一个解码网络，我把它画出来（上图编号 2 所示），它以编码网 络的输出作为输入，编码网络是左边的黑色部分（上图编号 1 所示），之后它可以被训练为 每次输出一个翻译后的单词，一直到它输出序列的结尾或者句子结尾标记，这个解码网络的 工作就结束了。和往常一样我们把每次生成的标记都传递到下一个单元中来进行预测，就像 之前用语言模型合成文本时一样。

深度学习在近期最卓越的成果之一就是这个模型确实有效，在给出足够的法语和英语文 本的情况下，如果你训练这个模型，通过输入一个法语句子来输出对应的英语翻译，这个模 型将会非常有效。这个模型简单地用一个编码网络来对输入的法语句子进行编码，然后用一 个解码网络来生成对应的英语翻译。

还有一个与此类似的结构被用来做图像描述，给出一张图片，比如这张猫的图片（上图 编号 1 所示），它能自动地输出该图片的描述，一只猫坐在椅子上，那么你如何训练出这样 的网络？通过输入图像来输出描述，像这个句子一样。

方法如下，在之前的卷积网络课程中，你已经知道了如何将图片输入到卷积神经网络中，比如一个预训练的 AlexNet 结构（上图编号 2 方框所示），然后让其学习图片的编码，或者 学习图片的一系列特征。现在幻灯片所展示的就是 AlexNet 结构，我们去掉最后的 softmax 单元（上图编号 3 所示），这个预训练的 AlexNet 结构会给你一个 4096 维的特征向量，向 量表示的就是这只猫的图片，所以这个预训练网络可以是图像的编码网络。现在你得到了一

615 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

个 4096 维的向量来表示这张图片，接着你可以把这个向量输入到 RNN 中（上图编号 4 方框 所示），RNN 要做的就是生成图像的描述，每次生成一个单词，这和我们在之前将法语译为 英语的机器翻译中看到的结构很像，现在你输入一个描述输入的特征向量，然后让网络生成 一个输出序列，或者说一个一个地输出单词序列。

事实证明在图像描述领域，这种方法相当有效，特别是当你想生成的描述不是特别长时。据我所知，这种模型首先是由 Junhua Mao，Wei Xu，Yi Yang，Jiang Wang，Zhiheng Huang 和 Alan Yuille 提出的，尽管有几个团队都几乎在同一时间构造出了非常相似的模型，因为还有 另外两个团队也在同一时间得出了相似的结论。我觉得有可能 Mao 的团队和 Oriol Vinyals，Alexander Toshev，Samy Bengio 和 Dumitru Erhan，还有 Andrej Karpathy 和 Fei-Fei Li 是同一 个团队。

现在你知道了基本的 seq2seq 模型是怎样运作的，以及 image to sequence 模型或者说 图像描述模型是怎样运作的。不过这两个模型运作方式有一些不同，主要体现在如何用语言 模型合成新的文本，并生成对应序列的方面。一个主要的区别就是你大概不会想得到一个随 机选取的翻译，你想要的是最准确的翻译，或者说你可能不想要一个随机选取的描述，你想 要的是最好的最贴切的描述，我们将在下节视频中介绍如何生成这些序列。

616 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

3.2 选择最可能的句子（Picking the most likely sentence）

在 seq2seq 机器翻译模型和我们在第一周课程所用的语言模型之间有很多相似的地方，但是它们之间也有许多重要的区别，让我们来一探究竟。

你可以把机器翻译想成是建立一个条件语言模型，在语言模型中上方是一个我们在第一 周所建立的模型，这个模型可以让你能够估计句子的可能性，这就是语言模型所做的事情。你也可以将它用于生成一个新的句子，如果你在图上的该处（下图编号 1 所示），有𝑥<1> 和 𝑥<2> ，那么在该例中𝑥 <2> = 𝑦<1> ，但是𝑥<1> 、𝑥<2> 等在这里并不重要。为了让图片看起来 更简洁，我把它们先抹去，可以理解为𝑥<1> 是一个全为 0 的向量，然后𝑥<2> 、𝑥<3> 等都等 于之前所生成的输出，这就是所说的语言模型。

而机器翻译模型是下面这样的，我这里用两种不同的颜色来表示，即绿色和紫色，用绿 色（上图编号 2 所示）表示 encoder 网络，用紫色（上图编号 3 所示）表示 decoder 网络。你会发现 decoder 网络看起来和刚才所画的语言模型几乎一模一样，机器翻译模型其实和语 言模型非常相似，不同在于语言模型总是以零向量（上图编号 4 所示）开始，而 encoder 网 络会计算出一系列向量（上图编号 2 所示）来表示输入的句子。有了这个输入句子，decoder 网络就可以以这个句子开始，而不是以零向量开始，所以我把它叫做条件语言模型 （conditional language model）。相比语言模型，输出任意句子的概率，翻译模型会输出句 子的英文翻译（上图编号 5 所示），这取决于输入的法语句子（上图编号 6 所示）。换句话 说，你将估计一个英文翻译的概率，比如估计这句英语翻译的概率，"Jane is visiting Africa in September."，这句翻译是取决于法语句子，"Jane visite I'Afrique en septembre."，这就是英

617 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

语句子相对于输入的法语句子的可能性，所以它是一个条件语言模型。

现在，假如你想真正地通过模型将法语翻译成英文，通过输入的法语句子模型将会告诉 你各种英文翻译所对应的可能性。𝑥在这里是法语句子 "Jane visite l'Afrique en septembre."，而它将告诉你不同的英语翻译所对应的概率。显然你不想让它随机地进行输出，如果你从这 个分布中进行取样得到𝑃(𝑦|𝑥)，可能取样一次就能得到很好的翻译，"Jane is visiting Africa in September."。但是你可能也会得到一个截然不同的翻译，"Jane is going to be visiting Africa in September."，这句话听起来有些笨拙，但它不是一个糟糕的翻译，只是不是最好的而已。有 时你也会偶然地得到这样的翻译，"In September, Jane will visit Africa."，或者有时候你还会 得到一个很糟糕的翻译，"Her African friend welcomed Jane in September."。所以当你使用这 个模型来进行机器翻译时，你并不是从得到的分布中进行随机取样，而是你要找到一个英语 句子𝑦（上图编号 1 所示），使得条件概率最大化。所以在开发机器翻译系统时，你需要做 的一件事就是想出一个算法，用来找出合适的𝑦值，使得该项最大化，而解决这种问题最通 用的算法就是束搜索 (Beam Search)，你将会在下节课见到它。

不过在了解束搜索之前，你可能会问一个问题，为什么不用贪心搜索 (Greedy Search) 呢？ 贪心搜索是一种来自计算机科学的算法，生成第一个词的分布以后，它将会根据你的条件语 言模型挑选出最有可能的第一个词进入你的机器翻译模型中，在挑选出第一个词之后它将会 继续挑选出最有可能的第二个词，然后继续挑选第三个最有可能的词，这种算法就叫做贪心 搜索，但是你真正需要的是一次性挑选出整个单词序列，从𝑦<1> 、𝑦<2> 到𝑦<𝑇 𝑦> 来使得整体 的概率最大化。所以这种贪心算法先挑出最好的第一个词，在这之后再挑最好的第二词，然 后再挑第三个，这种方法其实并不管用，为了证明这个观点，我们来考虑下面两种翻译。

618 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

第一串（上图编号 1 所示）翻译明显比第二个（上图编号 2 所示）好，所以我们希望机 器翻译模型会说第一个句子的𝑃(𝑦|𝑥) 比第二个句子要高，第一个句子对于法语原文来说更好 更简洁，虽然第二个也不错，但是有些啰嗦，里面有很多不重要的词。但如果贪心算法挑选 出了 "Jane is" 作为前两个词，因为在英语中 going 更加常见，于是对于法语句子来说 "Jane is going" 相比 "Jane is visiting" 会有更高的概率作为法语的翻译，所以很有可能如果你仅仅根据 前两个词来估计第三个词的可能性，得到的就是 going，最终你会得到一个欠佳的句子，在 𝑃(𝑦|𝑥) 模型中这不是一个最好的选择。

我知道这种说法可能比较粗略，但是它确实是一种广泛的现象，当你想得到单词序列 𝑦 <1> 、𝑦 <2> 一直到最后一个词总体的概率时，一次仅仅挑选一个词并不是最佳的选择。当 然，在英语中各种词汇的组合数量还有很多很多，如果你的字典中有 10,000 个单词，并且 你的翻译可能有 10 个词那么长，那么可能的组合就有 10,000 的 10 次方这么多，这仅仅是 10 个单词的句子，从这样大一个字典中来挑选单词，所以可能的句子数量非常巨大，不可 能去计算每一种组合的可能性。所以这时最常用的办法就是用一个近似的搜索算法，这个近 似的搜索算法做的就是它会尽力地，尽管不一定总会成功，但它将挑选出句子𝑦使得条件概 率最大化，尽管它不能保证找到的𝑦值一定可以使概率最大化，但这已经足够了。

最后总结一下，在本视频中，你看到了机器翻译是如何用来解决条件语言模型问题的，这个模型和之前的语言模型一个主要的区别就是，相比之前的模型随机地生成句子，在该模 型中你要找到最有可能的英语句子，最可能的英语翻译，但是可能的句子组合数量过于巨大，无法一一列举，所以我们需要一种合适的搜索算法，让我们在下节课中学习集束搜索。

619 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

3.3 集束搜索（Beam Search）

这节视频中你会学到集束搜索（beam search）算法，上节视频中我们讲了对于机器翻译 来说，给定输入，比如法语句子，你不会想要输出一个随机的英语翻译结果，你想要一个最 好的，最可能的英语翻译结果。对于语音识别也一样，给定一个输入的语音片段，你不会想 要一个随机的文本翻译结果，你想要最好的，最接近原意的翻译结果，集束搜索就是解决这 个最常用的算法。这节视频里，你会明白怎么把集束搜索算法应用到你自己的工作中，就用 我们的法语句子的例子来试一下集束搜索吧。

「Jane visite l'Afrique en Septembre.」（法语句子），我们希望翻译成英语，"Jane is visiting Africa in September".（英语句子），集束搜索算法首先做的就是挑选要输出的英语翻译中的 第一个单词。这里我列出了 10,000 个词的词汇表（下图编号 1 所示），为了简化问题，我 们忽略大小写，所有的单词都以小写列出来。在集束搜索的第一步中我用这个网络部分，绿 色是编码部分（下图编号 2 所示），紫色是解码部分（下图编号 3 所示），来评估第一个单 词的概率值，给定输入序列𝑥，即法语作为输入，第一个输出𝑦的概率值是多少。

贪婪算法只会挑出最可能的那一个单词，然后继续。而集束搜索则会考虑多个选择，集 束搜索算法会有一个参数 B，叫做集束宽（beam width）。在这个例子中我把这个集束宽设 成 3，这样就意味着集束搜索不会只考虑一个可能结果，而是一次会考虑 3 个，比如对第一 个单词有不同选择的可能性，最后找到 in、jane、september，是英语输出的第一个单词的 最可能的三个选项，然后集束搜索算法会把结果存到计算机内存里以便后面尝试用这三个 词。如果集束宽设的不一样，如果集束宽这个参数是 10 的话，那么我们跟踪的不仅仅 3 个，而是 10 个第一个单词的最可能的选择。所以要明白，为了执行集束搜索的第一步，你需要 输入法语句子到编码网络，然后会解码这个网络，这个 softmax 层（上图编号 3 所示）会输

620 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

出 10,000 个概率值，得到这 10,000 个输出的概率值，取前三个存起来。让我们看看集束搜索算法的第二步，已经选出了 in、jane、september 作为第一个单词 三个最可能的选择，集束算法接下来会针对每个第一个单词考虑第二个单词是什么，单词 in 后面的第二个单词可能是 a 或者是 aaron，我就是从词汇表里把这些词列了出来，或者是列 表里某个位置，september，可能是列表里的 visit，一直到字母 z，最后一个单词是 zulu（下 图编号 1 所示）。

为了评估第二个词的概率值，我们用这个神经网络的部分，绿色是编码部分（上图编号 2 所示），而对于解码部分，当决定单词 in 后面是什么，别忘了解码器的第一个输出𝑦<1> ，我把𝑦<1> 设为单词 in（上图编号 3 所示），然后把它喂回来，这里就是单词 in（上图编号 4 所示），因为它的目的是努力找出第一个单词是 in 的情况下，第二个单词是什么。这个输 出就是𝑦<2> （上图编号 5 所示），有了这个连接（上图编号 6 所示），就是这里的第一个单 词 in（上图编号 4 所示）作为输入，这样这个网络就可以用来评估第二个单词的概率了，在 给定法语句子和翻译结果的第一个单词 in 的情况下。

注意，在第二步里我们更关心的是要找到最可能的第一个和第二个单词对，所以不仅仅 是第二个单词有最大的概率，而是第一个、第二个单词对有最大的概率（上图编号 7 所示）。按照条件概率的准则，这个可以表示成第一个单词的概率（上图编号 8 所示）乘以第二个单 词的概率（上图编号 9 所示），这个可以从这个网络部分里得到（上图编号 10 所示），对 于已经选择的 in、jane、september 这三个单词，你可以先保存这个概率值（上图编号 8 所 示），然后再乘以第二个概率值（上图编号 9 所示）就得到了第一个和第二个单词对的概率 （上图编号 7 所示）。

621 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

现在你已经知道在第一个单词是 in 的情况下如何评估第二个单词的概率，现在第一个 单词是 jane，道理一样，句子可能是 "jane a"、"jane aaron"，等等到 "jane is"、"jane visits" 等

等（上图编号 1 所示）。你会用这个新的网络部分（上图编号 2 所示），我在这里画一条线，代表从𝑦<1> ，即 jane，𝑦<1> 连接 jane（上图编号 3 所示），那么这个网络部分就可以告诉 你给定输入𝑥和第一个词是 jane 下，第二个单词的概率了（上图编号 4 所示），和上面一样，

你可以乘以𝑃(𝑦 <1> |𝑥) 得到𝑃(𝑦<1> , 𝑦 <2> |𝑥)。

针对第二个单词所有 10,000 个不同的选择，最后对于单词 september 也一样，从单词 a 到单词 zulu，用这个网络部分，我把它画在这里。来看看如果第一个单词是 september，第二个单词最可能是什么。所以对于集束搜索的第二步，由于我们一直用的集束宽为 3，并 且词汇表里有 10,000 个单词，那么最终我们会有 3 乘以 10,000 也就是 30,000 个可能的结 果，因为这里（上图编号 1 所示）是 10,000，这里（上图编号 2 所示）是 10,000，这里（上

622 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

图编号 3 所示）是 10,000，就是集束宽乘以词汇表大小，你要做的就是评估这 30,000 个选 择。按照第一个词和第二个词的概率，然后选出前三个，这样又减少了这 30,000 个可能性，又变成了 3 个，减少到集束宽的大小。假如这 30,000 个选择里最可能的是「in September」（上 图编号 4 所示）和「jane is」（上图编号 5 所示），以及「jane visits」（上图编号 6 所示），画 的有点乱，但这就是这 30,000 个选择里最可能的三个结果，集束搜索算法会保存这些结果，然后用于下一次集束搜索。

注意一件事情，如果集束搜索找到了第一个和第二个单词对最可能的三个选择是「in September」或者「jane is」或者「jane visits」，这就意味着我们去掉了 september 作为英语翻译 结果的第一个单词的选择，所以我们的第一个单词现在减少到了两个可能结果，但是我们的 集束宽是 3，所以还是有𝑦<1> ，𝑦<2> 对的三个选择。

在我们进入集束搜索的第三步之前，我还想提醒一下因为我们的集束宽等于 3，每一步 我们都复制 3 个，同样的这种网络来评估部分句子和最后的结果，由于集束宽等于 3，我们 有三个网络副本（上图编号 7 所示），每个网络的第一个单词不同，而这三个网络可以高效 地评估第二个单词所有的 30,000 个选择。所以不需要初始化 30,000 个网络副本，只需要使 用 3 个网络的副本就可以快速的评估 softmax 的输出，即𝑦<2> 的 10,000 个结果。

让我们快速解释一下集束搜索的下一步，前面说过前两个单词最可能的选择是「in September」和「jane is」以及「jane visits」，对于每一对单词我们应该保存起来，给定输入𝑥，即 法语句子作为𝑥的情况下，𝑦<1> 和𝑦<2> 的概率值和前面一样，现在我们考虑第三个单词是什 么，可以是「in September a」，可以是「in September aaron」，一直到「in September zulu」。为 了评估第三个单词可能的选择，我们用这个网络部分，第一单词是 in（上图编号 1 所示），

623 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

第二个单词是 september（上图编号 2 所示），所以这个网络部分可以用来评估第三个单词 的概率，在给定输入的法语句子𝑥和给定的英语输出的前两个单词「in September」情况下（上 图编号 3 所示）。对于第二个片段来说也一样，就像这样一样（上图编号 4 所示），对于「jane visits」也一样，然后集束搜索还是会挑选出针对前三个词的三个最可能的选择，可能是「in september jane」（上图编号 5 所示），「Jane is visiting」也很有可能（上图编号 6 所示），也很可能是「Jane visits Africa」（上图编号 7 所示）。

然后继续，接着进行集束搜索的第四步，再加一个单词继续，最终这个过程的输出一次 增加一个单词，集束搜索最终会找到「Jane visits africa in september」这个句子，终止在句尾 符号（上图编号 8 所示），用这种符号的系统非常常见，它们会发现这是最有可能输出的一 个英语句子。在本周的练习中，你会看到更多的执行细节，同时，你会运用到这个集束算法，在集束宽为 3 时，集束搜索一次只考虑 3 个可能结果。注意如果集束宽等于 1，只考虑 1 种 可能结果，这实际上就变成了贪婪搜索算法，上个视频里我们已经讨论过了。但是如果同时 考虑多个，可能的结果比如 3 个，10 个或者其他的个数，集束搜索通常会找到比贪婪搜索 更好的输出结果。

你已经了解集束搜索是如何工作的了，事实上还有一些额外的提示和技巧的改进能够使 集束算法更高效，我们在下个视频中一探究竟。

624 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

3.4 改进集束搜索（Refinements to Beam Search）

上个视频中，你已经学到了基本的束搜索算法 (the basic beam search algorithm)，这个视 频里，我们会学到一些技巧，能够使算法运行的更好。长度归一化（Length normalization）就

是对束搜索算法稍作调整的一种方式，帮助你得到更好的结果，下面介绍一下它。

前面讲到束搜索就是最大化这个概率，这个乘积就是𝑃(𝑦 <1> … 𝑦 <𝑇 𝑦 |𝑋)，可以表示

成:𝑃(𝑦 <1> |𝑋) 𝑃(𝑦 <2> |𝑋, 𝑦 <1> ) 𝑃(𝑦 <3> |𝑋, 𝑦 <1> ,𝑦 <2> )…𝑃(𝑦 <𝑇 𝑦> |𝑋, 𝑦 <1> , 𝑦 <2> … 𝑦 <𝑇 𝑦 −1> )

这些符号看起来可能比实际上吓人，但这就是我们之前见到的乘积概率（the product probabilities）。如果计算这些，其实这些概率值都是小于 1 的，通常远小于 1。很多小于 1 的数乘起来，会得到很小很小的数字，会造成数值下溢（numerical underflow）。数值下溢 就是数值太小了，导致电脑的浮点表示不能精确地储存，因此在实践中，我们不会最大化这 个乘积，而是取𝑙𝑜𝑔值。如果在这加上一个𝑙𝑜𝑔，最大化这个𝑙𝑜𝑔求和的概率值，在选择最可 能的句子𝑦时，你会得到同样的结果。所以通过取𝑙𝑜𝑔，我们会得到一个数值上更稳定的算法，不容易出现四舍五入的误差，数值的舍入误差（rounding errors）或者说数值下溢（numerical underflow）。因为𝑙𝑜𝑔函数它是严格单调递增的函数，最大化𝑃(𝑦)，因为对数函数，这就是 𝑙𝑜𝑔函数，是严格单调递增的函数，所以最大化𝑙𝑜𝑔𝑃(𝑦|𝑥) 和最大化𝑃(𝑦|𝑥) 结果一样。如果一 个𝑦值能够使前者最大，就肯定能使后者也取最大。所以实际工作中，我们总是记录概率的 对数和（the sum of logs of the probabilities），而不是概率的乘积（the production of probabilities）。

对于目标函数（this objective function），还可以做一些改变，可以使得机器翻译表现的

625 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

更好。如果参照原来的目标函数（this original objective），如果有一个很长的句子，那么这 个句子的概率会很低，因为乘了很多项小于 1 的数字来估计句子的概率。所以如果乘起来很 多小于 1 的数字，那么就会得到一个更小的概率值，所以这个目标函数有一个缺点，它可能 不自然地倾向于简短的翻译结果，它更偏向短的输出，因为短句子的概率是由更少数量的小 于 1 的数字乘积得到的，所以这个乘积不会那么小。顺便说一下，这里也有同样的问题，概 率的𝑙𝑜𝑔值通常小于等于 1，实际上在𝑙𝑜𝑔的这个范围内，所以加起来的项越多，得到的结果 越负，所以对这个算法另一个改变也可以使它表现的更好，也就是我们不再最大化这个目标 函数了，我们可以把它归一化，通过除以翻译结果的单词数量（normalize this by the number of words in your translation）。这样就是取每个单词的概率对数值的平均了，这样很明显地 减少了对输出长的结果的惩罚（this significantly reduces the penalty for outputting longer translations.）。

在实践中，有个探索性的方法，相比于直接除𝑇 𝑦 ，也就是输出句子的单词总数，我们有 时会用一个更柔和的方法（a softer approach），在𝑇 𝑦 上加上指数𝑎，𝑎可以等于 0.7。如果𝑎 等于 1，就相当于完全用长度来归一化，如果𝑎等于 0，𝑇 𝑦 的 0 次幂就是 1，就相当于完全没 有归一化，这就是在完全归一化和没有归一化之间。𝑎就是算法另一个超参数（hyper parameter），需要调整大小来得到最好的结果。不得不承认，这样用𝑎实际上是试探性的，它并没有理论验证。但是大家都发现效果很好，大家都发现实践中效果不错，所以很多人都 会这么做。你可以尝试不同的𝑎值，看看哪一个能够得到最好的结果。

总结一下如何运行束搜索算法。当你运行束搜索时，你会看到很多长度等于 1 的句子，很多长度等于 2 的句子，很多长度等于 3 的句子，等等。可能运行束搜索 30 步，考虑输出 的句子可能达到，比如长度 30。因为束宽为 3，你会记录所有这些可能的句子长度，长度为 1、2、3、4 等等一直到 30 的三个最可能的选择。然后针对这些所有的可能的输出句子，用 这个式子（上图编号 1 所示）给它们打分，取概率最大的几个句子，然后对这些束搜索得到 的句子，计算这个目标函数。最后从经过评估的这些句子中，挑选出在归一化的𝑙𝑜𝑔 概率目 标函数上得分最高的一个（you pick the one that achieves the highest value on this normalized log probability objective.），有时这个也叫作归一化的对数似然目标函数（a normalized log likelihood objective）。这就是最终输出的翻译结果，这就是如何实现束搜索。这周的练习中 你会自己实现这个算法。

626 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

最后还有一些实现的细节，如何选择束宽 B。B 越大，你考虑的选择越多，你找到的句 子可能越好，但是 B 越大，你的算法的计算代价越大，因为你要把很多的可能选择保存起 来。最后我们总结一下关于如何选择束宽 B 的一些想法。接下来是针对或大或小的 B 各自 的优缺点。如果束宽很大，你会考虑很多的可能，你会得到一个更好的结果，因为你要考虑 很多的选择，但是算法会运行的慢一些，内存占用也会增大，计算起来会慢一点。而如果你 用小的束宽，结果会没那么好，因为你在算法运行中，保存的选择更少，但是你的算法运行 的更快，内存占用也小。在前面视频里，我们例子中用了束宽为 3，所以会保存 3 个可能选 择，在实践中这个值有点偏小。在产品中，经常可以看到把束宽设到 10，我认为束宽为 100 对于产品系统来说有点大了，这也取决于不同应用。但是对科研而言，人们想压榨出全部性 能，这样有个最好的结果用来发论文，也经常看到大家用束宽为 1000 或者 3000，这也是取 决于特定的应用和特定的领域。在你实现你的应用时，尝试不同的束宽的值，当 B 很大的时 候，性能提高会越来越少。对于很多应用来说，从束宽 1，也就是贪心算法，到束宽为 3、 到 10，你会看到一个很大的改善。但是当束宽从 1000 增加到 3000 时，效果就没那么明显 了。对于之前上过计算机科学课程的同学来说，如果你熟悉计算机科学里的搜索算法 （computer science search algorithms）, 比如广度优先搜索（BFS, Breadth First Search algorithms），或者深度优先搜索（DFS, Depth First Search），你可以这样想束搜索，不像其 他你在计算机科学算法课程中学到的算法一样。如果你没听说过这些算法也不要紧，但是如 果你听说过广度优先搜索和深度优先搜索，不同于这些算法，这些都是精确的搜索算法 （exact search algorithms），束搜索运行的更快，但是不能保证一定能找到 argmax 的准确 的最大值。如果你没听说过广度优先搜索和深度优先搜索，也不用担心，这些对于我们的目 标也不重要，如果你听说过，这就是束搜索和其他算法的关系。

627 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

好，这就是束搜索。这个算法广泛应用在多产品系统或者许多商业系统上，在深度学习 系列课程中的第三门课中，我们讨论了很多关于误差分析（error analysis）的问题。事实上 在束搜索上做误差分析是我发现的最有用的工具之一。有时你想知道是否应该增大束宽，我 的束宽是否足够好，你可以计算一些简单的东西来指导你需要做什么，来改进你的搜索算法。我们在下个视频里进一步讨论。

628 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

3.5 集束搜索的误差分析（Error analysis in beam search）

在这五门课中的第三门课里，你了解了误差分析是如何能够帮助你集中时间做你的项目 中最有用的工作，束搜索算法是一种近似搜索算法（an approximate search algorithm），也 被称作启发式搜索算法（a heuristic search algorithm），它不总是输出可能性最大的句子，它仅记录着 B 为前 3 或者 10 或是 100 种可能。那么如果束搜索算法出现错误会怎样呢？

本节视频中，你将会学习到误差分析和束搜索算法是如何相互起作用的，以及你怎样才 能发现是束搜索算法出现了问题，需要花时间解决，还是你的 RNN 模型出了问题，要花时 间解决。我们先来看看如何对束搜索算法进行误差分析。

我们来用这个例子说明：「Jane visite l'Afrique en septembre」。假如说，在你的机器翻 译的 dev 集中，也就是开发集（development set），人工是这样翻译的: Jane visits Africa in September, 我会将这个标记为𝑦∗ 。这是一个十分不错的人工翻译结果，不过假如说，当你在 已经完成学习的 RNN 模型，也就是已完成学习的翻译模型中运行束搜索算法时，它输出了 这个翻译结果：Jane visited Africa last September，我们将它标记为𝑦。这是一个十分糟糕的 翻译，它实际上改变了句子的原意，因此这不是个好翻译。

你的模型有两个主要部分，一个是神经网络模型，或说是序列到序列模型（sequence to sequence model），我们将这个称作是 RNN 模型，它实际上是个编码器和解码器（ an encoder and a decoder）。另一部分是束搜索算法，以某个集束宽度 B 运行。如果你能够找出造成这 个错误，这个不太好的翻译的原因，是两个部分中的哪一个，不是很好吗？RNN (循环神经网 络) 是更可能是出错的原因呢，还是束搜索算法更可能是出错的原因呢？你在第三门课中了

629 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

解到了大家很容易想到去收集更多的训练数据，这总归没什么坏处。所以同样的，大家也会 觉得不行就增大束宽，也是不会错的，或者说是很大可能是没有危害的。但是就像单纯获取 更多训练数据，可能并不能得到预期的表现结果。相同的，单纯增大束宽也可能得不到你想 要的结果，不过你怎样才能知道是不是值得花时间去改进搜索算法呢？下面我们来分解这个 问题弄清楚什么情况下该用什么解决办法。RNN (循环神经网络) 实际上是个编码器和解码器（the encoder and the decoder），它会 计算𝑃(𝑦|𝑥)。所以举个例子，对于这个句子：Jane visits Africa in September，你将 Jane visits Africa 填入这里（上图编号 1 所示），同样，我现在忽略了字母的大小写，后面也是一样，然后这个就会计算。𝑃(𝑦|𝑥) 结果表明，你此时能做的最有效的事就是用这个模型来计算 𝑃(𝑦 ∗ |𝑥)，同时也用你的 RNN 模型来计算𝑃(𝑦 |𝑥)，然后比较一下这两个值哪个更大。有可能 是左边大于右边，也有可能是𝑃(𝑦 ∗) 小于𝑃(𝑦)，其实应该是小于或等于，对吧。取决于实际 是哪种情况，你就能够更清楚地将这个特定的错误归咎于 RNN 或是束搜索算法，或说是哪 个负有更大的责任。我们来探究一下其中的逻辑。

这是之前幻灯片里的两个句子。记住，我们是要计算𝑃(𝑦 ∗ |𝑥) 和𝑃(𝑦 |𝑥)，然后比较这两 个哪个更大，所以就会有两种情况。

第一种情况，RNN 模型的输出结果𝑃(𝑦 ∗ |𝑥) 大于𝑃(𝑦 |𝑥)，这意味着什么呢？束搜索算法 选择了𝑦 ，对吧？你得到𝑦的方式是，你用一个 RNN 模型来计算𝑃(𝑦|𝑥)，然后束搜索算法做 的就是尝试寻找使𝑃(𝑦|𝑥) 最大的𝑦，不过在这种情况下，相比于𝑦，𝑦∗ 的值更𝑃(𝑦|𝑥) 大，因此 你能够得出束搜索算法实际上不能够给你一个能使𝑃(𝑦|𝑥) 最大化的𝑦值，因为束搜索算法的 任务就是寻找一个𝑦的值来使这项更大，但是它却选择了𝑦，而𝑦∗ 实际上能得到更大的值。因

630 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

此这种情况下你能够得出是束搜索算法出错了。那另一种情况是怎样的呢？

第二种情况是𝑃(𝑦 ∗ |𝑥) 小于或等于𝑃(𝑦 |𝑥) 对吧？这两者之中总有一个是真的。情况 1 或

是情况 2 总有一个为真。情况 2 你能够总结出什么呢？在我们的例子中，𝑦 ∗ 是比 𝑦更好的 翻译结果，不过根据 RNN 模型的结果，𝑃(𝑦∗) 是小于𝑃(𝑦) 的，也就是说，相比于𝑦，𝑦∗ 成为 输出的可能更小。因此在这种情况下，看来是 RNN 模型出了问题。同时可能值得在 RNN 模 型上花更多时间。这里我少讲了一些有关长度归一化（length normalizations）的细节。这里 我略过了有关长度归一化的细节，如果你用了某种长度归一化，那么你要做的就不是比较这 两种可能性大小，而是比较长度归一化后的最优化目标函数值。不过现在先忽略这种复杂的 情况。第二种情况表明虽然𝑦∗ 是一个更好的翻译结果，RNN 模型却赋予它更低的可能性，是 RNN 模型出现了问题。

所以误差分析过程看起来就像下面这样。你先遍历开发集，然后在其中找出算法产生的 错误，这个例子中，假如说𝑃(𝑦 ∗ |𝑥) 的值为 2 x 10-10，而𝑃(𝑦 |𝑥) 的值为 1 x10-10，根据上页 幻灯片中的逻辑关系，这种情况下我们得知束搜索算法实际上选择了比𝑦 ∗ 可能性更低的𝑦，因此我会说束搜索算法出错了。我将它缩写为 B。接着你继续遍历第二个错误，再来看这些 可能性。也许对于第二个例子来说，你认为是 RNN 模型出现了问题，我会用缩写 R 来代表 RNN。再接着你遍历了更多的例子，有时是束搜索算法出现了问题，有时是模型出现了问题，等等。通过这个过程，你就能够执行误差分析，得出束搜索算法和 RNN 模型出错的比例是 多少。有了这样的误差分析过程，你就可以对开发集中每一个错误例子，即算法输出了比人 工翻译更差的结果的情况，尝试确定这些错误，是搜索算法出了问题，还是生成目标函数 (束 搜索算法使之最大化) 的 RNN 模型出了问题。并且通过这个过程，你能够发现这两个部分中

631 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

哪个是产生更多错误的原因，并且只有当你发现是束搜索算法造成了大部分错误时，才值得 花费努力增大集束宽度。相反地，如果你发现是 RNN 模型出了更多错，那么你可以进行更 深层次的分析，来决定是需要增加正则化还是获取更多的训练数据，抑或是尝试一个不同的 网络结构，或是其他方案。你在第三门课中，了解到各种技巧都能够应用在这里。这就是束搜索算法中的误差分析，我认为这个特定的误差分析过程是十分有用的，它可 以用于分析近似最佳算法 (如束搜索算法)，这些算法被用来优化学习算法 (例如序列到序列模 型 / RNN) 输出的目标函数。也就是我们这些课中一直讨论的。学会了这个方法，我希望你能 够在你的应用里更有效地运用好这些类型的模型。

632 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

3.6 Bleu 得分（选修）（Bleu Score (optional)）

机器翻译（machine translation）的一大难题是一个法语句子可以有多种英文翻译而且 都同样好，所以当有多个同样好的答案时，怎样评估一个机器翻译系统呢？不像图像识别 （image recognition），只有一个正确答案，就只要测量准确性就可以了。如果有多个不错 的答案，要怎样衡量准确性呢？常见的解决办法是，通过一个叫做 BLEU 得分（the BLEU score） 的东西来解决。所以，在这个选修视频中，我想与你分享，我想让你了解 BLEU 得分是怎样 工作的。

假如给你一个法语句子：Le chat est sur le tapis，然后给你一个这个句子的人工翻译作 参考：The cat is on the mat。不过有多种相当不错的翻译。所以一个不同的人，也许会将其 翻译为：There is a cat on the mat，同时，实际上这两个都是很好的，都准确地翻译了这个 法语句子。BLEU 得分做的就是，给定一个机器生成的翻译，它能够自动地计算一个分数来 衡量机器翻译的好坏。直觉告诉我们，只要这个机器生成的翻译与任何一个人工翻译的结果 足够接近，那么它就会得到一个高的 BLEU 分数。顺便提一下 BLEU 代表 bilingual evaluation understudy (双语评估替补)。在戏剧界，侯补演员 (understudy) 学习资深的演员的角色，这样 在必要的时候，他们就能够接替这些资深演员。而 BLEU 的初衷是相对于请评估员（ask human evaluators），人工评估机器翻译系统（the machine translation system），BLEU 得分就相当 于一个侯补者，它可以代替人类来评估机器翻译的每一个输出结果。BLEU 得分是由 Kishore Papineni, Salim Roukos，Todd Ward 和 Wei-Jing Zhu 发表的这篇论文十分有影响力并且实际 上也是一篇很好读的文章。所以如果有时间的话，我推荐你读一下。BLEU 得分背后的理念 是观察机器生成的翻译，然后看生成的词是否出现在少一个人工翻译参考之中。因此这些人 工翻译的参考会包含在开发集或是测试集中。

633 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

（参考论文：Papineni, Kishore & Roukos, Salim & Ward, Todd & Zhu, Wei-jing. (2002). BLEU: a Method for Automatic Evaluation of Machine Translation. 10.3115/1073083.1073135.）

现在，我们来看一个极端的例子。我们假设机器翻译系统缩写为 MT。机器翻译 (MT) 的 输出是：the the the the the the the。这显然是一个十分糟糕的翻译。衡量机器翻译输出质量 的方法之一是观察输出结果的每一个词看其是否出现在参考中，这被称做是机器翻译的精确 度（a precision of the machine translation output）。这个情况下，机器翻译输出了七个单词 并且这七个词中的每一个都出现在了参考 1 或是参考 2。单词 the 在两个参考中都出现了，所以看上去每个词都是很合理的。因此这个输出的精确度就是 7/7，看起来是一个极好的精 确度。这就是为什么把出现在参考中的词在 MT 输出的所有词中所占的比例作为精确度评估 标准并不是很有用的原因。因为它似乎意味着，例子中 MT 输出的翻译有很高的精确度，因 此取而代之的是我们要用的这个改良后的精确度评估方法，我们把每一个单词的记分上限定 为它在参考句子中出现的最多次数。在参考 1 中，单词 the 出现了两次，在参考 2 中，单词 the 只出现了一次。而 2 比 1 大，所以我们会说，单词 the 的得分上限为 2。有了这个改良 后的精确度，我们就说，这个输出句子的得分为 2/7，因为在 7 个词中，我们最多只能给它 2 分。所以这里分母就是 7 个词中单词 the 总共出现的次数，而分子就是单词 the 出现的计 数。我们在达到上限时截断计数，这就是改良后的精确度评估（the modified precision measure）。

634 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

到目前为止，我们都只是关注单独的单词，在 BLEU 得分中，你不想仅仅考虑单个的单 词，你也许也想考虑成对的单词，我们定义一下二元词组（bigrams）的 BLEU 得分。bigram 的意思就是相邻的两个单词。现在我们来看看怎样用二元词组来定义 BLEU 得分，并且这仅 仅只是最终的 BLEU 得分的一部分。我们会考虑一元词组（unigrams）也就是单个单词以及 二元词组（bigrams），即成对的词，同时也许会有更长的单词序列，比如说三元词组（trigrams）。意思是三个挨在一起的词。我们继续刚才的例子，还是前面出现过的参考 1 和 2，不过现在 我们假定机器翻译输出了稍微好一点的翻译：The cat the cat on the mat，仍然不是一个好的 翻译，不过也许比上一个好一些。这里，可能的二元词组有 the cat ，忽略大小写，接着是 cat the，这是另一个二元词组，然后又是 the cat。不过我已经有了，所以我们跳过它，然 后下一个是 cat on，然后是 on the，再然后是 the mat。所以这些就是机器翻译中的二元词 组。好，我们来数一数每个二元词组出现了多少次。the cat 出现了两次 ，cat the 出现了一 次，剩下的都只出现了一次。最后 ，我们来定义一下截取计数（the clipped count）。也就 是 Count_clip。为了定义它，我们以这列的值为基础，但是给算法设置得分上限，上限值为 二元词组出现在参考 1 或 2 中的最大次数。the cat 在两个参考中最多出现一次，所以我将 截取它的计数为 1。cat the 它并没有出现在参考 1 和参考 2 中，所以我将它截取为 0。cat on ，好，它出现了一次，我们就记 1 分。on the 出现一次就记 1 分，the mat 出现了一次，所以这些就是截取完的计数（the clipped counts）。我们把所有的这些计数都截取了一遍，实际上就是将它们降低使之不大于二元词组出现在参考中的次数。最后，修改后的二元词组 的精确度就是 count_clip 之和。因此那就是 4 除以二元词组的总个数，也就是 6。因此是 4/6 也就是 2/3 为二元词组改良后的精确度。

635 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

现在我们将它公式化。基于我们在一元词组中学到的内容，我们将改良后的一元词组精 确度定义为𝑃 1 ，𝑃代表的是精确度。这里的下标 1 的意思是一元词组。不过它定义为一元词 组之和，也就是对机器翻译结果中所有单词求和，MT 输出就是𝑦，Countclip (unigram)。除 以机器翻译输出中的一元词组出现次数之和。因此这个就是最终结果应该是两页幻灯片前得 到的 2/7。这里的 1 指代的是一元词组，意思是我们在考虑单独的词，你也可以定义𝑃 𝑛 为𝑛 元词组精确度，用 n-gram 替代掉一元词组。所以这就是机器翻译输出中的𝑛元词组的 countclip 之和除以𝑛元词组的出现次数之和。因此这些精确度或说是这些改良后的精确度得 分评估的是一元词组或是二元词组。就是我们前页幻灯片中做的，或者是三元词组，也就是 由三个词组成的，甚至是𝑛取更大数值的𝑛元词组。这个方法都能够让你衡量机器翻译输出 中与参考相似重复的程度。另外，你能够确信如果机器翻译输出与参考 1 或是参考 2 完全一 致的话，那么所有的这些𝑃 1 、𝑃2 等等的值，都会等于 1.0。为了得到改良后的 1.0 的精确度，只要你的输出与参考之一完全相同就能满足，不过有时即使输出结果并不完全与参考相同，这也是有可能实现的。你可以将它们以另一种方式组合，但愿仍能得到不错的翻译结果。

636 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

最后，我们将这些组合一下来构成最终的 BLEU 得分。𝑃 𝑛 就是𝑛元词组这一项的 BLEU 得 分，也是计算出的𝑛元词组改良后的精确度，按照惯例，为了用一个值来表示你需要计算𝑃1 ，𝑃2 ，𝑃3 ，𝑃4 。然后将它们用这个公式组合在一起，就是取平均值。按照惯例 BLEU 得分被定 义为，𝑒𝑥𝑝(4 1 ∑ 𝑛=1 4 𝑃 𝑛)，对这个线性运算进行乘方运算，乘方是严格单调递增的运算，我们实 际上会用额外的一个叫做 BP 的惩罚因子（the BP penalty）来调整这项。BP 的意思是「简短 惩罚」（ brevity penalty）。这些细节也许并不是十分重要，但是你可以大致了解一下。事 实表明，如果你输出了一个非常短的翻译，那么它会更容易得到一个高精确度。因为输出的 大部分词可能都出现在参考之中，不过我们并不想要特别短的翻译结果。因此简短惩罚 (BP) 就是一个调整因子，它能够惩罚输出了太短翻译结果的翻译系统。BP 的公式如上图所示。如果你的机器翻译系统实际上输出了比人工翻译结果更长的翻译，那么它就等于 1，其他情 况下就是像这样的公式，惩罚所有更短的翻译，细节部分你能够在这篇论文中找到。

再说一句，在之前的视频中，你了解了拥有单一实数评估指标（a single real number evaluation metric）的重要性，因为它能够让你尝试两种想法，然后看一下哪个得分更高，尽 量选择得分更高的那个，BLEU 得分对于机器翻译来说，具有革命性的原因是因为它有一个 相当不错的虽然不是完美的但是非常好的单一实数评估指标，因此它加快了整个机器翻译领 域的进程，我希望这节视频能够让你了解 BLEU 得分是如何操作的。实践中，很少人会从零 实现一个 BLEU 得分（implement a BLEU score from scratch），有很多开源的实现结果，你可 以下载下来然后直接用来评估你的系统。不过今天，BLEU 得分被用来评估许多生成文本的 系统（systems that generate text），比如说机器翻译系统（machine translation systems），也有我之前简单提到的图像描述系统（image captioning systems）。也就是说你会用神经网 络来生成图像描述，然后使用 BLEU 得分来看一下，结果在多大程度上与参考描述或是多个 人工完成的参考描述内容相符。BLEU 得分是一个有用的单一实数评估指标，用于评估生成 文本的算法，判断输出的结果是否与人工写出的参考文本的含义相似。不过它并没有用于语 音识别（speech recognition）。因为在语音识别当中，通常只有一个答案，你可以用其他的 评估方法，来看一下你的语音识别结果，是否十分相近或是字字正确（pretty much, exactly word for word correct）。不过在图像描述应用中，对于同一图片的不同描述，可能是同样好 的。或者对于机器翻译来说，有多个一样好的翻译结果，BLEU 得分就给了你一个能够自动 评估的方法，帮助加快算法开发进程。说了这么多，希望你明白了 BLEU 得分是怎么运行的。

637 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

3.7 注意力模型直观理解（Attention Model Intuition）

在本周大部分时间中，你都在使用这个编码解码的构架（a Encoder-Decoder architecture） 来完成机器翻译。当你使用 RNN 读一个句子，于是另一个会输出一个句子。我们要对其做 一些改变，称为注意力模型（the Attention Model），并且这会使它工作得更好。注意力模 型或者说注意力这种思想（The attention algorithm, the attention idea）已经是深度学习中最 重要的思想之一，我们看看它是怎么运作的。

像这样给定一个很长的法语句子，在你的神经网络中，这个绿色的编码器要做的就是读 整个句子，然后记忆整个句子，再在感知机中传递（to read in the whole sentence and then memorize the whole sentences and store it in the activations conveyed her）。而对于这个紫色 的神经网络，即解码网络（the decoder network）将生成英文翻译，Jane 去年九月去了非洲，非常享受非洲文化，遇到了很多奇妙的人，她回来就嚷嚷道，她经历了一个多棒的旅行，并 邀请我也一起去。人工翻译并不会通过读整个法语句子，再记忆里面的东西，然后从零开始，机械式地翻译成一个英语句子。而人工翻译，首先会做的可能是先翻译出句子的部分，再看 下一部分，并翻译这一部分。看一部分，翻译一部分，一直这样下去。你会通过句子，一点 一点地翻译，因为记忆整个的像这样的的句子是非常困难的。你在下面这个编码解码结构中，会看到它对于短句子效果非常好，于是它会有一个相对高的 Bleu 分（Bleu score），但是对 于长句子而言，比如说大于 30 或者 40 词的句子，它的表现就会变差。Bleu 评分看起来就 会像是这样，随着单词数量变化，短的句子会难以翻译，因为很难得到所有词。对于长的句 子，效果也不好，因为在神经网络中，记忆非常长句子是非常困难的。在这个和下个视频中，

638 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

你会见识到注意力模型，它翻译得很像人类，一次翻译句子的一部分。而且有了注意力模型，机器翻译系统的表现会像这个一样，因为翻译只会翻译句子的一部分，你不会看到这个有一 个巨大的下倾（huge dip），这个下倾实际上衡量了神经网络记忆一个长句子的能力，这是 我们不希望神经网络去做的事情。在这个视频中，我想要给你们注意力机制运行的一些直观 的东西。然后在下个视频中，完善细节。

注意力模型源于 Dimitri, Bahdanau, Camcrun Cho, Yoshe Bengio。（Bahdanau D, Cho K, Bengio Y. Neural Machine Translation by Jointly Learning to Align and Translate [J]. Computer Science, 2014.）虽然这个模型源于机器翻译，但它也推广到了其他应用领域。我认为在深度学习领域，这个是个非常有影响力的，非常具有开创性的论文。

让我们用一个短句举例说明一下，即使这些思想可能是应用于更长的句子。但是用短句 来举例说明，讲解这些思想会更简单。我们有一个很平常的句子：(法语) Jane visite l'Afrique en Septembre。假定我们使用 RNN，在这个情况中，我们将使用一个双向的 RNN（a bidirectional RNN），为了计算每个输入单词的的特征集（set of features），你必须要理解输 出𝑦 到𝑦 一直到𝑦 的双向 RNN。但是我们并不是只翻译一个单词，让我们先去掉上

面的𝑌，就用双向的 RNN。我们要对单词做的就是，对于句子里的每五个单词，计算一个句 子中单词的特征集，也有可能是周围的词，让我们试试，生成英文翻译。我们将使用另一个 RNN 生成英文翻译，这是我平时用的 RNN 记号。我不用𝐴来表示感知机（the activation），这是为了避免和这里的感知机（the activations）混淆。我会用另一个不同的记号，我会用𝑆 来表示 RNN 的隐藏状态（the hidden state in this RNN），不用𝐴<1> ，而是用𝑆 <1> 。我们希

639 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

望在这个模型里第一个生成的单词将会是 Jane，为了生成 Jane visits Africa in September。于 是等式就是，当你尝试生成第一个词，即输出，那么我们应该看输入的法语句子的哪个部分？ 似乎你应该先看第一个单词，或者它附近的词。但是你别看太远了，比如说看到句尾去了。所以注意力模型就会计算注意力权重（a set of attention weights），我们将用𝑎 <1,1> 来表示 当你生成第一个词时你应该放多少注意力在这个第一块信息处。然后我们算第二个，这个叫 注意力权重，𝑎 <1,2> 它告诉我们当你尝试去计算第一个词 Jane 时，我们应该花多少注意力在 输入的第二个词上面。同理这里是𝑎 <1,3> ，接下去也同理。这些将会告诉我们，我们应该花 多少注意力在记号为𝐶的内容上。这就是 RNN 的一个单元，如何尝试生成第一个词的，这是 RNN 的其中一步，我们将会在下个视频中讲解细节。对于 RNN 的第二步，我们将有一个新 的隐藏状态𝑆 <2> ，我们也会用一个新的注意力权值集 (a new set of the attention weights), 我 们将用𝑎 <2,1> 来告诉我们什么时候生成第二个词，那么 visits 就会是第二个标签了 (the ground trip label)。我们应该花多少注意力在输入的第一个法语词上。然后同理𝑎 <2,2> ，接下 去也同理，我们应该花多少注意力在 visite 词上，我们应该花多少注意在词 l'Afique 上面。当然我们第一个生成的词 Jane 也会输入到这里，于是我们就有了需要花注意力的上下文。第二步，这也是个输入，然后会一起生成第二个词，这会让我们来到第三步𝑆 <3> ，这是输入，我们再有上下文 C，它取决于在不同的时间集（time sets）, 上面的𝑎<3> 。这个告诉了我们我 们要花注意力在不同的法语的输入词上面。然后同理。有些事情我还没说清楚，但是在下 个视频中，我会讲解一些细节，比如如何准确定义上下文，还有第三个词的上下文，是否真 的需要去注意句子中的周围的词。这里要用到的公式以及如何计算这些注意力权重（these attention weights），将会在下个视频中讲解到。在下个视频中你会看到𝑎 <3,𝑡> ，即当你尝试 去生成第三个词，应该是 l'Afique，就得到了右边这个输出，这个 RNN 步骤应该要花注意力 在𝑡时的法语词上，这取决于在𝑡时的双向 RNN 的激活值。那么它应该是取决于第四个激活 值，它会取决于上一步的状态，它会取决于𝑆 <2> 。然后这些一起影响你应该花多少注意在输 入的法语句子的某个词上面。我们会在下个视频中讲解这些细节。但是直观来想就是 RNN 向前进一次生成一个词，在每一步直到最终生成可能是 <EOS>。这些是注意力权重，即𝑎<𝑡,𝑡> 告诉你，当你尝试生成第𝑡个英文词，它应该花多少注意力在第𝑡个法语词上面。当生成一个 特定的英文词时，这允许它在每个时间步去看周围词距内的法语词要花多少注意力。

我希望这个视频传递了关于注意力模型的一些直观的东西。我们现在可能对算法的运行 有了大概的感觉，让我们进入到下个视频中，看看具体的细节。

注意力模型参考：

640 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

641 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

3.8 注意力模型（Attention Model）

在上个视频中你已经见到了，注意力模型如何让一个神经网络只注意到一部分的输入句 子。当它在生成句子的时候，更像人类翻译。让我们把这些想法转化成确切的式子，来实现 注意力模型。

跟上个视频一样，我们先假定有一个输入句子，并使用双向的 RNN，或者双向的 GRU 或者双向的 LSTM，去计算每个词的特征。实际上 GRU 和 LSTM 经常应用于这个，可能 LSTM 更经常一点。对于前向传播（the forward occurrence），你有第一个时间步的前向传播的激 活值（a forward occurrence first time step），第一个时间步后向传播的激活值，后向的激活 值，以此类推。他们一共向前了五个时间步，也向后了五个时间步，技术上我们把这里设置 为 0。我们也可以后向传播 6 次，设一个都是 0 的因子，实际上就是个都是 0 的因子。为了 简化每个时间步的记号，即使你在双向 RNN 已经计算了前向的特征值和后向的特征值，我 就用𝑎 <𝑡> 来一起表示这些联系。所以𝑎 <𝑡> 就是时间步𝑡上的特征向量。但是为了保持记号的 一致性，我们用第二个，也就是𝑡′，实际上我将用𝑡′来索引法语句子里面的词。接下来我们只 进行前向计算，就是说这是个单向的 RNN，用状态𝑆表示生成翻译。所以第一个时间步，它 应该生成𝑦<1> ，当你输入上下文𝐶的时候就会这样，如果你想用时间来索引它，你可以写𝐶<1> ，但有时候我就写个𝐶，就是没有上标的𝐶，这个会取决于注意力参数，即𝑎 <1,1> ，𝑎 <1,2> 以此 类推，告诉我们应该花多少注意力。同样的，这个𝑎参数告诉我们上下文有多少取决于我们 得到的特征，或者我们从不同时间步中得到的激活值。所以我们定义上下文的方式实际上来

642 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

源于被注意力权重加权的不同时间步中的特征值。于是更公式化的注意力权重将会满足非负 的条件，所以这就是个 0 或正数，它们加起来等于 1。我们等会会见到我们如何确保这个成 立，我们将会有上下文，或者说在𝑡 = 1 时的上下文，我会经常省略上标，这就会变成对𝑡′的 求和。这个权重的所有的𝑡′值，加上这些激活值。所以这里的这项（上图编号 1 所示）就是 注意力权重，这里的这项（上图编号 2）来自于这里（上图编号 3），于是𝑎<𝑡,𝑡′> 就是𝑦<𝑡> 应 该在𝑡′时花在𝑎上注意力的数量。换句话来说，当你在𝑡处生成输出词，你应该花多少注意力 在第𝑡′个输入词上面，这是生成输出的其中一步。然后下一个时间步，你会生成第二个输出。于是相似的，你现在有了一个新的注意力权重集，再找到一个新的方式将它们相加，这就产 生了一个新的上下文，这个也是输入，且允许你生成第二个词。只有现在才用这种方式相加，它会变成第二个时间步的上下文。即对𝑡′的𝑎<2,𝑡′> 进行求和，于是使用这些上下文向量，𝐶<1> 写到这里，𝐶<2> 也同理。这里的神经网络看起来很像相当标准的 RNN 序列，这里有着上下 文向量作为输出，我们可以一次一个词地生成翻译，我们也定义了如何通过这些注意力权重 和输入句子的特征值来计算上下文向量。剩下唯一要做的事情就是定义如何计算这些注意力 权重。让我们下张幻灯片看看。

回忆一下𝑎 <𝑡,𝑡′> ，是你应该花费在𝑎<𝑡′> 上的注意力的数量，当你尝试去生成第𝑡个输出 的翻译词，让我们先把式子写下来，再讨论它是怎么来的。这个式子你可以用来计算𝑎<𝑡,𝑡′> ，在此之前我们要先计算𝑒 <𝑡,𝑡′> ，关键要用 softmax，来确保这些权重加起来等于 1。如果你对 𝑡′求和，比如每一个固定的𝑡值，这些加起来等于 1。如果你对𝑡′求和，然后优先使用 softmax，确保这些值加起来等于 1。

现在我们如何计算这些𝑒项，一种我们可以用的方式是用下面这样的小的神经网络，于

643 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

是𝑠 <𝑡−1> 就是神经网络在上个时间步的状态，于是这里我们有一个神经网络，如果你想要生 成𝑦 <𝑡> ，那么𝑠 <𝑡−1> 就是上一时间步的隐藏状态，即𝑠 <𝑡> 。这是给小神经网络的其中一个输 入，也就是在神经网络中的一个隐藏层，因为你需要经常计算它们，然后𝑎<𝑡′> ，即上个时间 步的的特征是另一个输入。直观来想就是，如果你想要决定要花多少注意力在𝑡′的激活值上。于是，似乎它会很大程度上取决于你上一个时间步的的隐藏状态的激活值。你还没有当前状 态的激活值，因为上下文会输入到这里，所以你还没计算出来，但是看看你生成上一个翻译 的 RNN 的隐藏状态，然后对于每一个位置，每一个词都看向他们的特征值，这看起来很自 然，即𝑎<𝑡,𝑡′> 和𝑒<𝑡,𝑡′> 应该取决于这两个量。但是我们不知道具体函数是什么，所以我们可 以做的事情就是训练一个很小的神经网络，去学习这个函数到底是什么。相信反向传播算法，相信梯度下降算法学到一个正确的函数。这表示，如果你应用这整个的模型，然后用梯度下 降来训练它，这是可行的。这个小型的神经网络做了一件相当棒的事情，告诉你𝑦 <𝑡> 应该花 多少注意力在𝑎 <𝑡> 上面，然后这个式子确保注意力权重加起来等于 1，于是当你持续地一次 生成一个词，这个神经网络实际上会花注意力在右边的这个输入句子上，它会完全自动的通 过梯度下降来学习。这个算法的一个缺点就是它要花费三次方的时间，就是说这个算法的复杂是𝑂(𝑛3) 的，如果你有𝑇 𝑥 个输入单词和𝑇 𝑦 个输出单词，于是注意力参数的总数就会是𝑇 𝑥 × 𝑇 𝑦 ，所以这个算 法有着三次方的消耗。但是在机器翻译的应用上，输入和输出的句子一般不会太长，可能三 次方的消耗是可以接受，但也有很多研究工作，尝试去减少这样的消耗。那么讲解注意想法 在机器翻译中的应用，就到此为止了。虽然没有讲到太多的细节，但这个想法也被应用到了 其他的很多问题中去了，比如图片加标题（image captioning），图片加标题就是看一张图，写下这张图的标题。底下的这篇论文来源于 Kevin Chu，Jimmy Barr, Ryan Kiros, Kelvin Shaw, Aaron Korver, Russell Zarkutnov, Virta Zemo, 和 Andrew Benjo。他们也显示了你可以有一个 很相似的结构看图片，然后，当你在写图片标题的时候，一次只花注意力在一部分的图片上 面。如果你感兴趣，那么我鼓励你，也去看看这篇论文，做一些编程练习。

644 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

因为机器翻译是一个非常复杂的问题，在之前的练习中，你应用了注意力，在日期标准 化的问题（the date normalization problem）上面，问题输入了像这样的一个日期，这个日期 实际上是阿波罗登月的日期，把它标准化成标准的形式，或者这样的日期。用一个序列的神 经网络，即序列模型去标准化到这样的形式，这个日期实际上是威廉·莎士比亚的生日。一般 认为是这个日期正如你之前练习中见到的，你可以训练一个神经网络，输入任何形式的日期，生成标准化的日期形式。其他可以做的有意思的事情是看看可视化的注意力权重（the visualizations of the attention weights）。这个一个机器翻译的例子，这里被画上了不同的颜 色，不同注意力权重的大小，我不想在这上面花太多时间，但是你可以发现，对应的输入输 出词，你会发现注意力权重，会变高，因此这显示了当它生成特定的输出词时通常会花注意 力在输入的正确的词上面，包括学习花注意在哪。在注意力模型中，使用反向传播时，什 么时候学习完成。

这就是注意力模型，在深度学习中真的是个非常强大的想法。在本周的编程练习中，我 希望你可以享受自己应用它的过程。

645 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

3.9 语音识别（Speech recognition）

现今，最令人振奋的发展之一，就是 seq2seq 模型（sequence-to-sequence models）在 语音识别方面准确性有了很大的提升。这门课程已经接近尾声，现在我想通过剩下几节视频，来告诉你们，seq2seq 模型是如何应用于音频数据的（audio data），比如语音（the speech）。

什么是语音视频问题呢？现在你有一个音频片段𝑥（an audio clip,x），你的任务是自动 地生成文本𝑦。现在有一个音频片段，画出来是这样，该图的横轴是时间。一个麦克风的作 用是测量出微小的气压变化，现在你之所以能听到我的声音，是因为你的耳朵能够探测到这 些微小的气压变化，它可能是由你的扬声器或者耳机产生的，也就是像图上这样的音频片段，气压随着时间而变化。假如这个我说的音频片段的内容是："the quick brown fox"(敏捷的棕 色狐狸)，这时我们希望一个语音识别算法（a speech recognition algorithm），通过输入这段 音频，然后输出音频的文本内容。考虑到人的耳朵并不会处理声音的原始波形，而是通过一 种特殊的物理结构来测量这些，不同频率和强度的声波。音频数据的常见预处理步骤，就是 运行这个原始的音频片段，然后生成一个声谱图（a spectrogram），就像这样。同样地，横 轴是时间，纵轴是声音的频率（frequencies），而图中不同的颜色，显示了声波能量的大小 （the amount of energy），也就是在不同的时间和频率上这些声音有多大。通过这样的声谱 图，或者你可能还听过人们谈到过伪空白输出（the false blank outputs），也经常应用于预 处理步骤，也就是在音频被输入到学习算法之前，而人耳所做的计算和这个预处理过程非常 相似。语音识别方面，最令人振奋的趋势之一就是曾经有一段时间，语音识别系统是用音位 （phonemes）来构建的，也就是人工设计的基本单元（hand-engineered basic units of cells），如果用音位来表示 "the quick brown fox"，我这里稍微简化一些，"the" 含有 "th" 和 "e" 的音，

646 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

而 "quick" 有 "k" "w" "i" "k" 的音，语音学家过去把这些音作为声音的基本单元写下来，把这些

语音分解成这些基本的声音单元，而 "brown" 不是一个很正式的音位，因为它的音写起来比 较复杂，不过语音学家（linguists）们认为用这些基本的音位单元（basic units of sound called phonemes）来表示音频（audio），是做语音识别最好的办法。不过在 end-to-end 模型中，我们发现这种音位表示法（phonemes representations）已经不再必要了，而是可以构建一个 系统，通过向系统中输入音频片段（audio clip），然后直接输出音频的文本（a transcript），而不需要使用这种人工设计的表示方法。使这种方法成为可能的一件事就是用一个很大的数 据集，所以语音识别的研究数据集可能长达 300 个小时，在学术界，甚至 3000 小时的文本 音频数据集，都被认为是合理的大小。大量的研究，大量的论文所使用的数据集中，有几千 种不同的声音，而且，最好的商业系统现在已经训练了超过 1 万个小时的数据，甚至 10 万 个小时，并且它还会继续变得更大。在文本音频数据集中（Transcribe audio data sets）同时 包含𝑥和𝑦，通过深度学习算法大大推进了语音识别的进程。那么，如何建立一个语音识别系 统呢？

在上一节视频中，我们谈到了注意力模型，所以，一件你能做的事就是在横轴上，也就 是在输入音频的不同时间帧上，你可以用一个注意力模型，来输出文本描述，如 "the quick brown fox"，或者其他语音内容。

647 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

还有一种效果也不错的方法，就是用 CTC 损失函数（CTC cost）来做语音识别。CTC 就 是 Connectionist Temporal Classification，它是由 Alex Graves、Santiago Fernandes, Faustino Gomez、和 Jürgen Schmidhuber 提出的。（Graves A, Gomez F. Connectionist temporal classification:labelling unsegmented sequence data with recurrent neural networks [C]// International Conference on Machine Learning. ACM, 2006:369-376.）

算法思想如下:

假设语音片段内容是某人说："the quick brown fox"，这时我们使用一个新的网络，结构 像这个样子，这里输入𝑥和输出𝑦的数量都是一样的，因为我在这里画的，只是一个简单的单 向 RNN 结构。然而在实际中，它有可能是双向的 LSTM 结构，或者双向的 GIU 结构，并且 通常是很深的模型。但注意一下这里时间步的数量，它非常地大。在语音识别中，通常输入 的时间步数量（the number of input time steps）要比输出的时间步的数量（the number of output time steps）多出很多。举个例子，比如你有一段 10 秒的音频，并且特征（features） 是 100 赫兹的，即每秒有 100 个样本，于是这段 10 秒的音频片段就会有 1000 个输入，就 是简单地用 100 赫兹乘上 10 秒。所以有 1000 个输入，但可能你的输出就没有 1000 个字母 了，或者说没有 1000 个字符。这时要怎么办呢？CTC 损失函数允许 RNN 生成这样的输出： ttt，这是一个特殊的字符，叫做空白符，我们这里用下划线表示，这句话开头的音可表示为 h_eee_ _ _，然后这里可能有个空格，我们用这个来表示空格，之后是_ _ _qqq__，这样的输 出也被看做是正确的输出。下面这段输出对应的是 "the q"。CTC 损失函数的一个基本规则是 将空白符之间的重复的字符折叠起来，再说清楚一些，我这里用下划线来表示这个特殊的空

648 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

白符（a special blank character），它和空格（the space character）是不一样的。所以 the 和 quick 之间有一个空格符，所以我要输出一个空格，通过把用空白符所分割的重复的字符折 叠起来，然后我们就可以把这段序列折叠成 "the q"。这样一来你的神经网络因为有很多这种 重复的字符，和很多插入在其中的空白符（blank characters），所以最后我们得到的文本会 短上很多。于是这句 "the quick brown fox" 包括空格一共有 19 个字符，在这样的情况下，通 过允许神经网络有重复的字符和插入空白符使得它能强制输出 1000 个字符，甚至你可以输 出 1000 个𝑦值来表示这段 19 个字符长的输出。这篇论文来自于 Alex Grace 以及刚才提到的 那些人。我所参与的深度语音识别系统项目就使用这种思想来构建有效的语音识别系统。希望这能给你一个粗略的理解，理解语音识别模型是如何工作的：注意力模型是如何工 作的，以及 CTC 模型是如何工作的，以及这两种不同的构建这些系统的方法。现今，在生产 技术中，构建一个有效语音识别系统，是一项相当重要的工作，并且它需要很大的数据集，下节视频我想做的是告诉你如何构建一个触发字检测系统（a rigger word detection system），其中的关键字检测系统（keyword detection system）将会更加简单，它可以通过一个更简洁 的数量更合理的数据来完成。所以我们下节课再见。

649 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

3.10 触发字检测（Trigger Word Detection）

现在你已经学习了很多关于深度学习和序列模型的内容，于是我们可以真正去简便地描 绘出一个触发字系统（a trigger word system），就像上节视频中你看到的那样。随着语音识 别的发展，越来越多的设备可以通过你的声音来唤醒，这有时被叫做触发字检测系统（rigger word detection systems）。我们来看一看如何建立一个触发字系统。

触发字系统的例子包括 Amazon echo，它通过单词 Alexa 唤醒；还有百度 DuerOS 设备，通过 "小度你好" 来唤醒；苹果的 Siri 用 Hey Siri 来唤醒；Google Home 使用 Okay Google 来唤

醒，这就是触发字检测系统。假如你在卧室中，有一台 Amazon echo，你可以在卧室中简单 说一句: Alexa, 现在几点了？就能唤醒这个设备。它将会被单词 "Alexa" 唤醒，并回答你的询问。如果你能建立一个触发字检测系统，也许你就能让你的电脑通过你的声音来执行某些事，我 有个朋友也在做一种用触发字来打开的特殊的灯，这是个很有趣的项目。但我想教会你的，是如何构建一个触发字检测系统。

有关于触发字检测系统的文献，还处于发展阶段。对于触发字检测，最好的算法是什么，目前还没有一个广泛的定论。我这里就简单向你介绍一个你能够使用的算法好了。现在有一

650 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

个这样的 RNN 结构，我们要做的就是把一个音频片段（an audio clip）计算出它的声谱图特 征（spectrogram features）得到特征向量𝑥 <1> , 𝑥<2> , 𝑥<3> ..，然后把它放到 RNN 中，最后 要做的，就是定义我们的目标标签𝑦。假如音频片段中的这一点是某人刚刚说完一个触发字，比如 "Alexa"，或者 "小度你好" 或者 "Okay Google"，那么在这一点之前，你就可以在训练集 中把目标标签都设为 0，然后在这个点之后把目标标签设为 1。假如在一段时间之后，触发 字又被说了一次，比如是在这个点说的，那么就可以再次在这个点之后把目标标签设为 1。这样的标签方案对于 RNN 来说是可行的，并且确实运行得非常不错。不过该算法一个明显 的缺点就是它构建了一个很不平衡的训练集（a very imbalanced training set），0 的数量比 1 多太多了。

这里还有一个解决方法，虽然听起来有点简单粗暴，但确实能使其变得更容易训练。比 起只在一个时间步上去输出 1，其实你可以在输出变回 0 之前，多次输出 1，或说在固定的 一段时间内输出多个 1。这样的话，就稍微提高了 1 与 0 的比例，这确实有些简单粗暴。在 音频片段中，触发字刚被说完之后，就把多个目标标签设为 1，这里触发字又被说了一次。说完以后，又让 RNN 去输出 1。在之后的编程练习中，你可以进行更多这样的操作，我想你 应该会对自己学会了这么多东西而感到自豪。我们仅仅用了一张幻灯片来描述这种复杂的触 发字检测系统。在这个基础上，希望你能够实现一个能有效地让你能够检测出触发字的算法，不过在编程练习中你可以看到更多的学习内容。这就是触发字检测，希望你能对自己感到自 豪。因为你已经学了这么多深度学习的内容，现在你可以只用几分钟时间，就能用一张幻灯 片来描述触发字能够实现它，并让它发挥作用。你甚至可能在你的家里用触发字系统做一些 有趣的事情，比如打开或关闭电器，或者可以改造你的电脑，使得你或者其他人可以用触发 字来操作它。

这是深度学习课程最后一个技术视频，所以总结一下我们对序列模型的学习。我们学了 RNN，包括 GRU 和 LSTM，然后在上一周我们学了词嵌入（word embeddings），以及它们如 何学习词汇的表达（how they learn representations of words）。在这周还学了注意力模型 （the attention model）以及如何使用它来处理音频数据（audio data）。希望你在编程练习 中实现这些思想的时候，能够体会到诸多乐趣。接下来我们来看最后一个视频。

651 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

3.11 结论和致谢（Conclusion and thank you）

恭喜你能走到这一步，在最后这节视频中，只想做个总结，并给你一些最后的想法。

我们一起经历了一段很长的旅程，如果你已经学完了整个专业的课程，那么现在你已经 学会了神经网络和深度学习，如何改进深度神经网络，如何结构化机器学习项目，和卷积神 经网络。在最近的课程中还学了序列模型，我知道你为此非常努力，也希望你能对自己感到 自豪，为你的努力，为你所做的这一切。

我想向你传达一个对你来说可能很重要的想法。就是我觉得深度学习是一种超能力，通 过深度学习算法，你可以让计算机拥有 "视觉"（make a computer see），可以让计算机自己 合成小说（synthesize novel art），或者合成音乐（synthesized music），可以让计算机将一 种语言翻译成另一种（translate from one language to another），或者对放射影像进行定位 然后进行医疗诊断（Maybe have it located radiology image and render a medical diagnosis），或者构建自动驾驶系统（build pieces of a car that can drive itself）。如果说这还不是超能力 的话，那还能是什么呢？当我们结束这一系列课程的时候，结束整个专业的时候，我希望你 能够使用这些思想来发展你的事业，追逐你的梦想，但最重要的是，去做你认为最合适的能 对人类有贡献的事。这个世界现在面临着诸多挑战，但是在这种力量下，在人工智能和深度

652 

第五门课 序列模型 (Sequence Models)- 第三周 序列模型和注意力机制（Sequence models & Attention mechanism）

学习的力量下，我觉得我们可以让世界变得更美好。现在这种超能力就掌握在你的手中，去 突破障碍，让生活变得更好，这不单单是为自己，也是为了其他人。当然我也希望你能够对 自己取得的成就以及你学到的一切感到自豪。当你完成这一系列课程的学习后，我想你可以 把课程分享到社交媒体上，比如 Twitter 和 Facebook，让你的朋友也能知道这门课程。最后，我想告诉你的最后一件事，就是恭喜你完成了这门课程的学习，为自己所取得的 成就欢呼吧！同时也非常感谢你们，因为我知道大家都很忙，即便如此你们还是花了很多时 间来学习这些视频，可能还花了很多时间来做课堂测验还有编程练习，希望你们能够乐在其 中，并学到很多算法流程。我很荣幸你们能够花费时间，付诸精力，来学习这些东西，非常 感谢大家！

653 

附件 - 榜样的力量 - 吴恩达采访人工智能大师实录

附件

